Administration d'Oracle® Solaris : Systèmes
de fichiers ZFS

Référence : E25822–03
Février 2012

Copyright © 2006, 2012, Oracle et/ou ses affiliés. Tous droits réservés.

Ce logiciel et la documentation qui l'accompagne sont protégés par les lois sur la propriété intellectuelle. Ils sont concédés sous licence et soumis à des restrictions
d'utilisation et de divulgation. Sauf disposition de votre contrat de licence ou de la loi, vous ne pouvez pas copier, reproduire, traduire, diffuser, modifier, breveter,
transmettre, distribuer, exposer, exécuter, publier ou afficher le logiciel, même partiellement, sous quelque forme et par quelque procédé que ce soit. Par ailleurs, il est
interdit de procéder à toute ingénierie inverse du logiciel, de le désassembler ou de le décompiler, excepté à des fins d'interopérabilité avec des logiciels tiers ou tel que
prescrit par la loi.
Les informations fournies dans ce document sont susceptibles de modification sans préavis. Par ailleurs, Oracle Corporation ne garantit pas qu'elles soient exemptes
d'erreurs et vous invite, le cas échéant, à lui en faire part par écrit.
Si ce logiciel, ou la documentation qui l'accompagne, est concédé sous licence au Gouvernement des Etats-Unis, ou à toute entité qui délivre la licence de ce logiciel
ou l'utilise pour le compte du Gouvernement des Etats-Unis, la notice suivante s'applique :
U.S. GOVERNMENT END USERS:
Oracle programs, including any operating system, integrated software, any programs installed on the hardware, and/or documentation, delivered to U.S.
Government end users are "commercial computer software" pursuant to the applicable Federal Acquisition Regulation and agency-specific supplemental
regulations. As such, use, duplication, disclosure, modification, and adaptation of the programs, including any operating system, integrated software, any programs
installed on the hardware, and/or documentation, shall be subject to license terms and license restrictions applicable to the programs. No other rights are granted to
the U.S. Government.
Ce logiciel ou matériel a été développé pour un usage général dans le cadre d'applications de gestion des informations. Ce logiciel ou matériel n'est pas conçu ni n'est
destiné à être utilisé dans des applications à risque, notamment dans des applications pouvant causer des dommages corporels. Si vous utilisez ce logiciel ou matériel
dans le cadre d'applications dangereuses, il est de votre responsabilité de prendre toutes les mesures de secours, de sauvegarde, de redondance et autres mesures
nécessaires à son utilisation dans des conditions optimales de sécurité. Oracle Corporation et ses affiliés déclinent toute responsabilité quant aux dommages causés
par l'utilisation de ce logiciel ou matériel pour ce type d'applications.
Oracle et Java sont des marques déposées d'Oracle Corporation et/ou de ses affiliés. Tout autre nom mentionné peut correspondre à des marques appartenant à
d'autres propriétaires qu'Oracle.
Intel et Intel Xeon sont des marques ou des marques déposées d'Intel Corporation. Toutes les marques SPARC sont utilisées sous licence et sont des marques ou des
marques déposées de SPARC International, Inc. AMD, Opteron, le logo AMD et le logo AMD Opteron sont des marques ou des marques déposées d'Advanced
Micro Devices. UNIX est une marque déposée de The Open Group.
Ce logiciel ou matériel et la documentation qui l'accompagne peuvent fournir des informations ou des liens donnant accès à des contenus, des produits et des services
émanant de tiers. Oracle Corporation et ses affiliés déclinent toute responsabilité ou garantie expresse quant aux contenus, produits ou services émanant de tiers. En
aucun cas, Oracle Corporation et ses affiliés ne sauraient être tenus pour responsables des pertes subies, des coûts occasionnés ou des dommages causés par l'accès à
des contenus, produits ou services tiers, ou à leur utilisation.

120424@25097

Table des matières

Préface ...................................................................................................................................................11

1

Système de fichiers Oracle Solaris ZFS (introduction) .................................................................. 15
Nouveautés de ZFS .............................................................................................................................. 15
Page de manuel ZFS modifiée (zfs.1m) .................................................................................... 16
Propriété aclmode améliorée ...................................................................................................... 16
Identification des périphériques de pool en fonction de leur emplacement physique ........ 16
Migration shadow ZFS ................................................................................................................ 18
Améliorations du partage de fichiers ZFS ................................................................................. 18
Chiffrement de systèmes de fichiers ZFS ................................................................................... 18
Améliorations apportées au flux envoyé par ZFS ..................................................................... 19
Différences des instantanés ZFS (zfs diff) .............................................................................19
Récupération de pool de stockage ZFS et améliorations apportées aux performances ....... 20
Réglage du comportement synchrone ZFS ............................................................................... 20
Messages du pool ZFS améliorés ................................................................................................ 21
Améliorations de l'interopérabilité ACL ZFS ........................................................................... 22
Scission d'un pool de stockage ZFS mis en miroir (zpool split) ..........................................23
Modifications concernant iSCSI ZFS ........................................................................................ 23
Nouveau processus du système de fichiers ZFS ........................................................................ 23
Propriété de suppression des doublons ZFS ............................................................................. 24
Description d'Oracle Solaris ZFS ....................................................................................................... 24
Stockage ZFS mis en pool ............................................................................................................ 25
Sémantique transactionnelle ...................................................................................................... 25
Sommes de contrôle et données d'autorétablissement ............................................................ 26
Evolutitivé inégalée ...................................................................................................................... 26
Instantanés ZFS ............................................................................................................................ 26
Administration simplifiée ........................................................................................................... 27
Terminologie ZFS ................................................................................................................................ 27

3

Table des matières

Exigences d'attribution de noms de composants ZFS ..................................................................... 29

2 Mise en route d'Oracle Solaris ZFS .................................................................................................... 31
Profils de droits ZFS ............................................................................................................................ 31
Exigences et recommandations en matière de matériel et de logiciel ZFS ................................... 32
Création d'un système de fichiers ZFS basique ................................................................................ 32
Création d'un pool de stockage ZFS de base ..................................................................................... 33
▼ Identification des exigences de stockage du pool de stockage ZFS ........................................ 33
▼ Création d'un pool de stockage ZFS ........................................................................................... 34
Création d'une hiérarchie de systèmes de fichiers ZFS ................................................................... 35
▼ Détermination de la hiérarchie du système de fichiers ZFS .................................................... 35
▼ Création de systèmes de fichiers ZFS ......................................................................................... 36

3 Différences entre les systèmes de fichiers Oracle Solaris ZFS et classiques ...............................39
Granularité du système de fichiers ZFS ............................................................................................ 39
Comptabilisation de l'espace disque ZFS ......................................................................................... 40
Comportement d'espace saturé .................................................................................................. 41
Montage de système de fichiers ZFS .................................................................................................. 42
Gestion de volumes classique ............................................................................................................. 42
Nouveau modèle ACL Solaris ............................................................................................................ 42

4 Gestion des pools de stockage Oracle Solaris ZFS ......................................................................... 43
Composants d'un pool de stockage ZFS ........................................................................................... 43
Utilisation de disques dans un pool de stockage ZFS .............................................................. 43
Utilisation de tranches dans un pool de stockage ZFS ............................................................. 45
Utilisation de fichiers dans un pool de stockage ZFS ............................................................... 46
Remarques relatives aux pools de stockage ZFS ....................................................................... 47
Fonctions de réplication d'un pool de stockage ZFS ....................................................................... 47
Configuration de pool de stockage mis en miroir .................................................................... 48
Configuration de pool de stockage RAID-Z ............................................................................. 48
Pool de stockage ZFS hybride ..................................................................................................... 49
Données d'autorétablissement dans une configuration redondante ..................................... 50
Entrelacement dynamique dans un pool de stockage .............................................................. 50
Création et destruction de pools de stockage ZFS ........................................................................... 50

4

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Table des matières

Création de pools de stockage ZFS ............................................................................................. 51
Affichage des informations d'un périphérique virtuel de pool de stockage .......................... 58
Gestion d'erreurs de création de pools de stockage ZFS .......................................................... 59
Destruction de pools de stockage ZFS ....................................................................................... 62
Gestion de périphériques dans un pool de stockage ZFS ................................................................ 63
Ajout de périphériques à un pool de stockage .......................................................................... 63
Connexion et séparation de périphériques dans un pool de stockage ................................... 68
Création d'un pool par scission d'un pool de stockage ZFS mis en miroir ............................ 70
Mise en ligne et mise hors ligne de périphériques dans un pool de stockage ........................ 73
Effacement des erreurs de périphérique de pool de stockage ................................................. 76
Remplacement de périphériques dans un pool de stockage ................................................... 76
Désignation des disques hot spare dans le pool de stockage ................................................... 79
Gestion des propriétés de pool de stockage ZFS .............................................................................. 84
Requête d'état de pool de stockage ZFS ............................................................................................. 87
Affichage des informations des pools de stockage ZFS ........................................................... 87
Visualisation des statistiques d'E/S des pools de stockage ZFS ............................................... 92
Détermination de l'état de maintenance des pools de stockage ZFS ...................................... 95
Migration de pools de stockage ZFS .................................................................................................. 99
Préparatifs de migration de pool de stockage ZFS ................................................................. 100
Exportation d'un pool de stockage ZFS ................................................................................... 100
Définition des pools de stockage disponibles pour importation .......................................... 101
Importation de pools de stockage ZFS à partir d'autres répertoires .................................... 103
Importation de pools de stockage ZFS .................................................................................... 103
Récupération de pools de stockage ZFS détruits .................................................................... 107
Mise à niveau de pools de stockage ZFS .......................................................................................... 109

5 Gestion des composants du pool racine ZFS ................................................................................ 111
Gestion des composants du pool racine ZFS (présentation) ........................................................ 111
Configuration requise pour le pool racine ZFS ...................................................................... 112
Dépannage des problèmes d'installation d'un pool racine ZFS ............................................ 113
Gestion de votre pool racine ZFS ..................................................................................................... 114
Installation d'un pool racine ZFS ............................................................................................. 115
▼ Procédure de mise à jour de l'environnement d'initialisation ZFS ...................................... 116
▼ Montage d'un environnement d'initialisation alternatif ....................................................... 117
▼ Configuration d'un pool racine mis en miroir ........................................................................ 117

5

Table des matières

▼ Remplacement d'un disque dans un pool racine ZFS ............................................................ 119
▼ Création d'un environnement d'initialisation dans un pool racine différent ..................... 121
Gestion de vos périphériques de swap et de vidage ZFS ............................................................... 122
Ajustement de la taille de vos périphériques de swap et de vidage ZFS ............................... 123
Dépannage du périphérique de vidage ZFS ............................................................................ 124
Initialisation à partir d'un système de fichiers racine ZFS ............................................................ 125
Initialisation à partir d'un disque alternatif d'un pool racine ZFS mis en miroir ............... 126
Initialisation à partir d'un système de fichiers racine ZFS sur un système SPARC ............ 127
Initialisation à partir d'un système de fichiers racine ZFS sur un système x86 ................... 128
Initialisation à des fins de récupération dans un environnement racine ZFS ..................... 129

6 Gestion des systèmes de fichiers Oracle Solaris ZFS ................................................................... 133
Gestion des systèmes de fichiers ZFS (présentation) .................................................................... 133
Création, destruction et renommage de systèmes de fichiers ZFS .............................................. 134
Création d'un système de fichiers ZFS ..................................................................................... 134
Destruction d'un système de fichiers ZFS ............................................................................... 135
Modification du nom d'un système de fichiers ZFS ............................................................... 136
Présentation des propriétés ZFS ...................................................................................................... 137
Propriétés ZFS natives en lecture seule .................................................................................... 151
Propriétés ZFS natives définies ................................................................................................. 152
Propriétés ZFS définies par l'utilisateur ................................................................................... 159
Envoi de requêtes sur les informations des systèmes de fichiers ZFS .......................................... 160
Affichage des informations de base des systèmes ZFS ........................................................... 160
Création de requêtes ZFS complexes ....................................................................................... 161
Gestion des propriétés ZFS .............................................................................................................. 163
Définition des propriétés ZFS ................................................................................................... 163
Héritage des propriétés ZFS ...................................................................................................... 164
Envoi de requêtes sur les propriétés ZFS ................................................................................. 165
Montage de système de fichiers ZFS ................................................................................................ 168
Gestion des points de montage ZFS ......................................................................................... 168
Montage de système de fichiers ZFS ........................................................................................ 170
Utilisation de propriétés de montage temporaires ................................................................ 172
Démontage des systèmes de fichiers ZFS ................................................................................ 172
Activation et annulation du partage des systèmes de fichiers ZFS .............................................. 173
Syntaxe de partage ZFS héritée ................................................................................................. 175

6

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Table des matières

Nouvelle syntaxe de partage ZFS .............................................................................................. 175
Problèmes de migration/transition de partage ZFS ............................................................... 182
Définition des quotas et réservations ZFS ...................................................................................... 182
Définitions de quotas sur les systèmes de fichiers ZFS .......................................................... 183
Définition de réservations sur les systèmes de fichiers ZFS .................................................. 187
Chiffrement des systèmes de fichiers ZFS ....................................................................................... 188
Modification des clés d'un système de fichiers ZFS chiffré .................................................... 191
Montage d'un système de fichiers ZFS chiffré ......................................................................... 192
Interactions entre les propriétés de compression, de suppression des doublons et de
chiffrement ZFS .......................................................................................................................... 193
Exemples de chiffrement de systèmes de fichiers ZFS ........................................................... 193
Migration de systèmes de fichiers ZFS ............................................................................................ 195
▼ Migration d'un système de fichiers vers un système de fichiers ZFS .................................... 196
Dépannage des migrations de systèmes de fichiers ZFS ........................................................ 197
Mise à niveau des systèmes de fichiers ZFS .................................................................................... 198

7 Utilisation des instantanés et des clones ZFS Oracle Solaris ......................................................199
Présentation des instantanés ZFS .................................................................................................... 199
Création et destruction d'instantanés ZFS .............................................................................. 200
Affichage et accès des instantanés ZFS .................................................................................... 203
Restauration d'un instantané ZFS ............................................................................................ 205
Identification des différences entre des instantanés ZFS (zfs diff) .................................. 205
Présentation des clones ZFS ............................................................................................................. 206
Création d'un clone ZFS ............................................................................................................ 207
Destruction d'un clone ZFS ...................................................................................................... 208
Remplacement d'un système de fichiers ZFS par un clone ZFS ............................................ 208
Envoi et réception de données ZFS ................................................................................................. 209
Enregistrement de données ZFS à l'aide d'autres produits de sauvegarde .......................... 210
Reconnaissance des flux d'instantané ZFS .............................................................................. 210
Envoi d'un instantané ZFS ........................................................................................................ 212
Réception d'un instantané ZFS ................................................................................................. 213
Application de différentes valeurs de propriété à un flux d'instantané ZFS ........................ 214
Envoi et réception de flux d'instantanés ZFS complexes ....................................................... 216
Réplication distante de données ZFS ....................................................................................... 219

7

Table des matières

8 Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS ..................221
Nouveau modèle ACL Solaris .......................................................................................................... 221
Descriptions de syntaxe pour la configuration des ACL ....................................................... 223
Héritage d'ACL ........................................................................................................................... 227
Propriétés ACL ........................................................................................................................... 228
Configuration d'ACL dans des fichiers ZFS ................................................................................... 229
Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé ................................ 231
Configuration d'héritage d'ACL dans des fichiers ZFS en format détaillé .......................... 236
Configuration et affichage d'ACL dans des fichiers ZFS en format compact ............................. 242
Application d'attributs spéciaux aux fichiers ZFS .......................................................................... 248

9

10

Administration déléguée de ZFS dans Oracle Solaris ................................................................. 251
Présentation de l'administration déléguée de ZFS ........................................................................ 251
Désactivation des droits délégués de ZFS ................................................................................ 252
Délégation d'autorisations ZFS ........................................................................................................ 252
Délégation des autorisations ZFS (zfs allow) ...................................................................... 255
Suppression des autorisations déléguées de ZFS (zfs unallow) ......................................... 256
Délégation d'autorisations ZFS (exemples) ................................................................................... 257
Affichage des autorisations ZFS déléguées (exemples) ................................................................. 260
Suppression des autorisations ZFS déléguées (exemples) ............................................................ 262

Rubriques avancées Oracle Solaris ZFS .........................................................................................265
Volumes ZFS ...................................................................................................................................... 265
Utilisation d'un volume ZFS en tant que périphérique de swap ou de vidage .................... 266
Utilisation d'un volume ZFS en tant qu'unité logique de stockage iSCSI ............................ 267
Utilisation de ZFS dans un système Solaris avec zones installées ................................................ 268
Ajout de systèmes de fichiers ZFS à une zone non globale .................................................... 269
Délégation de jeux de données à une zone non globale ......................................................... 270
Ajout de volumes ZFS à une zone non globale ....................................................................... 270
Utilisation de pools de stockage ZFS au sein d'une zone ....................................................... 271
Gestion de propriétés ZFS au sein d'une zone ........................................................................ 271
Explication de la propriété zoned ............................................................................................. 272
Copie de zones vers d'autres systèmes ..................................................................................... 273
Utilisation de pools racine ZFS de remplacement ......................................................................... 274
Création de pools racine de remplacement ZFS ..................................................................... 274

8

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Table des matières

Importation de pools racine de remplacement ...................................................................... 275

11 Dépannage d'Oracle Solaris ZFS et récupération de pool .......................................................... 277
Identification des défaillances ZFS .................................................................................................. 277
Périphériques manquants dans un pool de stockage ZFS ..................................................... 278
Périphériques endommagés dans un pool de stockage ZFS ................................................. 278
Données ZFS corrompues ........................................................................................................ 278
Contrôle de l'intégrité d'un système de fichiers ZFS ..................................................................... 279
Réparation du système de fichiers ............................................................................................ 279
Validation du système de fichiers ............................................................................................. 279
Contrôle du nettoyage de données ZFS ................................................................................... 280
Résolution de problèmes avec le système de fichiers ZFS ............................................................. 281
Recherche de problèmes éventuels dans un pool de stockage ZFS ...................................... 283
Consultation de la sortie de zpool status ............................................................................. 283
Rapport système de messages d'erreur ZFS ............................................................................ 286
Réparation d'un configuration ZFS endommagée ........................................................................ 287
Réparation d'un périphérique manquant ....................................................................................... 287
Reconnexion physique d'un périphérique .............................................................................. 289
Notification relative à la disponibilité de périphériques dans ZFS ....................................... 289
Remplacement ou réparation d'un périphérique endommagé .................................................... 289
Détermination du type de panne de périphérique ................................................................. 290
Suppression des erreurs transitoires ........................................................................................ 291
Remplacement d'un périphérique dans un pool de stockage ZFS ........................................ 292
Réparation de données endommagées ........................................................................................... 300
Identification du type de corruption de données ................................................................... 300
Réparation d'un fichier ou répertoire corrompu .................................................................... 301
Réparation de dommages présents dans l'ensemble du pool de stockage ZFS ................... 303
Réparation d'un système impossible à réinitialiser ....................................................................... 304

12

Archivage des instantanés et récupération du pool racine ....................................................... 307
Présentation du processus de récupération ZFS ............................................................................ 307
Conditions pour la récupération de pools ZFS ....................................................................... 308
Création d'une archive d'instantanés ZFS pour la récupération .................................................. 308
▼ Création d'une archive d'instantanés ZFS ............................................................................... 309
Recréation du pool racine et récupération des instantanés de pool racine ................................. 310

9

Table des matières

▼ Recréation du pool racine sur le système de récupération .................................................... 310

13

Pratiques recommandées pour Oracle Solaris ZFS ...................................................................... 315
Pratiques recommandées pour les pools de stockage .................................................................... 315
Pratiques recommandées générales ......................................................................................... 315
Pratiques de création de pools de stockage ZFS ..................................................................... 316
Pratiques recommandées pour l'optimisation des performances des pools de stockage .. 319
Pratiques recommandées pour la maintenance et la gestion d'un pool de stockage ZFS .. 320
Pratiques recommandées pour les systèmes de fichiers ................................................................ 321
Pratiques recommandées pour la création de systèmes de fichiers ...................................... 321
Pratiques recommandées pour la surveillance de systèmes de fichiers ZFS ....................... 322

A Descriptions des versions d'Oracle Solaris ZFS ............................................................................ 325
Présentation des versions ZFS ......................................................................................................... 325
Versions de pool ZFS ........................................................................................................................ 325
Versions du système de fichiers ZFS ............................................................................................... 327

Index ................................................................................................................................................... 329

10

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Préface

Le manuel Administration d'Oracle Solaris : Systèmes de fichiers ZFS fournit des informations
sur la configuration et la gestion de systèmes de fichiers Oracle Solaris.

Ce guide contient des informations sur les systèmes SPARC et x86.

Remarque – Cette version d'Oracle Solaris prend en charge les systèmes utilisant les architectures
de processeur SPARC et x86. Les systèmes pris en charge sont répertoriés à la page Oracle
Solaris Hardware Compatibility List disponible à l'adresse http://www.oracle.com/webfolder/
technetwork/hcl/index.html. Ce document présente les différences d'implémentation en
fonction des divers types de plates-formes.

Utilisateurs de ce manuel

Ce guide est destiné à toute personne souhaitant configurer et gérer des systèmes de fichiers ZFS
Oracle Solaris. Il est recommandé de savoir utiliser le système d'exploitation Oracle Solaris ou
toute autre version UNIX.

Organisation de ce document

Le tableau suivant décrit les chapitres de ce document.

Chapitre

Description

Chapitre 1, “Système de fichiers
Oracle Solaris ZFS
(introduction)”

Chapitre 2, “Mise en route
d'Oracle Solaris ZFS”

Présente ZFS, ses fonctionnalités et ses avantages. Il aborde également des
concepts de base, ainsi que la terminologie.

Décrit étape par étape les instructions d'une configuration ZFS de base
contenant des pools et des systèmes de fichiers simples. Ce chapitre indique
également le matériel et logiciels requis pour la création de systèmes de
fichiers ZFS.

11

Préface

Chapitre

Description

Chapitre 3, “Différences entre
les systèmes de fichiers Oracle
Solaris ZFS et classiques”

Identifie les fonctionnalités importantes qui différencient le système de
fichiers ZFS des systèmes de fichiers standard. Lors de l'utilisation d'outils
classiques avec le système de fichiers ZFS, la compréhension de ces
différences clés permet d'éviter les confusions.

Chapitre 4, “Gestion des pools
de stockage Oracle Solaris ZFS”

Décrit en détail les méthodes de création et d'administration de pools de
stockage ZFS.

Chapitre 5, “Gestion des
composants du pool racine ZFS
”

Décrit les méthodes de gestion des composants de pool racine ZFS,
notamment la configuration d'un pool racine en miroir, la mise à niveau des
environnements d'initialisation ZFS et le redimensionnement de
périphériques de swap et de vidage.

Chapitre 6, “Gestion des
systèmes de fichiers
Oracle Solaris ZFS”

Chapitre 7, “Utilisation des
instantanés et des clones ZFS
Oracle Solaris”

Chapitre 8, “Utilisation des
ACL et des attributs pour
protéger les fichiers
Oracle Solaris ZFS”

Décrit en détail les méthodes de gestion de systèmes de fichiers ZFS. Ce
chapitre décrit des concepts tels que la disposition hiérarchique de systèmes
de fichiers, l'héritage de propriétés, la gestion automatique de points de
montage et les interactions de partage.

Décrit les méthodes de création et d'administration d'instantanés ZFS et de
clones.

Explique comment utiliser des listes de contrôle d'accès (ACL,
Access Control List) pour la protection des fichiers ZFS en fournissant des
autorisations à un niveau de granularité plus fin que les autorisations UNIX
standard.

Chapitre 9, “Administration
déléguée de ZFS dans Oracle
Solaris”

Explique comment utiliser les fonctions de l'administration déléguée de ZFS
pour permettre aux utilisateurs ne disposant pas des autorisations
nécessaires d'effectuer des tâches d'administration ZFS.

Chapitre 10, “Rubriques
avancées Oracle Solaris ZFS”

Explique comment utiliser des volumes et des systèmes de fichiers ZFS dans
un système Oracle Solaris comportant des zones et comment utiliser les
pools racine de remplacement.

Chapitre 11, “Dépannage
d'Oracle Solaris ZFS et
récupération de pool”

Explique comment identifier des modes de défaillance de ZFS et les solutions
existantes. Les étapes de prévention de ces défaillances sont également
abordées.

Chapitre 12, “Archivage des
instantanés et récupération du
pool racine”

Chapitre 13, “Pratiques
recommandées pour Oracle
Solaris ZFS”

Décrit comment archiver des instantanés de pool racine et comment
récupérer des pools racine.

Décrit les pratiques recommandées pour la création, la surveillance et la mise
à jour de pools de stockage ZFS et de systèmes de fichiers.

Annexe A, “Descriptions des
versions d'Oracle Solaris ZFS”

Décrit les versions ZFS disponibles, les fonctionnalités de chacune d'entre
elles et le SE Solaris fournissant la version et les fonctionnalités ZFS.

12

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Documentation connexe

Préface

Pour obtenir des informations générales sur l'administration de systèmes Oracle Solaris,
reportez-vous aux documents suivants :
■ Administration d’Oracle Solaris : Tâches courantes

■

System Administration Guide: Advanced Administration

■ Administration d’Oracle Solaris : Périphériques et systèmes de fichiers
■ Administration d’Oracle Solaris : services de sécurité

Accès au support technique Oracle

Les clients Oracle ont accès au support électronique via My Oracle Support. Pour plus
d'informations, visitez le site http://www.oracle.com/pls/topic/lookup?ctx=acc&id=info
ou le site http://www.oracle.com/pls/topic/lookup?ctx=acc&id=trs adapté aux
utilisateurs malentendants.

Conventions typographiques

Le tableau ci-dessous décrit les conventions typographiques utilisées dans ce manuel.

TABLEAU P–1 Conventions typographiques

Type de caractères

Signification

Exemple

AaBbCc123

Noms des commandes, fichiers et répertoires,
ainsi que messages système.

Modifiez votre fichier .login.
Utilisez ls -a pour afficher la liste
de tous les fichiers.

nom_machine% Vous avez reçu du

courrier.

nom_machine% su

Mot de passe :

Ce que vous entrez, par opposition à ce qui
s'affiche à l'écran.

AaBbCc123

aabbcc123

Paramètre fictif : à remplacer par un nom ou une
valeur réel(le).

La commande permettant de
supprimer un fichier est rm
nom_fichier.

13

Préface

TABLEAU P–1 Conventions typographiques
Type de caractères

Signification

(Suite)

AaBbCc123

Titres de manuel, nouveaux termes et termes
importants.

Exemple

Reportez-vous au chapitre 6 du
Guide de l'utilisateur.
Un cache est une copie des éléments
stockés localement.
N'enregistrez pas le fichier.
Remarque : en ligne, certains
éléments mis en valeur s'affichent en
gras.

Invites de shell dans les exemples de commandes

Le tableau suivant présente l'invite système UNIX par défaut et l'invite superutilisateur pour les
shells faisant partie du SE Oracle Solaris. L'invite système par défaut qui s'affiche dans les
exemples de commandes dépend de la version Oracle Solaris.

TABLEAU P–2

Invites de shell

Shell

Bash shell, korn shell et bourne shell

Bash shell, korn shell et bourne shell pour
superutilisateur

C shell

C shell pour superutilisateur

Invite

$

#

nom_machine%

nom_machine#

14

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

1C H A P I T R E

1

Système de fichiers Oracle Solaris ZFS
(introduction)

Ce chapitre présente le système de fichiers ZFS Oracle Solaris, ses fonctions et ses avantages. Il
aborde également la terminologie de base utilisée dans le reste de ce document.

Ce chapitre contient les sections suivantes :
■ “Nouveautés de ZFS” à la page 15
■ “Description d'Oracle Solaris ZFS” à la page 24
■ “Terminologie ZFS” à la page 27
■ “Exigences d'attribution de noms de composants ZFS” à la page 29

Nouveautés de ZFS

Cette section décrit les nouvelles fonctions du système de fichier ZFS.
■ “Page de manuel ZFS modifiée (zfs.1m)” à la page 16
■ “Propriété aclmode améliorée” à la page 16
■ “Identification des périphériques de pool en fonction de leur emplacement physique”

■ “Migration shadow ZFS” à la page 18
■ “Améliorations du partage de fichiers ZFS” à la page 18
■ “Chiffrement de systèmes de fichiers ZFS” à la page 18
■ “Améliorations apportées au flux envoyé par ZFS” à la page 19
■ “Différences des instantanés ZFS (zfs diff)” à la page 19
■ “Récupération de pool de stockage ZFS et améliorations apportées aux performances”

à la page 16

à la page 20

■ “Réglage du comportement synchrone ZFS” à la page 20
■ “Messages du pool ZFS améliorés” à la page 21
■ “Améliorations de l'interopérabilité ACL ZFS” à la page 22
■ “Scission d'un pool de stockage ZFS mis en miroir (zpool split) ” à la page 23
■ “Modifications concernant iSCSI ZFS” à la page 23
■ “Nouveau processus du système de fichiers ZFS” à la page 23

15

Nouveautés de ZFS

■ “Propriété de suppression des doublons ZFS” à la page 24

Page de manuel ZFS modifiée (zfs.1m)
Oracle Solaris 11 : la page de manuel zfs.1m a été révisée ; désormais, les fonctionnalités de
base du système de fichiers ZFS figurent toujours dans la page de manuel zfs.1m, mais
l'administration déléguée, le chiffrement, la syntaxe de partage et les exemples connexes sont
traités dans les pages suivantes :

■

■

■

zfs_allow(1M)
zfs_encrypt(1M)
zfs_share(1M)

Propriété aclmode améliorée
Oracle Solaris 11 : la propriété aclmode modifie le comportement des listes de contrôle d'accès
(ACL) lorsqu'un fichier est créé et contrôle la modification des ACL lors d'une opération chmod.
La propriété aclmode a été réintroduite avec les valeurs suivantes :

■

■

■

discard :un système de fichiers avec une propriété aclmode de discard supprime toutes les
entrées d'ACL qui ne représentent pas le mode du fichier. Il s'agit de la valeur par défaut.
mask : un système de fichiers avec une propriété aclmode de mask restreint les autorisations
d'utilisateur ou de groupe. Les autorisations sont réduites de manière à ne pas excéder les
bits d'autorisation du groupe, à moins qu'il ne s'agisse d'une entrée utilisateur possédant le
même UID que le propriétaire du fichier ou du répertoire. Dans ce cas, les autorisations
d'ACL sont réduites de manière à ne pas excéder les bits d'autorisation du propriétaire. La
valeur de masque préserve en outre l'ACL lors des modifications de mode successives, à
condition qu'aucune opération de jeu d'ACL explicite n'ait été effectuée.
passthrough : un système de fichiers avec une propriété aclmode de passthrough indique
qu'aucune modification n'est apportée à l'ACL en dehors de la génération des entrées d'ACL
nécessaires pour représenter le nouveau mode du fichier ou du répertoire.

Pour plus d'informations, reportez-vous à l'Exemple 8–14.

Identification des périphériques de pool en fonction
de leur emplacement physique
Oracle Solaris 11 : dans cette version de Solaris, utilisez la commande zpool status -l pour
afficher les informations relatives à l'emplacement du disque physique des périphériques de
pool, lesquelles sont accessibles dans le répertoire /dev/chassis. Ce répertoire contient les
valeurs de châssis, de réceptacle et d'occupant pour les périphériques de votre système.

16

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveautés de ZFS

En outre, vous pouvez utiliser la commande fmadm add-alias pour inclure un nom d'alias de
disque qui facilite l'identification de l'emplacement physique des disques dans votre
environnement. Par exemple :

# fmadm add-alias SUN-Storage-J4400.0912QAJ001 SUN-Storage-J4400.rack22

Par exemple :

% zpool status -l export

pool: export

state: ONLINE

scan: resilvered 379G in 8h31m with 0 errors on Thu Jan 27 23:10:20 2011

config:

NAME

export

mirror-0

STATE

READ WRITE CKSUM

ONLINE

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__2/disk

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__3/disk

ONLINE

mirror-1

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__4/disk

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__5/disk

ONLINE

mirror-2

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__6/disk

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__7/disk

ONLINE

mirror-3

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__8/disk

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__9/disk

ONLINE

mirror-4

ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__10/disk ONLINE

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__11/disk ONLINE

spares

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__0/disk

AVAIL

/dev/chassis/SUN-Storage-J4400.rack22/SCSI_Device__1/disk

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

La commande zpool iostat a également été mise à jour et fournit dorénavant des
informations sur l'emplacement physique des périphériques d'un pool. Par exemple :

# zpool iostat -lv

capacity

operations

bandwidth

pool

alloc

free

read write

read write

---------- ----- ----- ----- ----- ----- -----

export

2.39T 2.14T

mirror

490G

438G

13

2

27 42.7K

300K

5 8.53K 60.3K

/dev/chassis/...rack22/SCSI_Device__2/disk

/dev/chassis/...rack22/SCSI_Device__3/disk

mirror

490G

438G

2

5 8.62K 59.9K

/dev/chassis/...rack22/SCSI_Device__4/disk

/dev/chassis/...rack22/SCSI_Device__5/disk

mirror

490G

438G

2

5 8.60K 60.2K

/dev/chassis/...rack22/SCSI_Device__6/disk

/dev/chassis/...rack22/SCSI_Device__7/disk

mirror

490G

438G

2

5 8.47K 60.1K

-

-

-

-

-

-

.

-

-

-

-

-

-

1

1

1

1

1

1

0

0

0

0

0

0

4.47K 60.3K

4.45K 60.3K

4.52K 59.9K

4.48K 59.9K

4.50K 60.2K

4.49K 60.2K

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

17

Nouveautés de ZFS

.

.

Les commandes croinfo, diskinfo, format et prtconf fournissent elles aussi des informations
sur l'emplacement des disques physiques. Pour plus d'informations, reportez-vous à la section
“Identification des périphériques par emplacements physiques” du manuel Administration
d’Oracle Solaris : Périphériques et systèmes de fichiers.

Migration shadow ZFS
Oracle Solaris 11 : dans cette version, vous pouvez migrer des données à partir d'un ancien
système de fichiers vers un nouveau système de fichiers, le nouveau système de fichiers restant
accessible et modifiable pendant le processus de migration.
La définition de la propriété shadow sur un nouveau système de fichiers ZFS déclenche la
migration des anciennes données. La propriété shadow peut être définie de manière à
déclencher la migration de données depuis le système local ou depuis un système distant avec
l'une ou l'autre des valeurs suivantes :

file:///path
nfs://host:path
Pour plus d'informations, reportez-vous à la section “Migration de systèmes de fichiers ZFS”
à la page 195.

Améliorations du partage de fichiers ZFS
Oracle Solaris 11 : dans cette version de Solaris, le partage de systèmes de fichiers ZFS est un
processus en deux étapes au cours duquel les propriétés du partage sont définies à l'aide d'une
seule commande et le partage SMB ou NFS est publié au cours d'une seconde étape.
■ Création du partage NFS ou SMB d'un système de fichiers ZFS et identification des

propriétés de partage des fichiers à l'aide de la commande zfs set share.

■ Publication des partages NFS ou SMB par l'activation des propriétés sharenfs ou

sharesmb.

Pour plus d'informations, reportez-vous à la section “Activation et annulation du partage des
systèmes de fichiers ZFS ” à la page 173.

Chiffrement de systèmes de fichiers ZFS
Oracle Solaris 11 : dans cette version, vous pouvez chiffrer un système de fichiers ZFS.
Par exemple, le système de fichiers tank/cindy est créé avec la propriété de chiffrement activée.
La stratégie de chiffrement par défaut consiste en une invite à saisir une phrase de passe
comportant 8 caractères au minimum.

18

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveautés de ZFS

# zfs create -o encryption=on tank/cindy

Enter passphrase for ’tank/cindy’: xxx

Enter again: xxx

Une stratégie de chiffrement est définie au moment de la création d'un système de fichiers ZFS.
Les systèmes de fichiers descendants héritent de la stratégie de chiffrement du système de
fichiers parent, et celle-ci ne peut pas être supprimée.

Pour plus d'informations, reportez-vous à la section “Chiffrement des systèmes de fichiers ZFS”
à la page 188.

Améliorations apportées au flux envoyé par ZFS
Oracle Solaris 11 : dans cette version, vous pouvez définir les propriétés du système de fichiers
qui sont envoyées et reçues dans un flux d'instantané. Ces améliorations offrent davantage de
flexibilité pour appliquer des propriétés du système de fichiers dans un flux envoyé à un système
de fichiers récepteur ou pour déterminer si les propriétés du système de fichiers local, telles que
la valeur de propriété mountpoint, doivent être ignorées lorsqu'elles sont reçues.

Pour plus d'informations, reportez-vous à la section “Application de différentes valeurs de
propriété à un flux d'instantané ZFS” à la page 214.

Différences des instantanés ZFS (zfs diff)
Oracle Solaris 11 : dans cette version, vous pouvez déterminer les différences des instantanés
ZFS à l'aide de la commande zfs diff.

Supposons par exemple que les deux instantanés suivants sont créés :

$ ls /tank/cindy

fileA

$ zfs snapshot tank/cindy@0913

$ ls /tank/cindy

fileA fileB

$ zfs snapshot tank/cindy@0914

Par exemple, afin d'identifier les différences entre deux instantanés, utilisez une syntaxe
semblable à la suivante :

$ zfs diff tank/cindy@0913 tank/cindy@0914

M

+

/tank/cindy/

/tank/cindy/fileB

Dans la sortie, M indique que le répertoire a été modifié. Le + indique que fileB existe dans
l'instantané le plus récent.

Pour plus d'informations, reportez-vous à la section “Identification des différences entre des
instantanés ZFS (zfs diff)” à la page 205.

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

19

Nouveautés de ZFS

Récupération de pool de stockage ZFS et
améliorations apportées aux performances
Oracle Solaris 11 : dans cette version, les nouvelles fonctionnalités de pool de stockage ZFS
suivantes sont fournies :
■ Vous pouvez importer un pool avec un journal manquant en utilisant la commande zpool

import -m. Pour plus d'informations, reportez-vous à la section “Importation d'un pool avec
un périphérique de journalisation manquant” à la page 105.

■ Vous pouvez importer un pool en mode lecture seule. Cette fonction est principalement

destinée à la récupération de pool. Si un pool endommagé n'est pas accessible car les
périphériques sous-jacents le sont également, vous pouvez importer le pool en lecture seule
pour récupérer les données. Pour plus d'informations, reportez-vous à la section
“Importation d'un pool en mode lecture seule” à la page 106.

■ Un pool de stockage RAID-Z (raidz1, raidz2 ou raidz3) créé dans cette version et mis à
niveau vers la version 29 ou une version ultérieure du pool comportera des métadonnées
sensibles à la latence qui seront automatiquement mises en miroir pour améliorer les
performances de capacité de traitement d'E/S de lecture. Pour les pools RAID-Z existants
mis à niveau vers la version 29 ou une version ultérieure du pool, un certain nombre de
métadonnées seront mises en miroir pour toutes les données nouvellement écrites.
Les métadonnées mises en miroir dans un pool RAID-Z ne fournissent pas de protection
supplémentaire contre les pannes matérielles, comme c'est le cas pour un pool de stockage
mis en miroir. Les métadonnées mises en miroir consomment de l'espace supplémentaire,
mais la protection RAID-Z est la même que dans les versions précédentes. Cette
amélioration est destinée à l'amélioration des performances uniquement.

Réglage du comportement synchrone ZFS
Oracle Solaris 11 : dans cette version, vous pouvez déterminer un comportement synchrone du
système de fichiers ZFS à l'aide de la propriété sync.
Le comportement synchrone par défaut consiste à écrire toutes les transactions des systèmes de
fichiers synchrones dans le journal de tentatives et à vider tous les périphériques pour s'assurer
que les données sont stables. La désactivation du comportement synchrone par défaut n'est pas
recommandée. Elle pourrait avoir des répercussions sur les applications qui dépendent de la
prise en charge synchrone et risquerait d'entraîner des pertes de données.
La propriété sync peut être définie avant ou après la création du système de fichiers. Dans tous
les cas, la valeur de propriété prend effet immédiatement. Par exemple :

# zfs set sync=always tank/neil

Le paramètre zil_disable n'est plus disponible dans les versions Oracle Solaris incluant la
propriété sync.

20

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveautés de ZFS

Pour plus d'informations, reportez-vous au Tableau 6–1.

Messages du pool ZFS améliorés
Oracle Solaris 11 : dans cette version, vous pouvez utiliser l'option -T afin de fournir un
intervalle et une valeur de comptage pour les commandes zpool list et zpool status pour
l'affichage d'informations supplémentaires.

En outre, des nettoyages du pool et des informations de réargenture supplémentaires sont
disponibles via la commande zpool status comme suit :
■ Rapport de progression de la réargenture. Par exemple :

scan: resilver in progress since Thu May 26 11:26:32 2011

1.26G scanned out of 2.40G at 6.15M/s, 0h3m to go

1.26G resilvered, 56.3% done

■ Rapport de progression du nettoyage. Par exemple :

scan: scrub in progress since Fri May 27 08:24:17 2011

18.0M scanned out of 2.35G at 8.99M/s, 0h4m to go

0 repaired, 0.75% done

■ Message de fin de la réargenture. Par exemple :

scan: resilvered 2.34G in 1h2m with 0 errors on Thu May 26 11:56:40 2011

■ Message de fin du nettoyage. Par exemple :

scan: scrub repaired 512B in 1h2m with 0 errors on Fri May 27 08:54:50 2011

■ Message d'annulation du nettoyage en cours. Par exemple :

scan: scrub canceled on Wed Fri Jun 10 09:06:24 2011

■ Les messages de fin de la réargenture et du nettoyage subsistent après plusieurs

réinitialisation du système.

La syntaxe suivante utilise l'intervalle et l'option de comptage pour afficher en permanence les
informations relatives à la réargenture du pool en cours. Vous pouvez utiliser la valeur -T d
pour afficher les informations au format de date standard ou -T u pour les afficher dans un
format interne.

# zpool status -T d tank 3 2

Wed Jun 22 14:35:40 GMT 2011

pool: tank

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scan: resilver in progress since Wed Jun 22 14:33:29 2011

3.42G scanned out of 7.75G at 28.2M/s, 0h2m to go

3.39G resilvered, 44.13% done

config:

NAME

STATE

READ WRITE CKSUM

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

21

Nouveautés de ZFS

tank

ONLINE

mirror-0 ONLINE

c2t3d0 ONLINE

c2t4d0 ONLINE

mirror-1 ONLINE

c2t7d0 ONLINE

c2t8d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 (resilvering)

errors: No known data errors

Améliorations de l'interopérabilité ACL ZFS
Oracle Solaris 11 : cette version inclut les améliorations suivantes des ACL :
■ Les ACL triviales ne requièrent pas d'entrée de contrôle d'accès (ACE) de refus, à

l'exception des autorisations extraordinaires. Par exemple, un mode de 0644, 0755 ou 0664
ne requiert pas d'ACE de refus, tandis qu'un mode de 0705, 0060, et ainsi de suite, requiert
des ACE de refus.
L'ancien comportement inclut des ACE de refus dans une ACL triviale telle que 644. Par
exemple :

# ls -v file.1

-rw-r--r--

1 root

root

206663 Jun 14 11:52 file.1

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Le nouveau comportement pour une ACL triviale telle que 644 n'inclut pas les ACE de
refus. Par exemple :

# ls -v file.1

-rw-r--r--

1 root

root

206663 Jun 22 14:30 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

■ Les ACL ne plus sont scindées en plusieurs ACE pendant l'héritage pour tenter de conserver

l'autorisation d'origine inchangée. Au lieu de cela, les autorisations sont modifiées de
manière à appliquer le mode de création de fichier.

■ Le comportement de la propriété aclinherit inclut une réduction des autorisations lorsque

la propriété est définie sur restricted (restreint), ce qui signifie que les ACL ne sont plus
scindées en plusieurs ACE pendant l'héritage.

22

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveautés de ZFS

■ Une ACL existante est abandonnée durant les opérations chmod(2) par défaut. Cette

modification signifie que la propriété ZFS aclmode n'est plus disponible.

■ Une nouvelle règle de calcul du mode d'autorisation signifie que si une ACL possède une

entrée de contrôle d'accès user qui est également propriétaire du fichier, alors ces
autorisations sont incluses dans le calcul du mode d'autorisation. La même règle s'applique
lorsqu'une entrée de contrôle d'accès group est propriétaire de groupe du fichier.

Pour plus d'informations, reportez-vous au Chapitre 8, “Utilisation des ACL et des attributs
pour protéger les fichiers Oracle Solaris ZFS”.

Scission d'un pool de stockage ZFS mis en miroir
(zpool split)
Oracle Solaris 11 : dans cette version de , vous pouvez utiliser la commande zpool split pour
scinder un pool de stockage mis en miroir, ce qui déconnecte un ou plusieurs disques dans le
pool d'origine mis en miroir pour créer un autre pool identique.

Pour plus d'informations, reportez-vous à la section “Création d'un pool par scission d'un pool
de stockage ZFS mis en miroir” à la page 70.

Modifications concernant iSCSI ZFS
Oracle Solaris 11 : dans cette version, le démon cible iSCSI est remplacé par l'utilisation du
démon cible COMSTAR (Common Multiprotocol SCSI Target). Cette modification signifie
également que la propriété shareiscsi , qui servait à partager un volume ZFS en tant que LUN
iSCSI, n'est plus disponible. La commande stmfadm permet de configurer et de partager un
volume ZFS en tant que LUN iSCSI.

Pour plus d'informations, reportez-vous “Utilisation d'un volume ZFS en tant qu'unité logique
de stockage iSCSI” à la page 267.

Nouveau processus du système de fichiers ZFS
Oracle Solaris 11 : dans cette version, chaque pool de stockage ZFS est associé à un processus
zpool-poolname. Les threads dans ce processus sont les threads de traitement d'E/S du pool
permettant de gérer les tâches d'E/S, telles la compression et la validation de la somme de
contrôle, associées au pool. Le but de ce processus est d'indiquer l'utilisation de la CPU de
chaque pool de stockage.

Des informations relatives à ces processus en cours d'exécution peuvent être consultées à l'aide
des commandes ps et prstat. Ces processus sont uniquement disponibles dans la zone globale.
Pour de plus amples informations, reportez-vous à la section SDC(7).

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

23

Description d'Oracle Solaris ZFS

Propriété de suppression des doublons ZFS
Oracle Solaris 11 : dans cette version, vous pouvez utiliser la propriété de suppression des
doublons (dedup) pour supprimer les données redondantes des systèmes de fichiers ZFS. Si la
propriété dedup est activée pour un système de fichiers, les blocs de données dupliquées sont
supprimés de façon synchrone. Par conséquent, seules les données uniques sont stockées et les
composants communs sont partagés entre les fichiers.

Vous pouvez activer cette propriété comme suit :

# zfs set dedup=on tank/home

Bien que la suppression des doublons soit définie en tant que propriété du système de fichiers,
elle s'étend à l'échelle du pool. Par exemple, vous pouvez identifier le ratio de suppression des
doublons comme suit :

# zpool list tank

NAME

SIZE ALLOC

FREE

CAP DEDUP HEALTH ALTROOT

tank

136G 55.2G 80.8G

40% 2.30x ONLINE -

La sortie zpool list a été mise à jour pour prendre en charge la propriété de suppression des
doublons.

Pour plus d'informations sur la définition de la propriété de suppression des doublons,
reportez-vous à la section “Propriété dedup” à la page 156.

N'activez pas la propriété dedup sur des systèmes de fichiers résidant sur des systèmes de
production avant d'avoir passé en revue les points suivants :
■ Déterminez si vos données bénéficieraient de gains d'espace grâce à la suppression des

doublons.

■ Assurez-vous que votre système dispose de suffisamment de mémoire physique pour

prendre en charge la suppression des doublons.

■ Déterminez l'incidence possible sur les performances du système.

Pour plus d'informations sur ces considérations, reportez-vous à la section “Propriété dedup”
à la page 156.

Description d'Oracle Solaris ZFS

Le système de fichiers ZFS Oracle Solaris présente des fonctions et des avantages uniques au
monde. Ce système de fichiers modifie radicalement les méthodes d'administration des
systèmes de fichiers. Le système ZFS est robuste, évolutif et facile à administrer.

24

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Description d'Oracle Solaris ZFS

Stockage ZFS mis en pool
ZFS utilise le concept de pools de stockage pour la gestion du stockage physique. Auparavant,
l'élaboration des systèmes de fichiers reposait sur un périphérique physique unique. Afin de
traiter plusieurs périphériques et d'assurer la redondance de données, le concept de gestionnaire
de volume a été introduit pour fournir la représentation d'un périphérique. Ainsi, il n'est plus
nécessaire de modifier les systèmes de fichiers pour bénéficier de plusieurs périphériques. Cette
conception ajoutait un niveau de complexité supplémentaire et empêchait finalement les
avancées de certains systèmes de fichiers, car le système de fichiers ne pouvait pas contrôler le
placement physique des données dans les volumes virtualisés.

Le système de fichiers ZFS élimine la gestion du volume. Plutôt que de vous obliger à créer des
volumes virtualisés, ZFS regroupe les périphériques dans un pool de stockage. Le pool de
stockage décrit les caractéristiques physiques du stockage (disposition de périphérique,
redondance de données, etc.) et agit en tant qu'espace de stockage de données arbitraires à
partir duquel il est possible de créer des systèmes de fichiers. Désormais, les systèmes de fichiers
ne sont plus limités à des périphériques individuels. Ainsi, ils peuvent partager l'espace disque
avec l'ensemble des systèmes de fichiers du pool. Il n'est plus nécessaire de prédéterminer la
taille des systèmes de fichiers, car celle-ci augmente automatiquement au sein de l'espace disque
alloué au pool de stockage. En cas d'ajout d'espace de stockage, tous les systèmes de fichiers du
pool peuvent immédiatement utiliser l'espace disque supplémentaire, sans requérir des tâches
supplémentaires. Le pool de stockage fonctionne de la même manière qu'un système de
mémoire virtuelle sous plusieurs aspects : lors de l'ajout d'un module DIMM à un système, le
système d'exploitation ne force pas l'exécution de commandes pour configurer la mémoire et
pour l'assigner aux processus. Tous les processus du système utilisent automatiquement la
mémoire supplémentaire.

Sémantique transactionnelle
ZFS étant un système de fichiers transactionnel, l'état du système de fichiers reste toujours
cohérent sur le disque. Les systèmes de fichiers classiques écrasent les données en place. Ainsi,
en cas de réduction de la puissance du système, par exemple, entre le moment où un bloc de
données est alloué et celui où il est lié à un répertoire, le système de fichiers reste incohérent.
Auparavant, la commande fsck permettait de résoudre ce problème. Cette commande
permettait de vérifier l'état du système de fichiers et de tenter de réparer les incohérences
détectées au cours du processus. Les incohérences dans les systèmes de fichiers pouvaient poser
de sérieux problèmes aux administrateurs. La commande fsck ne garantissait pas la résolution
de tous les problèmes. Plus récemment, les systèmes de fichiers ont introduit le concept de
journalisation. Le processus de journalisation enregistre les actions dans un journal séparé,
lequel peut ensuite être lu en toute sécurité en cas de panne du système. Ce processus requiert
un temps système inutile car les données doivent être écrites deux fois. En outre, il entraîne
souvent d'autres problèmes, par exemple l'impossibilité de relire correctement le journal.

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

25

Description d'Oracle Solaris ZFS

Avec un système de fichiers transactionnel, la gestion de données s'effectue avec une sémantique
de copie lors de l'écriture. Les données ne sont jamais écrasées et toute séquence d'opération est
entièrement validée ou entièrement ignorée. La corruption du système de fichier en raison
d'une coupure de courant ou d'un arrêt du système est impossible. Même s'il se peut que les
éléments les plus récents écrits sur les données soient perdus, le système de fichiers reste
cohérent. De plus, les données synchrones (écrites avec l'indicateur O_DSYNC) sont toujours
écrites avant le renvoi. Ainsi, toute perte est impossible.

Sommes de contrôle et données d'autorétablissement
Avec ZFS, toutes les données et métadonnées sont vérifiées selon un algorithme de somme de
contrôle sélectionné par l'utilisateur. Les systèmes de fichiers classiques fournissant le contrôle
de sommes l'effectuaient par bloc, en raison de la couche de gestion de volumes et de la
conception classique de système de fichiers. Le terme classique signifie que certaines pannes,
comme l'écriture d'un bloc complet dans un emplacement incorrect, peuvent entraîner des
incohérences dans les données, sans pour autant entraîner d'erreur dans les sommes de
contrôle. Les sommes de contrôle ZFS sont stockées de façon à détecter ces pannes et à effectuer
une récupération de manière appropriée. Toutes les opérations de contrôle de somme et de
récupération des données sont effectuées sur la couche du système de fichiers et sont
transparentes aux applications.

De plus, ZFS fournit des données d'autorétablissement. ZFS assure la prise en charge de pools
de stockage avec différents niveaux de redondance de données. Lorsqu'un bloc de données
endommagé est détecté, ZFS récupère les données correctes à partir d'une autre copie
redondante et répare les données endommagées en les remplaçant par celles de la copie.

Evolutitivé inégalée
L'évolutivité de ZFS représente l'un des éléments clés de sa conception. La taille du système de
fichiers lui-même est de 128 bits et vous pouvez utiliser jusqu'à 256 quadrillions de zettaoctets
de stockage. L'ensemble des métadonnées est alloué de façon dynamique. Il est donc inutile de
pré-allouer des inodes ou de limiter l'évolutivité du système de fichiers lors de sa création. Tous
les algorithmes ont été écrits selon cette exigence d'évolutivité. Les répertoires peuvent contenir
jusqu'à 248 (256 trillions) d'entrées et le nombre de systèmes de fichiers ou de fichiers contenus
dans un système de fichiers est illimité.

Instantanés ZFS
Un instantané est une copie en lecture seule d'un système de fichiers ou d'un volume. La
création d'instantanés est rapide et facile. Ils n'utilisent initialement aucun espace disque
supplémentaire dans le pool.

26

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Terminologie ZFS

A mesure que le jeu de données actif est modifié, l'espace disque occupé par l'instantané
augmente tandis que l'instantané continue de référencer les anciennes données. Par
conséquent, l'instantané évite que les données soit libérées à nouveau dans le pool.

Administration simplifiée
Point le plus important, ZFS fournit un modèle administration qui a été énormément simplifié.
Grâce à une disposition hiérarchique des systèmes de fichiers, à l'héritage des propriétés et à la
gestion automatique des points de montage et de la sémantique de partage NFS, ZFS facilite la
création et la gestion de systèmes de fichiers sans requérir de nombreuses commandes, ni la
modification de fichiers de configuration. Vous pouvez définir des quotas ou des réservations,
activer ou désactiver la compression ou encore gérer les point de montage pour plusieurs
systèmes de fichiers avec une seule commande. Vous pouvez vérifier ou remplacer les
périphériques sans devoir apprendre un jeu de commandes de gestion de volumes spécifique.
Vous pouvez envoyer et recevoir des flux d'instantanés du système de fichiers.

ZFS assure la gestion des systèmes de fichiers par le biais d'une hiérarchie qui facilite la gestion
des propriétés telles que les quotas, les réservations, la compression et les points de montage.
Dans ce modèle, les systèmes de fichiers constituent le point de contrôle central. Les systèmes de
fichiers eux-mêmes étant très peu coûteux (autant que la création d'un nouveau répertoire), il
est recommandé de créer un système de fichiers pour chaque utilisateur, projet, espace de
travail, etc. Cette conception permet de définir des points de gestion détaillés.

Terminologie ZFS

Cette section décrit la terminologie de base utilisée dans ce document :
environnement d'initialisation

Un environnement d'initialisation est un environnement
Oracle Solaris initialisable se composant d'un système de
fichiers racine ZFS et, en option, d'autres systèmes de
fichiers montés sous celui-ci. Il ne peut y avoir plus d'un
environnement d'initialisation actif à la fois.
Hachage de 256 bits des données dans un bloc de système de
données. La fonctionnalité de contrôle de somme regroupe
entre autres, le contrôle de somme simple et rapide fletcher4
(paramètre par défaut), ainsi que les puissantes fonctions de
hachage cryptographique telles que SHA256.
Système de fichiers dont le contenu initial est identique à
celui d'un instantané.

somme de contrôle

clone

Pour plus d'informations sur les clones, reportez-vous à la
section “Présentation des clones ZFS” à la page 206.

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

27

Terminologie ZFS

jeu de données

Nom générique pour les composants ZFS suivants : clones,
systèmes de fichiers, instantanés et volumes.

Chaque jeu de données est identifié par un nom unique dans
l'espace de noms ZFS. Les jeux de données sont identifiés à
l'aide du format suivant :

pool/path[ @snapshot]
pool

Identifie le nom d'un pool de stockage
contenant le jeu de données.
Nom de chemin délimité par slash pour le
composant de jeu de données
Composant optionnel identifiant l'instantané
d'un jeu de données.

path

snapshot

système de fichiers

miroir

pool

RAID-Z

Pour plus d'informations sur les jeux de données,
reportez-vous au Chapitre 6, “Gestion des systèmes de
fichiers Oracle Solaris ZFS”.
Jeu de données ZFS de type filesystem monté au sein de
l'espace de noms système standard et se comportant comme
les autres systèmes de fichiers.

Pour plus d'informations sur les systèmes de fichiers,
reportez-vous au Chapitre 6, “Gestion des systèmes de
fichiers Oracle Solaris ZFS”.
Périphérique virtuel stockant des copies identiques de
données sur un ou plusieurs disques. Lorsqu'un disque d'un
miroir est défaillant, tout autre disque du miroir est en
mesure de fournir les mêmes données.
Groupe logique de périphériques décrivant la disposition et
les caractéristiques physiques du stockage disponible.
L'espace disque pour les jeux de données est alloué à partir
d'un pool.

Pour plus d'informations sur les pools de stockage,
reportez-vous au Chapitre 4, “Gestion des pools de stockage
Oracle Solaris ZFS”.
Périphérique virtuel stockant les données et la parité sur
plusieurs disques. Pour plus d'informations sur RAID-Z,
reportez-vous à la section “Configuration de pool de
stockage RAID-Z” à la page 48.

28

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

réargenture

instantané

périphérique virtuel

volume

Exigences d'attribution de noms de composants ZFS

Processus de copie de données d'un périphérique à un autre,
connu sous le nom de resynchronisation. Par exemple, si un
périphérique de miroir est remplacé ou mis hors ligne, les
données du périphérique de miroir le plus actuel sont
copiées dans le périphérique de miroir nouvellement
restauré. Dans les produits de gestion de volumes classiques,
ce processus est appelé resynchronisation de miroir.

Pour plus d'informations sur la réargenture ZFS,
reportez-vous à la section “Affichage de l'état de
réargenture” à la page 298.
Copie ponctuelle en lecture seule d'un système de fichiers ou
d'un volume.

Pour plus d'informations sur les instantanés, reportez-vous
à la section “Présentation des instantanés ZFS” à la page 199.
Périphérique logique dans un pool. il peut s'agir d'un
périphérique physique, d'un fichier ou d'une collection de
périphériques.

Pour plus d'informations sur les périphériques virtuels,
reportez-vous à la section “Affichage des informations d'un
périphérique virtuel de pool de stockage” à la page 58.
Jeu de données représentant un périphérique en mode bloc.
Vous pouvez par exemple créer un volume ZFS en tant que
périphérique de swap.

Pour plus d'informations sur les volumes ZFS,
reportez-vous à la section “Volumes ZFS” à la page 265.

Exigences d'attribution de noms de composants ZFS

L'attribution de noms de chaque composant ZFS, tels que les jeux de données et les pools, doit
respecter les règles suivantes :
■ Chaque composant ne peut contenir que des caractères alphanumériques en plus des quatre

caractères spéciaux suivants :

■

Soulignement (_)
■ Trait d'union (-)
■ Deux points (:)
■ Point (.)

Chapitre 1 • Système de fichiers Oracle Solaris ZFS (introduction)

29

Exigences d'attribution de noms de composants ZFS

■ Les noms de pools doivent commencer par une lettre, à l'exception des restrictions

suivantes :
■ La séquence de début c[0-9] n'est pas autorisée.
■ Le nom log est réservé.
■ Vous ne pouvez pas utiliser un nom commençant par mirror, raidz , raidz1, raidz2,

raidz3 ou spare car ces noms sont réservés.

■ Les noms de pools ne doivent pas contenir le signe de pourcentage (%).

■ Les noms de jeux de données doivent commencer par un caractère alphanumérique.
■ Les noms de jeux de données ne doivent pas contenir le signe de pourcentage (%).

De plus, les composants vides ne sont pas autorisés.

30

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

2C H A P I T R E

2

Mise en route d'Oracle Solaris ZFS

Ce chapitre fournit des instructions détaillées sur la configuration de base d'Oracle Solaris ZFS.
Il offre une vision globale du fonctionnement des commandes ZFS et explique les méthodes de
création de pools et de systèmes de fichiers de base. Ce chapitre ne constitue pas une
présentation exhaustive. Pour des informations plus détaillées, reportez-vous aux autres
chapitres, comme indiqué.
Ce chapitre contient les sections suivantes :
■ “Profils de droits ZFS” à la page 31
■ “Exigences et recommandations en matière de matériel et de logiciel ZFS ” à la page 32
■ “Création d'un système de fichiers ZFS basique” à la page 32
■ “Création d'un pool de stockage ZFS de base” à la page 33
■ “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 35

Profils de droits ZFS

Si vous souhaitez effectuer des tâches de gestion ZFS sans utiliser le compte superutilisateur
(racine), vous pouvez adopter un rôle disposant de l'une des propriétés suivantes afin d'effectuer
des tâches d'administration ZFS :
■ Gestion de stockage ZFS : permet de créer, détruire, manipuler des périphériques au sein

d'un pool de stockage ZFS.

■ Gestion de système de fichiers ZFS : spécifie les autorisations de création, de destruction et

de modification des systèmes de fichiers ZFS.

Pour plus d'informations sur la création ou l'assignation de rôles, reportez-vous au manuel
Administration d’Oracle Solaris : services de sécurité.
Outre les rôles RBAC permettant de gérer les systèmes de fichiers ZFS, vous pouvez également
vous servir de l'administration déléguée de ZFS pour effectuer des tâches d'administration ZFS
distribuée. Pour plus d'informations, reportez-vous au Chapitre 9, “Administration déléguée de
ZFS dans Oracle Solaris”.

31

Exigences et recommandations en matière de matériel et de logiciel ZFS

Exigences et recommandations en matière de matériel et de
logiciel ZFS

Avant d'utiliser le logiciel ZFS, passez en revue les exigences et recommandations matérielles et
logicielles suivantes :
■ Utilisez un système SPARC ou un système x86 exécutant une version d'Oracle Solaris prise

en charge.

■ L'espace disque minimum requis pour un pool de stockage est de 64 Mo. La taille minimale

du disque est de 128 Mo.

■ La quantité minimale de mémoire nécessaire pour installer un système Solaris est d'1 Go.
Toutefois, pour des performances ZFS optimales, évaluez la taille de mémoire requise en
fonction de votre charge de travail.
Si vous créez une configuration de pool mis en miroir, utilisez plusieurs contrôleurs.

■

Création d'un système de fichiers ZFS basique

L'administration de ZFS a été conçue dans un but de simplicité. L'un des objectifs principaux est
de réduire le nombre de commandes nécessaires à la création d'un système de fichiers utilisable.
Par exemple, lors de la création d'un pool, un système de fichiers ZFS est automatiquement créé
et monté.

L'exemple suivant illustre la création d'un pool de stockage à miroir simple appelé tank et d'un
système de fichiers ZFS appelé tank, en une seule commande. Supposons que l'intégralité des
disques /dev/dsk/c1t0d0 et /dev/dsk/c2t0d0 puissent être utilisés.

# zpool create tank mirror c1t0d0 c2t0d0

Pour plus d'informations sur les configurations redondantes de pools ZFS, reportez-vous à la
section “Fonctions de réplication d'un pool de stockage ZFS” à la page 47.

Le nouveau système de fichiers ZFS, tank, peut utiliser autant d'espace disque disponible que
nécessaire et est monté automatiquement sur /tank.

# mkfile 100m /tank/foo

# df -h /tank

Filesystem

size

used avail capacity Mounted on

tank

80G

100M

80G

1%

/tank

Au sein d'un pool, vous souhaiterez probablement créer des systèmes de fichiers
supplémentaires. Les systèmes de fichiers fournissent des points d'administration qui
permettent de gérer différents jeux de données au sein du même pool.

L'exemple illustre la création d'un système de fichiers nommé fs dans le pool de stockage tank.

32

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création d'un pool de stockage ZFS de base

# zfs create tank/fs

Le nouveau système de fichiers ZFS, tank/fs, peut utiliser autant d'espace disque disponible
que nécessaire et est monté automatiquement sur /tank/fs.

# mkfile 100m /tank/fs/foo

# df -h /tank/fs

Filesystem

size

used avail capacity Mounted on

tank/fs

80G

100M

80G

1%

/tank/fs

Généralement, vous souhaitez créer et organiser une hiérarchie de systèmes de fichiers
correspondant à des besoins spécifiques en matière d'organisation. Pour de plus amples
informations sur la création d'une hiérarchie de systèmes de fichiers ZFS, reportez-vous à la
section “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 35.

Création d'un pool de stockage ZFS de base

L'exemple suivant illustre la simplicité de ZFS. Vous trouverez dans la suite de cette section un
exemple plus complet, similaire à ce qui pourrait exister dans votre environnement. Les
premières tâches consistent à identifier les besoins en matière de stockage et à créer un pool de
stockage. Le pool décrit les caractéristiques physiques du stockage et doit être créé
préalablement à tout système de fichiers.

▼ Identification des exigences de stockage du pool de

stockage ZFS

1

Déterminez les périphériques disponibles pour le pool de stockage.
Avant de créer un pool de stockage, vous devez définir les périphériques à utiliser pour stocker
les données. Ces périphériques doivent être des disques de 128 Mo minimum et ne doivent pas
être en cours d'utilisation par d'autres parties du système d'exploitation. Il peut s'agir de
tranches individuelles d'un disque préformaté ou de disques entiers formatés par ZFS sous
forme d'une seule grande tranche.
Pour l'exemple de stockage utilisé dans la section “Création d'un pool de stockage ZFS”
à la page 34, partez du principe que les disques entiers /dev/dsk/c1t0d0 et /dev/dsk/c2t0d0
sont disponibles.
Pour de plus amples informations sur les disques, leur utilisation et leur étiquetage,
reportez-vous à la section “Utilisation de disques dans un pool de stockage ZFS” à la page 43.

Chapitre 2 • Mise en route d'Oracle Solaris ZFS

33

Création d'un pool de stockage ZFS de base

2

Sélectionnez la réplication de données.
Le système de fichiers ZFS assure la prise en charge de plusieurs types de réplication de données.
Cela permet de déterminer les types de panne matérielle supportés par le pool. ZFS assure la
prise en charge des configurations non redondantes (entrelacées), ainsi que la mise en miroir et
RAID-Z (une variante de RAID-5).
Pour l'exemple de stockage utilisé dans la section “Création d'un pool de stockage ZFS”
à la page 34 utilise la mise en miroir de base de deux disques disponibles.
Pour de plus amples informations sur les fonctions de réplication ZFS, reportez-vous à la
section “Fonctions de réplication d'un pool de stockage ZFS” à la page 47.

▼ Création d'un pool de stockage ZFS

1

2

3

4

Connectez-vous en tant qu'utilisateur root ou endossez un rôle équivalent avec un profil de
droits ZFS adéquat.
Pour de plus amples informations sur les profils de droits ZFS, reportez-vous à la section
“Profils de droits ZFS” à la page 31.

Assignez un nom au pool de stockage.
Le nom sert à identifier le pool de stockage lorsque vous exécutez les commandes zpool et zfs.
Entrez le nom de votre choix, mais celui-ci doit respecter les conventions d'attribution de nom
définies dans la section “Exigences d'attribution de noms de composants ZFS” à la page 29.

Créez le pool.
Par exemple, la commande suivante crée un pool mis en miroir nommé tank :

# zpool create tank mirror c1t0d0 c2t0d0

Si des périphériques contiennent un autre système de fichiers ou sont en cours d'utilisation, la
commande ne peut pas créer le pool.
Pour plus d'informations sur la création de pools de stockage, reportez-vous à la section
“Création de pools de stockage ZFS” à la page 51. Pour plus d'informations sur la détection de
l'utilisation de périphériques, reportez-vous à la section “Détection des périphériques utilisés”
à la page 59.

Affichez les résultats.
Vous pouvez déterminer si votre pool a été correctement créé à l'aide de la commande zpool
list.

# zpool list

NAME

tank

SIZE

ALLOC

FREE

CAP HEALTH

ALTROOT

80G

137K

80G

0% ONLINE

-

Pour de plus amples informations sur la vérification de l'état de pool, reportez-vous à la section
“Requête d'état de pool de stockage ZFS” à la page 87.

34

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création d'une hiérarchie de systèmes de fichiers ZFS

Création d'une hiérarchie de systèmes de fichiers ZFS

Une fois le pool de stockage, vous pouvez créer la hiérarchie du système de fichiers. Les
hiérarchies sont des mécanismes d'organisation des informations à la fois simples et puissants.
Elles sont connues de toute personne ayant utilisé un système de fichiers.

ZFS permet d'organiser en hiérarchies les systèmes de fichiers. Chaque système de cette
hiérarchie ne compte qu'un seul parent. La racine de la hiérarchie correspond toujours au nom
du pool. ZFS exploite cette hiérarchie en assurant la prise en charge de l'héritage de propriétés.
Ainsi, vous pouvez définir les propriétés communes rapidement et facilement dans des
arborescences représentant l'intégralité des systèmes de fichiers.

▼ Détermination de la hiérarchie du système de fichiers

ZFS

1

2

3

Choisissez la granularité du système de fichiers.
Les systèmes de fichiers ZFS sont le point central d'administration. Ils sont légers et se créent
facilement. Pour ce faire, nous vous recommandons d'établir un système de fichiers par
utilisateur ou par projet car cela permet de contrôler les propriétés, les instantanés et les
sauvegardes par utilisateur ou par projet.
Deux systèmes de fichiers ZFS, jeff et bill, sont créés à la section “Création de systèmes de
fichiers ZFS” à la page 36.
Pour plus d'informations sur la gestion des systèmes de fichiers, reportez-vous au Chapitre 6,
“Gestion des systèmes de fichiers Oracle Solaris ZFS”.

Regroupez les systèmes de fichiers similaires.
ZFS permet d'organiser les systèmes de fichiers en hiérarchie, pour regrouper les systèmes de
fichiers similaires. Ce modèle fournit un point d'administration central pour le contrôle des
propriétés et l'administration de systèmes de fichiers. Il est recommandé de créer les systèmes
de fichiers similaires sous un nom commun.
Dans l'exemple de la section “Création de systèmes de fichiers ZFS” à la page 36, les deux
systèmes de fichiers sont placés sous un système de fichiers appelé home.

Choisissez les propriétés du système de fichiers.
La plupart des caractéristiques de systèmes de fichiers se contrôlent à l'aide de propriétés. Ces
propriétés assurent le contrôle de divers comportements, y compris l'emplacement de montage
des systèmes de fichiers, leur méthode de partage, l'utilisation de la compression et l'activation
des quotas.

Chapitre 2 • Mise en route d'Oracle Solaris ZFS

35

Création d'une hiérarchie de systèmes de fichiers ZFS

Dans l'exemple de la section “Création de systèmes de fichiers ZFS” à la page 36, tous les
répertoires de base sont montés dans /export/zfs/ user. Ils sont partagés à l'aide de NFS et la
compression est activée. De plus, un quota de 10 Go est appliqué pour l'utilisateur jeff.
Pour de plus amples informations sur les propriétés, reportez-vous à la section “Présentation
des propriétés ZFS” à la page 137.

▼ Création de systèmes de fichiers ZFS

1

2

3

Connectez-vous en tant qu'utilisateur root ou endossez un rôle équivalent avec un profil de
droits ZFS adéquat.
Pour de plus amples informations sur les profils de droits ZFS, reportez-vous à la section
“Profils de droits ZFS” à la page 31.

Créez la hiérarchie souhaitée.
Dans cet exemple, un système de fichiers agissant en tant que conteneur de systèmes de fichiers
individuels est créé.

# zfs create tank/home

Définissez les propriétés héritées.
Une fois la hiérarchie du système de fichiers établie, définissez toute propriété destinée à être
partagée par l'ensemble des utilisateurs :

# zfs set mountpoint=/export/zfs tank/home

# zfs set share=name=home,path=/export/zfs,prot=nfs tank/home

name=home,path=/export/zfs,prot=nfs

# zfs set sharenfs=on tank/home

# zfs set compression=on tank/home

# zfs get compression tank/home

NAME

PROPERTY

VALUE

tank/home

compression

on

SOURCE

local

Il est possible de définir les propriétés du système de fichiers lors de la création de ce dernier. Par
exemple :

# zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home

Pour plus d'informations sur les propriétés et l'héritage des propriétés, reportez-vous à la
section “Présentation des propriétés ZFS” à la page 137.
Ensuite, les systèmes de fichiers sont regroupés sous le système de fichiers home dans le pool
tank.

36

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création d'une hiérarchie de systèmes de fichiers ZFS

4

Créez les systèmes de fichiers individuels.
Il est possible que les systèmes de fichiers aient été créés et que leurs propriétés aient ensuite été
modifiées au niveau home. Vous pouvez modifier les propriétés de manière dynamique lorsque
les systèmes de fichiers sont en cours d'utilisation.

# zfs create tank/home/jeff

# zfs create tank/home/bill

Les valeurs de propriétés de ces systèmes de fichiers sont héritées de leur parent. Elles sont donc
montées sur /export/zfs/ utilisateur et partagées via NFS. Il est inutile de modifier le fichier
/etc/vfstab ou /etc/dfs/dfstab.
Pour de plus amples informations sur les systèmes de fichiers, reportez-vous à la section
“Création d'un système de fichiers ZFS” à la page 134.
Pour plus d'informations sur le montage et le partage de systèmes de fichiers, reportez-vous à la
section “Montage de système de fichiers ZFS” à la page 168.

5

6

Définissez les propriétés spécifiques au système.
Dans cet exemple, l'utilisateur jeff se voit assigner un quota de 10 Go. Cette propriété place
une limite sur la quantité d'espace qu'il peut utiliser, indépendamment de l'espace disponible
dans le pool.

# zfs set quota=10G tank/home/jeff

Affichez les résultats.
La commande zfs list permet de visualiser les informations disponibles sur le système de
fichiers :

# zfs list

NAME

tank

USED AVAIL REFER MOUNTPOINT

92.0K 67.0G

9.5K /tank

tank/home

24.0K 67.0G

8K /export/zfs

tank/home/bill

8K 67.0G

8K /export/zfs/bill

tank/home/jeff

8K 10.0G

8K /export/zfs/jeff

Notez que l'utilisateur jeff dispose d'uniquement 10 Go d'espace disponible, tandis que
l'utilisateur bill peut utiliser la totalité du pool (67 Go).
Pour de plus amples informations sur la visualisation de l'état du système de fichiers,
reportez-vous à la section “Envoi de requêtes sur les informations des systèmes de fichiers ZFS”
à la page 160.
Pour de plus amples informations sur l'utilisation et le calcul de l'espace disque, reportez-vous à
la section “Comptabilisation de l'espace disque ZFS” à la page 40.

Chapitre 2 • Mise en route d'Oracle Solaris ZFS

37

38

3C H A P I T R E

3

Différences entre les systèmes de fichiers
Oracle Solaris ZFS et classiques

Ce chapitre aborde les différences significatives entre Oracle Solaris ZFS et les systèmes de
fichiers classiques. Lors de l'utilisation d'outils classiques avec le système de fichiers ZFS, la
compréhension de ces différences clés permet d'éviter les confusions.
Ce chapitre contient les sections suivantes :
■ “Granularité du système de fichiers ZFS” à la page 39
■ “Comptabilisation de l'espace disque ZFS” à la page 40
■ “Comportement d'espace saturé” à la page 41
■ “Montage de système de fichiers ZFS” à la page 42
■ “Gestion de volumes classique” à la page 42
■ “Nouveau modèle ACL Solaris” à la page 42

Granularité du système de fichiers ZFS

Traditionnellement, les systèmes de fichiers étaient restreints à un périphérique et par
conséquent à la taille de ce périphérique. Les créations successives de systèmes de fichiers
classiques dues aux contraintes de taille demandent du temps et s'avèrent parfois difficile. Les
produits de gestion de volume traditionnels aident à gérer ce processus.
Les systèmes de fichiers ZFS n'étant pas limités à des périphériques spécifiques, leur création est
facile et rapide, tout comme celle des répertoires. La taille des systèmes de fichiers ZFS
augmente automatiquement dans l'espace disque alloué au pool de stockage sur lequel ils se
trouvent.
Au lieu de créer un système de fichier, comme /export/home, pour la gestion de plusieurs
sous-répertoires d'utilisateurs, vous pouvez créer un système de fichiers par utilisateur. Vous
pouvez facilement définir et gérer plusieurs systèmes de fichiers en appliquant des propriétés
pouvant être héritées par le système de fichiers descendant au sein de la hiérarchie.
Pour obtenir un exemple de création d'une hiérarchie de système de fichiers, reportez-vous à la
section “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 35.

39

Comptabilisation de l'espace disque ZFS

Comptabilisation de l'espace disque ZFS

Le système de fichiers ZFS repose sur le concept de stockage de pools. Contrairement aux
systèmes de fichiers classiques, qui sont mappés vers un stockage physique, tous les systèmes de
fichiers ZFS d'un pool partagent le stockage disponible dans le pool. Ainsi, l'espace disponible
indiqué par des utilitaires tels que df peut changer alors même que le système de fichiers est
inactif, parce que d'autres systèmes de fichiers du pool utilisent ou libèrent de l'espace.

Notez que la taille maximale du système de fichiers peut être limitée par l'utilisation des quotas.
Pour obtenir des informations sur les quotas, reportez-vous à la section “Définitions de quotas
sur les systèmes de fichiers ZFS” à la page 183. Vous pouvez allouer une certaine quantité
d'espace disque à un système de fichiers à l'aide des réservations. Pour obtenir des informations
sur les réservations, reportez-vous à la rubrique “Définition de réservations sur les systèmes de
fichiers ZFS” à la page 187. Ce modèle est très similaire au modèle NFS dans lequel plusieurs
répertoires sont montés à partir du même système de fichiers (par exemple : /home).

Toutes les métadonnées dans ZFS sont allouées dynamiquement. La plupart des autres
systèmes de fichiers pré-allouent une grande partie de leurs métadonnées. Par conséquent, lors
de la création du système de fichiers, ces métadonnées ont besoin d'une partie de l'espace
disque. En outre, en raison de ce comportement, le nombre total de fichiers pris en charge par le
système de fichiers est prédéterminé. Dans la mesure où ZFS alloue les métadonnées lorsqu'il en
a besoin, aucun coût d'espace initial n'est requis et le nombre de fichiers n'est limité que par
l'espace disponible. Dans le cas de ZFS, la sortie de la commande df -g ne s'interprète pas de la
même manière que pour les autres systèmes de fichiers. Le nombre de fichiers (total files)
indiqué n'est qu'une estimation basée sur la quantité de stockage disponible dans le pool.

ZFS est un système de fichiers transactionnel. La plupart des modifications apportées au
système de fichier sont rassemblées en groupes de transaction et validées sur le disque de façon
asynchrone. Tant que ces modifications ne sont pas validées sur le disque, elles sont considérées
comme des modifications en attente. La quantité d'espace disque utilisé disponible et référencé
par un fichier ou un système de fichier ne tient pas compte des modifications en attente. Ces
modifications sont généralement prises en compte au bout de quelques secondes. Même si vous
validez une modification apportée au disque avec la commande fsync(3c) ou O_SYNC, les
informations relatives à l'utilisation d'espace disque ne sont pas automatiquement mises à jour.

Sur un système de fichiers UFS, la commande du indique la taille des blocs de données au sein
du fichier. Sur un système de fichiers ZFS, la commande du indique la taille réelle du fichier, telle
qu'elle est stockée sur le disque. La taille prend en compte les métadonnées et la compression.
Ces informations vous aident à déterminer l'espace supplémentaire dont vous disposerez si
vous supprimez un fichier donné. Par conséquent, même lorsque la compression est désactivée,
vous obtenez des résultats différents entre ZFS et UFS.

Lorsque vous comparez la consommation d'espace renvoyée par la commande df avec celle
renvoyée par la commande zfs list, n'oubliez pas que df indique la taille du pool et pas
seulement la taille des systèmes de fichiers. En outre, df ne reconnaît pas les systèmes de fichiers
descendants ni ne détecte la présence d'instantanés. Si des propriétés ZFS telles que la

40

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

compression et les quotas sont définies sur les systèmes de fichiers, le rapprochement de la
consommation d'espace renvoyée par df peut s'avérer difficile.

Comptabilisation de l'espace disque ZFS

Considérez les scénarios suivants qui peuvent également avoir un impact sur la consommation
d'espace signalée :
■ Pour les fichiers de volume supérieur à recordsize, le dernier bloc du fichier est

généralement à moitié plein. Lorsque recordsize est défini par défaut sur 128 Ko, environ
64 Ko sont perdus par fichier, ce qui peut avoir un impact considérable. L'intégration de
RFE 6812608 permet de remédier à ce problème. Une solution de contournement consiste à
activer la compression. Même si vos données sont déjà compressées, la partie non utilisée du
dernier bloc sera remplie de zéros et sera compressée sans difficulté.
Sur un pool RAIDZ-2, chaque bloc consomme au moins 2 secteurs (par blocs de 512 octets)
d'informations de parité. L'espace utilisé par les informations de parité n'est pas signalé ;
toutefois, il peut varier et représenter un pourcentage beaucoup plus élevé pour les blocs de
petite taille, si bien qu'il peut avoir une incidence sur les valeurs d'espace renvoyées.
L'impact est plus important lorsque recordsize est défini sur 512 octets, où chaque bloc
logique de 512 octets consomme 1,5 Ko (3 fois l'espace). Quelles que soient les données
stockées, si une utilisation efficace de l'espace est primordiale, il est recommandé de
conserver la valeur par défaut de recordsize (128 KB) et d'activer la compression (sur la
valeur par défaut lzjb).

■

■ La commande df n'a pas connaissance des données de fichiers dédupliquées.

Comportement d'espace saturé
La création d'instantanés de systèmes de fichiers est peu coûteuse et facile dans ZFS. Les
instantanés sont communs à la plupart des environnements ZFS. Pour plus d'informations sur
les instantanés ZFS, reportez-vous au Chapitre 7, “Utilisation des instantanés et des clones ZFS
Oracle Solaris”.

La présence d'instantanés peut entraîner des comportements inattendus lors des tentatives de
libération d'espace disque. En règle générale, si vous disposez des autorisations adéquates, vous
pouvez supprimer un fichier d'un système de fichiers plein, ce qui entraîne une augmentation
de la quantité d'espace disque disponible dans le système de fichiers. Cependant, si le fichier à
supprimer existe dans un instantané du système de fichiers, sa suppression ne libère pas
d'espace disque. Les blocs utilisés par le fichier continuent à être référencés à partir de
l'instantané.

Par conséquent, la suppression du fichier peut occuper davantage d'espace disque car une
nouvelle version du répertoire doit être créée afin de refléter le nouvel état de l'espace de noms.
En raison de ce comportement, une erreur ENOSPC ou EDQUOT inattendue peut se produire
lorsque vous tentez de supprimer un fichier.

Chapitre 3 • Différences entre les systèmes de fichiers Oracle Solaris ZFS et classiques

41

Montage de système de fichiers ZFS

Montage de système de fichiers ZFS

Le système de fichiers ZFS réduit la complexité et facilite l'administration. Par exemple, avec des
systèmes de fichiers standard, vous devez modifier le fichier /etc/vfstab à chaque fois que
vous ajoutez un système de fichiers. Avec ZFS, cela n'est plus nécessaire, grâce au montage et
démontage automatique en fonction des propriétés du système de fichiers. Vous n'avez pas
besoin de gérer les entrées ZFS dans le fichier /etc/vfstab.

Pour plus d'informations sur le montage et le partage des systèmes de fichiers ZFS,
reportez-vous à la section “Montage de système de fichiers ZFS” à la page 168.

Gestion de volumes classique

Comme décrit à la section “Stockage ZFS mis en pool” à la page 25, ZFS élimine la nécessité d'un
gestionnaire de volume séparé. ZFS opérant sur des périphériques bruts, il est possible de créer
un pool de stockage composé de volumes logiques logiciels ou matériels. Cette configuration est
déconseillée, car ZFS fonctionne mieux avec des périphériques bruts physiques. L'utilisation de
volumes logiques peut avoir un impact négatif sur les performances, la fiabilité, voire les deux,
et doit de ce fait être évitée.

Nouveau modèle ACL Solaris

Les versions précédentes du système d'exploitation Solaris assuraient la prise en charge d'une
implémentation ACL reposant principalement sur la spécification d'ACL POSIX-draft. Les
ACL POSIX-draft sont utilisées pour protéger des fichiers UFS. Un nouveau modèle ACL basé
sur la spécification NFSv4 est utilisé pour protéger les fichiers ZFS.

Les principales différences présentées par le nouveau modèle ACL Solaris sont les suivantes :
■ Le modèle est basé sur la spécification NFSv4 et similaire aux ACL de type Windows NT.
■ Ce modèle fournit un jeu d'autorisations d'accès plus détaillé.
■ Les ACL sont définies et affichées avec les commandes chmod et ls plutôt qu'avec les

commandes setfacl et getfacl .

■ Une sémantique d'héritage plus riche désigne la manière dont les privilèges d'accès sont

appliqués d'un répertoire à un sous-répertoire, et ainsi de suite.

Pour plus d'informations sur l'utilisation des ACL avec des fichiers ZFS, reportez-vous au
Chapitre 8, “Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS”.

42

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

4C H A P I T R E

4

Gestion des pools de stockage
Oracle Solaris ZFS

Ce chapitre explique comment créer et administrer des pools de stockage dans
Oracle Solaris ZFS.

Ce chapitre contient les sections suivantes :
■ “Composants d'un pool de stockage ZFS” à la page 43
■ “Fonctions de réplication d'un pool de stockage ZFS” à la page 47
■ “Création et destruction de pools de stockage ZFS” à la page 50
■ “Gestion de périphériques dans un pool de stockage ZFS” à la page 63
■ “Gestion des propriétés de pool de stockage ZFS” à la page 84
■ “Requête d'état de pool de stockage ZFS” à la page 87
■ “Migration de pools de stockage ZFS” à la page 99
■ “Mise à niveau de pools de stockage ZFS” à la page 109

Composants d'un pool de stockage ZFS

Les sections ci-dessous contiennent des informations détaillées sur les composants de pools de
stockage suivants :
■ “Utilisation de disques dans un pool de stockage ZFS” à la page 43
■ “Utilisation de tranches dans un pool de stockage ZFS” à la page 45
■ “Utilisation de fichiers dans un pool de stockage ZFS” à la page 46

Utilisation de disques dans un pool de stockage ZFS
Le composant le plus basique d'un pool de stockage est le stockage physique. Le stockage
physique peut être constitué de tout périphérique en mode bloc d'une taille supérieure à
128 Mo. En règle générale, ce périphérique est un disque dur visible pour le système dans le
répertoire /dev/dsk.

43

Composants d'un pool de stockage ZFS

Un disque entier (c1t0d0) ou une tranche individuelle (c0t0d0s7) peuvent constituer un
périphérique de stockage. La manière d'opérer recommandée consiste à utiliser un disque
entier. Dans ce cas, il est inutile de formater spécifiquement le disque. ZFS formate le disque à
l'aide d'une étiquette EFI de façon à ce qu'il contienne une grande tranche unique. Utilisé de
cette façon, le tableau de partition affiché par la commande format s'affiche comme suit :

Current partition table (original):

Total disk sectors available: 286722878 + 16384 (reserved sectors)

Part

Tag

Flag

First Sector

Size

Last Sector

0

usr

1 unassigned

2 unassigned

3 unassigned

4 unassigned

5 unassigned

6 unassigned

8

reserved

wm

wm

wm

wm

wm

wm

wm

wm

34

136.72GB

286722911

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

286722912

8.00MB

286739295

Tenez compte des points suivants lorsque vous utilisez des disques entiers dans vos pools de
stockage ZFS :
■ Lorsqu'un disque entier est utilisé, le disque est généralement nommé à l'aide de la

convention de nommage /dev/dsk/cNtNdN. Certains pilotes tiers suivent une convention
de nom différente ou placent les disques à un endroit autre que le répertoire /dev/dsk.
Pour utiliser ces disques, vous devez les étiqueter manuellement et fournir une tranche à
ZFS.
Sur les systèmes x86, le disque doit avoir un Solaris valide fdisk partition. Pour plus
d'informations sur la création ou la modification d'une partition fdisk Solaris,
reportez-vous au Chapitre 13, “Système x86 : Configuration des disques (tâches)” du
manuel Administration d’Oracle Solaris : Périphériques et systèmes de fichiers.

■

■ ZFS applique une étiquette EFI lorsque vous créez un pool de stockage avec des disques

entiers. Pour plus d'informations sur les étiquettes EFI, reportez-vous à la section “Etiquette
de disque EFI” du manuel Administration d’Oracle Solaris : Périphériques et systèmes de
fichiers.

■ Un disque destiné à un pool racine ZFS doit être créé avec une étiquette SMI (VTOC) et non

une étiquette EFI. Vous pouvez réattribuer une étiquette SMI à un disque à l'aide de la
commande format - e. Sinon, vous pouvez utiliser les raccourcis de commande suivants
pour modifier l'étiquette d'un disque. Sachez toutefois que les raccourcis de commande
n'incluent pas de vérification des erreurs.
Les commandes suivantes peuvent être utilisées sur un système x86 pour remplacer une
étiquette par une étiquette SMI. La deuxième commande crée une partition fdisk Solaris
qui utilise l'ensemble du disque.

x86# format -L vtoc -d c0t1d0

x86# fdisk -B /dev/rdsk/c0t1d0p0

44

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Composants d'un pool de stockage ZFS

La commande suivante remplace l'étiquette du disque par une étiquette SMI et la table de
partition par défaut. La s0 tranche dans la table de partition par défaut n'est peut-être pas
suffisamment grande pour le pool racine.

sparc# format -L vtoc -d c0t1d0

Pour plus d'informations sur la conversion d'une étiquette EFI en étiquette SMI (VTOC) ou
la modification de la table de partition par défaut, reportez-vous au Chapitre 12, “Système
SPARC : Configuration des disques (tâches)” du manuel Administration d’Oracle Solaris :
Périphériques et systèmes de fichiers.

Vous pouvez spécifier les disques soit en utilisant le chemin complet (/dev/dsk/c1t0d0, par
exemple) ou un nom abrégé composé du nom du périphérique dans le répertoire /dev/dsk
(c1t0d0, par exemple). Les exemples suivants constituent des noms de disques valides :

■

■

■

c1t0d0

/dev/dsk/c1t0d0

/dev/foo/disk

Utilisation de tranches dans un pool de stockage ZFS
Les disques peuvent être étiquetés avec une étiquette VTOC Solaris (SMI) classique lorsque
vous créez un pool de stockage avec une tranche de disque.

Pour un pool racine ZFS initialisable, les disques du pool doivent contenir des tranches et
doivent être étiquetés avec une étiquette SMI. La plus simple configuration consiste à placer
toute la capacité du disque dans la tranche 0 et à utiliser cette tranche pour le pool racine.

Sur un système SPARC, un disque de 72 Go dispose de 68 Go d'espace utilisable situé dans la
tranche 0, comme illustré dans la sortie format suivante :

# format

.

.

.

Specify disk (enter its number): 4

selecting c1t1d0

partition> p

Current partition table (original):

Total disk cylinders available: 14087 + 2 (reserved cylinders)

Part

Tag

Flag

Cylinders

Size

Blocks

0

root

1 unassigned

2

backup

3 unassigned

4 unassigned

5 unassigned

6 unassigned

7 unassigned

wm

wm

wm

wm

wm

wm

wm

wm

0 - 14086

68.35GB

(14087/0/0) 143349312

0

0

(0/0/0)

0

0 - 14086

68.35GB

(14087/0/0) 143349312

0

0

0

0

0

0

0

0

0

0

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

45

Composants d'un pool de stockage ZFS

Sur un système x86, un disque de 72 Go dispose de 68 Go d'espace disque utilisable situé dans la
tranche 0, comme illustré dans la sortie format suivante : Une petite quantité d'informations
d'initialisation est contenue dans la tranche 8. La tranche 8 ne nécessite aucune administration
et ne peut pas être modifiée.

# format

.

.

.

selecting c1t0d0

partition> p

Current partition table (original):

Total disk cylinders available: 49779 + 2 (reserved cylinders)

Part

Tag

Flag

Cylinders

Size

Blocks

0

root

1 unassigned

2

backup

3 unassigned

4 unassigned

5 unassigned

6 unassigned

7 unassigned

8

boot

9 unassigned

wm

wu

wm

wu

wu

wu

wu

wu

wu

wu

1 - 49778

68.36GB

(49778/0/0) 143360640

0

0

(0/0/0)

0

0 - 49778

68.36GB

(49779/0/0) 143363520

0

0

0

0

0

0

0

0

0

0

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

0 -

0

1.41MB

(1/0/0)

0

0

(0/0/0)

0

0

0

0

0

2880

0

Une partition fdisk existe également sur les systèmes x86 Solaris. Une partition fdisk est
représentée par un nom de périphérique /dev/dsk/cN[tN]dNpN et fait office de conteneur pour
les tranches disponibles du disque. N'utilisez pas de périphérique cN[tN]dNpN pour un
composant de pool de stockage ZFS car cette configuration n'est ni testée ni prise en charge.

Utilisation de fichiers dans un pool de stockage ZFS
ZFS permet également d'utiliser des fichiers en tant que périphériques virtuels dans le pool de
stockage. Cette fonction est destinée principalement aux tests et à des essais simples, et non pas
à être utilisée dans un contexte de production.

■

■

Si vous créez un pool ZFS sauvegardé par des fichiers dans un système de fichiers UFS, vous
vous basez implicitement sur UFS pour la garantie de l'exactitude et de la synchronisation de
la sémantique.
Si vous créez un pool ZFS à partir de fichiers ou de volumes créés sur un autre pool ZFS, le
système peut générer un interblocage ou paniquer.

Cependant, les fichiers peuvent s'avérer utiles lorsque vous employez ZFS pour la première fois
ou en cas de configuration complexe, lorsque les périphériques physiques présents ne sont pas
suffisants. Tous les fichiers doivent être spécifiés avec leur chemin complet et leur taille doit être
de 64 Mo minimum.

46

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Fonctions de réplication d'un pool de stockage ZFS

Remarques relatives aux pools de stockage ZFS
Tenez compte des points suivants lors de la création et de la gestion de pools de stockage ZFS.
■ L'utilisation de disques physiques constitue la méthode de création de pools de stockage ZFS

la plus simple. Les configurations ZFS deviennent de plus en plus complexes, en termes de
gestion, de fiabilité et de performance. Lorsque vous construisez des pools à partir de
tranches de disques, de LUN dans des baies RAID matérielles ou de volumes présentés par
des gestionnaires de volume basés sur des logiciels. Les considérations suivantes peuvent
vous aider à configurer ZFS avec d'autres solutions de stockage matérielles ou logicielles :

■

Si vous élaborez une configuration ZFS sur des LUN à partir de baies RAID matérielles,
vous devez comprendre la relation entre les fonctionnalités de redondance ZFS et les
fonctionnalités de redondance proposées par la baie. Certaines configurations peuvent
fournir une redondance et des performances adéquates, mais d'autres non.

■ Vous pouvez construire des périphériques logiques pour ZFS à l'aide des volumes

présentés par des gestionnaires de volumes logiciels. Ces configurations sont cependant
déconseillées. Même si le système de fichiers ZFS fonctionne correctement sur ces
périphériques, il se peut que les performances ne soient pas optimales.
Pour plus d'informations sur les recommandations relatives aux pools de stockage,
reportez-vous au Chapitre 13, “Pratiques recommandées pour Oracle Solaris ZFS”.

■ Les disques sont identifiés par leur chemin et par l'ID de leur périphérique, s'il est

disponible. Pour les systèmes sur lesquels les informations de l'ID du périphérique sont
disponibles, cette méthode d'identification permet de reconfigurer les périphériques sans
mettre à jour ZFS. Etant donné que la génération et la gestion d'ID de périphérique peuvent
varier d'un système à l'autre, vous devez commencer par exporter le pool avant tout
déplacement de périphériques, par exemple, le déplacement d'un disque d'un contrôleur à
un autre. Un événement système, tel que la mise à jour du microprogramme ou toute autre
modification apportée au matériel, peut modifier les ID de périphérique du pool de stockage
ZFS, ce qui peut entraîner l'indisponibilité des périphériques.

Fonctions de réplication d'un pool de stockage ZFS

Le système de fichiers ZFS offre une redondance des données, ainsi que des propriétés
d'auto-rétablissement dans des configurations RAID-Z ou mises en miroir.
■ “Configuration de pool de stockage mis en miroir” à la page 48
■ “Configuration de pool de stockage RAID-Z” à la page 48
■ “Données d'autorétablissement dans une configuration redondante” à la page 50
■ “Entrelacement dynamique dans un pool de stockage” à la page 50
■ “Pool de stockage ZFS hybride” à la page 49

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

47

Fonctions de réplication d'un pool de stockage ZFS

Configuration de pool de stockage mis en miroir
Une configuration de pool de stockage en miroir requiert deux disques minimum, situés de
préférence dans des contrôleurs séparés. Vous pouvez utiliser un grand nombre de disques dans
une configuration en miroir. En outre, vous pouvez créer plusieurs miroirs dans chaque pool.
Conceptuellement, une configuration en miroir de base devrait ressembler à ce qui suit :

mirror c1t0d0 c2t0d0

Conceptuellement, une configuration en miroir plus complexe devrait ressembler à ce qui suit :

mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0

Pour obtenir des informations sur les pools de stockage mis en miroir, reportez-vous à la
section “Création d'un pool de stockage mis en miroir” à la page 51.

Configuration de pool de stockage RAID-Z
En plus d'une configuration en miroir de pool de stockage, ZFS fournit une configuration
RAID-Z disposant d'une tolérance de pannes à parité simple, double ou triple. Une
configuration RAID-Z à parité simple (raidz ou raidz1) équivaut à une configuration RAID-5.
Une configuration RAID-Z à double parité (raidz2) est similaire à une configuration RAID-6.

Pour plus d'informations sur la fonction RAIDZ-3 (raidz3), consultez le blog suivant :

http://blogs.oracle.com/ahl/entry/triple_parity_raid_z

Tous les algorithmes similaires à RAID-5 (RAID-4, RAID-6, RDP et EVEN-ODD, par exemple)
peuvent souffrir d'un problème connu sous le nom de RAID-5 write hole, ou trou d'écriture de
RAID-5. Si seule une partie d'un entrelacement RAID-5 est écrite, et qu'une perte
d'alimentation se produit avant que tous les blocs aient été écrits sur le disque, la parité n'est pas
synchronisée avec les données, et est par conséquent inutile à tout jamais (à moins qu'elle ne
soit écrasée ultérieurement par une écriture d'entrelacement total). Dans RAID-Z, ZFS utilise
des entrelacements RAID de largeur variable pour que toutes les écritures correspondent à des
entrelacements entiers. Cette conception n'est possible que parce que ZFS intègre le système de
fichiers et la gestion de périphérique de telle façon que les métadonnées du système de fichiers
disposent de suffisamment d'informations sur le modèle de redondance de données pour gérer
les entrelacements RAID de largeur variable. RAID-Z est la première solution au monde pour le
trou d'écriture de RAID-5.

Une configuration RAID-Z avec N disques de taille X et des disques de parité P présente une
contenance d'environ (N-P)*X octets et peut supporter la panne d'un ou de plusieurs
périphériques P avant que l'intégrité des données ne soit compromise. Vous devez disposer d'au
moins deux disques pour une configuration RAID-Z à parité simple et d'au moins trois disques
pour une configuration RAID-Z à double parité, et ainsi de suite. Par exemple, si vous disposez

48

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Fonctions de réplication d'un pool de stockage ZFS

de trois disques pour une configuration RAID-Z à parité simple, les données de parité occupent
un espace disque égal à l'un des trois disques. Dans le cas contraire, aucun matériel spécifique
n'est requis pour la création d'une configuration RAID-Z.

Conceptuellement, une configuration RAID-Z à trois disques serait similaire à ce qui suit :

raidz c1t0d0 c2t0d0 c3t0d0

Conceptuellement, une configuration RAID-Z plus complexe devrait ressembler à ce qui suit :

raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0

raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0c12t0d0 c13t0d0 c14t0d0

Si vous créez une configuration RAID-Z avec un nombre important de disques, vous pouvez
scinder les disques en plusieurs groupes. Par exemple, il est recommandé d'utiliser une
configuration RAID-Z composée de 14 disques au lieu de la scinder en 2 groupes de 7 disques.
Les configurations RAID-Z disposant de groupements de moins de 10 disques devraient
présenter de meilleures performances.

Pour obtenir des informations sur les pools de stockage RAID-Z, reportez-vous à la section
“Création d'un pool de stockage RAID-Z” à la page 53.

Pour obtenir des informations supplémentaires afin de choisir une configuration en miroir ou
une configuration RAID-Z en fonction de considérations de performances et d'espace disque,
consultez le blog suivant :

http://blogs.oracle.com/roch/entry/when_to_and_not_to

Pour plus d'informations sur les recommandations relatives aux pools de stockage RAID-Z,
reportez-vous au Chapitre 13, “Pratiques recommandées pour Oracle Solaris ZFS”.

Pool de stockage ZFS hybride
Le pool de stockage ZFS hybride est disponible dans la gamme de produits Oracle Sun
Storage 7000. Il s'agit d'un pool de stockage spécial combinant de la RAM dynamique, des
disques électroniques et des disques durs, qui permet d'améliorer les performances et
d'augmenter la capacité, tout en réduisant la consommation électrique. Grâce à l'interface de
gestion de ce produit, vous pouvez sélectionner la configuration de redondance ZFS du pool de
stockage et gérer facilement d'autres options de configuration.

Pour plus d'informations sur ce produit, reportez-vous au Sun Storage Unified Storage System
Administration Guide.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

49

Création et destruction de pools de stockage ZFS

Données d'autorétablissement dans une
configuration redondante
Le système de fichiers ZFS fournit des données d'auto-rétablissement dans une configuration
RAID-Z ou mise en miroir.

Lorsqu'un bloc de données endommagé est détecté, ZFS récupère les données correctes à partir
d'une copie redondante et de plus, répare les données incorrectes en les remplaçant par celles de
la copie.

Entrelacement dynamique dans un pool de stockage
Le système de fichiers ZFS entrelace de façon dynamique les données de tous les périphériques
virtuels de niveau supérieur. Le choix de l'emplacement des données est effectué lors de
l'écriture ; ainsi, aucun entrelacement de largeur fixe n'est créé lors de l'allocation.

Lorsque de nouveaux périphériques virtuels sont ajoutés à un pool, ZFS attribue graduellement
les données au nouveau périphérique afin de maintenir les performances et les stratégies
d'allocation d'espace disque. Chaque périphérique virtuel peut également être constitué d'un
miroir ou d'un périphérique RAID-Z contenant d'autres périphériques de disques ou d'autres
fichiers. Cette configuration vous offre un contrôle flexible des caractéristiques par défaut du
pool. Par exemple, vous pouvez créer les configurations suivantes à partir de quatre disques :
■ Quatre disques utilisant l'entrelacement dynamique
■ Une configuration RAID-Z à quatre directions
■ Deux miroirs bidirectionnels utilisant l'entrelacement dynamique

Même si le système de fichiers ZFS prend en charge différents types de périphériques virtuels au
sein du même pool, cette pratique n'est pas recommandée. Vous pouvez par exemple créer un
pool avec un miroir bidirectionnel et une configuration RAID-Z à trois directions. Cependant,
le niveau de tolérance de pannes est aussi bon que le pire périphérique virtuel (RAID-Z dans ce
cas). Nous vous recommandons d'utiliser des périphériques virtuels de niveau supérieur du
même type avec le même niveau de redondance pour chaque périphérique.

Création et destruction de pools de stockage ZFS

Les sections suivantes illustrent différents scénarios de création et de destruction de pools de
stockage ZFS :
■ “Création de pools de stockage ZFS” à la page 51
■ “Affichage des informations d'un périphérique virtuel de pool de stockage” à la page 58
■ “Gestion d'erreurs de création de pools de stockage ZFS” à la page 59
■ “Destruction de pools de stockage ZFS” à la page 62

50

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

La création et la destruction de pools est rapide et facile. Cependant, ces opérations doivent être
réalisées avec prudence. Des vérifications sont effectuées pour éviter une utilisation de
périphériques déjà utilisés dans un nouveau pool, mais ZFS n'est pas systématiquement en
mesure de savoir si un périphérique est déjà en cours d'utilisation. Il est plus facile de détruire
un pool que d'en créer un. Utilisez la commande zpool destroy avec précaution. L'exécution
de cette commande simple a des conséquences considérables.

Création de pools de stockage ZFS
Pour créer un pool de stockage, exécutez la commande zpool create. Cette commande prend
un nom de pool et un nombre illimité de périphériques virtuels en tant qu'arguments. Le nom
de pool doit se conformer aux conventions d'attribution de noms décrites à la section
“Exigences d'attribution de noms de composants ZFS” à la page 29.

Création d'un pool de stockage de base
La commande suivante crée un pool appelé tank et composé des disques c1t0d0 et c1t1d0:

# zpool create tank c1t0d0 c1t1d0

Ces noms de périphériques représentant les disques entiers se trouvent dans le répertoire
/dev/dsk et ont été étiquetés de façon adéquate par ZFS afin de contenir une tranche unique de
grande taille. Les données sont entrelacées de façon dynamique sur les deux disques.

Création d'un pool de stockage mis en miroir
Pour créer un pool mis en miroir, utilisez le mot-clé mirror suivi du nombre de périphériques
de stockage que doit contenir le miroir. Pour spécifier plusieurs miroirs, répétez le mot-clé
mirror dans la ligne de commande. La commande suivante crée un pool avec deux miroirs
bidirectionnels :

# zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0

Le second mot-clé mirror indique qu'un nouveau périphérique virtuel de niveau supérieur est
spécifié. Les données sont dynamiquement entrelacées sur les deux miroirs, ce qui les rend
redondantes sur chaque disque.

Pour plus d'informations sur les configurations en miroir recommandées, reportez-vous au
Chapitre 13, “Pratiques recommandées pour Oracle Solaris ZFS”.

Actuellement, les opérations suivantes sont prises en charge dans une configuration ZFS en
miroir :
■ Ajout d'un autre jeu de disques comme périphérique virtuel (vdev) supplémentaire de

niveau supérieur à une configuration en miroir existante. Pour plus d'informations,
reportez-vous à la rubrique “Ajout de périphériques à un pool de stockage” à la page 63.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

51

Création et destruction de pools de stockage ZFS

■ Connexion de disques supplémentaires à une configuration en miroir existante ou

connexion de disques supplémentaires à une configuration non répliquée pour créer une
configuration en miroir. Pour plus d'informations, reportez-vous à la section “Connexion et
séparation de périphériques dans un pool de stockage ” à la page 68.

■ Remplacement d'un ou de plusieurs disques dans une configuration en miroir existante, à
condition que les disques de remplacement soient d'une taille supérieure ou égale à celle du
périphérique remplacé. Pour plus d'informations, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 76.

■ Retrait d'un ou de plusieurs disques dans une configuration en miroir, à condition que les
périphériques restants procurent la redondance qui convient à la configuration. Pour plus
d'informations, reportez-vous à la section “Connexion et séparation de périphériques dans
un pool de stockage ” à la page 68.
Scission d'une configuration mise en miroir en déconnectant l'un des disques en vue de
créer un nouveau pool identique. Pour plus d'informations, reportez-vous à la section
“Création d'un pool par scission d'un pool de stockage ZFS mis en miroir” à la page 70.

■

Vous ne pouvez pas forcer la suppression d'un périphérique qui n'est pas un périphérique de
rechange, un périphérique de journalisation ou un périphérique de cache d'un pool de stockage
mis en miroir.

Création d'un pool racine ZFS
Tenez compte des exigences suivantes applicables à la configuration du pool racine :
■ Les disques utilisés pour le pool racine doivent avoir une étiquette VTOC (SMI) et le pool

doit être créé avec des tranches de disque.

■ Le pool racine doit être créé sous la forme d'une configuration en miroir ou d'une

configuration à disque unique. Vous ne pouvez pas ajouter d'autres disques mis en miroir
pour créer plusieurs périphériques virtuels de niveau supérieur à l'aide de la commande
zpool add. Toutefois, vous pouvez étendre un périphérique virtuel mis en miroir à l'aide de
la commande zpool attach.

■ Les configurations RAID-Z ou entrelacées ne sont pas prises en charge.
■ Un pool racine ne peut pas avoir de périphérique de journalisation distinct.

■

Si vous tentez d'utiliser une configuration non prise en charge pour un pool racine, un
message tel que le suivant s'affiche :

ERROR: ZFS pool <pool-name> does not support boot environments

# zpool add -f rpool log c0t6d0s0

cannot add to ’rpool’: root pool can not have multiple vdevs or separate logs

Pour plus d'informations sur l'installation et l'initialisation d'un système de fichiers racine ZFS,
reportez-vous au Chapitre 5, “Gestion des composants du pool racine ZFS ”.

52

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

Création d'un pool de stockage RAID-Z
La création d'un pool RAID-Z à parité simple est identique à celle d'un pool mis en miroir, à la
seule différence que le mot-clé raidz ou raidz1 est utilisé à la place du mot-clé mirror. Les
exemples suivants illustrent la création d'un pool avec un périphérique RAID-Z unique
composé de cinq disques :

# zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0

Cet exemple montre que les disques peuvent être spécifiés à l'aide de leurs noms de
périphérique abrégés ou complets. Les deux éléments /dev/dsk/c5t0d0 et c5t0d0 font
référence au même disque.

Vous pouvez créer une configuration RAID-Z à double ou à triple parité à l'aide du mot-clé
raidz2 ou raidz3 lors de la création du pool. Par exemple :

# zpool create tank raidz2 c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0

# zpool status -v tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

raidz2-0 ONLINE

c1t0d0 ONLINE

c2t0d0 ONLINE

c3t0d0 ONLINE

c4t0d0 ONLINE

c5t0d0 ONLINE

errors: No known data errors

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

# zpool create tank raidz3 c0t0d0 c1t0d0 c2t0d0 c3t0d0 c4t0d0

c5t0d0 c6t0d0 c7t0d0 c8t0d0

# zpool status -v tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

raidz3-0 ONLINE

c0t0d0 ONLINE

c1t0d0 ONLINE

c2t0d0 ONLINE

c3t0d0 ONLINE

c4t0d0 ONLINE

c5t0d0 ONLINE

c6t0d0 ONLINE

c7t0d0 ONLINE

c8t0d0 ONLINE

errors: No known data errors

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

53

Création et destruction de pools de stockage ZFS

Actuellement, les opérations suivantes sont prises en charge dans une configuration RAID-Z
ZFS :
■ Ajout d'un autre jeu de disques comme périphérique virtuel (vdev) supplémentaire de

niveau supérieur à une configuration RAID-Z existante. Pour plus d'informations,
reportez-vous à la rubrique “Ajout de périphériques à un pool de stockage” à la page 63.
■ Remplacement d'un ou de plusieurs disques dans une configuration RAID-Z existante, à

condition que les disques de remplacement soient d'une taille supérieure ou égale au celle du
périphérique remplacé. Pour plus d'informations, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 76.

Actuellement, les opérations suivantes ne sont pas prises en charge dans une configuration
RAID-Z :
■ Connexion d'un disque supplémentaire à une configuration RAID-Z existante.
■ Déconnexion d'un disque d'une configuration RAID-Z, sauf lorsque vous déconnectez un

disque qui est remplacé par un disque de rechange ou lorsque vous avez besoin de
déconnecter un disque de rechange.

■ Vous ne pouvez pas forcer la suppression d'un périphérique qui n'est pas un périphérique de

journalisation ni de cache à partir d'une configuration RAID-Z. Cette fonction fait l'objet
d'une demande d'amélioration.

Pour obtenir des informations supplémentaire, reportez-vous à la section “Configuration de
pool de stockage RAID-Z” à la page 48.

Création d'un pool de stockage ZFS avec des périphériques de
journalisation
Le journal d'intention ZFS (ZIL) permet de répondre aux exigences de la norme POSIX dans le
cadre de transactions synchronisées. Par exemple, les transactions de base de données doivent
souvent se trouver sur des périphériques de stockage stables lorsqu'elles sont obtenues à partir
d'un appel système. NFS et d'autres applications peuvent également utiliser fsync() pour
assurer la stabilité des données.

Par défaut, le ZIL est attribué à partir de blocs dans le pool principal. Il est cependant possible
d'obtenir de meilleures performances en utilisant des périphériques de journalisation
d'intention distincts, notamment une NVRAM ou un disque dédié.

Considérez les points suivants pour déterminer si la configuration d'un périphérique de
journalisation ZFS convient à votre environnement :
■ Les périphériques de journalisation du ZIL ne sont pas liés aux fichiers journaux de base de

données.

54

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

■ Toute amélioration des performances observée suite à l'implémentation d'un périphérique
de journalisation distinct dépend du type de périphérique, de la configuration matérielle du
pool et de la charge de travail de l'application. Pour des informations préliminaires sur les
performances, consultez le blog suivant :

http://blogs.oracle.com/perrin/entry/slog_blog_or_blogging_on

■ Les périphériques de journalisation peuvent être mis en miroir et leur réplication peut être

annulée, mais RAID-Z n'est pas pris en charge pour les périphériques de journalisation.
Si un périphérique de journalisation distinct n'est pas mis en miroir et que le périphérique
contenant le journal échoue, le stockage des blocs de journal retourne sur le pool de
stockage.

■

■ Les périphériques de journalisation peuvent être ajoutés, remplacés, connectés,

déconnectés, importés et exportés en tant que partie du pool de stockage.

■ Vous pouvez connecter un périphérique de journalisation à un périphérique de

journalisation existant afin de créer un périphérique mis en miroir. Cette opération est
similaire à la connexion d'un périphérique à un pool de stockage qui n'est pas mis en miroir.

■ La taille minimale d'un périphérique de journalisation correspond à la taille minimale de

chaque périphérique d'un pool, à savoir 64 Mo. La quantité de données en jeu pouvant être
stockée sur un périphérique de journalisation est relativement petite. Les blocs de journal
sont libérés lorsque la transaction du journal (appel système) est validée.

■ La taille maximale d'un périphérique de journalisation doit être approximativement égale à
la moitié de la taille de la mémoire physique car il s'agit de la quantité maximale de données
en jeu potentielles pouvant être stockée. Si un système dispose par exemple de 16 Go de
mémoire physique, considérez une taille maximale de périphérique de journalisation de
8 Go.

Vous pouvez installer un périphérique de journalisation ZFS au moment de la création du pool
de stockage ou après sa création.

L'exemple suivant explique comment créer un pool de stockage mis en miroir contenant des
périphériques de journalisation mis en miroir :

# zpool create datap mirror c1t1d0 c1t2d0 mirror c1t3d0 c1t4d0

log mirror c1t5d0 c1t8d0

# zpool status datap

pool: datap

state: ONLINE

scrub: none requested

config:

NAME

datap

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c1t1d0 ONLINE

c1t2d0 ONLINE

mirror-1 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

55

Création et destruction de pools de stockage ZFS

c1t4d0 ONLINE

logs

mirror-2 ONLINE

c1t5d0 ONLINE

c1t8d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Pour plus d'informations sur la récupération suite à une défaillance de périphérique de
journalisation, reportez-vous à l'Exemple 11–2.

Création d'un pool de stockage ZFS avec des périphériques de cache
Les périphériques de cache fournissent une couche de mise en cache supplémentaire entre la
mémoire principale et le disque. L'utilisation de périphériques de cache constitue la meilleure
amélioration de performances pour les charges de travail de lecture aléatoire constituées
principalement de contenu statique.

Vous pouvez créer un pool de stockage avec des périphériques de cache afin de mettre en cache
des données de pool de stockage. Par exemple :

# zpool create tank mirror c2t0d0 c2t1d0 c2t3d0 cache c2t5d0 c2t8d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

cache

c2t5d0

ONLINE

c2t8d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Une fois les périphériques de cache ajoutés, ils se remplissent progressivement de contenu
provenant de la mémoire principale. En fonction de la taille du périphérique de cache, le
remplissage peut prendre plus d'une heure. La capacité et les lectures sont contrôlables à l'aide
de la commande zpool iostat comme indiqué ci-dessous :

# zpool iostat -v pool 5

Une fois le pool créé, vous pouvez y ajouter des périphériques de cache ou les en supprimer.

56

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

Tenez compte des points suivants lorsque vous envisagez de créer un pool de stockage ZFS avec
des périphériques de cache :
■ L'utilisation de périphériques de cache constitue la meilleure amélioration de performances

pour les charges de travail de lecture aléatoire constituées principalement de contenu
statique.

■ La capacité et les lectures sont contrôlables à l'aide de la commande zpool iostat.
■ Lors de la création du pool, vous pouvez ajouter un ou plusieurs caches. Ils peuvent

également être ajoutés ou supprimés après la création du pool. Pour plus d'informations,
reportez-vous à l'Exemple 4–4.

■

■ Les périphériques de cache ne peuvent pas être mis en miroir ou faire partie d'une

configuration RAID-Z.
Si une erreur de lecture est détectée sur un périphérique de cache, cette E/S de lecture est à
nouveau exécutée sur le périphérique de pool de stockage d'origine, qui peut faire partie
d'une configuration RAID-Z ou en miroir. Le contenu des périphériques de cache est
considéré comme volatile, comme les autres caches système.

Précautions pour la création de pools de stockage
Tenez compte des mises en garde suivantes lors de la création et de la gestion de pools de
stockage ZFS.
■ Ne repartitionnez ou ne réétiquetez pas des disques qui font partie d'un pool de stockage
existant. Si vous tentez de repartitionner ou de réétiqueter un disque de pool racine, vous
devrez peut-être réinstaller le système d'exploitation.

■ Ne créez pas de pool de stockage contenant des composants d'un autre pool de stockage, tels

que des fichiers ou des volumes. Des interblocages peuvent se produire dans cette
configuration non prise en charge.

■ Un pool créé avec une tranche unique ou un disque unique n'a aucune redondance et risque

de perdre des données. Un pool créé avec plusieurs tranches mais sans redondance risque
également de perdre des données. Un pool créé avec plusieurs tranches réparties sur
plusieurs disques est plus difficile à gérer qu'un pool créé avec des disques entiers.

■ Un pool créé sans redondance ZFS (RAIDZ ou miroir) peut uniquement signaler les

incohérences de données. Il ne peut pas réparer les incohérences de données.

■ Bien qu'un pool créé avec redondance ZFS permette de réduire le temps d'inactivité dû à des

pannes matérielles, il n'est pas à l'abri de pannes matérielles, de pannes de courant ou de
déconnexions de câbles. Veillez à sauvegarder régulièrement vos données. Il est important
d'effectuer des sauvegardes de routine des données de pools si le matériel n'est pas de niveau
professionnel.

■ Un pool ne peut pas être partagé par différents systèmes. ZFS n'est pas un système de fichiers

de cluster.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

57

Création et destruction de pools de stockage ZFS

Affichage des informations d'un périphérique virtuel
de pool de stockage
Chaque pool de stockage contient un ou plusieurs périphériques virtuels. Un périphérique
virtuel est une représentation interne du pool de stockage qui décrit la disposition du stockage
physique et les caractéristiques par défaut du pool de stockage. Ainsi, un périphérique virtuel
représente les périphériques de disque ou les fichiers utilisés pour créer le pool de stockage. Un
pool peut contenir un nombre quelconque de périphériques virtuels dans le niveau supérieur de
la configuration. Ces périphériques sont appelés top-level vdev.

Si le périphérique virtuel de niveau supérieur contient deux ou plusieurs périphériques
physiques, la configuration assure la redondance des données en tant que périphériques virtuels
RAID-Z ou miroir. Ces périphériques virtuels se composent de disques, de tranches de disques
ou de fichiers. Un disque de rechange (spare) est un périphérique virtuel spécial qui effectue le
suivi des disques hot spare disponibles d'un pool.

L'exemple suivant illustre la création d'un pool composé de deux périphériques virtuels de
niveau supérieur, chacun étant un miroir de deux disques :

# zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0

L'exemple suivant illustre la création d'un pool composé d'un périphérique virtuel de niveau
supérieur comportant quatre disques :

# zpool create mypool raidz2 c1d0 c2d0 c3d0 c4d0

Vous pouvez ajouter un autre périphérique virtuel de niveau supérieur à ce pool en utilisant la
commande zpool add. Par exemple :

# zpool add mypool raidz2 c2d1 c3d1 c4d1 c5d1

Les disques, tranches de disque ou fichiers utilisés dans des pools non redondants fonctionnent
en tant que périphériques virtuels de niveau supérieur. Les pools de stockage contiennent en
règle générale plusieurs périphériques virtuels de niveau supérieur. ZFS entrelace
automatiquement les données entre l'ensemble des périphériques virtuels de niveau supérieur
dans un pool.

Les périphériques virtuels et les périphériques physiques contenus dans un pool de stockage
ZFS s'affichent avec la commande zpool status. Par exemple :

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

0

0

0

58

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror-1 ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror-2 ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Gestion d'erreurs de création de pools de stockage ZFS
Les erreurs de création de pool peuvent se produire pour de nombreuses raisons. Certaines
raisons sont évidentes, par exemple lorsqu'un périphérique spécifié n'existe pas, mais d'autres le
sont moins.

Détection des périphériques utilisés
Avant de formater un périphérique, ZFS vérifie que le disque n'est pas utilisé par ZFS ou une
autre partie du système d'exploitation. Si le disque est en cours d'utilisation, les erreurs
suivantes peuvent se produire :

# zpool create tank c1t0d0 c1t1d0

invalid vdev specification

use ’-f’ to override the following errors:

/dev/dsk/c1t0d0s0 is currently mounted on /. Please see umount(1M).

/dev/dsk/c1t0d0s1 is currently mounted on swap. Please see swap(1M).

/dev/dsk/c1t1d0s0 is part of active ZFS pool zeepool. Please see zpool(1M).

Certaines erreurs peuvent être ignorées à l'aide de l'option -f, mais pas toutes. Les conditions
suivantes ne peuvent pas à être ignorées via l'option - f et doivent être corrigées
manuellement :
Système de fichiers monté

Système de fichiers dans /etc/vfstab

Périphérique de vidage dédié

Le disque ou une de ses tranches contient un système
de fichiers actuellement monté. La commande umount
permet de corriger cette erreur.
Le disque contient un système de fichiers répertorié
dans le fichier /etc/vfstab, mais le système de
fichiers n'est pas monté. Pour corriger cette erreur,
supprimez ou commentez la ligne dans le fichier
/etc/vfstab.
Le disque est utilisé en tant que périphérique de vidage
dédié pour le système. La commande dumpadm permet
de corriger cette erreur.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

59

Création et destruction de pools de stockage ZFS

Elément d'un pool ZFS

Le disque ou fichier fait partie d'un pool de stockage
ZFS. Pour corriger cette erreur, utilisez la commande
zpool destroy afin de détruire l'autre pool s'il est
obsolète. Utilisez sinon la commande zpool detach
pour déconnecter le disque de l'autre pool. Vous
pouvez déconnecter un disque que s'il est connecté à
un pool de stockage mis en miroir.

Les vérifications en cours d'utilisation suivantes constituent des avertissements. Pour les
ignorer, appliquez l'option -f afin de créer le pool :
Contient un système de fichiers

Elément d'un volume

Elément d'un pool ZFS exporté

Le disque contient un système de fichiers connu bien qu'il
ne soir pas monté et n'apparaisse pas comme étant en
cours d'utilisation.
Le disque fait partie d'un volume
Solaris Volume Manager.
Le disque fait partie d'un pool de stockage exporté ou
supprimé manuellement d'un système. Dans le deuxième
cas, le pool est signalé comme étant potentiellement
actif, dans la mesure où il peut s'agir d'un disque
connecté au réseau en cours d'utilisation par un autre
système. Faites attention lorsque vous ignorez un pool
potentiellement activé.

L'exemple suivant illustre l'utilisation de l'option -f :

# zpool create tank c1t0d0

invalid vdev specification

use ’-f’ to override the following errors:

/dev/dsk/c1t0d0s0 contains a ufs filesystem.

# zpool create -f tank c1t0d0

Si possible, corrigez les erreurs au lieu d'utiliser l'option -f pour les ignorer.

Niveaux de réplication incohérents
Il est déconseillé de créer des pools avec des périphériques virtuels de niveau de réplication
différents. La commande zpool tente de vous empêcher de créer par inadvertance un pool
comprenant des niveaux de redondance différents. Si vous tentez de créer un pool avec un telle
configuration, les erreurs suivantes s'affichent :

# zpool create tank c1t0d0 mirror c2t0d0 c3t0d0

invalid vdev specification

use ’-f’ to override the following errors:

mismatched replication level: both disk and mirror vdevs are present

# zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0

60

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création et destruction de pools de stockage ZFS

invalid vdev specification

use ’-f’ to override the following errors:

mismatched replication level: 2-way mirror and 3-way mirror vdevs are present

Vous pouvez ignorer ces erreurs avec l'option -f. Toutefois, cette pratique est déconseillée. La
commande affiche également un avertissement relatif à la création d'un pool RAID-Z ou mis en
miroir à l'aide de périphériques de tailles différentes. Même si cette configuration est autorisée,
les niveaux de redondance sont incohérents. Par conséquent, l'espace disque du périphérique de
plus grande taille n'est pas utilisé. Vous devez spécifier l'option -f pour ignorer l'avertissement.

Réalisation d'un test à la création d'un pool de stockage
Les tentatives de création d'un pool peuvent échouer soudainement de plusieurs façons ; vous
pouvez formater les disques, mais cela peut avoir des conséquences négatives. C'est pourquoi la
commande zpool create dispose d'une option supplémentaire, à savoir l'option -n, qui simule
la création du pool sans écrire les données sur le périphérique. Cette option de test vérifie le
périphérique en cours d'utilisation et valide le niveau de réplication, puis répertorie les erreurs
survenues au cours du processus. Si aucune erreur n'est détectée, la sortie est similaire à la
suivante :

# zpool create -n tank mirror c1t0d0 c1t1d0

would create ’tank’ with the following layout:

tank

mirror

c1t0d0

c1t1d0

Certaines erreurs sont impossibles à détecter sans création effective du pool. L'exemple le plus
courant consiste à spécifier le même périphérique deux fois dans la même configuration. Cette
erreur ne peut pas être détectée de façon fiable sans l'enregistrement effectif des données. Par
conséquent, la commande zpool create -n peut indiquer que l'opération a réussi sans pour
autant parvenir à créer le pool, lors de son exécution sans cette option.

Point de montage par défaut pour les pools de stockage
Lors de la création d'un pool, le point de montage par défaut du système de fichiers de niveau
supérieur est /pool-name . Le répertoire doit être inexistant ou vide. Le répertoire est créé
automatiquement s'il n'existe pas. Si le répertoire est vide, le système de fichiers racine est
monté sur le répertoire existant. Pour créer un pool avec un point de montage par défaut
différent, utilisez l'option - m de la commande zpool create : Par exemple :

# zpool create home c1t0d0

default mountpoint ’/home’ exists and is not empty

use ’-m’ option to provide a different default

# zpool create -m /export/zfs home c1t0d0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

61

Création et destruction de pools de stockage ZFS

Cette commande crée le pool home et le système de fichiers home avec le point de montage
/export/zfs.

Pour de plus amples informations sur les points de montage, reportez-vous à la section
“Gestion des points de montage ZFS” à la page 168.

Destruction de pools de stockage ZFS
La commande zpool destroy permet de détruire les pools. Cette commande détruit le pool
même s'il contient des jeux de données montés.

# zpool destroy tank

Attention – Faites très attention lorsque vous détruisez un pool. Assurez-vous de détruire le pool
souhaité et de toujours disposer de copies de vos données. En cas de destruction accidentelle
d'un pool, vous pouvez tenter de le récupérer. Pour obtenir des informations supplémentaires,
reportez-vous à la section “Récupération de pools de stockage ZFS détruits” à la page 107.

Si vous détruisez un pool à l'aide de la commande zpool destroy, le pool reste disponible pour
l'importation, comme décrit dans la section “Récupération de pools de stockage ZFS détruits”
à la page 107. Cela signifie que des données confidentielles peuvent subsister sur les disques qui
faisaient partie du pool. Si vous souhaitez détruire les données placées sur les disques du pool
détruit, vous devez utiliser une fonctionnalité telle que l'option analyze->purge de l'utilitaire
format sur tous les disques du pool détruit.

Une autre possibilité pour préserver la confidentialité de données de systèmes de fichiers est de
créer des systèmes de fichiers ZFS chiffrés. Lorsqu'un pool contenant un système de fichiers
chiffré est détruit, les données ne sont pas accessibles sans les clés de chiffrement, même si le
pool détruit est récupéré. Pour plus d'informations, reportez-vous à la section “Chiffrement des
systèmes de fichiers ZFS” à la page 188.

Destruction d'un pool avec des périphériques défaillants
La destruction d'un pool requiert l'écriture des données sur le disque pour indiquer que le pool
n'est désormais plus valide. Ces informations d'état évitent que les périphériques ne s'affichent
en tant que pool potentiel lorsque vous effectuez une importation. La destruction du pool est
tout de même possible si un ou plusieurs périphériques ne sont pas disponibles. Cependant, les
informations d'état requises ne sont pas écrites sur ces périphériques indisponibles.

Ces périphériques, lorsqu'ils sont correctement réparés, sont signalés comme potentiellement
actifs, lors de la création d'un pool. Lorsque vous recherchez des pools à importer, ils s'affichent
en tant que périphériques valides. Si un pool a tant de périphérique défaillants que le pool

62

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

lui-même est défaillant (en d'autres termes, un périphérique virtuel de niveau supérieur est
défaillant), alors la commande émet un avertissement et ne peut pas s'exécuter sans l'option -f.
Cette option est requise car l'ouverture du pool est impossible et il est impossible de savoir si des
données y sont stockées. Par exemple :

# zpool destroy tank

cannot destroy ’tank’: pool is faulted

use ’-f’ to force destruction anyway

# zpool destroy -f tank

Pour de plus amples informations sur les pools et la maintenance des périphériques,
reportez-vous à la section “Détermination de l'état de maintenance des pools de stockage ZFS”
à la page 95.

Pour de plus amples informations sur l'importation de pools, reportez-vous à la section
“Importation de pools de stockage ZFS” à la page 103.

Gestion de périphériques dans un pool de stockage ZFS

Vous trouverez la plupart des informations de base concernant les périphériques dans la section
“Composants d'un pool de stockage ZFS” à la page 43. Après la création d'un pool, vous pouvez
effectuer plusieurs tâches de gestion des périphériques physiques au sein du pool.
■ “Ajout de périphériques à un pool de stockage” à la page 63
■ “Connexion et séparation de périphériques dans un pool de stockage ” à la page 68
■ “Création d'un pool par scission d'un pool de stockage ZFS mis en miroir” à la page 70
■ “Mise en ligne et mise hors ligne de périphériques dans un pool de stockage” à la page 73
■ “Effacement des erreurs de périphérique de pool de stockage” à la page 76
■ “Remplacement de périphériques dans un pool de stockage” à la page 76
■ “Désignation des disques hot spare dans le pool de stockage” à la page 79

Ajout de périphériques à un pool de stockage
Vous pouvez ajouter de l'espace disque à un pool de façon dynamique, en ajoutant un
périphérique virtuel de niveau supérieur. Cet espace disque est disponible immédiatement pour
l'ensemble des jeux de données du pool. Pour ajouter un périphérique virtuel à un pool, utilisez
la commande zpool add. Par exemple :

# zpool add zeepool mirror c2t1d0 c2t2d0

Le format de spécification des périphériques virtuels est le même que pour la commande zpool
create. Une vérification des périphériques est effectuée afin de déterminer s'ils sont en cours

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

63

Gestion de périphériques dans un pool de stockage ZFS

d'utilisation et la commande ne peut pas modifier le niveau de redondance sans l'option -f. La
commande prend également en charge l'option -n, ce qui permet d'effectuer un test. Par
exemple :

# zpool add -n zeepool mirror c3t1d0 c3t2d0

would update ’zeepool’ to the following configuration:

zeepool

mirror

c1t0d0

c1t1d0

mirror

c2t1d0

c2t2d0

mirror

c3t1d0

c3t2d0

Cette syntaxe de commande ajouterait les périphériques en miroir c3t1d0 et c3t2d0 à la
configuration existante du pool zeepool.

Pour plus d'informations sur la validation des périphériques virtuels, reportez-vous à la section
“Détection des périphériques utilisés” à la page 59.

EXEMPLE 4–1 Ajout de disques à une configuration ZFS mise en miroir
Dans l'exemple suivant, un autre miroir est ajouté à une configuration ZFS mise en miroir
existante.

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

STATE

READ WRITE CKSUM

NAME

tank

ONLINE

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror-1 ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add tank mirror c0t3d0 c1t3d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

0

0

0

0

0

0

64

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

EXEMPLE 4–1 Ajout de disques à une configuration ZFS mise en miroir

(Suite)

Gestion de périphériques dans un pool de stockage ZFS

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror-1 ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror-2 ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

EXEMPLE 4–2 Ajout de disques à une configuration RAID-Z
De la même façon, vous pouvez ajouter des disques supplémentaires à une configuration
RAID-Z. L'exemple suivant illustre la conversion d'un pool de stockage avec un périphérique
RAID–Z composé de trois disques en pool de stockage avec deux périphériques RAID-Z
composés de trois disques chacun.

# zpool status rzpool

pool: rzpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rzpool

ONLINE

raidz1-0 ONLINE

c1t2d0 ONLINE

c1t3d0 ONLINE

c1t4d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add rzpool raidz c2t2d0 c2t3d0 c2t4d0

# zpool status rzpool

pool: rzpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rzpool

ONLINE

raidz1-0 ONLINE

c1t0d0 ONLINE

c1t2d0 ONLINE

c1t3d0 ONLINE

raidz1-1 ONLINE

c2t2d0 ONLINE

c2t3d0 ONLINE

c2t4d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

65

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–3 Ajout et suppression d'un périphérique de journalisation mis en miroir
L'exemple suivant indique comment ajouter un périphérique de journalisation mis en miroir
dans un pool de stockage mis en miroir.

# zpool status newpool

pool: newpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

newpool

ONLINE

mirror-0 ONLINE

c0t4d0 ONLINE

c0t5d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add newpool log mirror c0t6d0 c0t7d0

# zpool status newpool

pool: newpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

newpool

ONLINE

mirror-0 ONLINE

c0t4d0 ONLINE

c0t5d0 ONLINE

logs

mirror-1 ONLINE

c0t6d0 ONLINE

c0t7d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Vous pouvez connecter un périphérique de journalisation à un périphérique de journalisation
existant afin de créer un périphérique mis en miroir. Cette opération est similaire à la
connexion d'un périphérique à un pool de stockage qui n'est pas mis en miroir.

Vous pouvez supprimer les périphériques de journalisation en utilisant la commande zpool
remove. Le périphérique de journalisation mis en miroir dans l'exemple précédent peut être
supprimé en spécifiant l'argument miroir-1. Par exemple :

# zpool remove newpool mirror-1

# zpool status newpool

pool: newpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

newpool

ONLINE

mirror-0 ONLINE

0

0

0

0

0

0

66

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–3 Ajout et suppression d'un périphérique de journalisation mis en miroir

(Suite)

c0t4d0 ONLINE

c0t5d0 ONLINE

0

0

0

0

0

0

errors: No known data errors

Si votre configuration de pool contient un seul périphérique de journalisation, supprimez-le en
saisissant le nom du périphérique. Par exemple :

# zpool status pool

pool: pool

state: ONLINE

scrub: none requested

config:

NAME

pool

ONLINE

STATE

READ WRITE CKSUM

raidz1-0 ONLINE

c0t8d0 ONLINE

c0t9d0 ONLINE

logs

c0t10d0

ONLINE

errors: No known data errors

# zpool remove pool c0t10d0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

EXEMPLE 4–4 Ajout et suppression des périphériques de cache
Vous pouvez ajouter des périphériques de cache à votre pool de stockage ZFS et les supprimer
s'ils ne sont plus nécessaires.

Utilisez la commande zpool add pour ajouter des périphériques de cache. Par exemple :

# zpool add tank cache c2t5d0 c2t8d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

mirror-0 ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

cache

c2t5d0

ONLINE

c2t8d0

ONLINE

errors: No known data errors

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

67

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–4 Ajout et suppression des périphériques de cache

(Suite)

Les périphériques de cache ne peuvent pas être mis en miroir ou faire partie d'une configuration
RAID-Z.

Utilisez la commande zpool remove pour supprimer des périphériques de cache. Par exemple :

# zpool remove tank c2t5d0 c2t8d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

mirror-0 ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Actuellement, la commande zpool remove prend uniquement en charge la suppression des
disques hot spare, des périphériques de journalisation et des périphériques de cache. Les
périphériques faisant partie de la configuration de pool mis en miroir principale peuvent être
supprimés à l'aide de la commande zpool detach. Les périphériques non redondants et
RAID-Z ne peuvent pas être supprimés d'un pool.

Pour plus d'informations sur l'utilisation des périphériques de cache dans un pool de stockage
ZFS, reportez-vous à la section “Création d'un pool de stockage ZFS avec des périphériques de
cache” à la page 56.

Connexion et séparation de périphériques dans un
pool de stockage
Outre la commande zpool add, vous pouvez utiliser la commande zpool attach pour ajouter
un périphérique à un périphérique existant, en miroir ou non.

Si vous connectez un disque pour créer un pool racine mis en miroir, reportez-vous à la section
“Configuration d'un pool racine mis en miroir” à la page 117.

Si vous remplacez un disque dans le pool racine ZFS, reportez-vous à la section “Remplacement
d'un disque dans un pool racine ZFS” à la page 119.

68

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–5 Conversion d'un pool de stockage bidirectionnel mis en miroir en un pool de stockage
tridirectionnel mis en miroir
Dans cet exemple, zeepool est un miroir bidirectionnel. Il est converti en un miroir
tridirectionnel via la connexion de c2t1d0, le nouveau périphérique, au périphérique existant,
c1t1d0.

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool attach zeepool c1t1d0 c2t1d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Fri Jan 8 12:59:20 2010

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

c2t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 592K resilvered

errors: No known data errors

Si le périphérique existant fait partie d'un miroir tridirectionnel, la connexion d'un nouveau
périphérique crée un miroir quadridirectionnel, et ainsi de suite. Dans tous les cas, la
resynchronisation du nouveau périphérique commence immédiatement.

EXEMPLE 4–6 Conversion d'un pool de stockage ZFS non redondant en pool de stockage ZFS en miroir
En outre, vous pouvez convertir un pool de stockage non redondant en pool de stockage
redondant à l'aide de la commande zpool attach. Par exemple :

# zpool create tank c0t1d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

c0t1d0

ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

69

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–6 Conversion d'un pool de stockage ZFS non redondant en pool de stockage ZFS en miroir
(Suite)

errors: No known data errors

# zpool attach tank c0t1d0 c1t1d0

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Fri Jan 8 14:28:23 2010

config:

NAME

tank

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0 73.5K resilvered

Vous pouvez utiliser la commande zpool detach pour séparer un périphérique d'un pool de
stockage mis en miroir. Par exemple :

# zpool detach zeepool c2t1d0

Cependant, en l'absence de répliques de données valides, cette opération échoue. Par exemple :

# zpool detach newpool c1t2d0

cannot detach c1t2d0: only applicable to mirror and replacing vdevs

Création d'un pool par scission d'un pool de stockage
ZFS mis en miroir
Un pool de stockage ZFS mis en miroir peut être rapidement cloné en tant que pool de
sauvegarde à l'aide de la commande zpool split. Vous pouvez utiliser cette fonctionnalité
pour scinder un pool de stockage mis en miroir. Toutefois, le pool qui a été séparé n'est pas
initialisable.

Vous pouvez utiliser la commande zpool split pour déconnecter un ou plusieurs disques à
partir d'un pool de stockage ZFS mis en miroir afin de créer un pool de stockage avec l'un des
disques déconnectés. Le nouveau pool contiendra les mêmes données que le pool de stockage
ZFS d'origine mis en miroir.

Par défaut, une opération zpool split sur un pool mis en miroir déconnecte le dernier disque
du nouveau pool. Une fois l'opération de scission terminée, importez le nouveau pool. Par
exemple :

70

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

mirror-0 ONLINE

c1t0d0 ONLINE

c1t2d0 ONLINE

errors: No known data errors

# zpool split tank tank2

# zpool import tank2

# zpool status tank tank2

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

c1t0d0

ONLINE

errors: No known data errors

pool: tank2

state: ONLINE

scrub: none requested

config:

NAME

tank2

ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

c1t2d0

ONLINE

errors: No known data errors

Vous pouvez identifier le disque à utiliser par le nouveau pool en le définissant avec la
commande zpool split. Par exemple :

# zpool split tank tank2 c1t0d0

Avant l'opération de scission, les données de la mémoire sont vidées vers les disques mis en
miroir. Une fois les données vidées, le disque est déconnecté du pool et reçoit un nouveau
GUID de pool. Un pool GUID est généré pour que le pool puisse être importé sur le même
système que celui sur lequel il a été scindé.

Si le pool à scinder possède des points de montage de système de fichiers autres que celui par
défaut et si le nouveau pool est créé sur le même système, utilisez l'option zpool split -R pour
identifier un autre répertoire racine pour le nouveau pool afin d'éviter tout conflit entre les
points de montage existants. Par exemple :

# zpool split -R /tank2 tank tank2

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

71

Gestion de périphériques dans un pool de stockage ZFS

Si vous n'utilisez pas l'option zpool split -R et si des points de montage entrent en conflit
lorsque vous tentez d'importer le nouveau pool, importez celui-ci à l'aide de l'option -R. Si le
nouveau pool est créé sur un autre système, il n'est pas nécessaire de spécifier un autre
répertoire racine, sauf en cas de conflits entre les points de montage.

Avant d'utiliser la fonctionnalité zpool split, veuillez prendre en compte les points suivants :
■ Cette fonction n'est pas disponible dans une configuration RAID-Z ou un pool non

redondant composé de plusieurs disques.

■ Avant de tenter une opération zpool split, les opérations des données et des applications

doivent être suspendues.
Si vous voulez que les disques effectuent les opérations demandées et qu'ils ne les ignorent
pas, la commande de mise en cache des enregistrements de vidage des disques est
importante.

■

■ Vous ne pouvez pas scinder un pool si une réargenture est en cours.
■ Lorsqu'un pool mis en miroir est composé de deux à trois disques et que le dernier disque du
pool d'origine est utilisé pour le nouveau pool créé, la meilleure solution consiste à scinder le
pool mis en miroir. Vous pouvez ensuite utiliser la commande zpool attach pour recréer
votre pool de stockage d'origine mis en miroir ou convertir votre nouveau pool dans un pool
de stockage mis en miroir. Il n'existe actuellement aucun moyen de créer un nouveau pool
mis en miroir à partir d'un pool mis en miroir existant à l'aide de cette fonction.
Si le pool existant est un miroir tridirectionnel, le nouveau pool contiendra un disque après
l'opération de scission. Si le pool existant est un miroir bidirectionnel composé de deux
disques, cela donne deux pools non redondants composés de deux disques. Vous devez
connecter deux disques supplémentaires pour convertir les pools non redondants en pools
mis en miroir.

■

■ Pour conserver vos données redondantes lors d'une scission, scindez un pool de stockage
mis en miroir composé de trois disques pour que le pool d'origine soit composé de deux
disques mis en miroir après la scission.

EXEMPLE 4–7 Scission d'un pool ZFS mis en miroir
Dans l'exemple suivant, un pool de stockage mis en miroir nommé trinity et contenant trois
disques (c1t0d0, c1t2d0 et c1t3d0) est scindé. Les deux pools correspondants sont le pool mis
en miroir trinity contenant les disques c1t0d0 et c1t2d0 et le nouveau pool neo contenant le
disque c1t3d0. Chaque pool contient les mêmes données.

# zpool status trinity

pool: trinity

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

trinity

ONLINE

0

0

0

72

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–7 Scission d'un pool ZFS mis en miroir

(Suite)

mirror-0 ONLINE

c1t0d0 ONLINE

c1t2d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool split trinity neo

# zpool import neo

# zpool status trinity neo

pool: neo

state: ONLINE

scrub: none requested

config:

NAME

neo

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

c1t3d0

ONLINE

errors: No known data errors

pool: trinity

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

trinity

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

c1t2d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Mise en ligne et mise hors ligne de périphériques dans
un pool de stockage
ZFS permet la mise en ligne ou hors ligne de périphériques. Lorsque le matériel n'est pas fiable
ou fonctionne mal, ZFS continue de lire ou d'écrire les données dans le périphérique en partant
du principe que le problème est temporaire. Dans le cas contraire, vous pouvez indiquer à ZFS
d'ignorer le périphérique en le mettant hors ligne. Le système de fichiers ZFS n'envoie aucune
demande à un périphérique déconnecté.

Remarque – Il est inutile de mettre les périphériques hors ligne pour les remplacer.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

73

Gestion de périphériques dans un pool de stockage ZFS

Mise hors ligne d'un périphérique
La commande zpool offline permet de mettre un périphérique hors ligne. Vous pouvez
spécifier le périphérique via son chemin ou via son nom abrégé s'il s'agit d'un disque. Par
exemple :

# zpool offline tank c1t0d0

bringing device c1t0d0 offline

Lors de la déconnexion d'un périphérique, veuillez prendre en compte les points suivants :
■ Vous ne pouvez pas mettre un périphérique hors ligne au point où il devient défaillant. Vous

ne pouvez par exemple pas mettre hors ligne deux périphériques d'une configuration
raid-z1, ni ne pouvez mettre hors ligne un périphérique virtuel de niveau supérieur.

# zpool offline tank c1t0d0

cannot offline c1t0d0: no valid replicas

■ Par défaut, l'état OFFLINE est persistant. Le périphérique reste hors ligne lors de la

réinitialisation du système.
Pour mettre un périphérique hors ligne temporairement, utilisez l'option -t de la
commande zpool offline. Par exemple :

# zpool offline -t tank c1t0d0

bringing device ’c1t0d0’ offline

En cas de réinitialisation du système, ce périphérique revient automatiquement à l'état
ONLINE.

■ Lorsqu'un périphérique est mis hors ligne, il n'est pas séparé du pool de stockage. En cas de

tentative d'utilisation du périphérique hors ligne dans un autre pool, même en cas de
destruction du pool d'origine, un message similaire au suivant s'affiche :
device is part of exported or potentially active ZFS pool. Please see zpool(1M)
Si vous souhaitez utiliser le périphérique hors ligne dans un autre pool de stockage après
destruction du pool de stockage d'origine, remettez le périphérique en ligne puis détruisez le
pool de stockage d'origine.

Une autre mode d'utilisation d'un périphérique provenant d'un autre pool de stockage si
vous souhaitez conserver le pool de stockage d'origine consiste à remplacer le périphérique
existant dans le pool de stockage d'origine par un autre périphérique similaire. Pour obtenir
des informations sur le remplacement de périphériques, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 76.

Les périphériques mis hors ligne s'affichent dans l'état OFFLINE en cas de requête de l'état de
pool. Pour obtenir des informations sur les requêtes d'état de pool, reportez-vous à la section
“Requête d'état de pool de stockage ZFS” à la page 87.

74

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

Pour de plus amples informations sur la maintenance des périphériques, reportez-vous à la
section “Détermination de l'état de maintenance des pools de stockage ZFS” à la page 95.

Mise en ligne d'un périphérique
Lorsqu'un périphérique est mis hors ligne, il peut être restauré grâce à la commande zpool
online. Par exemple :

# zpool online tank c1t0d0

bringing device c1t0d0 online

Lorsqu'un périphérique est mis en ligne, toute donnée écrite dans le pool est resynchronisée sur
le périphérique nouvellement disponible. Notez que vous ne pouvez pas utiliser la mise en ligne
d'un périphérique pour remplacer un disque. Si vous mettez un périphérique hors ligne, le
remplacez, puis tentez de le mettre en ligne, son état continue à indiquer qu'il est défaillant.

Si vous tentez de mettre un périphérique défaillant en ligne, un message similaire au suivant
s'affiche :

# zpool online tank c1t0d0

warning: device ’c1t0d0’ onlined, but remains in faulted state

use ’zpool replace’ to replace devices that are no longer present

Vous pouvez également afficher les messages de disques erronés dans la console ou les messages
enregistrés dans le fichier /var/adm/messages. Par exemple :

SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Wed Sep 21 11:11:27 GMT 2011

PLATFORM: Sun-Fire-X4140, CSN: 0904QAD02C, HOSTNAME: tardis

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: d9e3469f-8d84-4a03-b8a3-d0beb178c017

DESC: A ZFS device failed. Refer to http://sun.com/msg/ZFS-8000-D3

for more information.

AUTO-RESPONSE: No automated response will occur.

IMPACT: Fault tolerance of the pool may be compromised.

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Pour obtenir des informations sur le remplacement d'un périphérique défaillant, reportez-vous
à la section “Réparation d'un périphérique manquant” à la page 287.

Vous pouvez utiliser la commande zpool online -e pour étendre un LUN. Par défaut, un LUN
ajouté à un pool n'est pas étendu à sa taille maximale, à moins que la propriété autoexpand du
pool ne soit activée. Vous pouvez étendre automatiquement le LUN à l'aide de la commande
zpool online -e, même si le LUN est déjà en ligne ou s'il est actuellement hors ligne. Par
exemple :

# zpool online -e tank c1t13d0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

75

Gestion de périphériques dans un pool de stockage ZFS

Effacement des erreurs de périphérique de pool de
stockage
Si un périphérique est mis hors ligne en raison d'une défaillance qui entraîne l'affichage
d'erreurs dans la sortie zpool status, la commande zpool clear permet d'effacer les nombres
d'erreurs.

Si elle est spécifiée sans argument, cette commande efface toutes les erreurs de périphérique
dans le pool. Par exemple :

# zpool clear tank

Si un ou plusieurs périphériques sont spécifiés, cette commande n'efface que les erreurs
associées aux périphériques spécifiés. Par exemple :

# zpool clear tank c1t0d0

Pour de plus amples informations sur l'effacement d'erreurs de zpool reportez-vous à la section
“Suppression des erreurs transitoires” à la page 291.

Remplacement de périphériques dans un pool de
stockage
Vous pouvez remplacer un périphérique dans un pool de stockage à l'aide de la commande
zpool replace.

Pour remplacer physiquement un périphérique par un autre, en conservant le même
emplacement dans le pool redondant, il vous suffit alors d'identifier le périphérique remplacé.
Sur certains matériels, ZFS reconnaît que le périphérique est un disque différent au même
emplacement. Par exemple, pour remplacer un disque défaillant (c1t1d0), supprimez-le, puis
ajoutez le disque de rechange au même emplacement en respectant la syntaxe suivante :

# zpool replace tank c1t1d0

Si vous remplacez un périphérique dans un pool de stockage par un disque dans un autre
emplacement physique, vous devez spécifier les deux périphériques. Par exemple :

# zpool replace tank c1t1d0 c1t2d0

Si vous remplacez un disque dans le pool racine ZFS, reportez-vous à la section “Remplacement
d'un disque dans un pool racine ZFS” à la page 119.

76

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

Voici les étapes de base pour remplacer un disque :
1. Le cas échéant, mettez le disque hors ligne à l'aide de la commande zpool offline.
2. Enlevez le disque à remplacer.
3.
4. Exécutez la commande zpool replace. Par exemple :

Insérez le disque de remplacement.

# zpool replace tank c1t1d0

5. Remettez le disque en ligne à l'aide de la commande zpool online.
6.

Informez FMA du remplacement du périphérique.

# fmadm faulty

# fmadm repair fmri

Sur certains systèmes avec des disques SATA, vous devez annuler la configuration d'un disque
avant de pouvoir mettre hors ligne. Si vous remplacez un disque dans le même emplacement sur
ce système, vous pouvez exécuter la commande zpool replace comme décrit dans le premier
exemple de cette section.

Pour consulter un exemple de remplacement d'un disque SATA, reportez-vous à
l'Exemple 11–1.

Lorsque vous remplacez des périphériques dans un pool de stockage ZFS, veuillez prendre en
compte les points suivants :

■

Si vous définissez la propriété de pool autoreplace sur on, tout nouveau périphérique
détecté au même emplacement physique qu'un périphérique appartenant précédemment au
pool est automatiquement formaté et remplacé. Lorsque cette propriété est activée, vous
n'êtes pas obligé d'utiliser la commande zpool replace. Cette fonction n'est pas disponible
sur tous les types de matériel.

■ L'état de pool de stockage REMOVED est fourni en cas de retrait physique du périphérique ou

d'un disque hot spare alors que le système est en cours d'exécution. Si un disque hot spare est
disponible, il remplace le périphérique retiré.
Si un périphérique est retiré, puis réinséré, il est mis en ligne. Si un disque hot spare est
activé lors de la réinsertion du périphérique, le disque hot spare est retiré une fois l'opération
en ligne terminée.

■

■ La détection automatique du retrait ou de l'insertion de périphériques dépend du matériel

utilisé. Il est possible qu'elle ne soit pas prise en charge sur certaines plates-formes. Par
exemple, les périphériques USB sont configurés automatiquement après insertion. Il peut
être toutefois nécessaire d'utiliser la commande cfgadm -c configure pour configurer un
lecteur SATA.

■ Les disques hot spare sont consultés régulièrement afin de vérifier qu'ils sont en ligne et

disponibles.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

77

Gestion de périphériques dans un pool de stockage ZFS

■ La taille du périphérique de remplacement doit être égale ou supérieure au disque le plus

petit d'une configuration RAID-Z ou mise en miroir.

■ Lorsqu'un périphérique de remplacement dont la taille est supérieure à la taille du

périphérique qu'il remplace est ajouté à un pool, ce dernier n'est pas automatiquement
étendu à sa taille maximale. La valeur de la propriété autoexpand du pool détermine si un
LUN de remplacement est étendu à sa taille maximale lorsque le disque est ajouté au pool.
Par défaut, la propriété autoexpand est désactivée. Vous pouvez activer cette propriété pour
augmenter la taille du LUN avant ou après avoir ajouté le plus grand LUN au pool.
Dans l'exemple suivant, deux disques de 16 Go d'un pool mis en miroir sont remplacés par
deux disques de 72 Go. La propriété autoexpand est activée après le remplacement du
disque pour étendre le disque à sa taille maximale.

# zpool create pool mirror c1t16d0 c1t17d0

# zpool status

pool: pool

state: ONLINE

scrub: none requested

config:

NAME

pool

ONLINE

mirror

ONLINE

c1t16d0 ONLINE

c1t17d0 ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

zpool list pool

NAME

SIZE

ALLOC FREE

CAP HEALTH ALTROOT

pool 16.8G 76.5K 16.7G

0% ONLINE -

# zpool replace pool c1t16d0 c1t1d0

# zpool replace pool c1t17d0 c1t2d0

# zpool list pool

NAME

SIZE

ALLOC FREE

CAP HEALTH ALTROOT

pool 16.8G 88.5K 16.7G

0% ONLINE -

# zpool set autoexpand=on pool

# zpool list pool

NAME

SIZE

ALLOC FREE

CAP HEALTH ALTROOT

pool 68.2G

117K 68.2G

0% ONLINE -

■ Le remplacement d'un grand nombre de disques dans un pool volumineux prend du temps,

en raison de la resynchronisation des données sur les nouveaux disques. En outre, il peut
s'avérer utile d'exécuter la commande zpool scrub entre chaque remplacement de disque
afin de garantir le fonctionnement des périphériques de remplacement et l'exactitude des
données écrites.
Si un disque défectueux a été remplacé automatiquement par un disque hot spare, il se peut
que vous deviez déconnecter le disque hot spare une fois le disque défectueux remplacé.
Vous pouvez utiliser la commande zpool detach pour déconnecter le disque hot spare d'un
pool RAID-Z ou mis en miroir. Pour plus d'informations sur la déconnexion d'un disque
hot spare, reportez-vous à la section “Activation et désactivation de disque hot spare dans le
pool de stockage” à la page 80.

■

78

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

Pour plus d'informations sur le remplacement de périphériques, reportez-vous aux sections
“Réparation d'un périphérique manquant” à la page 287 et “Remplacement ou réparation d'un
périphérique endommagé ” à la page 289.

Désignation des disques hot spare dans le pool de
stockage
La fonction de disque hot spare permet d'identifier les disques utilisables pour remplacer un
périphérique défaillant dans un pool de stockage. Un périphérique désigné en tant que disque
hot spare n'est pas actif dans un pool, mais en cas d'échec d'un périphérique actif du pool, le
disque hot spare le remplace automatiquement

Pour désigner des périphériques en tant que disques hot spare, vous avez le choix entre les
méthodes suivantes :

■

■

lors de la création du pool à l'aide de la commande zpool create ;
après la création du pool à l'aide de la commande zpool create.

L'exemple suivant explique comment désigner des périphériques en tant que disques hot spare
lorsque le pool est créé :

# zpool create trinity mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0

# zpool status trinity

pool: trinity

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

trinity

ONLINE

mirror-0 ONLINE

c1t1d0 ONLINE

c2t1d0 ONLINE

spares

c1t2d0

AVAIL

c2t2d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

L'exemple suivant explique comment désigner des disques hot spare en les ajoutant à un pool
après la création du pool :

# zpool add neo spare c5t3d0 c6t3d0

# zpool status neo

pool: neo

state: ONLINE

scrub: none requested

config:

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

79

Gestion de périphériques dans un pool de stockage ZFS

NAME

neo

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c3t3d0 ONLINE

c4t3d0 ONLINE

spares

c5t3d0

AVAIL

c6t3d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Vous pouvez supprimer les disques hot spare d'un pool de stockage à l'aide de la commande
zpool remove. Par exemple :

# zpool remove zeepool c2t3d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0 ONLINE

c1t1d0 ONLINE

c2t1d0 ONLINE

spares

c1t3d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Vous ne pouvez pas supprimer un disque hot spare si ce dernier est actuellement utilisé par un
pool de stockage.

Lorsque vous utilisez des disques hot spare ZFS, veuillez prendre en compte les points suivants :
■ Actuellement, la commande zpool remove ne peut être utilisée que pour la suppression de

disques hot spare, de périphériques de journalisation et de périphériques de cache.

■ Pour ajouter un disque en tant que disque hot spare, la taille du disque hot spare doit être

égale ou supérieure à la taille du plus grand disque du pool. L'ajout d'un disque de rechange
plus petit dans le pool est autorisé. Toutefois, lorsque le plus petit disque de rechange est
activé, automatiquement ou via la commande zpool replace, l'opération échoue et une
erreur du type suivant s'affiche :
cannot replace disk3 with disk4: device is too small

Activation et désactivation de disque hot spare dans le pool de
stockage
Les disques hot spare s'activent des façons suivantes :

80

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

■ Remplacement manuel : remplacez un périphérique défaillant dans un pool de stockage par

un disque hot spare à l'aide de la commande zpool replace.

■ Remplacement automatique : en cas de détection d'une défaillance, un agent FMA examine
le pool pour déterminer s'il y a des disques hot spare. Dans ce cas, le périphérique défaillant
est remplacé par un disque hot spare disponible.
En cas de défaillance d'un disque hot spare en cours d'utilisation, l'agent FMA sépare le
disque hot spare et annule ainsi le remplacement. L'agent tente ensuite de remplacer le
périphérique par un autre disque hot spare s'il y en a un de disponible. Cette fonction est
actuellement limitée par le fait que le moteur de diagnostics ZFS ne génère des défaillances
qu'en cas de disparition d'un périphérique du système.
Si vous remplacez physiquement un périphérique défaillant par un disque spare actif, vous
pouvez réactiver le périphérique original en utilisant la commande zpool detach pour
déconnecter le disque spare. Si vous définissez la propriété de pool autoreplace sur on, le
disque spare est automatiquement déconnecté et retourne au pool de disques spare lorsque
le nouveau périphérique est inséré et que l'opération en ligne s'achève.

Tout périphérique défaillant est remplacé automatiquement si un disque hot spare est
disponible. Par exemple :

# zpool status -x

pool: zeepool

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: resilver completed after 0h0m with 0 errors on Mon Jan 11 10:20:35 2010

config:

NAME

STATE

READ WRITE CKSUM

zeepool

DEGRADED

mirror-0

DEGRADED

c1t2d0

ONLINE

spare-1

DEGRADED

c2t1d0 UNAVAIL

c2t3d0 ONLINE

spares

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 cannot open

0 88.5K resilvered

c2t3d0

INUSE

currently in use

errors: No known data errors

Vous pouvez actuellement désactiver un disque hot spare en recourant à l'une des méthodes
suivantes :

■

Suppression du disque hot spare du pool de stockage.

■ Déconnexion du disque hot spare après avoir remplacé physiquement un disque

défectueux. Reportez-vous à l'Exemple 4–8.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

81

Gestion de périphériques dans un pool de stockage ZFS

■ Remplacement temporaire ou permanent par un autre disque hot spare. Reportez-vous à

l'Exemple 4–9.

EXEMPLE 4–8 Déconnexion d'un disque hot spare après le remplacement du disque défectueux
Dans cet exemple, le disque défectueux (c2t1d0) est remplacé physiquement et ZFS est averti à
l'aide de la commande zpool replace.

# zpool replace zeepool c2t1d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Wed Jan 20 10:08:44 2010

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0

ONLINE

c1t2d0

ONLINE

spare-1

ONLINE

c2t3d0 ONLINE

c2t1d0 ONLINE

spares

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 90K resilvered

0

c2t3d0

INUSE

currently in use

errors: No known data errors

Vous pouvez ensuite utiliser la commande zpool detach pour retourner le disque hot spare au
pool de disques hot spare. Par exemple :

# zpool detach zeepool c2t3d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed with 0 errors on Wed Jan 20 10:08:44 2010

config:

NAME

zeepool

mirror

c1t2d0

c2t1d0

spares

STATE

READ WRITE CKSUM

ONLINE

ONLINE

ONLINE

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

c2t3d0

AVAIL

errors: No known data errors

EXEMPLE 4–9 Déconnexion d'un disque défectueux et utilisation d'un disque hot spare
Si vous souhaitez remplacer un disque défectueux par un swap temporaire ou permanent dans
le disque hot spare qui le remplace actuellement, vous devez déconnecter le disque d'origine
(défectueux). Si le disque défectueux finit par être remplacé, vous pouvez l'ajouter de nouveau
au groupe de stockage en tant que disque hot spare. Par exemple :

82

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–9 Déconnexion d'un disque défectueux et utilisation d'un disque hot spare

(Suite)

# zpool status zeepool

pool: zeepool

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: resilver in progress for 0h0m, 70.47% done, 0h0m to go

config:

NAME

STATE

READ WRITE CKSUM

zeepool

DEGRADED

mirror-0

DEGRADED

c1t2d0

ONLINE

spare-1

DEGRADED

c2t1d0 UNAVAIL

c2t3d0 ONLINE

spares

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 cannot open

0 70.5M resilvered

c2t3d0

INUSE

currently in use

errors: No known data errors

# zpool detach zeepool c2t1d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Wed Jan 20 13:46:46 2010

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0 ONLINE

c1t2d0 ONLINE

c2t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0 70.5M resilvered

errors: No known data errors

(Original failed disk c2t1d0 is physically replaced)

# zpool add zeepool spare c2t1d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Wed Jan 20 13:48:46 2010

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror-0 ONLINE

c1t2d0 ONLINE

c2t3d0 ONLINE

spares

c2t1d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0 70.5M resilvered

errors: No known data errors

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

83

Gestion des propriétés de pool de stockage ZFS

Gestion des propriétés de pool de stockage ZFS

Vous pouvez vous servir de la commande zpool get pour afficher des informations sur les
propriétés du pool. Par exemple :

# zpool get all zeepool

NAME

PROPERTY

zeepool size

zeepool capacity

zeepool altroot

VALUE

33.8G

0%

-

SOURCE

-

-

default

zeepool health

ONLINE

-

zeepool guid

8588873752016230819 default

zeepool version

zeepool bootfs

zeepool delegation

31

-

on

zeepool autoreplace

off

zeepool cachefile

-

default

default

default

default

default

zeepool failmode

wait

default

zeepool listsnapshots off

zeepool autoexpand

off

zeepool dedupditto

0

zeepool dedupratio

1.00x

zeepool free

zeepool allocated

zeepool readonly

33.7G

104K

off

default

default

default

-

-

-

-

Les propriétés d'un pool de stockage peuvent être définies à l'aide de la commande zpool set.
Par exemple :

# zpool set autoreplace=on zeepool

# zpool get autoreplace zeepool

NAME

PROPERTY

VALUE

SOURCE

zeepool autoreplace on

local

Si vous tentez de définir une propriété de pool sur un pool à 100% de sa capacité, un message
semblable à celui-ci s'affiche :

# zpool set autoreplace=on tank

cannot set property for ’tank’: out of space

Pour plus d'informations sur la prévention des problèmes de capacité d'espace des pools,
reportez-vous au Chapitre 13, “Pratiques recommandées pour Oracle Solaris ZFS”.

TABLEAU 4–1 Description des propriétés d'un pool ZFS

Nom de propriété

Type

Valeur par
défaut

Description

allocated

Chaîne

SO

Valeur en lecture seule permettant d'identifier l'espace de
stockage disponible physiquement alloué dans le pool.

84

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

TABLEAU 4–1 Description des propriétés d'un pool ZFS

(Suite)

Nom de propriété

Type

Valeur par
défaut

Description

Gestion des propriétés de pool de stockage ZFS

altroot

Chaîne

off

autoreplace

Booléen

off

bootfs

Booléen

SO

cachefile

Chaîne

SO

capacity

Valeur
numérique

SO

dedupditto

Chaîne

SO

dedupratio

Chaîne

SO

delegation

Booléen

on

Identifie un répertoire racine alternatif. S'il est défini, ce
répertoire est ajouté au début de tout point de montage
figurant dans le pool. Cette propriété peut être utilisée lors de
l'examen d'un pool inconnu si vous ne pouvez pas faire
confiance aux points de montage ou dans un environnement
d'initialisation alternatif dans lequel les chemins types sont
incorrects.

Contrôle le remplacement automatique d'un périphérique. Si
la valeur off est définie, le remplacement du périphérique doit
être initié à l'aide de la commande zpool replace. Si la valeur
est définie sur on, tout nouveau périphérique se trouvant au
même emplacement physique qu'un périphérique qui
appartenait au pool est automatiquement formaté et remplacé.
L'abréviation de la propriété est la suivante : replace.

Identifie le système de fichiers d'initialisation par défaut du
pool racine. Cette propriété est généralement définie par les
programmes d'installation.

Contrôle l'emplacement de la mise en cache du pool. Tous les
pools du cache sont importés automatiquement à
l'initialisation du système. Toutefois, dans les environnements
d'installation et de clustering, il peut s'avérer nécessaire de
placer ces informations en cache à un autre endroit afin
d'éviter l'importation automatique des pools. Vous pouvez
définir cette propriété pour mettre en cache les informations
de configuration du pool dans un autre emplacement. Ces
informations peuvent être importées ultérieurement à l'aide de
la commande zpool import -c. Pour la plupart des
configurations ZFS, cette propriété n'est pas utilisée.

Valeur en lecture seule identifiant le pourcentage d'espace
utilisé du pool.
L'abréviation de la propriété est cap.

Définit un seuil ; si le nombre de références pour un bloc
dédupliqué dépasse ce seuil, une autre copie ditto du bloc est
automatiquement stockée

Ratio de suppression des doublons en lecture seule obtenu
pour un pool, exprimé sous la forme d'un multiplicateur

Contrôle si un utilisateur non privilégié peut bénéficier des
autorisations d'accès définies pour un système de fichiers. Pour
plus d'informations, reportez-vous au Chapitre 9,
“Administration déléguée de ZFS dans Oracle Solaris”.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

85

Gestion des propriétés de pool de stockage ZFS

TABLEAU 4–1 Description des propriétés d'un pool ZFS

(Suite)

Nom de propriété

Type

Valeur par
défaut

Description

failmode

Chaîne

wait

free

guid

health

Chaîne

Chaîne

Chaîne

SO

SO

SO

listsnapshots Chaîne

off

Contrôle le comportement du système en cas de panne grave
d'un pool. Cette condition résulte habituellement d'une perte
de connectivité aux périphériques de stockage sous-jacents ou
d'une panne de tous les périphériques au sein du pool. Le
comportement d'un événement de ce type est déterminé par
l'une des valeurs suivantes :

■

■

■

wait : bloque toutes les demandes d'E/S vers le pool
jusqu'au rétablissement de la connectivité et jusqu'à
l'effacement des erreurs à l'aide de la commande zpool
clear. Dans cet état, les opérations d'E/S du pool sont
bloquées mais les opérations de lecture peuvent aboutir.
Un pool renvoie l'état wait jusqu'à ce que le problème du
périphérique soit résolu.
continue : renvoie une erreur EIO à toute nouvelle
demande d'E/S d'écriture, mais autorise les lectures de tout
autre périphérique fonctionnel. Toute demande d'écriture
devant encore être validée sur disque est bloquée. Une fois
le périphérique reconnecté ou remplacé, les erreurs
doivent être effacées à l'aide de la commande zpool
clear.
panic : affiche un message sur la console et génère un
vidage sur incident du système.

Valeur en lecture seule identifiant le nombre de blocs non
alloués au sein du pool.

Propriété en lecture seule identifiant l'identificateur unique du
pool.

Propriété en lecture seule indiquant l'état actuel du pool ; les
valeurs possibles sont : ONLINE, DEGRADED, FAULTED,
OFFLINE, REMOVED ou UNAVAIL.

Détermine si les informations sur les instantanés associées à ce
groupe s'affichent avec la commande zfs list. Si cette
propriété est désactivée, les informations sur les instantanés
peuvent être affichées à l'aide de la commande zfs list
- t snapshot.

86

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

TABLEAU 4–1 Description des propriétés d'un pool ZFS

(Suite)

Nom de propriété

Type

Valeur par
défaut

Description

readonly

Booléen

off

size

version

Valeur
numérique

Valeur
numérique

SO

SO

Indique si un pool peut être modifié. Cette propriété est
uniquement activée lorsqu'un pool a été importé en mode
lecture seule. Lorsqu'elle est activée, les données synchrones
éventuellement présentes dans le journal d'intention ne sont
pas accessibles tant que le pool n'a pas réimporté en mode
lecture-écriture.

Propriété en lecture seule identifiant la taille totale du pool de
stockage.

Identifie la version actuelle sur disque du pool. La méthode
recommandée de mise à jour des pools consiste à utiliser la
commande zpool upgrade, bien que cette propriété puisse
être utilisée lorsqu'une version spécifique est requise pour des
raisons de compatibilité ascendante. Cette propriété peut être
définie sur tout numéro compris entre 1 et la version actuelle
signalée par la commande zpool upgrade -v.

Requête d'état de pool de stockage ZFS

La commande zpool list offre plusieurs moyens d'effectuer des demandes sur l'état du pool.
Les informations disponibles se répartissent généralement en trois catégories : informations
d'utilisation de base, statistiques d'E/S et état de maintenance. Les trois types d'information sur
un pool de stockage sont traités dans cette section.
■ “Affichage des informations des pools de stockage ZFS” à la page 87
■ “Visualisation des statistiques d'E/S des pools de stockage ZFS ” à la page 92
■ “Détermination de l'état de maintenance des pools de stockage ZFS” à la page 95

Affichage des informations des pools de stockage ZFS
La commande zpool list permet d'afficher les informations de base relatives aux pools.

Affichage des informations concernant tous les pools de stockage ou
un pool spécifique
En l'absence d'arguments, la commande zpool list affiche les informations suivantes pour
tous les pools du système :

# zpool list

NAME

tank

dozer

SIZE

ALLOC

FREE

CAP HEALTH

ALTROOT

80.0G

22.3G

47.7G

28% ONLINE

1.2T

384G

816G

32% ONLINE

-

-

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

87

Requête d'état de pool de stockage ZFS

La sortie de cette commande affiche les informations suivantes :

NAME

SIZE

ALLOC

FREE

CAP (CAPACITY)

HEALTH

ALTROOT

Nom du pool.
Taille totale du pool, égale à la somme de la taille de tous les périphériques
virtuels de niveau supérieur.
Quantité d'espace physique utilisée, c'est-à-dire allouée à tous les jeux de
données et métadonnées internes. Notez que cette quantité d'espace disque
est différente de celle qui est rapportée au niveau des systèmes de fichiers.

Pour de plus amples informations sur la détermination de l'espace de
systèmes de fichiers disponible, reportez-vous à la section
“Comptabilisation de l'espace disque ZFS” à la page 40.
Quantité d'espace disponible, c'est-à-dire non allouée dans le pool.
Quantité d'espace disque utilisée, exprimée en tant que pourcentage de
l'espace disque total.
Etat de maintenance actuel du pool.

Pour de plus amples informations sur la maintenance des pools,
reportez-vous à la section “Détermination de l'état de maintenance des
pools de stockage ZFS” à la page 95.
Racine de remplacement, le cas échéant.

Pour de plus amples informations sur les pools racine de remplacement,
reportez-vous à la section “Utilisation de pools racine ZFS de
remplacement” à la page 274.

Vous pouvez également rassembler des statistiques pour un pool donné en spécifiant le nom du
pool. Par exemple :

# zpool list tank

NAME

tank

SIZE

ALLOC

FREE

CAP

HEALTH

ALTROOT

80.0G

22.3G

47.7G

28% ONLINE

-

Vous pouvez utiliser l'intervalle zpool list et les options de comptage pour rassembler les
statistiques d'une période précise. En outre, vous pouvez afficher un horodatage en utilisant
l'option -T. Par exemple :

# zpool list -T d 3 2

Tue Nov 2 10:36:11 MDT 2010

NAME

SIZE ALLOC

FREE

CAP DEDUP HEALTH ALTROOT

pool

33.8G 83.5K 33.7G

0% 1.00x ONLINE -

rpool 33.8G 12.2G 21.5G

36% 1.00x ONLINE -

Tue Nov 2 10:36:14 MDT 2010

pool

33.8G 83.5K 33.7G

0% 1.00x ONLINE -

rpool 33.8G 12.2G 21.5G

36% 1.00x ONLINE -

88

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

Affichage des périphériques de pool par emplacement physique
Vous pouvez utiliser l'option zpool status -l pour afficher des informations sur
l'emplacement physique des périphériques de pool. Les informations sur l'emplacement
physique sont utiles si vous devez supprimer ou remplacer un disque physiquement.

En outre, vous pouvez utiliser la commande fmadm add-alias pour inclure un nom d'alias de
disque qui facilite l'identification de l'emplacement physique des disques dans votre
environnement. Par exemple :

# fmadm add-alias SUN-Storage-J4400.1002QCQ015 Lab10Rack5...

# zpool status -l tank

pool: tank

state: ONLINE

scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 27 08:24:17 2011

config:

NAME

tank

mirror-0

STATE

READ WRITE CKSUM

ONLINE

ONLINE

/dev/chassis/Lab10Rack5.../DISK_02/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_20/disk ONLINE

mirror-1

ONLINE

/dev/chassis/Lab10Rack5.../DISK_22/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_14/disk ONLINE

mirror-2

ONLINE

/dev/chassis/Lab10Rack5.../DISK_10/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_16/disk ONLINE

mirror-3

ONLINE

/dev/chassis/Lab10Rack5.../DISK_01/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_21/disk ONLINE

mirror-4

ONLINE

/dev/chassis/Lab10Rack5.../DISK_23/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_15/disk ONLINE

mirror-5

ONLINE

/dev/chassis/Lab10Rack5.../DISK_09/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_04/disk ONLINE

mirror-6

ONLINE

/dev/chassis/Lab10Rack5.../DISK_08/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_05/disk ONLINE

mirror-7

ONLINE

/dev/chassis/Lab10Rack5.../DISK_07/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_11/disk ONLINE

mirror-8

ONLINE

/dev/chassis/Lab10Rack5.../DISK_06/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_19/disk ONLINE

mirror-9

ONLINE

/dev/chassis/Lab10Rack5.../DISK_00/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_13/disk ONLINE

mirror-10

ONLINE

/dev/chassis/Lab10Rack5.../DISK_03/disk ONLINE

/dev/chassis/Lab10Rack5.../DISK_18/disk ONLINE

spares

/dev/chassis/Lab10Rack5.../DISK_17/disk

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

89

Requête d'état de pool de stockage ZFS

/dev/chassis/Lab10Rack5.../DISK_12/disk

AVAIL

errors: No known data errors

Affichage de statistiques spécifiques à un pool de stockage
L'option -o permet d'effectuer une demande concernant des statistiques spécifiques. Cette
option permet de générer des rapports personnalisés ou de générer rapidement une liste
d'informations pertinentes. Par exemple, pour ne répertorier que le nom et la taille de chaque
pool, respectez la syntaxe suivante :

# zpool list -o name,size

NAME

tank

dozer

SIZE

80.0G

1.2T

Les noms de colonne correspondent aux propriétés répertoriées à la section “Affichage des
informations concernant tous les pools de stockage ou un pool spécifique” à la page 87.

Script de sortie du pool de stockage ZFS
La sortie par défaut de la commande zpool list a été conçue pour améliorer la lisibilité. Elle
n'est pas facile à utiliser en tant que partie d'un script shell. Pour faciliter l'utilisation de la
commande dans le cadre de la programmation, l'option -H permet de supprimer les en-têtes de
colonnes et de séparer les champs par des onglets plutôt que par des espaces. La syntaxe
suivante permet d'obtenir la liste des noms de pool du système :

# zpool list -Ho name

tank

dozer

Voici un autre exemple :

# zpool list -H -o name,size

tank

80.0G

dozer 1.2T

Affichage de l'historique des commandes du pool de stockage ZFS
ZFS consigne automatiquement les commandes zfs et zpool ayant pour effet de modifier les
informations d'état du pool. Cette information peut être affichée à l'aide de la commande zpool
history.

Par exemple, la syntaxe suivante affiche la sortie de la commande pour le pool racine :

# zpool history

History for ’rpool’:

2010-05-11.10:18:54 zpool create -f -o failmode=continue -R /a -m legacy -o

cachefile=/tmp/root/etc/zfs/zpool.cache rpool mirror c1t0d0s0 c1t1d0s0

90

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

2010-05-11.10:18:55 zfs set canmount=noauto rpool

2010-05-11.10:18:55 zfs set mountpoint=/rpool rpool

2010-05-11.10:18:56 zfs create -o mountpoint=legacy rpool/ROOT

2010-05-11.10:18:57 zfs create -b 8192 -V 2048m rpool/swap

2010-05-11.10:18:58 zfs create -b 131072 -V 1536m rpool/dump

2010-05-11.10:19:01 zfs create -o canmount=noauto rpool/ROOT/zfsBE

2010-05-11.10:19:02 zpool set bootfs=rpool/ROOT/zfsBE rpool

2010-05-11.10:19:02 zfs set mountpoint=/ rpool/ROOT/zfsBE

2010-05-11.10:19:03 zfs set canmount=on rpool

2010-05-11.10:19:04 zfs create -o mountpoint=/export rpool/export

2010-05-11.10:19:05 zfs create rpool/export/home

2010-05-11.11:11:10 zpool set bootfs=rpool rpool

2010-05-11.11:11:10 zpool set bootfs=rpool/ROOT/zfsBE rpool

Vous pouvez utiliser une sortie similaire sur votre système pour identifier l'ensemble réel de
commandes ZFS exécutées pour résoudre les conditions d'erreur.

Les caractéristiques de l'historique sont les suivantes :
■ Le journal ne peut pas être désactivé.
■ Le journal est enregistré en permanence sur disque, c'est-à-dire qu'il est conservé d'une

réinitialisation système à une autre.

■ Le journal est implémenté en tant que tampon d'anneau. La taille minimale est de 128 Ko. La

taille maximale est de 32 Mo.

■ Pour des pools de taille inférieure, la taille maximale est plafonnée à 1 % de la taille du pool,

la valeur size étant déterminée lors de la création du pool.

■ Le journal ne nécessite aucune administration, ce qui signifie qu'il n'est pas nécessaire

d'ajuster la taille du journal ou de modifier son emplacement.

Pour identifier l'historique des commandes d'un pool de stockage spécifique, utilisez une
syntaxe similaire à la suivante :

# zpool history tank

History for ’tank’:

2011-05-27.13:10:43 zpool create tank mirror c8t1d0 c8t2d0

2011-06-01.12:05:23 zpool scrub tank

2011-06-13.16:26:07 zfs create tank/users

2011-06-13.16:26:27 zfs create tank/users/finance

2011-06-13.16:27:15 zfs set users:dept=finance tank/users/finance

Utilisez l'option -l pour afficher un format détaillé comprenant le nom d'utilisateur, le nom de
l'hôte et la zone dans laquelle l'opération a été effectuée. Par exemple :

# zpool history -l tank

2011-05-27.13:10:43 zpool create tank mirror c8t1d0 c8t2d0 [user root on neo:global]

2011-06-01.12:05:23 zpool scrub tank [user root on neo:global]

2011-06-13.16:26:07 zfs create tank/users [user root on neo:global]

2011-06-13.16:26:27 zfs create tank/users/finance [user root on neo:global]

2011-06-13.16:27:15 zfs set users:dept=finance tank/users/finance [user root ...]

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

91

Requête d'état de pool de stockage ZFS

L'option -i permet d'afficher des informations relatives aux événements internes utilisables
pour établir des diagnostics. Par exemple :

# zpool history -i tank

History for ’tank’:

2011-05-27.13:10:43 zpool create tank mirror c8t1d0 c8t2d0

2011-05-27.13:10:43 [internal pool create txg:5] pool spa 33; zfs spa 33; zpl 5;...

2011-05-31.15:02:39 [internal pool scrub done txg:11828] complete=1

2011-06-01.12:04:50 [internal pool scrub txg:14353] func=1 mintxg=0 maxtxg=14353

2011-06-01.12:05:23 zpool scrub tank

2011-06-13.16:26:06 [internal create txg:29879] dataset = 52

2011-06-13.16:26:07 zfs create tank/users

2011-06-13.16:26:07 [internal property set txg:29880] $share2=2 dataset = 52

2011-06-13.16:26:26 [internal create txg:29881] dataset = 59

2011-06-13.16:26:27 zfs create tank/users/finance

2011-06-13.16:26:27 [internal property set txg:29882] $share2=2 dataset = 59

2011-06-13.16:26:45 [internal property set txg:29883] users:dept=finance dataset = 59

2011-06-13.16:27:15 zfs set users:dept=finance tank/users/finance

Visualisation des statistiques d'E/S des pools de
stockage ZFS
La commande zpool iostat permet d'effectuer une demande de statistiques d'E/S pour un
pool ou des périphériques virtuels spécifiques. Cette commande est similaire à la commande
iostat. Elle permet d'afficher un instantané statique de toutes les activités d'E/S, ainsi que les
statistiques mises à jour pour chaque intervalle spécifié. Les statistiques suivantes sont
rapportées :

alloc capacity

free capacity

read operations

write operations

Capacité utilisée, c'est-à-dire quantité de données actuellement stockées
dans le pool ou le périphérique. Cette quantité diffère quelque peu de la
quantité d'espace disque disponible pour les systèmes de fichiers
effectifs en raison de détails d'implémentation interne.

Pour de plus amples informations sur la différence entre l'espace de
pool et l'espace de jeux de données, reportez-vous à la section
“Comptabilisation de l'espace disque ZFS” à la page 40.
Capacité disponible, c'est-à-dire quantité d'espace disque disponible
dans le pool ou le périphérique. Comme la statistique used, cette
quantité diffère légèrement de la quantité d'espace disque disponible
pour les jeux de données.
Nombre d'opérations de lecture d'E/S envoyées au pool ou au
périphérique, y compris les demandes de métadonnées.
Nombre d'opérations d'écriture d'E/S envoyées au pool ou au
périphérique.

92

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

read bandwidth

write bandwidth

Bande passante de toutes les opérations de lecture (métadonnées
incluses), exprimée en unités par seconde.
Bande passante de toutes les opérations d'écriture, exprimée en unités
par seconde.

Liste des statistiques d'E/S à l'échelle du pool
Sans options, la commande zpool iostat affiche les statistiques accumulées depuis
l'initialisation pour tous les pools du système. Par exemple :

# zpool iostat

capacity

operations

bandwidth

pool

alloc

free

read write

read write

---------- ----- ----- ----- ----- ----- -----

rpool

tank

6.05G 61.9G

31.3G 36.7G

0

4

0

1

786

107

296K 86.1K

---------- ----- ----- ----- ----- ----- -----

Comme ces statistiques sont cumulatives depuis l'initialisation, la bande passante peut sembler
basse si l'activité du pool est relativement faible. Vous pouvez effectuer une demande pour une
vue plus précise de l'utilisation actuelle de la bande passante en spécifiant un intervalle. Par
exemple :

# zpool iostat tank 2

capacity

operations

bandwidth

pool

alloc

free

read write

read write

---------- ----- ----- ----- ----- ----- -----

tank

tank

tank

tank

18.5G 49.5G

18.5G 49.5G

18.5G 49.5G

18.8G 49.2G

0

0

0

0

187

464

457

435

0 23.3M

0 57.7M

0 56.6M

0 51.3M

Dans l'exemple ci-dessus, la commande affiche les statistiques d'utilisation pour le pool tank
toutes les deux secondes, jusqu'à ce que vous saisissiez Ctrl-C. Vous pouvez également spécifier
un argument count supplémentaire pour entraîner l'interruption de la commande une fois le
nombre spécifié d'itérations atteint.

Par exemple, zpool iostat 2 3 imprimerait un résumé toutes les deux secondes pour trois
itérations, pendant six secondes. S'il n'y a qu'un pool unique, les statistiques s'affichent sur des
lignes consécutives. S'il existe plusieurs pools, une ligne pointillée supplémentaire délimite
chaque itération pour fournir une séparation visuelle.

Liste des statistiques d'E/S des périphériques virtuels
Outre les statistiques d'E/S à l'échelle du pool, la commande zpool iostat permet d'afficher des
statistiques d'E/S pour des périphériques virtuels. Ainsi, vous pouvez identifier les
périphériques anormalement lents ou consulter la répartition d'E/S générées par ZFS. Pour

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

93

Requête d'état de pool de stockage ZFS

effectuer une demande relative à la disposition complète des périphériques virtuels, ainsi que
l'ensemble des statistiques d'E/S, utilisez la commande zpool iostat -v. Par exemple :

# zpool iostat -v

capacity

operations

bandwidth

pool

alloc

free

read write

read write

---------- ----- ----- ----- ----- ----- -----

rpool

6.05G 61.9G

mirror

6.05G 61.9G

c1t0d0s0

c1t1d0s0

-

-

-

-

0

0

0

0

0

0

0

0

785

785

578

595

107

107

109

109

---------- ----- ----- ----- ----- ----- -----

tank

36.5G 31.5G

4

1

295K

146K

mirror

36.5G 31.5G

126

45 8.13M 4.01M

c1t2d0

c1t3d0

-

-

-

-

0

0

3

3

100K

386K

104K

386K

---------- ----- ----- ----- ----- ----- -----

Lors de la visualisation des statistiques d'E/S des périphériques virtuels, vous devez prendre en
compte deux points importants :
■ Dans un premier temps, les statistiques d'utilisation de l'espace disque sont uniquement

disponibles pour les périphériques virtuels de niveau supérieur. L'allocation d'espace disque
entre les périphériques virtuels RAID-Z et en miroir est spécifique à l'implémentation et ne
s'exprime pas facilement en tant que chiffre unique.

■ De plus, il est possible que les chiffres s'additionnent de façon inattendue. En particulier, les

opérations au sein des périphériques RAID-Z et mis en miroir ne sont pas parfaitement
identiques. Cette différence se remarque particulièrement après la création d'un pool, car
une quantité significative d'E/S est réalisée directement sur les disques dans le cadre de la
création du pool, qui n'est pas comptabilisée au niveau du miroir. Ces chiffres s'égalisent
graduellement dans le temps. Cependant, les périphériques hors ligne, ne répondant pas, ou
en panne peuvent également affecter cette symétrie.

Vous pouvez utiliser les mêmes options (interval et count) lorsque vous étudiez les statistiques
de périphériques virtuels.

En outre, vous pouvez afficher des informations sur l'emplacement physique des périphériques
virtuels du pool. Par exemple :

# zpool iostat -lv

capacity

operations

bandwidth

pool

alloc

free

read write

read write

---------- ----- ----- ----- ----- ----- -----

export

2.39T 2.14T

mirror

490G

438G

13

2

27 42.7K

300K

5 8.53K 60.3K

/dev/chassis/lab10rack15/SCSI_Device__2/disk

/dev/chassis/lab10rack15/SCSI_Device__3/disk

mirror

490G

438G

2

5 8.62K 59.9K

/dev/chassis/lab10rack15/SCSI_Device__4/disk

/dev/chassis/lab10rack15/SCSI_Device__5/disk

mirror

490G

438G

2

5 8.60K 60.2K

-

-

-

-

-

-

-

-

1

1

1

1

0

0

0

0

4.47K 60.3K

4.45K 60.3K

4.52K 59.9K

4.48K 59.9K

94

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

/dev/chassis/lab10rack15/SCSI_Device__6/disk

/dev/chassis/lab10rack15/SCSI_Device__7/disk

mirror

490G

438G

2

5 8.47K 60.1K

/dev/chassis/lab10rack15/SCSI_Device__8/disk

/dev/chassis/lab10rack15/SCSI_Device__9/disk

-

-

-

-

-

-

-

-

1

1

1

1

0

0

0

0

4.50K 60.2K

4.49K 60.2K

4.42K 60.1K

4.43K 60.1K

.

.

.

Détermination de l'état de maintenance des pools de
stockage ZFS
ZFS offre une méthode intégrée pour examiner la maintenance des pools et des périphériques.
La maintenance d'un pool se détermine par l'état de l'ensemble de ses périphériques. La
commande zpool status permet d'afficher ces informations d'état. En outre, les défaillances
potentielles des pools et des périphériques sont rapportées par la commande fmd, s'affichent
dans la console système et sont consignées dans le fichier /var/adm/messages.

Cette section décrit les méthodes permettant de déterminer la maintenance des pools et des
périphériques. Ce chapitre n'aborde cependant pas les méthodes de réparation ou de
récupération de pools en mauvais état de maintenance. Pour plus d'informations sur le
dépannage et la récupération des données, reportez-vous au Chapitre 11, “Dépannage
d'Oracle Solaris ZFS et récupération de pool”.

Chaque périphérique peut se trouver dans l'un des états suivants :

ONLINE

DEGRADED

FAULTED

OFFLINE

Le périphérique ou le périphérique virtuel fonctionne normalement. Même si
certaines erreurs transitoires peuvent encore survenir, le périphérique fonctionne
correctement.
Le périphérique virtuel a connu un panne. Toutefois, il continue de fonctionner.
Cet état est le plus commun lorsqu'un miroir ou un périphérique RAID-Z a
perdu un ou plusieurs périphériques le constituant. La tolérance de pannes du
pool peut être compromise dans la mesure où une défaillance ultérieure d'un
autre périphérique peut être impossible à résoudre.
Le périphérique ou le périphérique virtuel est totalement inaccessible. Cet état
indique en règle générale une défaillance totale du périphérique, de telle façon
que ZFS est incapable d'y envoyer des données ou d'en recevoir de lui. Si un
périphérique virtuel de niveau supérieur se trouve dans cet état, le pool est
totalement inaccessible.
Le périphérique a été mis hors ligne explicitement par l'administrateur.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

95

Requête d'état de pool de stockage ZFS

UNAVAIL

REMOVED

L'ouverture du périphérique ou du périphérique virtuel est impossible. Dans
certains cas, les pools avec des périphériques en état UNAVAIL s'affichent en mode
DEGRADED. Si un périphérique de niveau supérieur est en état UNAVAIL, aucun
élément du pool n'est accessible.
Le périphérique a été retiré alors que le système était en cours d'exécution. La
détection du retrait d'un périphérique dépend du matériel et n'est pas pris en
charge sur toutes les plates-formes.

La maintenance d'un pool est déterminée à partir de celle de l'ensemble de ses périphériques
virtuels. Si l'état de tous les périphériques virtuels est ONLINE, l'état du pool est également
ONLINE. Si l'état d'un des périphériques virtuels est DEGRADED ou UNAVAIL, l'état du pool est
également DEGRADED. Si l'état d'un des périphériques virtuels est FAULTED ou OFFLINE, l'état du
pool est également FAULTED. Un pool renvoyant l'état FAULTED est totalement inaccessible.
Aucune donnée ne peut être récupérée tant que les périphériques nécessaires n'ont pas été
connectés ou réparés. Un pool renvoyant l'état DEGRADED continue à être exécuté. Cependant, il
se peut que vous ne puissiez pas atteindre le même niveau de redondance ou de capacité de
données que s'il se trouvait en ligne.

La commande zpool status fournit également des informations détaillées sur les opérations
de réargenture et de nettoyage.
■ Rapport de progression de la réargenture. Par exemple :

scan: resilver in progress since Thu May 26 11:26:32 2011

1.26G scanned out of 2.40G at 6.15M/s, 0h3m to go

1.26G resilvered, 56.3% done

■ Rapport de progression du nettoyage. Par exemple :

scan: scrub in progress since Fri May 27 08:24:17 2011

18.0M scanned out of 2.35G at 8.99M/s, 0h4m to go

0 repaired, 0.75% done

■ Message de fin de la réargenture. Par exemple :

scan: resilvered 2.34G in 1h2m with 0 errors on Thu May 26 11:56:40 2011

■ Message de fin du nettoyage. Par exemple :

scan: scrub repaired 512B in 1h2m with 0 errors on Fri May 27 08:54:50 2011

■ Message d'annulation du nettoyage en cours. Par exemple :

scan: scrub canceled on Wed Fri Jun 10 09:06:24 2011

■ Les messages de fin de la réargenture et du nettoyage subsistent après plusieurs

réinitialisation du système.

Etat de maintenance de base de pool de stockage
Vous pouvez vérifier rapidement l'état de maintenance d'un pool en utilisant la commande
zpool status comme suit :

96

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Requête d'état de pool de stockage ZFS

# zpool status -x

all pools are healthy

Il est possible d'examiner des pools spécifiques en spécifiant un nom de pool dans la syntaxe de
commande. Tout pool n'étant pas en état ONLINE doit être passé en revue pour vérifier tout
problème potentiel, comme décrit dans la section suivante.

Etat de maintenance détaillé
Vous pouvez demander un résumé de l'état plus détaillé en utilisant l'option -v. Par exemple :

# zpool status -v tank

pool: tank

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: scrub completed after 0h0m with 0 errors on Wed Jan 20 15:13:59 2010

config:

NAME

tank

mirror-0 DEGRADED

c1t0d0 ONLINE

c1t1d0 UNAVAIL

errors: No known data errors

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0

0 cannot open

Cette sortie affiche une description complète des raisons de l'état actuel du pool, y compris une
description lisible du problème et un lien vers un article de connaissances contenant de plus
amples informations. Les articles de connaissances donnent les informations les plus récentes
vous permettant de résoudre le problème. Les informations détaillées de configuration doivent
vous permettre de déterminer les périphériques endommagés et la manière de réparer le pool.

Dans l'exemple précédent, le périphérique défaillant devrait être remplacé. Une fois le
périphérique remplacé, exécutez la commande zpool online pour le remettre en ligne. Par
exemple :

# zpool online tank c1t0d0

Bringing device c1t0d0 online

# zpool status -x

all pools are healthy

Si la propriété autoreplace est activée, vous n'êtes pas obligé de mettre en ligne le périphérique
remplacé.

Si un périphérique d'un pool est hors ligne, la sortie de commande identifie le pool qui pose
problème. Par exemple :

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

97

Requête d'état de pool de stockage ZFS

# zpool status -x

pool: tank

state: DEGRADED

status: One or more devices has been taken offline by the administrator.

Sufficient replicas exist for the pool to continue functioning in a

degraded state.

action: Online the device using ’zpool online’ or replace the device with

’zpool replace’.

scrub: resilver completed after 0h0m with 0 errors on Wed Jan 20 15:15:09 2010

config:

NAME

tank

mirror-0 DEGRADED

c1t0d0 ONLINE

c1t1d0 OFFLINE

errors: No known data errors

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0

0 48K resilvered

Les colonnes READ et WRITE indiquent le nombre d'erreurs d'E/S détectées dans le périphérique,
tandis que la colonne CKSUM indique le nombre d'erreurs de somme de contrôle impossible à
corriger qui se sont produites sur le périphérique. Ces deux comptes d'erreurs indiquent une
défaillance potentielle du périphérique et que des actions correctives sont requises. Si le nombre
d'erreurs est non nul pour un périphérique virtuel de niveau supérieur, il est possible que des
parties de vos données soient inaccessibles.

Le champ errors: identifie toute erreur de données connue.

Dans l'exemple de sortie précédent, le périphérique mis en ligne ne cause aucune erreur de
données.

Pour plus d'informations sur le diagnostic et la réparation de pools et de données défaillants,
reportez-vous au Chapitre 11, “Dépannage d'Oracle Solaris ZFS et récupération de pool”.

Collecte des informations sur l'état du pool de stockage ZFS
Vous pouvez utiliser l'intervalle zpool status et les options de comptage pour rassembler des
statistiques sur une période précise. En outre, vous pouvez afficher un horodatage en utilisant
l'option -T. Par exemple :

# zpool status -T d 3 2

zpool status -T d 3 2

Tue Nov 2 10:38:18 MDT 2010

pool: pool

state: ONLINE

scan: none requested

config:

NAME

pool

STATE

READ WRITE CKSUM

ONLINE

c3t3d0

ONLINE

0

0

0

0

0

0

98

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de pools de stockage ZFS

errors: No known data errors

pool: rpool

state: ONLINE

scan: resilvered 12.2G in 0h14m with 0 errors on Thu Oct 28 14:55:57 2010

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

config:

NAME

rpool

mirror-0

ONLINE

c3t0d0s0 ONLINE

c3t2d0s0 ONLINE

errors: No known data errors

Tue Nov 2 10:38:21 MDT 2010

pool: pool

state: ONLINE

scan: none requested

config:

NAME

pool

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

c3t3d0

ONLINE

errors: No known data errors

pool: rpool

state: ONLINE

scan: resilvered 12.2G in 0h14m with 0 errors on Thu Oct 28 14:55:57 2010

config:

NAME

rpool

mirror-0

ONLINE

c3t0d0s0 ONLINE

c3t2d0s0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

Migration de pools de stockage ZFS

Parfois, il est possible que vous deviez déplacer un pool de stockage d'un système à l'autre. Pour
ce faire, les périphériques de stockage doivent être déconnectés du système d'origine et
reconnectés au système de destination. Pour accomplir cette tâche, vous pouvez raccorder
physiquement les périphériques ou utiliser des périphériques multiport, par exemple les
périphériques d'un SAN. Le système de fichiers ZFS permet d'exporter le pool à partir d'un
système et de l'importer sur le système de destination, même si l'endianisme de l'architecture
des systèmes est différente. Pour plus d'informations sur la réplication ou la migration de
systèmes de fichiers d'un pool de stockage à un autre résidant éventuellement sur des systèmes
différents, reportez-vous à la section “Envoi et réception de données ZFS” à la page 209.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

99

Migration de pools de stockage ZFS

■ “Préparatifs de migration de pool de stockage ZFS” à la page 100
■ “Exportation d'un pool de stockage ZFS” à la page 100
■ “Définition des pools de stockage disponibles pour importation” à la page 101
■ “Importation de pools de stockage ZFS à partir d'autres répertoires” à la page 103
■ “Importation de pools de stockage ZFS” à la page 103
■ “Récupération de pools de stockage ZFS détruits” à la page 107

Préparatifs de migration de pool de stockage ZFS
Il est conseillé d'exporter les pools de stockage explicitement afin d'indiquer qu'ils sont prêts à la
migration. Cette opération vide toute donnée non écrite sur le disque, écrit les données sur le
disque en indiquant que l'exportation a été effectuée et supprime toute information sur le pool
du système.

Si vous retirez les disques manuellement, au lieu d'exporter le pool explicitement, vous pouvez
toujours importer le pool résultant dans un autre système. Cependant, vous pourriez perdre les
dernières secondes de transactions de données et le pool s'affichera alors comme étant défaillant
sur le système d'origine dans la mesure où les périphériques ne sont plus présents. Par défaut, le
système de destination refuse d'importer un pool qui n'a pas été exporté implicitement. Cette
condition est nécessaire car elle évite les importations accidentelles d'un pool actif composé de
stockage connecté au réseau toujours en cours d'utilisation sur un autre système.

Exportation d'un pool de stockage ZFS
La commande zpool export permet d'exporter un pool. Par exemple :

# zpool export tank

La commande tente de démonter tout système de fichiers démonté au sein du pool avant de
continuer. Si le démontage d'un des systèmes de fichiers est impossible, vous pouvez le forcer à
l'aide de l'option -f. Par exemple :

# zpool export tank

cannot unmount ’/export/home/eric’: Device busy

# zpool export -f tank

Une fois la commande exécutée, le pool tank n'est plus visible sur le système.

Si les périphériques ne sont pas disponibles lors de l'export, les périphériques ne peuvent pas
être identifiés comme étant exportés sans défaut. Si un de ces périphériques est connecté
ultérieurement à un système sans aucun des périphériques en mode de fonctionnement, il
s'affiche comme étant "potentiellement actif".

100

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de pools de stockage ZFS

Si des volumes ZFS sont utilisés dans le pool, ce dernier ne peut pas être exporté, même avec
l'option -f. Pour exporter un pool contenant un volume ZFS, vérifiez au préalable que tous les
utilisateurs du volume ne sont plus actifs.

Pour de plus amples informations sur les volumes ZFS, reportez-vous à la section “Volumes
ZFS” à la page 265.

Définition des pools de stockage disponibles pour
importation
Une fois le pool supprimé du système (soit par le biais d'une exportation explicite, soit par le
biais d'une suppression forcée des périphériques), vous pouvez connecter les périphériques au
système cible. Le système de fichiers ZFS peut gérer des situations dans lesquelles seuls certains
périphériques sont disponibles. Cependant, pour migrer correctement un pool, les
périphériques doivent fonctionner correctement. En outre, il n'est pas nécessaire que les
périphériques soient connectés sous le même nom de périphérique. ZFS détecte tout
périphérique déplacé ou renommé et ajuste la configuration de façon adéquate. Pour connaître
les pools disponibles, exécutez la commande zpool import sans option. Par exemple :

# zpool import

pool: tank

id: 11809215114195894163

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

tank

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

Dans cet exemple, le pool tank est disponible pour être importé dans le système cible. Chaque
pool est identifié par un nom et un identifiant numérique unique. Si plusieurs pools à importer
portent le même nom, vous pouvez utiliser leur identifiant numérique afin de les distinguer.

Tout comme la sortie de la commande zpool status, la sortie de la commande zpool import
se rapporte à un article de connaissances contenant les informations les plus récentes sur les
procédures de réparation pour les problèmes qui empêchent l'importation d'un pool. Dans ce
cas, l'utilisateur peut forcer l'importation du pool. Cependant, l'importation d'un pool en cours
d'utilisation par un autre système au sein d'un réseau de stockage peut entraîner une corruption
des données et des erreurs graves si les deux systèmes tentent d'écrire dans le même stockage. Si
certains périphériques dans le pool ne sont pas disponibles, mais que des données redondantes
suffisantes sont disponibles pour obtenir un pool utilisable, le pool s'affiche dans l'état
DEGRADED. Par exemple :

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

101

Migration de pools de stockage ZFS

# zpool import

pool: tank

id: 11809215114195894163

state: DEGRADED

status: One or more devices are missing from the system.

action: The pool can be imported despite missing or damaged devices. The

fault tolerance of the pool may be compromised if imported.

see: http://www.sun.com/msg/ZFS-8000-2Q

config:

NAME

tank

DEGRADED

mirror-0 DEGRADED

c1t0d0 UNAVAIL

c1t3d0 ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0 cannot open

0

Dans cet exemple, le premier disque est endommagé ou manquant, mais il est toujours possible
d'importer le pool car les données mises en miroir restent accessibles. Si le nombre de
périphériques défaillants ou manquant est trop importants, l'importation du pool est
impossible. Par exemple :

# zpool import

pool: dozer

id: 9784486589352144634

state: FAULTED

action: The pool cannot be imported. Attach the missing

devices and try again.

see: http://www.sun.com/msg/ZFS-8000-6X

config:

raidz1-0

FAULTED

c1t0d0

ONLINE

c1t1d0

FAULTED

c1t2d0

ONLINE

c1t3d0

FAULTED

Dans cet exemple, deux disques manquent dans un périphérique virtuel RAID-Z, ce qui signifie
que les données redondantes disponibles ne sont pas suffisantes pour reconstruire le pool. Dans
certains cas, les périphériques présents ne sont pas suffisants pour déterminer la configuration
complète. Dans ce cas, ZFS ne peut pas déterminer quels autres périphériques faisaient partie
du pool, mais fournit autant d'informations que possible sur la situation. Par exemple :

# zpool import

pool: dozer

id: 9784486589352144634

state: FAULTED

status: One or more devices are missing from the system.

action: The pool cannot be imported. Attach the missing

devices and try again.

see: http://www.sun.com/msg/ZFS-8000-6X

config:

dozer

FAULTED

missing device

raidz1-0

ONLINE

c1t0d0

ONLINE

c1t1d0

ONLINE

102

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de pools de stockage ZFS

c1t2d0

ONLINE

c1t3d0

ONLINE

Additional devices are known to be part of this pool, though their

exact configuration cannot be determined.

Importation de pools de stockage ZFS à partir d'autres
répertoires
Par défaut, la commande zpool import ne recherche les périphériques que dans le répertoire
/dev/dsk. Si les périphériques existent dans un autre répertoire, ou si vous utilisez des pools
sauvegardés dans des fichiers, utilisez l'option -d pour effectuer des recherches dans d'autres
répertoires. Par exemple :

# zpool create dozer mirror /file/a /file/b

# zpool export dozer

# zpool import -d /file

pool: dozer

id: 7318163511366751416

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

dozer

ONLINE

mirror-0

ONLINE

/file/a ONLINE

/file/b ONLINE

# zpool import -d /file dozer

Si les périphériques se trouvent dans plusieurs répertoires, vous pouvez utiliser plusieurs
options - d.

Importation de pools de stockage ZFS
Une fois le pool identifié pour l'importation, vous pouvez l'importer en spécifiant son nom ou
son identifiant numérique en tant qu'argument pour la commande zpool import. Par
exemple :

# zpool import tank

Si plusieurs pools disponibles possèdent le même nom, vous devez spécifier le pool à importer à
l'aide de l'identifiant numérique. Par exemple :

# zpool import

pool: dozer

id: 2704475622193776801

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

103

Migration de pools de stockage ZFS

config:

dozer

ONLINE

c1t9d0

ONLINE

pool: dozer

id: 6223921996155991199

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

dozer

ONLINE

c1t8d0

ONLINE

# zpool import dozer

cannot import ’dozer’: more than one matching pool

import by numeric ID instead

# zpool import 6223921996155991199

Si le nom du pool est en conflit avec un nom de pool existant, vous pouvez importer le pool sous
un nom différent. Par exemple :

# zpool import dozer zeepool

Cette commande importe le pool dozer exporté sous le nouveau nom zeepool. Le nouveau
nom de pool est persistant.

Si l'exportation du pool ne s'effectue pas correctement, l'indicateur -f est requis par ZFS pour
empêcher les utilisateurs d'importer par erreur un pool en cours d'utilisation dans un autre
système. Par exemple :

# zpool import dozer

cannot import ’dozer’: pool may be in use on another system

use ’-f’ to import anyway

# zpool import -f dozer

Remarque – N'essayez pas d'importer un pool actif sur un seul système vers un autre système.
ZFS n'est pas un systèmes de fichiers de cluster natifs, distribués ou parallèles et ne peut pas
fournir d'accès simultané à plusieurs hôtes différents.

Les pools peuvent également être importés sous une racine de remplacement à l'aide de l'option
-R. Pour plus d'informations sur les pools racine de remplacement, reportez-vous à la section
“Utilisation de pools racine ZFS de remplacement” à la page 274.

104

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de pools de stockage ZFS

Importation d'un pool avec un périphérique de journalisation
manquant
Par défaut, un pool avec un périphérique de journalisation manquant ne peut pas être importé.
Vous pouvez utiliser la commande zpool import -m pour forcer l'importation d'un pool avec
un périphérique de journalisation manquant. Par exemple :

# zpool import dozer

The devices below are missing, use ’-m’ to import the pool anyway:

c3t3d0 [log]

cannot import ’dozer’: one or more devices is currently unavailable

Importez le pool avec le périphérique de journalisation manquant. Par exemple :

# zpool import -m dozer

# zpool status dozer

pool: dozer

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scan: scrub repaired 0 in 0h0m with 0 errors on Fri Oct 15 16:43:03 2010

config:

STATE

READ WRITE CKSUM

DEGRADED

ONLINE

ONLINE

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 was c3t3d0

NAME

dozer

mirror-0

c3t1d0

c3t2d0

logs

14685044587769991702 UNAVAIL

Après avoir connecté le périphérique de journalisation manquant, exécutez la commande zpool
clear pour effacer les erreurs du pool.

Une récupération similaire peut être tentée avec des périphériques de journalisation mis en
miroir manquant. Par exemple :

# zpool import dozer

The devices below are missing, use ’-m’ to import the pool anyway:

mirror-1 [log]

c3t3d0

c3t4d0

cannot import ’dozer’: one or more devices is currently unavailable

# zpool import -m dozer

# zpool status dozer

pool: dozer

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

105

Migration de pools de stockage ZFS

see: http://www.sun.com/msg/ZFS-8000-2Q

scan: scrub repaired 0 in 0h0m with 0 errors on Fri Oct 15 16:51:39 2010

config:

NAME

dozer

mirror-0

c3t1d0

c3t2d0

logs

mirror-1

UNAVAIL

13514061426445294202 UNAVAIL

16839344638582008929 UNAVAIL

STATE

READ WRITE CKSUM

DEGRADED

ONLINE

ONLINE

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 insufficient replicas

0 was c3t3d0

0 was c3t4d0

Après avoir connecté les périphériques de journalisation manquant, exécutez la commande
zpool clear pour effacer les erreurs du pool.

Importation d'un pool en mode lecture seule
Vous pouvez importer un pool en mode lecture seule. Si un pool est tellement endommagé qu'il
ne peut pas être accessible, cette fonction peut vous permettre de récupérer les données du pool.
Par exemple :

# zpool import -o readonly=on tank

# zpool scrub tank

cannot scrub tank: pool is read-only

Lorsqu'un pool est importé en mode lecture seule, les conditions suivantes s'appliquent :
■ Tous les systèmes de fichiers et les volumes sont montés en mode lecture seule.
■ Le traitement de la transaction du pool est désactivé. Cela signifie également que les

écritures synchrones en attente dans le journal de tentatives ne sont pas lues jusqu'à ce que le
pool soit importé en lecture-écriture.

■ Les tentatives de définition d'une propriété de pool au cours de l'importation en lecture seule

ne sont pas prises en compte.

Un pool en lecture seule peut être redéfini en mode lecture-écriture via l'exportation et
l'importation du pool. Par exemple :

# zpool export tank

# zpool import tank

# zpool scrub tank

Importation d'un pool via le chemin d'accès au périphérique
La commande suivante permet d'importer le pool dpool en identifiant l'un des périphériques
spécifiques du pool, /dev/dsk/c2t3d0, dans cet exemple.

106

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de pools de stockage ZFS

# zpool import -d /dev/dsk/c2t3d0s0 dpool

# zpool status dpool

pool: dpool

state: ONLINE

scan: resilvered 952K in 0h0m with 0 errors on Thu Mar 10 10:28:46 2011

config:

NAME

dpool

ONLINE

mirror-0 ONLINE

c2t3d0 ONLINE

c2t1d0 ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

Même si ce pool est composé de disques entiers, la commande doit inclure l'identificateur de
tranche du périphérique concerné.

Récupération de pools de stockage ZFS détruits
La commande zpool import -D permet de récupérer un pool de stockage détruit. Par exemple :

# zpool destroy tank

# zpool import -D

pool: tank

id: 5154272182900538157

state: ONLINE (DESTROYED)

action: The pool can be imported using its name or numeric identifier.

config:

tank

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

Dans la sortie de zpool import, vous pouvez identifier le pool tank comme étant le pool détruit
en raison des informations d'état suivantes :

state: ONLINE (DESTROYED)

Pour récupérer le pool détruit, exécutez la commande zpool import -D à nouveau avec le pool à
récupérer. Par exemple :

# zpool import -D tank

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

107

Migration de pools de stockage ZFS

c1t1d0 ONLINE

errors: No known data errors

Même si l'un des périphériques du pool détruit est défaillant ou indisponible, vous devriez être
en mesure de récupérer le pool détruit en incluant l'option -f. Dans ce cas, importez le pool
défaillant et tentez ensuite de réparer la défaillance du périphérique. Par exemple :

# zpool destroy dozer

# zpool import -D

pool: dozer

id: 13643595538644303788

state: DEGRADED (DESTROYED)

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0

0

35

0

0

0

0

0

0

1 cannot open

0

config:

NAME

dozer

raidz2-0

DEGRADED

c2t8d0

ONLINE

c2t9d0

ONLINE

c2t10d0 ONLINE

c2t11d0 UNAVAIL

c2t12d0 ONLINE

errors: No known data errors

# zpool import -Df dozer

# zpool status -x

pool: dozer

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: scrub completed after 0h0m with 0 errors on Thu Jan 21 15:38:48 2010

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0

0

37

0

0

0

0

0

0

0 cannot open

0

config:

NAME

dozer

raidz2-0

DEGRADED

c2t8d0

ONLINE

c2t9d0

ONLINE

c2t10d0 ONLINE

c2t11d0 UNAVAIL

c2t12d0 ONLINE

errors: No known data errors

# zpool online dozer c2t11d0

Bringing device c2t11d0 online

# zpool status -x

all pools are healthy

108

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Mise à niveau de pools de stockage ZFS

Mise à niveau de pools de stockage ZFS

Si certains pools de stockage ZFS proviennent d'une version antérieure de Solaris, vous pouvez
mettre les pools à niveau à l'aide de la commande zpool upgrade pour bénéficier des fonctions
des pools de la version actuelle. De plus, la commande zpool status vous avertit lorsque la
version de vos pools est plus ancienne. Par exemple :

# zpool status

pool: tank

state: ONLINE

status: The pool is formatted using an older on-disk format. The pool can

still be used, but some features are unavailable.

action: Upgrade the pool using ’zpool upgrade’. Once this is done, the

pool will no longer be accessible on older software versions.

scrub: none requested

config:

NAME

tank

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

Vous pouvez utiliser la syntaxe suivante afin d'identifier des informations supplémentaires sur
une version donnée et sur les versions prises en charge.

# zpool upgrade -v

This system is currently running ZFS pool version 33.

The following versions are supported:

VER DESCRIPTION

--- --------------------------------------------------------

1

2

3

4

5

6

7

8

9

Initial ZFS version

Ditto blocks (replicated metadata)

Hot spares and double parity RAID-Z

zpool history

Compression using the gzip algorithm

bootfs pool property

Separate intent log devices

Delegated administration

refquota and refreservation properties

10 Cache devices

11 Improved scrub performance

12 Snapshot properties

13 snapused property

14 passthrough-x aclinherit

15 user/group space accounting

16 stmf property support

17 Triple-parity RAID-Z

18 Snapshot user holds

19 Log device removal

20 Compression using zle (zero-length encoding)

21 Deduplication

Chapitre 4 • Gestion des pools de stockage Oracle Solaris ZFS

109

Mise à niveau de pools de stockage ZFS

22 Received properties

23 Slim ZIL

24 System attributes

25 Improved scrub stats

26 Improved snapshot deletion performance

27 Improved snapshot creation performance

28 Multiple vdev replacements

29 RAID-Z/mirror hybrid allocator

30 Encryption

31 Improved ’zfs list’ performance

32 One MB blocksize

33 Improved share support

For more information on a particular version, including supported releases,

see the ZFS Administration Guide.

Vous pouvez ensuite mettre tous vos pools à niveau en exécutant la commande zpool upgrade.
Par exemple :

# zpool upgrade -a

Remarque – Si vous mettez à niveau votre pool vers une version ZFS ultérieure, le pool ne sera
pas accessible sur un système qui exécute une version ZFS plus ancienne.

110

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

5C H A P I T R E

5

Gestion des composants du pool racine ZFS

Ce chapitre décrit comment gérer les composants du pool racine ZFS Oracle Solaris, par
exemple pour connecter un miroir de pool racine, cloner un environnement d'initialisation ZFS
et redimensionner des périphériques de swap et de vidage.

Ce chapitre contient les sections suivantes :
■ “Gestion des composants du pool racine ZFS (présentation)” à la page 111
■ “Configuration requise pour le pool racine ZFS” à la page 112
■ “Gestion de votre pool racine ZFS” à la page 114
■ “Gestion de vos périphériques de swap et de vidage ZFS” à la page 122
■ “Initialisation à partir d'un système de fichiers racine ZFS” à la page 125

Pour plus d'informations sur la récupération de pool racine, reportez-vous au Chapitre 12,
“Archivage des instantanés et récupération du pool racine”.

Pour toute informations récentes importantes, reportez-vous au manuel Oracle Solaris 11
release notes.

Gestion des composants du pool racine ZFS (présentation)

ZFS est le système de fichiers racine par défaut dans la version Oracle Solaris 11. Passez en revue
les considérations suivantes lorsque vous installez la version d'Oracle Solaris.

■

Installation : dans la version Oracle Solaris 11, vous pouvez installer et initialiser à partir
d'un système de fichiers racine ZFS de l'une des manières suivantes :
■ Live CD (x86 uniquement) : installe un pool racine ZFS sur un seul disque. Vous pouvez
utiliser le menu de partition fdisk au cours de l'installation afin de partitionner le disque
pour votre environnement.
Installation en mode texte (SPARC et x86) : installe un pool racine ZFS sur un seul
disque à partir d'un média ou sur le réseau. Vous pouvez utiliser le menu de partition
fdisk au cours de l'installation afin de partitionner le disque pour votre environnement.

■

111

Gestion des composants du pool racine ZFS (présentation)

■ Programme d'installation automatisée (AI) (SPARC et x86) : installe automatiquement
un pool racine ZFS. Vous pouvez utiliser un fichier manifeste AI afin de déterminer le
disque et les partitions de disque à utiliser pour le pool racine ZFS.

■ Périphériques de swap et de vidage : créés automatiquement sur les volumes ZFS dans le
pool racine ZFS par toutes les méthodes d'installation ci-dessus. Pour plus d'informations
sur la gestion des périphériques de swap et de vidage, reportez-vous à la section “Gestion de
vos périphériques de swap et de vidage ZFS” à la page 122.

■ Configuration d'un pool racine mis en miroir : vous pouvez configurer un pool racine mis
en miroir lors d'une installation automatique. Pour plus d'informations sur la configuration
d'un pool racine mis en miroir après une installation, reportez-vous à la section
“Configuration d'un pool racine mis en miroir” à la page 117.

■ Gestion de l'espace de pool racine : après l'installation du système, envisagez de définir un

quota sur le système de fichiers racine ZFS pour empêcher qu'il ne se remplisse. A l'heure
actuelle, aucun espace de pool racine ZFS n'est réservé en tant que filet de sécurité pour un
système de fichiers plein. Par exemple, si vous avez un disque de 68 Go pour le pool racine,
définissez un quota de 67 Go sur le système de fichiers racine ZFS, ce qui permet au système
de fichiers de conserver 1 Go d'espace.

Configuration requise pour le pool racine ZFS
Consultez les sections ci-après décrivant l'espace de pool racine ZFS et la configuration requise.

Espace de pool de stockage ZFS requis
Lorsqu'un système est installé, la taille du volume de swap et du périphérique de vidage dépend
de la quantité de mémoire physique. L'espace de pool minimal d'un système de fichiers racine
ZFS d'initialisation dépend de la quantité de mémoire physique, de l'espace disque disponible et
du nombre d'environnements d'initialisation à créer.

■

Consultez les exigences en termes d'espace de pool de stockage ZFS suivantes :
■ Pour une description de la mémoire requise pour les différentes méthodes d'installation,

reportez-vous aux Notes de version Oracle Solaris 11 .
7-13 Go d'espace disque minimum sont recommandés. L'espace est utilisé comme suit :
■ Zone de swap et périphérique de vidage : les capacités par défaut des volumes de swap
et de vidage créés par les programmes d'installation de Solaris varient en fonction de la
quantité de mémoire disponible sur le système et d'autres variables. La taille du
périphérique de swap correspond généralement à 1/4 de la mémoire physique et la taille
du périphérique de vidage est environ égale à la moitié de la mémoire physique.
Vous pouvez ajuster librement les tailles respectives des volumes de swap et de vidage,
dès lors que celles-ci permettent au programme de fonctionner correctement pendant et
après l'installation. Pour plus d'informations, reportez-vous à la section “Ajustement de
la taille de vos périphériques de swap et de vidage ZFS” à la page 123.

112

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion des composants du pool racine ZFS (présentation)

■ Environnement d'initialisation (BE) : un environnement d'initialisation ZFS est
d'environ 4 à 6 Go. Un environnement d'initialisation ZFS cloné à partir d'un autre
environnement d'initialisation ZFS ne requiert pas d'espace disque supplémentaire.
Tenez compte du fait que la taille de l'environnement d'initialisation augmente lorsqu'il
est mis à jour, ce en fonction des mises à jour. Tous les environnements d'initialisation
ZFS d'un même pool racine utilisent les mêmes périphériques de swap et de vidage.

■ Composants du SE Oracle Solaris : à l'exception de /var, tous les sous-répertoires du

système de fichiers racine qui font partie de l'image du SE doivent se trouver dans le
système de fichiers racine. En outre, tous les composants du SE Solaris doivent se trouver
dans le pool racine, à l'exception des périphériques de swap et de vidage.

Configuration requise pour le pool racine ZFS
Vérifiez la configuration requise suivante pour le pool de stockage ZFS :
■ Le disque destiné au pool racine doit contenir une étiquette SMI (VTOC).
■ Le disque destiné au pool racine doit être inférieur à 2 To afin que le système d'exploitation

Solaris s'initialise correctement.

■ Le pool doit exister sur une tranche de disque ou sur des tranches de disque qui sont mises
en miroir. Si vous tentez d'utiliser une configuration de pool non prise en charge lors d'une
opération beadm, un message du type suivant s'affiche :
ERROR: ZFS pool name does not support boot environments
Pour obtenir une description détaillée des configurations de pool racine ZFS prises en
charge, reportez-vous à la section “Création d'un pool racine ZFS” à la page 52.
Sur un système x86, le disque doit contenir une partition fdisk Solaris. Une partitionfdisk
Solaris est créée automatiquement lors de l'installation du système x86. Pour plus
d'informations sur les partitions fdisk de Solaris, reportez-vous à la section “Consignes
pour la création d’une partition fdisk” du manuel Administration d’Oracle Solaris :
Périphériques et systèmes de fichiers.

■

■ Les propriétés d'un pool ou les propriétés du système de fichiers peuvent être définies sur un
pool racine au cours d'une installation automatique. L'algorithme de compression gzip n'est
pas pris en charge sur les pools racine.

■ Ne renommez pas le pool racine une fois qu'il a été créé par une installation initiale. Si vous

renommez le pool racine, cela peut empêcher l'initialisation du système.

Dépannage des problèmes d'installation d'un pool
racine ZFS
Examinez les problèmes suivants si vous tentez d'installer la version Oracle Solaris 11.

Chapitre 5 • Gestion des composants du pool racine ZFS

113

Gestion de votre pool racine ZFS

■ Le disque du pool racine est trop petit : si la tranche de disque destinée au pool racine est

trop petite, vous devrez quitter le programme d'installation, augmenter la taille de la tranche
de disque à l'aide de l'utilitaire format et redémarrer l'installation. Par exemple, au cours
d'une installation automatique, un message semblable au message suivant peut s'afficher sur
la console système :

15:43:54

Space required for installation: 5.00gb

15:43:54

Total available space: 4.55gb

15:43:54

Error occurred during execution of ’target-selection’ checkpoint.

15:43:54

Failed Checkpoints:

15:43:54

15:43:54

target-selection

15:43:54

15:43:54

Checkpoint execution error:

15:43:54

15:43:54

Error determining swap/dump requirements.

15:43:54

15:43:54

Automated Installation Failed. See install log at

/system/volatile/install_log

■ Pour plus d'informations sur l'extension de la tranche de disque d'un pool racine,

reportez-vous à la section “Création d’une tranche de disque pour un système de fichiers
racine ZFS” du manuel Administration d’Oracle Solaris : Périphériques et systèmes de
fichiers ou à la section “Création d’une tranche de disque pour un système de fichiers
racine ZFS” du manuel Administration d’Oracle Solaris : Périphériques et systèmes de
fichiers.

■ Une fois la tranche de disque du pool racine étendue, redémarrez le programme

d'installation automatique :

# svcadm clear auto-installer

Appuyez sur la touche Retour. Vous pouvez observer l'installation à l'aide de cette
commande :

# tail -f /system/volatile/install_log

■ L'installation s'arrête : si l'installation s'arrête et que le disque du pool racine est trop
petit, par exemple 16 Go, et que la mémoire système est grande, par exemple 32 Go, le
disque est trop petit pour créer un volume de swap et un volume de vidage. Sur un
système à mémoire importante, le disque du pool racine doit être suffisamment grand
pour contenir l'environnement d'initialisation, le volume de swap et un périphérique de
vidage qui représente la moitié ou les 3/4 de la taille de la mémoire physique.

Gestion de votre pool racine ZFS

Les sections suivantes fournissent des informations sur l'installation et la mise à jour d'un pool
racine ZFS et la configuration d'un pool racine en miroir.

114

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de votre pool racine ZFS

Installation d'un pool racine ZFS
La méthode d'installation Live CD d'Oracle Solaris 11 installe un pool racine ZFS par défaut sur
un disque unique. La méthode d'installation automatisée (AI, Automated Install)
d'Oracle Solaris 11 vous permet de créer un manifeste AI pour identifier le disque ou les disques
mis en miroir du pool racine ZFS.

Le programme d'installation automatisée permet de disposer d'une certaine souplesse en
installant un pool racine ZFS sur le disque d'initialisation par défaut ou sur un disque cible que
vous identifiez. Vous pouvez spécifier le périphérique logique, tel que c1t0d0s0, ou le chemin
du périphérique physique. En outre, vous pouvez utiliser l'identificateur MPxIO ou l'ID du
périphérique à installer.

Après l'installation, examinez les informations de votre pool de stockage ZFS et du système de
fichiers, qui peuvent varier selon le type d'installation et les personnalisations. Par exemple :

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

c1t3d0s0 ONLINE

0

0

0

0

0

0

errors: No known data errors

# zfs list

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

6.49G 60.4G

40K /rpool

rpool/ROOT

3.46G 60.4G

31K legacy

rpool/ROOT/solaris

3.46G 60.4G 3.16G /

rpool/ROOT/solaris/var

303M 60.4G

216M /var

rpool/dump

2.00G 60.5G 1.94G -

rpool/export

96.5K 60.4G

32K /rpool/export

rpool/export/home

64.5K 60.4G

32K /rpool/export/home

rpool/export/home/admin 32.5K 60.4G 32.5K /rpool/export/home/admin

rpool/swap

1.03G 60.5G 1.00G -

Passez en revue les informations sur l'environnement d'initialisation ZFS. Par exemple :

# beadm list

# beadm list

BE

--

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

solaris NR

/

3.85G static 2011-09-26 08:37

Dans la sortie ci-dessus, le champ Active indique si l'environnement d'initialisation est actif
maintenant (lettre N), actif lors de la réinitialisation (lettre R), ou les deux (lettres NR).

Chapitre 5 • Gestion des composants du pool racine ZFS

115

Gestion de votre pool racine ZFS

▼ Procédure de mise à jour de l'environnement

d'initialisation ZFS
L'environnement d'initialisation ZFS par défaut est nommé solaris par défaut. Vous pouvez
identifier votre environnement d'initialisation en utilisant la commande beadm list. Par
exemple :

# beadm list

BE

--

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

solaris NR

/

8.41G static 2011-01-13 15:31

Dans la sortie ci-dessus, NR signifie que l'environnement d'initialisation est actuellement actif
et qu'il sera l'environnement d'initialisation actif après la réinitialisation.

La commande pkg update vous permet de mettre à jour votre environnement d'initialisation
ZFS. Si vous mettez à jour votre environnement d'initialisation ZFS à l'aide de la commande pkg
update, un nouvel environnement d'initialisation est créé et activé automatiquement, sauf si les
mises à jour appliquées à l'environnement d'initialisation existant sont très minimes.

1

Mettez à jour votre environnement d'initialisation ZFS.

# pkg update

DOWNLOAD

Completed

.

.

.

PKGS

FILES

XFER (MB)

707/707 10529/10529 194.9/194.9

Un nouvel environnement d'initialisation, solaris-1, est automatiquement créé et activé.

2

Réinitialisez le système pour terminer l'activation de l'environnement d'initialisation. Ensuite,
confirmez le statut de l'environnement d'initialisation.

# init 6

.

.

.

# beadm list

BE

--

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

solaris

-

solaris-1 NR

-

/

6.25M static 2011-09-26 08:37

3.92G static 2011-09-26 09:32

3

Si une erreur se produit lors de l'initialisation du nouvel environnement d'initialisation, activez
et initialisez sur l'environnement d'initialisation précédent.

# beadm activate solaris

# init 6

116

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de votre pool racine ZFS

▼ Montage d'un environnement d'initialisation

alternatif
A des fins de récupération, vous pouvez être amené à copier ou à accéder à un fichier à partir
d'un autre environnement d'initialisation.

1

2

3

Connectez-vous en tant qu'administrateur.

Montez l'environnement d'initialisation alternatif.

# beadm mount solaris-1 /mnt

Accédez à l'environnement d'initialisation.

# ls /mnt

bin

boot

dev

devices

java

export

media

pkg

rpool

home

mine

platform

sbin

import

mnt

net

proc

scde

project

shared

tmp

usr

var

doe

etc

kernel

nfs4

re

src

lib

opt

root

system

4

Démontez l'environnement d'initialisation alternatif lorsque vous avez terminé de l'utiliser.

# beadm umount solaris-1

▼ Configuration d'un pool racine mis en miroir

Si vous ne configurez pas de pool racine mis en miroir au cours d'une installation automatique,
vous pouvez facilement configurer un pool racine mis en miroir après l'installation.

Pour plus d'informations sur le remplacement d'un disque dans un pool racine, reportez-vous à
la section “Remplacement d'un disque dans un pool racine ZFS” à la page 119.

1

Affichez l'état du pool racine actuel.

# zpool status rpool

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

c2t0d0s0 ONLINE

0

0

0

0

0

0

errors: No known data errors

2

Préparez un second disque à raccorder au pool racine, si nécessaire.

Chapitre 5 • Gestion des composants du pool racine ZFS

117

Gestion de votre pool racine ZFS

■

■

SPARC : confirmez que le disque dispose d'une étiquette de disque SMI (VTOC) et d'une
tranche 0. Si vous devez réétiqueter le disque et créer une tranche 0, reportez-vous à la
section “Création d’une tranche de disque pour un système de fichiers racine ZFS” du
manuel Administration d’Oracle Solaris : Périphériques et systèmes de fichiers.
x86 : confirmez que le disque dispose d'une partition fdisk, d'une étiquette de disque SMI et
d'une tranche 0. Si vous devez repartitionner le disque et créer une tranche 0, reportez-vous
à la section “Création d’une tranche de disque pour un système de fichiers racine ZFS” du
manuel Administration d’Oracle Solaris : Périphériques et systèmes de fichiers.

3

Connectez un deuxième disque pour configurer un pool racine mis en miroir.

# zpool attach rpool c2t0d0s0 c2t1d0s0

Make sure to wait until resilver is done before rebooting.

4

Affichez l'état du pool racine pour confirmer la fin de la réargenture.

# zpool status rpool

pool: rpool

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scan: resilver in progress since Thu Sep 29 18:09:09 2011

1.55G scanned out of 5.36G at 36.9M/s, 0h1m to go

1.55G scanned out of 5.36G at 36.9M/s, 0h1m to go

1.55G resilvered, 28.91% done

config:

NAME

rpool

STATE

READ WRITE CKSUM

ONLINE

mirror-0

ONLINE

c2t0d0s0 ONLINE

c2t1d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0 (resilvering)

errors: No known data errors

Dans la sortie ci-dessus, le processus de réargenture n'est pas terminé. La réargenture est
terminée lorsque des messages similaires aux suivants s'affichent :

5

6

resilvered 5.36G in 0h10m with 0 errors on Thu Sep 29 18:19:09 2011

Assurez-vous que vous pouvez initialiser correctement à partir du nouveau disque.

Configurez le système de manière à ce qu'il s'initialise automatiquement à partir du nouveau
disque.

■

■

SPARC : configurez le système de manière à ce qu'il s'initialise automatiquement à partir du
nouveau disque, soit en utilisant la commande eeprom, soit en utilisant la commande
setenv de la PROM d'initialisation.
x86 : reconfigurez le BIOS du système.

118

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de votre pool racine ZFS

▼ Remplacement d'un disque dans un pool racine ZFS

Vous pouvez être amené à remplacer un disque dans le pool racine pour les raisons suivantes :
■ Le pool racine est trop petit et vous souhaitez le remplacer par un disque plus grand.
■ Le disque du pool racine est défectueux. Dans un pool non redondant, si le disque est

défectueux et empêche l'initialisation du système, vous devez initialiser votre système à
partir d'un autre média, par exemple un CD ou le réseau, avant de remplacer le disque du
pool racine.

Dans une configuration de pool racine en miroir, vous pouvez peut-être tenter un
remplacement de disque sans avoir à initialiser à partir d'un autre média. Vous pouvez
remplacer un disque défaillant en utilisant la commande zpool replace ou, si vous avez un
disque supplémentaire, la commande zpool attach. Pour savoir comment connecter un autre
disque et déconnecter un disque de pool racine, reportez-vous aux étapes ci-dessous.

Sur les systèmes équipés de disques SATA, vous devez déconnecter le disque et en supprimer la
configuration avant de tenter d'utiliser la commande zpool replace pour remplacer un disque
défectueux. Par exemple :

# zpool offline rpool c1t0d0s0

# cfgadm -c unconfigure c1::dsk/c1t0d0

<Physically remove failed disk c1t0d0>

<Physically insert replacement disk c1t0d0>

# cfgadm -c configure c1::dsk/c1t0d0

<Confirm that the new disk has an SMI label and a slice 0>

# zpool replace rpool c1t0d0s0

# zpool online rpool c1t0d0s0

# zpool status rpool

<Let disk resilver before installing the boot blocks>

SPARC# installboot -F zfs /usr/platform/‘uname -i‘/lib/fs/zfs/bootblk /dev/rdsk/c1t0d0s0

x86# installgrub /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c1t0d0s0

Avec certains composants matériels, il n'est pas nécessaire de connecter le disque, ni de
reconfigurer son remplacement après son insertion.

Connectez physiquement le disque de remplacement.

Confirmez que le (nouveau) disque de remplacement dispose d'une étiquette SMI (VTOC) et
d'une tranche 0.
Pour plus d'informations sur le réétiquetage d'un disque destiné au pool racine, reportez-vous à
la section “Etiquetage d’un disque” du manuel Administration d’Oracle Solaris : Périphériques et
systèmes de fichiers.

Associez le nouveau disque au pool racine.
Par exemple :

# zpool attach rpool c2t0d0s0 c2t1d0s0

Make sure to wait until resilver is done before rebooting.

1

2

3

Chapitre 5 • Gestion des composants du pool racine ZFS

119

Gestion de votre pool racine ZFS

4

Confirmez le statut du pool racine.
Par exemple :

# zpool status rpool

pool: rpool

state: ONLINE

scan: resilvered 5.36G in 0h2m with 0 errors on Thu Sep 29 18:11:53 2011

config:

NAME

rpool

mirror-0

ONLINE

c2t0d0s0 ONLINE

c2t1d0s0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

5

Vérifiez que vous pouvez initialiser à partir du nouveau disque une fois la réargenture terminée.
Par exemple, sur un système SPARC :

ok boot /pci@1f,700000/scsi@2/disk@1,0

Identifiez les chemins d'accès du périphérique d'initialisation du nouveau disque et du disque
actuel afin de tester l'initialisation à partir du disque de remplacement et afin de pouvoir
initialiser manuellement le système à partir du disque existant, en cas de dysfonctionnement du
disque de remplacement. Dans l'exemple suivant, le disque du pool racine actuel (c2t0d0s0)
est :

6

7

/pci@1f,700000/scsi@2/disk@0,0

Dans l'exemple suivant, le disque d'initialisation de remplacement est (c2t1d0s0) :

boot /pci@1f,700000/scsi@2/disk@1,0

Si le système s'initialise à partir du nouveau disque, déconnectez l'ancien disque.
Par exemple :

# zpool detach rpool c2t0d0s0

Configurez le système de manière à ce qu'il s'initialise automatiquement à partir du nouveau
disque.

■

■

SPARC : configurez le système de manière à ce qu'il s'initialise automatiquement à partir du
nouveau disque, soit en utilisant la commande eeprom, soit en utilisant la commande
setenv de la PROM d'initialisation.
x86 : reconfigurez le BIOS du système.

120

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de votre pool racine ZFS

▼ Création d'un environnement d'initialisation dans un

pool racine différent
Si vous souhaitez recréer votre environnement d'initialisation existant dans un autre pool
racine, effectuez les étapes décrites dans cette procédure. Vous pouvez modifier les étapes en
fonction de ce que vous souhaitez obtenir :deux pools racine dotés d'environnements
d'initialisation similaires ayant des périphériques de swap et de vidage indépendants ou un
environnement d'initialisation dans un autre pool racine qui partage les périphériques de swap
et de vidage.

Une fois que vous avez activé et initialisé à partir du nouvel environnement d'initialisation dans
le second pool racine, celui-ci ne disposera d'aucune information sur l'environnement
d'initialisation précédent du premier pool racine. Si vous souhaitez revenir à l'environnement
d'initialisation d'origine, réinitialisez le système manuellement à partir du disque d'initialisation
du pool racine d'origine.

1

2

3

4

5

6

7

8

Créez un second pool racine avec un disque étiqueté SMI (VTOC). Par exemple :

# zpool create rpool2 c4t2d0s0

Créez le nouvel environnement d'initialisation dans le deuxième pool racine. Par exemple :

# beadm create -p rpool2 solaris2

Définissez la propriété bootfs sur le deuxième pool racine. Par exemple :

# zpool set bootfs=rpool2/ROOT/solaris2 rpool2

Activez le nouvel environnement d'initialisation. Par exemple :

# beadm activate solaris2

Initialisez à partir du nouvel environnement d'initialisation mais veillez à le faire à partir du
périphérique d'initialisation du deuxième pool racine.

ok boot disk2

Votre système doit s'exécuter sous le nouvel environnement d'initialisation.

Recréez le volume de swap. Par exemple :

# zfs create -V 4g rpool2/swap

Mettez à jour l'entrée /etc/vfstab pour le nouveau périphérique de swap. Par exemple :

/dev/zvol/dsk/rpool2/swap

-

-

swap -

no

-

Recréez le volume de vidage. Par exemple :

# zfs create -V 4g rpool2/dump

Chapitre 5 • Gestion des composants du pool racine ZFS

121

Gestion de vos périphériques de swap et de vidage ZFS

9

Réinitialisez le périphérique de vidage. Par exemple :

# dumpadm -d /dev/zvol/dsk/rpool2/dump

10

Réinitialisez votre périphérique d'initialisation par défaut afin qu'il démarre à partir du disque
d'initialisation du deuxième pool racine.

■

■

SPARC : configurez le système de manière à ce qu'il s'initialise automatiquement à partir du
nouveau disque, soit en utilisant la commande eeprom, soit en utilisant la commande
setenv de la PROM d'initialisation.
x86 : reconfigurez le BIOS du système.

11

Réinitialisez pour effacer les périphériques de swap et de vidage du pool racine d'origine.

# init 6

Gestion de vos périphériques de swap et de vidage ZFS

Au cours du processus d'installation, une zone de swap est créée sur un volume ZFS du pool
racine ZFS. Par exemple :

# swap -l

swapfile

dev

swaplo

blocks

free

/dev/zvol/dsk/rpool/swap 145,2

16 16646128 16646128

Au cours du processus d'installation, un périphérique de vidage est créé sur un volume ZFS du
pool racine ZFS. En règle générale, un périphérique de vidage ne nécessite aucune
administration car il est créé automatiquement lors de l'installation. Par exemple :

# dumpadm

Dump content: kernel pages

Dump device: /dev/zvol/dsk/rpool/dump (dedicated)

Savecore directory: /var/crash/

Savecore enabled: yes

Save compressed: on

Si vous désactivez et supprimez le périphérique de vidage, vous devrez l'activer avec la
commande dumpadm après sa recréation. Dans la plupart des cas, vous devrez uniquement
ajuster la taille du périphérique de vidage à l'aide de la commande zfs.
Pour plus d'informations sur la taille des volumes de swap et de vidage créés par les programmes
d'installation, reportez-vous à la section “Configuration requise pour le pool racine ZFS”
à la page 112.
La taille des volumes de swap et de vidage peut être ajustée après l'installation. Pour plus
d'informations, reportez-vous à la section “Ajustement de la taille de vos périphériques de swap
et de vidage ZFS” à la page 123.
Tenez compte des points suivants lorsque vous travaillez avec des périphériques de swap et de
vidage ZFS :

122

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion de vos périphériques de swap et de vidage ZFS

■

■ Vous devez utiliser des volumes ZFS distincts pour les périphériques de swap et de vidage.
■ L'utilisation d'un fichier swap sur un système de fichiers ZFS n'est actuellement pas prise en

charge.
Si vous devez modifier votre zone de swap ou votre périphérique de vidage après
l'installation du système, utilisez les commandes swap et dumpadm de la même manière que
dans les versions précédentes de Solaris. Pour plus d'informations, reportez-vous au
Chapitre 19, “Extension de l’espace de swap (tâches)” du manuel Administration d’Oracle
Solaris : Périphériques et systèmes de fichiers et au Chapitre 17, “Gestion des informations sur
les pannes système (tâches)” du manuel Administration d’Oracle Solaris : Tâches courantes .

Ajustement de la taille de vos périphériques de swap
et de vidage ZFS
Il peut s'avérer nécessaire d'ajuster la taille des périphériques de swap et de vidage après
l'installation ou éventuellement de recréer les volumes de swap et de vidage.
■ Ajustez la taille de vos volumes de swap et de vidage.
■ Vous pouvez rétablir la propriété volsize du périphérique de vidage après l'installation

d'un système. Par exemple :

# zfs set volsize=2G rpool/dump

# zfs get volsize rpool/dump

NAME

PROPERTY VALUE

SOURCE

rpool/dump volsize

2G

-

■ Vous pouvez redimensionner le volume de swap, ou le recréer si le système n'est pas occupé.

Par exemple :

# swap -d /dev/zvol/dsk/rpool/swap

# zfs volsize=2G rpool/swap

# swap -a /dev/zvol/dsk/rpool/swap

Pour plus d'informations sur la suppression d'un périphérique de swap sur un système actif,
reportez-vous à la section “Ajout d’espace de swap dans un environnement racine ZFS
Oracle Solaris” du manuel Administration d’Oracle Solaris : Périphériques et systèmes de
fichiers.
Si vous avez besoin de plus d'espace de swap sur un système déjà installé et le périphérique
de swap est occupé, il suffit d'ajouter un autre volume de swap. Par exemple :

■

# zfs create -V 2G rpool/swap2

■ Activez le nouveau volume de swap. Par exemple :

# swap -a /dev/zvol/dsk/rpool/swap2

# swap -l

swapfile

dev swaplo

blocks

free

/dev/zvol/dsk/rpool/swap 256,1

16 1058800 1058800

/dev/zvol/dsk/rpool/swap2 256,3

16 4194288 4194288

Chapitre 5 • Gestion des composants du pool racine ZFS

123

Gestion de vos périphériques de swap et de vidage ZFS

■ Ajoutez une entrée pour le deuxième volume de swap dans le fichier /etc/vfstab. Par

exemple :

/dev/zvol/dsk/rpool/swap

-

-

swap

-

no

-

■

Sélectionnez l'une des méthodes suivantes si vous avez besoin de recréer votre zone de
swap :

■

■

Sur un système SPARC, créez la zone de swap. Configurez la taille du bloc sur 8 Ko.

# zfs create -V 2G -b 8k rpool/swap

Sur un système x86, créez la zone de swap. Configurez la taille du bloc sur 4 Ko.

# zfs create -V 2G -b 4k rpool/swap

■ Vous devez activer la zone de swap lorsqu'un nouveau périphérique de swap est ajouté ou

modifié.

■ Ajoutez une entrée pour le volume d'échange dans le fichier /etc/vfstab.

Dépannage du périphérique de vidage ZFS
Vérifiez les éléments suivants si vous rencontrez des problèmes soit lors de la capture d'un
vidage mémoire sur incident du système, soit lors du redimensionnement du périphérique de
vidage.

■

Si un vidage mémoire sur incident n'a pas été automatiquement créé, vous pouvez utiliser la
commande savecore pour enregistrer le vidage mémoire sur incident.

■ Lorsque vous installez un système de fichiers ZFS racine ou lorsque vous effectuez une

migration vers un système de fichiers racine ZFS pour la première fois, un périphérique de
vidage est automatiquement créé. Dans la plupart des cas, vous devez uniquement ajuster la
taille par défaut du périphérique de vidage si celle-ci est trop petite. Par exemple, vous
pouvez augmenter la taille du périphérique de vidage jusqu'à 40 Go sur un système
contenant une quantité de mémoire importante comme suit :

# zfs set volsize=40G rpool/dump

Le redimensionnement d'un périphérique de vidage de grande capacité peut prendre un
certain temps.

Si, pour une raison quelconque, vous devez activer un périphérique de vidage après l'avoir
créé manuellement, utilisez une syntaxe semblable à la suivante :

# dumpadm -d /dev/zvol/dsk/rpool/dump

Dump content: kernel pages

Dump device: /dev/zvol/dsk/rpool/dump (dedicated)

Savecore directory: /var/crash/

Savecore enabled: yes

Save compressed: on

124

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Initialisation à partir d'un système de fichiers racine ZFS

■ Un système avec 128 Go de mémoire ou plus nécessite un périphérique de vidage plus

important que celui créé par défaut. Si le périphérique de vidage est trop petit pour capturer
un vidage mémoire sur incident existant, un message semblable au suivant s'affiche :

# dumpadm -d /dev/zvol/dsk/rpool/dump

dumpadm: dump device /dev/zvol/dsk/rpool/dump is too small to hold a system dump

dump size 36255432704 bytes, device size 34359738368 bytes

Pour plus d'informations sur le dimensionnement des périphériques de swap et de vidage,
reportez-vous à la section “Planification de l’espace de swap” du manuel Administration
d’Oracle Solaris : Périphériques et systèmes de fichiers.

■ Vous ne pouvez pas ajouter actuellement un périphérique de vidage à un pool avec plusieurs

périphériques de niveau supérieur. Un message similaire à celui figurant ci-dessous
s'affiche :

# dumpadm -d /dev/zvol/dsk/datapool/dump

dump is not supported on device ’/dev/zvol/dsk/datapool/dump’:

’datapool’ has multiple top level vdevs

Ajoutez le périphérique de vidage au pool racine. Ce dernier ne peut pas contenir plusieurs
périphériques de niveau supérieur.

Initialisation à partir d'un système de fichiers racine ZFS

Les systèmes SPARC et les systèmes x86 s'initialisent à l'aide d'une archive d'amorçage, qui est
une image de système de fichiers contenant les fichiers requis pour l'initialisation. Lorsque vous
initialisez un système à partir d'un système de fichiers racine ZFS, les noms de chemin de
l'archive d'amorçage et du fichier noyau sont résolus dans le système de fichiers racine
sélectionné pour l'initialisation.

L'initialisation à partir d'un système de fichiers ZFS diffère de celle effectuée à partir d'un
système de fichiers UFS car avec ZFS, un spécificateur de périphérique identifie un pool de
stockage par opposition à un seul système de fichiers racine. Un pool de stockage peut contenir
plusieurs systèmes de fichiers racine ZFS initialisables. Lorsque vous initialisez un système à
partir de ZFS, vous devez spécifier un périphérique d'initialisation et un système de fichiers
racine contenu dans le pool qui a été identifié par le périphérique d'initialisation.

Par défaut, le système de fichiers sélectionné pour l'initialisation est celui qui est identifié par la
propriété bootfs du pool. Il est possible de passer outre à cette sélection par défaut en spécifiant
un autre système de fichiers initialisable inclus dans la commande boot -Z sur un système
SPARC ou en sélectionnant un autre périphérique d'initialisation à partir du BIOS sur un
système x86.

Chapitre 5 • Gestion des composants du pool racine ZFS

125

Initialisation à partir d'un système de fichiers racine ZFS

Initialisation à partir d'un disque alternatif d'un pool
racine ZFS mis en miroir
Vous pouvez connecter un disque pour créer un pool racine ZFS en miroir après l'installation.
Pour plus d'informations sur la création d'un pool racine en miroir, reportez-vous à la section
“Configuration d'un pool racine mis en miroir” à la page 117.

Consultez les problèmes connus suivants relatifs aux pools racine ZFS mis en miroir :

■

Si vous remplacez un disque de pool racine en utilisant la commande zpool replace, vous
devez installer les informations d'initialisation sur le nouveau disque à l'aide de la
commande installboot ou installgrub. Si vous créez un pool racine ZFS mis en miroir à
l'aide de la méthode d'installation initiale ou si vous utilisez la commande zpool attach
pour connecter un disque au pool racine, cette étape n'est pas nécessaire. La commande
installboot ou installgrub présente la syntaxe suivante :

sparc# installboot -F zfs /usr/platform/‘uname -i‘/lib/fs/zfs/bootblk

/dev/rdsk/c0t1d0s0

x86# installgrub /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c0t1d0s0

■ Vous pouvez effectuer l'initialisation à partir de divers périphériques d'un pool racine ZFS
mis en miroir. Selon la configuration matérielle, la mise à jour de la PROM ou du BIOS peut
s'avérer nécessaire pour spécifier un périphérique d'initialisation différent.
Vous pouvez par exemple effectuer l'initialisation à partir de l'un des deux disques
(c1t0d0s0 ou c1t1d0s0) de ce pool.

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

rpool

STATE

READ WRITE CKSUM

ONLINE

mirror-0

ONLINE

c1t0d0s0 ONLINE

c1t1d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

Sur un système SPARC, saisissez le disque alternatif à l'invite ok.

ok boot /pci@7c0/pci@0/pci@1/pci@0,2/LSILogic,sas@2/disk@1

Une fois le système réinitialisé, confirmez le périphérique d'initialisation actif. Par exemple :

SPARC# prtconf -vp | grep bootpath

bootpath: ’/pci@7c0/pci@0/pci@1/pci@0,2/LSILogic,sas@2/disk@1,0:a’

Sur un système x86, utilisez une syntaxe semblable à ce qui suit :

x86# prtconf -v|sed -n ’/bootpath/,/value/p’

name=’bootpath’ type=string items=1

value=’/pci@0,0/pci8086,25f8@4/pci108e,286@0/disk@0,0:a’

126

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Initialisation à partir d'un système de fichiers racine ZFS

■

Sur un système x86, sélectionnez un disque alternatif dans le pool racine ZFS mis en miroir
dans le menu approprié du BIOS.

Initialisation à partir d'un système de fichiers racine
ZFS sur un système SPARC
Sur un système SPARC avec environnements d'initialisation ZFS multiples, vous pouvez
initialiser à partir de tout environnement d'initialisation en utilisant la commande beadm
activate.
Au cours de l'installation et de la procédure d'activation beadm, le système de fichiers racine ZFS
est automatiquement désigné avec la propriété bootfs.
Un pool peut contenir plusieurs systèmes de fichiers d'initialisation. Par défaut, l'entrée du
système de fichiers d'initialisation dans le fichier /pool-name/boot/menu.lst est identifiée par
la propriété bootfs du pool. Cependant, une entrée menu.lst peut contenir une commande
bootfs spécifiant un autre système de fichiers du pool. Le fichier menu.lst peut ainsi contenir
les entrées de plusieurs systèmes de fichiers racine du pool.
Lorsqu'un système est installé à l'aide d'un système de fichiers racine ZFS, une entrée similaire à
l'entrée suivante est ajoutée au fichier menu.lst :

title Oracle Solaris 11 solaris SPARC

bootfs rpool/ROOT/solaris

Lorsqu'un nouvel environnement d'initialisation est créé, le fichier menu.lst est mis à jour
automatiquement.
Sur un système SPARC, deux options d'initialisation sont disponibles :
■ Une fois qu'un environnement d'initialisation ZFS a été activé, vous pouvez utiliser la
commande d'initialisation -L pour afficher la liste des systèmes de fichiers initialisables
contenus dans un pool ZFS. Vous pouvez ensuite sélectionner l'un des systèmes de fichiers
d'initialisation de la liste. Des instructions détaillées concernant l'initialisation de ce système
de fichiers s'affichent. Vous pouvez initialiser le système de fichiers sélectionné en suivant
ces instructions.

■ Utilisez la commande -Z système de fichiers pour initialiser un système de fichiers ZFS

spécifique.

Initialisation à partir d'un environnement d'initialisation ZFS spécifique

EXEMPLE 5–1
Si vous disposez de plusieurs environnements d'initialisation ZFS dans un pool de stockage ZFS
situé sur le périphérique d'initialisation de votre système, vous pouvez utiliser la commande
beadm activate pour spécifier un environnement d'initialisation par défaut.
Par exemple, les environnements d'initialisation ZFS suivants sont disponibles comme décrit
par la sortie de beadm :

Chapitre 5 • Gestion des composants du pool racine ZFS

127

Initialisation à partir d'un système de fichiers racine ZFS

EXEMPLE 5–1

Initialisation à partir d'un environnement d'initialisation ZFS spécifique

(Suite)

# beadm list

BE

--

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

solaris

-

solaris-1 NR

-

/

19.18M static 2011-01-13 15:31

8.48G static 2011-01-13 15:44

Si vous disposez de plusieurs environnements d'initialisation ZFS sur votre système SPARC,
vous pouvez utiliser la commande boot -L. Par exemple :

ok boot -L

Boot device: /pci@0/pci@0/pci@2/scsi@0/disk@3,0:a File and args: -L

1 solaris

2 solaris-1

Select environment to boot: [ 1 - 2 ]: 2

To boot the selected entry, invoke:

boot [<root-device>] -Z rpool/ROOT/solaris-1

Program terminated

ok boot -Z rpool/ROOT/solaris-1

Gardez à l'esprit que l'environnement d'initialisation initialisé à l'aide de la commande ci-dessus
n'est pas activé lors de la prochaine réinitialisation. Si vous souhaitez initialiser
automatiquement à partir de l'environnement d'initialisation sélectionné lors de l'opération
boot -Z, vous devez l'activer.

Initialisation à partir d'un système de fichiers racine
ZFS sur un système x86
Les entrées suivantes sont ajoutées au fichier /pool-name /boot/grub/menu.lst au cours du
processus d'installation ou de l'opération beadm activate pour initialiser ZFS
automatiquement :

title solaris

bootfs rpool/ROOT/solaris

kernel$ /platform/i86pc/kernel/amd64/unix -B $ZFS-BOOTFS

module$ /platform/i86pc/amd64/boot_archive

title solaris-1

bootfs rpool/ROOT/solaris-1

kernel$ /platform/i86pc/kernel/amd64/unix -B $ZFS-BOOTFS

module$ /platform/i86pc/amd64/boot_archive

Si le périphérique identifié par GRUB comme périphérique d'initialisation contient un pool de
stockage ZFS, le fichier menu.lst est utilisé pour créer le menu GRUB.
Sur un système x86 contenant plusieurs environnements d'initialisation ZFS, vous pouvez
sélectionner un environnement d'initialisation à partir du menu GRUB. Si le système de fichiers
racine correspondant à cette entrée de menu est un système de fichiers ZFS, l'option suivante est
ajoutée.

128

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Initialisation à partir d'un système de fichiers racine ZFS

-B $ZFS-BOOTFS

EXEMPLE 5–2 x86 : Initialisation d'un système de fichiers ZFS
Lors de l'initialisation à partir d'un système de fichiers ZFS, le périphérique racine est spécifié
par le paramètre d'initialisation -B $ZFS-BOOTFS. Par exemple :

title solaris

bootfs rpool/ROOT/solaris

kernel$ /platform/i86pc/kernel/amd64/unix -B $ZFS-BOOTFS

module$ /platform/i86pc/amd64/boot_archive

title solaris-1

bootfs rpool/ROOT/solaris-1

kernel$ /platform/i86pc/kernel/amd64/unix -B $ZFS-BOOTFS

module$ /platform/i86pc/amd64/boot_archive

EXEMPLE 5–3 x86 : réinitialisation rapide d'un système de fichiers racine ZFS
La fonctionnalité de réinitialisation rapide offre la possibilité de réinitialiser en quelques
secondes sur les systèmes x86. La fonctionnalité de réinitialisation rapide vous permet de
réinitialiser vers un nouveau noyau sans subir les longs délais imposés par le BIOS et le
programme d'amorçage. La possibilité de réinitialiser rapidement un système réduit
considérablement les indisponibilités, tout en améliorant l'efficacité.

Vous devez continuer à utiliser la commande init 6 lors du passage d'un environnement
d'initialisation à un autre à l'aide de la commande beadm activate. Pour d'autres opérations
système où la commande reboot est nécessaire, vous pouvez utiliser la commande reboot -f.
Par exemple :

# reboot -f

Initialisation à des fins de récupération dans un
environnement racine ZFS
Suivez la procédure suivante si vous devez initialiser le système pour pouvoir récupérer un mot
de passe root perdu ou tout problème similaire.

▼ Initialisation du système à des fins de récupération

La procédure ci-dessous vous permet de résoudre un problème lié à menu.lst ou à un mot de
passe root. Si vous devez remplacer un disque dans un pool racine, reportez-vous à la section
“Remplacement d'un disque dans un pool racine ZFS” à la page 119. Si vous devez effectuer une
récupération complète du système (à chaud), reportez-vous à la section Chapitre 12, “Archivage
des instantanés et récupération du pool racine”.

1

Sélectionnez la méthode d'initialisation appropriée :

Chapitre 5 • Gestion des composants du pool racine ZFS

129

Initialisation à partir d'un système de fichiers racine ZFS

■

■

■

■

■

x86 - Live Media : initialisez le système à partir du média d'installation et utilisez un terminal
GNOME pour la procédure de récupération.
SPARC - Installation en mode texte : initialisez le système à partir du média d'installation ou
du réseau, puis sélectionnez l'option 3 Shell dans l'écran d'installation en mode texte.
x86 - Installation en mode texte : dans le menu GRUB, sélectionnez l'entrée Text Installer
and command line, puis l'option 3 Shell dans l'écran d'installation en mode texte.
SPARC - Programme d'installation automatisée : exécutez la commande suivante pour
initialiser le système directement à partir d'un menu d'installation qui vous permet de
quitter et d'accéder à un shell.

ok boot net:dhcp

x86 - Installation automatisée : l'initialisation à partir d'un serveur d'installation sur le réseau
requiert une initialisation PXE. Sélectionnez l'entrée Text Installer and command line
du menu GRUB. Sélectionnez ensuite l'option 3 Shell à partir de l'écran d'installation en
mode texte.

Par exemple, une fois que le système est initialisé, sélectionnez l'option 3 Shell.

1 Install Oracle Solaris

2 Install Additional Drivers

3 Shell

4 Terminal type (currently xterm)

5 Reboot

Please enter a number [1]: 3

To return to the main menu, exit the shell

#

2

Sélectionnez le problème de récupération de réinitialisation :
■ Résolvez un shell racine incorrect en initialisant le système en mode monoutilisateur et en

corrigeant l'entrée de shell dans le fichier /etc/passwd.
Sur un système x86, modifiez l'entrée d'initialisation sélectionnée et ajoutez l'option -s.
Par exemple, sur un système SPARC, éteignez le système et initialisez en mode
monoutilisateur. Une fois connecté en tant qu'utilisateur root, modifiez le fichier
/etc/passwd et réparez l'entrée de shell racine.

# init 0

ok boot -s

Boot device: /pci@780/pci@0/pci@9/scsi@0/disk@0,0:a File and args: -s

SunOS Release 5.11 Version 11.0 64-bit

Copyright (c) 1983, 2011, Oracle and/or its affiliates. All rights

reserved.

Booting to milestone "milestone/single-user:default".

Hostname: tardis.central

Requesting System Maintenance Mode

SINGLE USER MODE

Enter user name for system maintenance (control-d to bypass): root

130

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Initialisation à partir d'un système de fichiers racine ZFS

Enter root password (control-d to bypass): xxxx

single-user privilege assigned to root on /dev/console.

Entering System Maintenance Mode

Jan 24 13:23:54 su: ’su root’ succeeded for root on /dev/console

Oracle Corporation SunOS 5.11 11.0 November 2011

su: No shell /usr/bin/mybash. Trying fallback shell /sbin/sh.

root@tardis.central:~# TERM =vt100; export TERM

root@tardis.central:~# vi /etc/passwd

root@tardis.central:~# <Press control-d>

logout

svc.startd: Returning to milestone all.

■ Résolvez un problème avec une entrée d'initialisation menu.lst.

Tout d'abord, vous devez initialiser le système à partir d'un média ou du réseau à l'aide de
l'une des méthodes d'initialisation répertoriées à l'étape 1. Ensuite, importez le pool racine et
réparez l'entrée menu.lst.

x86# zpool import -f rpool

x86# cd /rpool/boot/grub

x86# vi menu.lst

x86# exit

1 Install Oracle Solaris

2 Install Additional Drivers

3 Shell

4 Terminal type (currently sun-color)

5 Reboot

Please enter a number [1]: 5

Confirmez la réussite de l'initialisation du système.

■ Résolvez un mot de passe root inconnu qui vous empêche de vous connecter au système.
Tout d'abord, vous devez initialiser le système à partir d'un média ou du réseau à l'aide de
l'une des méthodes d'initialisation répertoriées à l'étape 1. Ensuite, importez le pool racine
(rpool) et montez l'environnement d'initialisation afin de supprimer l'entrée de mot de
passe root. Ce processus est identique sur les plates-formes SPARC et x86.

# zpool import -f rpool

# beadm list

be_find_current_be: failed to find current BE name

be_find_current_be: failed to find current BE name

BE

--

solaris

solaris-2

# mkdir /a

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

-

R

-

-

11.45M static 2011-10-22 00:30

12.69G static 2011-10-21 21:04

# beadm mount solaris-2 /a

# TERM=vt100

# export TERM

# cd /a/etc

# vi shadow

<Carefully remove the unknown password>

# cd /

# beadm umount solaris-2

# halt

Chapitre 5 • Gestion des composants du pool racine ZFS

131

Initialisation à partir d'un système de fichiers racine ZFS

Accédez à l'étape suivante pour définir le mot de passe root.

3

Définissez le mot de passe root en initialisant le système en mode monoutilisateur et en
définissant le mot de passe.
Cette étape suppose que vous avez supprimé un mot de passe root inconnu à l'étape précédente.
Sur un système x86, modifiez l'entrée d'initialisation sélectionnée et ajoutez l'option -s.
Sur un système SPARC, initialisez le système en mode monoutilisateur, connectez-vous en tant
qu'utilisateur root et définissez le mot de passe root. Par exemple :

ok boot -s

Boot device: /pci@780/pci@0/pci@9/scsi@0/disk@0,0:a File and args: -s

SunOS Release 5.11 Version 11.0 64-bit

Copyright (c) 1983, 2011, Oracle and/or its affiliates. All rights

reserved.

Booting to milestone "milestone/single-user:default".

Hostname: tardis.central

Requesting System Maintenance Mode

SINGLE USER MODE

Enter user name for system maintenance (control-d to bypass): root

Enter root password (control-d to bypass): <Press return>

single-user privilege assigned to root on /dev/console.

Entering System Maintenance Mode

Jan 24 13:23:54 su: ’su root’ succeeded for root on /dev/console

Oracle Corporation SunOS 5.11 11.0 November 2011

root@tardis.central:~# passwd -r files root

New Password: xxxxxx

Re-enter new Password: xxxxxx

passwd: password successfully changed for root

root@tardis.central:~# <Press control-d>

logout

svc.startd: Returning to milestone all.

132

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

6C H A P I T R E

6

Gestion des systèmes de fichiers
Oracle Solaris ZFS

Ce chapitre contient des informations détaillées sur la gestion des systèmes de fichiers ZFS
Oracle Solaris. Il aborde notamment les concepts d'organisation hiérarchique des systèmes de
fichiers, d'héritage des propriétés, de gestion automatique des points de montage et
d'interaction sur les partages.

Ce chapitre contient les sections suivantes :
■ “Gestion des systèmes de fichiers ZFS (présentation)” à la page 133
■ “Création, destruction et renommage de systèmes de fichiers ZFS” à la page 134
■ “Présentation des propriétés ZFS” à la page 137
■ “Envoi de requêtes sur les informations des systèmes de fichiers ZFS” à la page 160
■ “Gestion des propriétés ZFS” à la page 163
■ “Montage de système de fichiers ZFS” à la page 168
■ “Activation et annulation du partage des systèmes de fichiers ZFS ” à la page 173
■ “Définition des quotas et réservations ZFS” à la page 182
■ “Chiffrement des systèmes de fichiers ZFS” à la page 188
■ “Migration de systèmes de fichiers ZFS” à la page 195
■ “Mise à niveau des systèmes de fichiers ZFS” à la page 198

Gestion des systèmes de fichiers ZFS (présentation)

La création d'un système de fichiers ZFS s'effectue sur un pool de stockage. La création et la
destruction des systèmes de fichiers peuvent s'effectuer de manière dynamique, sans allocation
ni formatage manuels de l'espace disque sous-jacent. En raison de leur légèreté et de leur rôle
central dans l'administration du système ZFS, la création de ces systèmes de fichiers constitue
généralement une opération extrêmement courante.

La gestion des systèmes de fichiers ZFS s'effectue à l'aide de la commande zfs. La commande
zfs offre un ensemble de sous-commandes permettant d'effectuer des opérations spécifiques
sur les systèmes de fichiers. Chacune de ces sous-commandes est décrite en détail dans ce
chapitre. Cette commande permet également de gérer les instantanés, les volumes et les clones.

133

Création, destruction et renommage de systèmes de fichiers ZFS

Toutefois, ces fonctionnalités sont uniquement traitées de manière succincte dans ce chapitre.
Pour plus d'informations sur les instantanés et les clones, reportez-vous au Chapitre 7,
“Utilisation des instantanés et des clones ZFS Oracle Solaris”. Pour de plus amples informations
sur les volumes ZFS, reportez-vous à la section “Volumes ZFS” à la page 265.

Remarque – Dans ce chapitre, le terme jeu de données désigne de manière générique un système
de fichiers, un instantané, un clone ou un volume.

Création, destruction et renommage de systèmes de fichiers
ZFS

La création et la destruction des systèmes de fichiers ZFS s'effectuent respectivement à l'aide des
commandes zfs create et zfs destroy. Vous pouvez renommer les systèmes de fichiers ZFS
en utilisant la commande zfs rename.
■ “Création d'un système de fichiers ZFS” à la page 134
■ “Destruction d'un système de fichiers ZFS” à la page 135
■ “Modification du nom d'un système de fichiers ZFS” à la page 136

Création d'un système de fichiers ZFS
La création des systèmes de fichiers ZFS s'effectue à l'aide de la commande zfs create. La
sous-commande create ne peut contenir qu'un argument : le nom du système de fichiers à
créer. Le nom de ce système de fichiers permet également de définir le nom du chemin par
rapport au nom du pool, comme suit :

nom-pool/[nom-système-fichiers/]nom-système-fichiers

Le nom du pool et les noms des systèmes de fichiers existants mentionnés dans le chemin
déterminent l'emplacement du nouveau système de fichiers dans la structure hiérarchique. Le
dernier nom mentionné dans le chemin correspond au nom du système de fichiers à créer. Ce
nom doit respecter les conventions d'attribution de nom définies à la section “Exigences
d'attribution de noms de composants ZFS” à la page 29.

Le chiffrement d'un système de fichiers ZFS doit être activé au moment de sa création. Pour plus
d'informations sur le chiffrement d'un système de fichiers ZFS, reportez-vous à la section
“Chiffrement des systèmes de fichiers ZFS” à la page 188.

Dans l'exemple suivant, un système de fichiers nommé jeff est créé dans le système de fichiers
tank/home.

# zfs create tank/home/jeff

134

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création, destruction et renommage de systèmes de fichiers ZFS

Si le processus de création se déroule correctement, le système de fichiers ZFS est
automatiquement monté. Par défaut, les systèmes de fichiers sont montés sous /jeu-données, à
l'aide du chemin défini pour le nom du système dans la commande create. Dans cet exemple, le
système de fichiers jeff créé est monté sous /tank/home/jeff. Pour plus d'informations sur les
points de montage gérés automatiquement, reportez-vous à la section “Gestion des points de
montage ZFS” à la page 168.

Pour plus d'informations sur la commande zfs create, reportez-vous à la page de manuel
zfs(1M).

Il est possible de définir les propriétés du système de fichiers lors de la création de ce dernier.

Dans l'exemple ci-dessous, le point de montage /export/zfs est créé pour le système de fichiers
tank/home :

# zfs create -o mountpoint=/export/zfs tank/home

Pour plus d'informations sur les propriétés des systèmes de fichiers, reportez-vous à la section
“Présentation des propriétés ZFS” à la page 137.

Destruction d'un système de fichiers ZFS
La destruction d'un système de fichiers ZFS s'effectue à l'aide de la commande zfs destroy. Les
systèmes de fichiers détruits sont automatiquement démontés et ne sont plus partagés. Pour
plus d'informations sur les montages ou partages gérés automatiquement, reportez-vous à la
section “Points de montage automatiques” à la page 169.

L'exemple suivant illustre la destruction du système de fichiers tank/home/mark :

# zfs destroy tank/home/mark

Attention – Aucune invite de confirmation ne s'affiche lors de l'exécution de la sous-commande
destroy. Son utilisation requiert une attention particulière.

Si le système de fichiers à détruire est occupé et ne peut pas être démonté, la commande zfs
destroy échoue. Pour détruire un système de fichiers actif, indiquez l'option -f. L'utilisation
de cette option requiert une attention particulière. En effet, elle permet de démonter, d'annuler
le partage et de détruire des systèmes de fichiers actifs, ce qui risque d'affecter le comportement
de certaines applications.

# zfs destroy tank/home/matt

cannot unmount ’tank/home/matt’: Device busy

# zfs destroy -f tank/home/matt

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

135

Création, destruction et renommage de systèmes de fichiers ZFS

La commande zfs destroy échoue également si le système de fichiers possède des descendants.
Pour détruire un système de fichiers et l'ensemble des descendants de ce système de fichiers,
indiquez l'option -r. Ce type d'opération de destruction récursive entraîne également la
destruction des instantanés ; l'utilisation de cette option requiert donc une attention
particulière.

# zfs destroy tank/ws

cannot destroy ’tank/ws’: filesystem has children

use ’-r’ to destroy the following datasets:

tank/ws/jeff

tank/ws/bill

tank/ws/mark

# zfs destroy -r tank/ws

Si le système de fichiers à détruire possède des systèmes indirectement dépendants, même la
commande de destruction récursive échoue. Pour forcer la destruction de tous les systèmes
dépendants, y compris des systèmes de fichiers clonés situés en dehors de la structure
hiérarchique cible, vous devez indiquer l'option -R. Utilisez cette option avec précaution.

# zfs destroy -r tank/home/eric

cannot destroy ’tank/home/eric’: filesystem has dependent clones

use ’-R’ to destroy the following datasets:

tank//home/eric-clone

# zfs destroy -R tank/home/eric

Attention – Aucune invite de confirmation ne s'affiche lors de l'utilisation des options -f, - r ou
-R avec la commande zfs destroy. L'utilisation de ces options requiert donc une attention
particulière.

Pour plus d'informations sur les instantanés et les clones, reportez-vous au Chapitre 7,
“Utilisation des instantanés et des clones ZFS Oracle Solaris”.

Modification du nom d'un système de fichiers ZFS
La modification du nom d'un système de fichiers ZFS s'effectue à l'aide de la commande zfs
rename. La commande rename permet d'effectuer les opérations suivantes :
■ Modifier le nom d'un système de fichiers.
■ Déplacer le système de fichiers au sein de la hiérarchie ZFS.
■ Modifier le nom d'un système de fichiers et son emplacement au sein de la hiérarchie ZFS.

L'exemple suivant utilise la sous-commande rename pour renommer un système de fichiers
eric en eric_old :

# zfs rename tank/home/eric tank/home/eric_old

136

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

L'exemple ci-dessous illustre la modification de l'emplacement d'un système de fichiers à l'aide
de la sous-commande zfs rename :

# zfs rename tank/home/mark tank/ws/mark

Dans cet exemple, le système de fichiers mark est déplacé de tank/home vers tank/ws. Lorsque
vous modifiez l'emplacement d'un système de fichiers à l'aide de la commande rename, le
nouvel emplacement doit se trouver au sein du même pool et l'espace disque disponible doit
être suffisant pour contenir le nouveau système de fichiers. Lorsque le nouvel emplacement ne
dispose pas de suffisamment d'espace disque, l'opération rename échoue.
Pour plus d'informations sur les quotas, reportez-vous à la section “Définition des quotas et
réservations ZFS” à la page 182.
L'opération rename tente de démonter, puis de remonter le système de fichiers ainsi que ses
éventuels systèmes de fichiers descendants. Si la commande rename ne parvient pas à démonter
un système de fichiers actif, l'opération échoue. Si ce problème survient, vous devez forcer le
démontage du système de fichiers.
Pour plus d'informations sur la modification du nom des instantanés, reportez-vous à la section
“Renommage d'instantanés ZFS” à la page 202.

Présentation des propriétés ZFS

Les propriétés constituent le mécanisme principal de contrôle du comportement des systèmes
de fichiers, des volumes, des instantanés et des clones. Sauf mention contraire, les propriétés
définies dans cette section s'appliquent à tous les types de jeux de données.
■ “Propriétés ZFS natives en lecture seule” à la page 151
■ “Propriétés ZFS natives définies” à la page 152
■ “Propriétés ZFS définies par l'utilisateur” à la page 159
Les propriétés se divisent en deux catégories : les propriétés natives et les propriétés définies par
l'utilisateur. Les propriétés natives permettent de fournir des statistiques internes ou de
contrôler le comportement du système de fichiers ZFS. Certaines de ces propriétés peuvent être
définies tandis que d'autres sont en lecture seule. Les propriétés définies par l'utilisateur n'ont
aucune incidence sur le comportement des systèmes de fichiers ZFS. En revanche, elles
permettent d'annoter les jeux de données avec des informations adaptées à votre
environnement. Pour plus d'informations sur les propriétés définies par l'utilisateur,
reportez-vous à la section “Propriétés ZFS définies par l'utilisateur” à la page 159.
La plupart des propriétés pouvant être définies peuvent également être héritées. Les propriétés
héritables sont des propriétés qui, une fois définies sur un système de fichiers parent,
s'appliquent à l'ensemble de ses descendants.
Toutes ces propriétés héritables sont associées à une source indiquant la façon dont la propriété
a été obtenue. Les sources de propriétés peuvent être définies sur les valeurs suivantes :

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

137

Présentation des propriétés ZFS

local

inherited from dataset-name

default

Indique que la propriété été définie de manière explicite sur
le jeu de données à l'aide de la commande zfs set, selon la
procédure décrite à la section “Définition des propriétés
ZFS” à la page 163.
Indique que la propriété a été héritée à partir de l'ascendant
indiqué.
Indique que la valeur de la propriété n'a été ni héritée, ni
définie en local. Cette source est définie lorsque la propriété
n'est pas définie en tant que source local sur aucun système
ascendant.

Le tableau suivant répertorie les propriétés de système de fichiers ZFS natives en lecture seule et
pouvant être définies. Les propriétés natives en lecture seule sont signalées comme tel. Les
autres propriétés natives répertoriées dans le tableau peuvent être définies. Pour plus
d'informations sur les propriétés définies par l'utilisateur, reportez-vous à la section “Propriétés
ZFS définies par l'utilisateur” à la page 159.

TABLEAU 6–1 Description des propriétés ZFS natives

Valeur par défaut Description

Contrôle le processus d'héritage des entrées ACL lors
de la création de fichiers et de répertoires. Les valeurs
possibles sont discard, noallow, secure et
passthrough. Pour une description de ces valeurs,
reportez-vous à la section “Propriétés ACL”
à la page 228.

Contrôle le processus de modification des entrées ACL
lors des opérations chmod. Les valeurs possibles sont
discard, groupmask et passthrough. Pour une
description de ces valeurs, reportez-vous à la section
“Propriétés ACL” à la page 228.

Détermine si l'heure d'accès aux fichiers est mise à jour
lorsqu'ils sont consultés. La désactivation de cette
propriété évite de produire du trafic d'écriture lors de la
lecture de fichiers et permet parfois d'améliorer
considérablement les performances ; elle risque
cependant de perturber les logiciels de messagerie et
autres utilitaires du même type.

Nom de propriété

aclinherit

Type

Chaîne

secure

aclmode

Chaîne

groupmask

atime

Booléen

on

138

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

available

Valeur
numérique

SO

canmount

Booléen

on

Propriété en lecture seule indiquant la quantité
d'espace disque disponible pour un système de fichiers
et l'ensemble de ses enfants, sans tenir compte des
autres activités du pool. L'espace disque étant partagé
au sein d'un pool, l'espace disponible peut être limité
par divers facteurs, y compris la taille du pool physique,
les quotas, les réservations ou les autres jeux de
données présents au sein du pool.
L'abréviation de la propriété est avail.
Pour plus d'informations sur la détermination de
l'espace disque, reportez-vous à la section
“Comptabilisation de l'espace disque ZFS” à la page 40.

Détermine si un système de fichiers donné peut être
monté à l'aide de la commande zfs mount. Cette
propriété peut être définie sur tous les systèmes de
fichiers et ne peut pas être héritée. En revanche, lorsque
cette propriété est définie sur off, un point de montage
peut être hérité par des systèmes de fichiers
descendants. Le système de fichiers à proprement
parler n'est toutefois pas monté.
Lorsque l'option noauto est définie, un système de
fichiers peut uniquement être monté et démonté de
manière explicite. Le système de fichiers n'est pas
monté automatiquement lorsqu'il est créé ou importé,
et il n'est pas monté par la commande zfs mount-a ni
démonté par la commande zfs unmount-a.
Pour plus d'informations, reportez-vous à la section
“Propriété canmount” à la page 154.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

139

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

casesensitivity

Chaîne

mixte

checksum

Chaîne

on

Cette propriété indique si l'algorithme de
correspondance de nom de fichiers utilisé par le
système de fichiers doit être casesensitive (respecter
la casse), caseinsensitive (ne pas tenir compte de la
casse) ou autoriser une combinaison des deux styles de
correspondance (mixed). Traditionnellement, UNIX et
systèmes de fichiers POSIX ont sensibles à la casse les
noms de fichier.
La valeur mixed de cette propriété indique que le
système de fichiers peut prendre en charge les
demandes pour le comportement de correspondance
respectant la casse et pour le comportement de
correspondance ne tenant pas compte de la casse.
Actuellement, le comportement de correspondance ne
tenant pas compte de la casse sur un système de fichiers
prenant en charge un comportement mixte est limité
au produit serveur SMB d'Oracle Solaris. Pour plus
d'informations sur l'utilisation de la valeur mixed,
reportez-vous à la section “Propriété
casesensitivity” à la page 154.
Quel que soit le réglage de la propriété
casesensitivity, le système de fichiers conserve la
casse du nom indiqué pour créer un fichier. Cette
propriété ne peut pas être modifiée après la création du
système de fichiers.

Détermine la somme de contrôle permettant de vérifier
l'intégrité des données. La valeur par défaut est définie
sur on. Cette valeur permet de sélectionner
automatiquement l'algorithme approprié, actuellement
fletcher4. Les valeurs possibles sont on, off,
fletcher2, fletcher4, sha256 et sha256+mac. La
valeur off entraîne la désactivation du contrôle
d'intégrité des données utilisateur. La valeur off n'est
pas recommandée.

140

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

compression

Chaîne

off

compressratio

Valeur
numérique

SO

copies

Valeur
numérique

1

creation

Chaîne

SO

dedup

Chaîne

off

Active ou désactive la compression d'un jeu de
données. Les valeurs sont on , off et lzjb, gzip ou
gzip-N. Donner à cette propriété la valeur lzjb, gzip
ou la valeur gzip- N a actuellement le même effet que
la valeur on. L'activation de la compression sur un
système de fichiers contenant des données existantes
entraîne uniquement la compression des nouvelles
données. Les données actuelles restent non
compressées.
L'abréviation de la propriété est compress.

Propriété en lecture seule indiquant le ratio de
compression obtenu pour un jeu de données, exprimé
sous la forme d'un multiple. La compression peut être
activée en exécutant la commande zfs set
compression=on dataset.
Cette valeur est calculée sur la base de la taille logique
de l'ensemble des fichiers et de la quantité de données
physiques indiquée. Elle induit un gain explicite basé
sur l'utilisation de la propriété compression.

Définit le nombre de copies des données utilisateur par
système de fichiers. Les valeurs disponibles sont 1, 2 ou
3. Ces copies viennent s'ajouter à toute redondance au
niveau du pool. L'espace disque utilisé par plusieurs
copies de données utilisateur est chargé dans le fichier
et le jeu de données correspondants et pénalise les
quotas et les réservations. En outre, la propriété used
est mise à jour lorsque plusieurs copies sont activées.
Considérez la définition de cette propriété à la création
du système de fichiers car lorsque vous la modifiez sur
un système de fichiers existant, les modifications ne
s'appliquent qu'aux nouvelles données.

Propriété en lecture seule identifiant la date et l'heure
de création d'un jeu de données.

Contrôle la possibilité de supprimer les données en
double dans un système de fichiers ZFS. Les valeurs
possibles sont sur, off, vérifier, et
sha256[,vérifier]. La somme de contrôle par défaut
pour la suppression des doublons est sha256.
Pour plus d'informations, reportez-vous à la section
“Propriété dedup” à la page 156.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

141

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

devices

Booléen

on

encryption

Booléen

off

exec

Booléen

on

keysource

Chaîne

none

keystatus

Chaîne

none

logbias

Chaîne

latency

Contrôle si les fichiers de périphérique d'un système de
fichiers peuvent être ouverts.

Contrôle si un système de fichiers est chiffré. Un
système de fichiers chiffré signifie que les données sont
codées et qu'une clé est requise par le propriétaire du
système de fichiers pour accéder aux données.

Contrôle l'autorisation d'exécuter les programmes
dans un système de fichiers. Par ailleurs, lorsqu'elle est
définie sur off, les appels de la commande mmap(2)
avec PROT_EXEC ne sont pas autorisés.

Identifie le format et l'emplacement de la clé
encapsulant les clés du système de fichiers. Les valeurs
de propriété valides sont raw, hex,
passphrase,prompt, ou file. La clé doit être présente
lorsque le système de fichiers est créé, monté, ou chargé
en utilisant la commande zfs key -l. Si le chiffrement
est activé pour un nouveau système de fichiers, la
keysource par défaut est passphrase,prompt.

Propriété en lecture seule identifiant le statut de la clé
de chiffrement du système de fichiers. La disponibilité
de la clé d'un système de fichiers est indiquée par
available ou unavailable. Pour les systèmes de
fichiers où le chiffrement n'est pas activé, none s'affiche.

Contrôle la manière dont ZFS optimise les demandes
synchrones pour ce système de fichiers. Si la propriété
logbias est définie sur latency, ZFS utilise des
périphériques de journalisation distincts dans le pool
pour gérer les demandes à faible latence. Si la propriété
logbias est définie sur throughput, le système de
fichiers ZFS n'utilise pas de périphériques de
journalisation distincts dans le pool. Au lieu de cela, le
système de fichiers ZFS optimise les opérations
synchrones pour traiter globalement le pool et utiliser
efficacement les ressources. La valeur par défaut est
latency.

142

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

mlslabel

Chaîne

Aucune

mounted

Booléen

SO

mountpoint

Chaîne

SO

primarycache

Chaîne

all

Fournit une étiquette de sensibilité qui détermine si un
système de fichiers peut être monté dans une zone
Trusted Extensions. Si le système de fichiers étiqueté
correspond à la zone étiquetée, le système de fichiers
peut être monté et atteint depuis la zone étiquetée. La
valeur par défaut est none. Cette propriété peut
uniquement être modifiée lorsque Trusted Extensions
est activé et que l'utilisateur dispose du privilège
approprié.

Propriété en lecture seule indiquant si un système de
fichiers, un clone ou un instantané est actuellement
monté. Cette propriété ne s'applique pas aux volumes.
Les valeurs possibles sont yes ou no.

Détermine le point de montage utilisé pour le système
de fichiers. Lorsque la propriété mountpoint d'un
système de fichiers est modifiée, ce système de fichiers
ainsi que les éventuels systèmes descendants héritant
du point de montage sont démontés. Si la nouvelle
valeur est définie sur legacy, ces systèmes restent
démontés. Dans le cas contraire, ils sont
automatiquement remontés au nouvel emplacement si
la propriété était précédemment définie sur legacy ou
sur none ou s'ils étaient montés avant la modification
de la propriété. D'autre part, le partage de tout système
de fichiers est annulé puis rétabli au nouvel
emplacement.
Pour plus d'informations sur l'utilisation de cette
propriété, reportez-vous à la section “Gestion des
points de montage ZFS” à la page 168.

Contrôle les éléments mis en cache dans le cache
principal (ARC). Les valeurs possibles sont all, none et
metadata. Si elles sont définies sur all, les données
d'utilisateur et les métadonnées sont mises en cache. Si
elle est définie sur none, ni les données d'utilisateur ni
les métadonnées ne sont mises en cache. Si elles sont
définies sur metadata, seules les métadonnées sont
mises en cache. Lorsque ces propriétés sont définies sur
des systèmes de fichiers existants, seule la nouvelle E/S
est mise en cache en fonction de la valeur de ces
propriétés. Certains environnements de bases de
données pourraient bénéficier de la non-mise en cache
des données d'utilisateur. Vous devez déterminer si la
configuration des paramètres du cache est adaptée à
votre environnement.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

143

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

nbmand

Booléen

off

normalization

Chaîne

Aucune

origin

Chaîne

SO

quota

Valeur
numérique
(ou none)

none

Contrôle si le système de fichiers doit être monté avec
des verrous nbmand (obligatoires non bloquants). Cette
propriété est réservée aux clients SMB. Les
modifications apportées à cette propriété s'appliquent
uniquement lorsque le système de fichiers est démonté
puis remonté.

Cette propriété indique si un système de fichiers doit
effectuer une normalisation Unicode des noms de
fichiers dès lors que deux noms de fichier sont
comparés, et précise l'algorithme de normalisation à
utiliser. Les noms de fichier sont toujours stockés sans
être modifiés, et sont normalisés dans le cadre de
n'importe quel processus de comparaison. Si la valeur
de cette propriété est définie sur une valeur légale autre
que none et que la propriété utf8only n'est pas
renseignée, la propriété utf8only est
automatiquement définie sur on. La valeur par défaut
de la normalisation propriété est aucun. Cette
propriété ne peut pas être modifiée après la création du
système de fichiers.

Propriété en lecture seule appliquée aux systèmes de
fichiers ou aux volumes clonés et indiquant l'instantané
à partir duquel le clone a été créé. Le système d'origine
ne peut pas être détruit (même à l'aide des options -r
ou -f) tant que le clone existe.
Les systèmes de fichiers non clonés indiquent un
système d'origine none.

Limite la quantité d'espace disque qu'un système de
fichiers et ses descendants peuvent consommer. Cette
propriété permet d'appliquer une limite fixe à la
quantité d'espace disque utilisée, y compris l'espace
utilisé par les descendants, qu'il s'agisse de systèmes de
fichiers ou d'instantanés. La définition d'un quota sur
un descendant de système de fichiers déjà associé à un
quota n'entraîne pas le remplacement du quota du
système ascendant. Cette opération entraîne au
contraire l'application d'une limite supplémentaire. Les
quotas ne peuvent pas être définis pour les volumes car
la propriété volsize sert de quota implicite.
Pour plus d'informations concernant la définition de
quotas, reportez-vous à la section “Définitions de
quotas sur les systèmes de fichiers ZFS” à la page 183.

144

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

rekeydate

Chaîne

SO

readonly

Booléen

off

recordsize

Valeur
numérique

128K

referenced

Valeur
numérique

SO

refquota

Valeur
numérique
(ou none)

none

Propriété en lecture seule indiquant la date de la
dernière modification de la clé de chiffrement des
données résultant d'une opération zfs key -K ou zfs
clone -K sur ce système de fichiers. Si aucune
opération rekey n'a été effectuée, la valeur de cette
propriété est identique à la date de creation .

Contrôle l'autorisation de modifier un jeu de données.
Lorsqu'elle est définie sur on, aucune modification ne
peut être apportée.
L'abréviation de la propriété est rdonly.

Spécifie une taille de bloc suggérée pour les fichiers
d'un système de fichiers.
L'abréviation de la propriété est recsize. Pour obtenir
une description détaillée de cette propriété,
reportez-vous à la section “Propriété recordsize”
à la page 157.

Propriété en lecture seule identifiant la quantité de
données à laquelle un jeu de données a accès, lesquelles
peuvent ou non être partagées avec d'autres jeux de
données du pool.
Lorsqu'un instantané ou un clone est créé, il indique
dans un premier temps la même quantité d'espace
disque que le système de fichiers ou l'instantané à partir
duquel il a été créé. En effet, son contenu est identique.
L'abréviation de la propriété est refer.

Définit la quantité d'espace disque pouvant être utilisé
par un jeu de données. Cette propriété définit une
quantité d'espace maximale. Cette limite fixe n'inclut
pas l'espace disque utilisé par les descendants, tels que
les instantanés et les clones.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

145

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

refreservation

Valeur
numérique
(ou none)

none

reservation

Valeur
numérique
(ou none)

none

rstchown

Booléen

on

secondarycache

Chaîne

all

Définit la quantité d'espace disque minimale garantie
pour un jeu de données, à l'exclusion des descendants,
notamment les instantanés et les clones. Lorsque la
quantité d'espace disque utilisée est inférieure à cette
valeur, le système considère que le jeu de donnée utilise
la quantité d'espace spécifiée par refreservation La
réservation refreservation est prise en compte dans
l'espace disque utilisé des jeux de donnés parent et
vient en déduction de leurs quotas et réservations.
Lorsque la propriété refreservation est définie, un
instantané n'est autorisé que si suffisamment d'espace
est disponible dans le pool au-delà de cette réservation
afin de pouvoir contenir le nombre actuel d'octets
référencés dans le jeu de données.
L'abréviation de la propriété est refserv.

Définit la quantité d'espace disque minimale garantie à
un système de fichiers et ses descendants. Lorsque la
quantité d'espace disque utilisée est inférieure à cette
valeur, le système considère que le système de fichiers
utilise la quantité d'espace réservée. Les réservations
sont prises en compte dans l'espace disque utilisé du
système de fichiers parent, et viennent en déduction
des quotas et réservations de celui-ci.
L'abréviation de la propriété est reserv.
Pour plus d'informations, reportez-vous à la section
“Définition de réservations sur les systèmes de fichiers
ZFS” à la page 187.

Indique si le propriétaire du système de fichiers peut
autoriser la modification du ou des propriétaires de
fichiers. Par défaut, les opérations chown sont
restreintes. Lorsque rstchown est défini sur off,
l'utilisateur dispose du privilège
PRIV_FILE_CHOWN_SELF pour les opérations chown.

Contrôle les éléments qui sont mis en cache dans le
cache secondaire (L2ARC). Les valeurs possibles sont
all, none et metadata. Si elles sont définies sur all, les
données d'utilisateur et les métadonnées sont mises en
cache. Si elle est définie sur none, ni les données
d'utilisateur ni les métadonnées ne sont mises en cache.
Si elles sont définies sur metadata, seules les
métadonnées sont mises en cache.

146

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

setuid

shadow

Booléen

on

Chaîne

None

sharenfs

Chaîne

off

sharesmb

Chaîne

off

Contrôle l'application du bit setuid dans un système
de fichiers.

Identifie un système de fichiers ZFS en tant que shadow
du système de fichiers décrit par l'URI. Les données
sont migrées vers un système de fichiers shadow pour
lequel cette propriété est activée à partir du système de
fichiers identifié par l'URI. Le système de fichiers à
migrer doit être en lecture seule pour une migration
complète.

Contrôle si un système de fichiers ZFS est publié en
tant que partage NFS. Vous pouvez également publier
et annuler la publication d'un partage NFS de système
de fichiers NFS à l'aide des commandes zfs share et
zfs unshare. Les deux méthodes de publication d'un
partage NFS nécessitent que les propriétés du partage
NFS soient déjà définies. Pour plus d'informations sur
les propriétés du partage NFS, reportez-vous à la
commande zfs set share.
Lorsque la propriété sharenfs est modifiée, le partage
du système de fichiers et tout enfant héritant de la
propriété sont republiés avec toutes les nouvelles
options définies à l'aide de la commande zfs set
share, uniquement si la propriété était précédemment
désactivée ou si la publication des partages avait eu lieu
avant la modification de la propriété. Si la nouvelle
valeur de propriété est désactivée, les partages du
système de fichiers ne sont pas publiés.
Pour plus d'informations sur le partage des systèmes de
fichiers ZF, reportez-vous à la section “Activation et
annulation du partage des systèmes de fichiers ZFS ”
à la page 173.

Contrôle si un système de fichiers ZFS est publié en
tant que partage SMB. Vous pouvez également publier
et annuler la publication d'un partage SMB ou d'un
système de fichiers ZFS à l'aide des commandes zfs
share et zfs unshare. Les deux méthodes de
publication d'un partage SMB nécessitent que les
propriétés du partage SMB soient également définies.
Pour plus d'informations sur les propriétés du partage
SMB, reportez-vous à la commande zfs set share.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

147

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

snapdir

Chaîne

hidden

sync

Chaîne

standard

type

Chaîne

SO

Détermine si le répertoire .zfs doit être affiché ou
masqué au niveau de la racine du système de fichiers.
Pour plus d'informations sur l'utilisation des
instantanés, reportez-vous à la section “Présentation
des instantanés ZFS” à la page 199.

Détermine le comportement synchrone des
transactions d'un système de fichiers. Les valeurs
possibles sont :

■

■

■

standard, la valeur par défaut, avec laquelle les
transactions des systèmes de fichiers synchrones,
telles que fsync, O_DSYNC, O_SYNC, etc. sont
consignées dans le journal d'intention.
always, qui garantit que chaque transaction de
système de fichiers est consignée et validée sur le
stockage stable par un appel système retourné.
Cette valeur réduit sensiblement les performances.
disabled, signifie que les demandes synchrones
sont désactivées. Les transactions du système de
fichiers sont uniquement validées sur le stockage
stable lors de la validation de groupe de
transactions suivante, qui peut intervenir après un
grand nombre de secondes. Cette valeur permet les
meilleures performances, et tout risque de
corruption du pool est exclu.

Attention – Cette valeur disabled est très
dangereuse car le système de fichiers ZFS ignore les
demandes de transaction synchrones des
applications, telles que les bases de données ou les
opérations NFS. Le réglage de cette valeur sur le
système de fichiers racine ou /var actuellement
actif peut entraîner un comportement inattendu,
une perte des données de l'application ou un
accroissement de la vulnérabilité aux attaques en
boucle. N'utilisez cette valeur que si vous
parfaitement averti de tous les risques associés.

Propriété en lecture seule identifiant le type de jeu de
données comme étant un système de fichiers
(filesystem ; système de fichiers à proprement parler
ou clone), un volume (volume) ou un instantané
(snapshot).

148

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

used

Valeur
numérique

SO

usedbychildren

Valeur
numérique

off

usedbydataset

Valeur
numérique

off

usedbyrefreservationValeur

numérique

off

usedbysnapshots

Valeur
numérique

off

version

Valeur
numérique

SO

Propriété en lecture seule identifiant la quantité
d'espace disque utilisée par le jeu de données et tous ses
descendants.
Pour obtenir une description détaillée de cette
propriété, reportez-vous à la section “Propriété used”
à la page 151.

Propriété en lecture seule indiquant la quantité
d'espace disque utilisée par les enfants de ce jeu de
données, qui serait libérée si tous ses enfants étaient
détruits. L'abréviation de la propriété est usedchild.

Propriété en lecture seule indiquant la quantité
d'espace disque utilisée par le jeu de données lui-même,
qui serait libérée si ce dernier était détruit, après la
destruction préalable de tous les instantanés et la
suppression de toutes les réservations
refreservation. L'abréviation de la propriété est
usedds.

Propriété en lecture seule indiquant la quantité
d'espace disque utilisée par un jeu refreservation sur
un jeu de données, qui serait libérée si le jeu
refreservation était supprimé. L'abréviation de la
propriété est usedrefreserv.

Propriété en lecture seule indiquant la quantité
d'espace disque utilisée par les instantanés de ce jeu de
données. En particulier, elle correspond à la quantité
d'espace disque qui serait libérée si l'ensemble des
instantanés de ce jeu de données était supprimé. Notez
que cette valeur ne correspond pas simplement à la
somme des propriétés used des instantanés, car
l'espace peut être partagé par plusieurs instantanés.
L'abréviation de la propriété est usedsnap.

Identifie la version du disque d'un système de fichiers.
Cette information n'est pas liée à la version du pool.
Cette propriété peut uniquement être définie avec une
version supérieure prise en charge par la version du
logiciel. Pour plus d'informations, reportez-vous à la
commande zfs upgrade.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

149

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives
Nom de propriété

(Suite)
Valeur par défaut Description

Type

utf8only

Booléen

Off

volsize

Valeur
numérique

SO

volblocksize

Valeur
numérique

8 KB

vscan

Booléen

Off

zoned

Booléen

SO

xattr

Booléen

on

Cette propriété indique si un système de fichiers doit
rejeter des noms de fichiers contenant des caractères
non inclus dans l'ensemble de caractères au format
UTF-8. Si cette propriété est définie de façon explicite
sur off, la propriété normalization doit soit être
définie de façon non explicite soit être définie sur none
. La valeur par défaut pour la propriété utf8only est
off. Cette propriété ne peut pas être modifiée après la
création du système de fichiers.

Spécifie la taille logique des volumes.
Pour obtenir une description détaillée de cette
propriété, reportez-vous à la section “Propriété
volsize” à la page 158.

(Volumes) Spécifie la taille de bloc du volume. Une fois
que des données ont été écrites sur un volume, la taille
de bloc ne peut plus être modifiée. Vous devez donc
définir cette valeur lors de la création du volume. La
taille de bloc par défaut des volumes est de 8 Ko. Toute
puissance de deux comprise entre 512 octets et 128 Ko
est correcte.
L'abréviation de la propriété est volblock.

Contrôle si des recherches de virus doivent être
effectuées sur les fichiers standard lorsqu'un fichier est
ouvert et fermé. En plus de cette propriété, un service
d'analyse des virus doit également être activé afin de
permettre le lancement des analyses sur des logiciels
anti-virus tiers, si vous en possédez. La valeur par
défaut est off.

Indique si un système de fichiers a été ajouté ou non à
une zone globale. Si cette propriété est activée, le point
de montage ne figure pas dans la zone globale et le
système ZFS ne peut pas monter le système de fichiers
en réponse aux requêtes. Lors de la première
installation d'une zone, cette propriété est définie pour
tous les systèmes de fichiers ajoutés.
Pour plus d'informations sur l'utilisation du système
ZFS avec des zones installées, reportez-vous à la section
“Utilisation de ZFS dans un système Solaris avec zones
installées” à la page 268.

Indique si les attributs étendus sont activés (on) ou
désactivés (off) pour ce système de fichiers.

150

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

Propriétés ZFS natives en lecture seule
Les propriétés natives en lecture seule peuvent être récupérées, mais ne peuvent pas être
définies. Elles ne peuvent pas non plus être héritées. Certaines propriétés natives sont
spécifiques à un type de jeu de données. Dans ce cas, le type de jeu de données est mentionné
dans la description figurant dans le Tableau 6–1.

Les propriétés natives en lecture seule sont répertoriées dans cette section et décrites dans le
Tableau 6–1.

■

■

■

■

■

■

■

■

■

■

■

■

■

■

available

compressratio

creation

keystatus

mounted

origin

referenced

rekeydate

type

used

Pour plus d'informations sur cette propriété, reportez-vous à la section “Propriété used”
à la page 151.

usedbychildren

usedbydataset

usedbyrefreservation

usedbysnapshots

Pour plus d'informations sur la détermination de l'espace disque, notamment sur les propriétés
used, referenced et available, reportez-vous à la section “Comptabilisation de l'espace
disque ZFS” à la page 40.

Propriété used
La propriété used est une propriété en lecture seule indiquant la quantité d'espace disque
utilisée par le jeu de données et l'ensemble de ses descendants. Cette valeur est comparée au
quota et à la réservation définis pour le jeu de données. La quantité d'espace disque utilisé
n'inclut pas la réservation du jeu de données. En revanche, elle prend en compte les réservations
définies pour les éventuels jeux de données descendants. La quantité d'espace disque utilisée sur
le parent par un jeu de données, ainsi que la quantité d'espace disque libérée si le jeu de données
est détruit de façon récursive, constituent la plus grande partie de son espace utilisé et sa
réservation.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

151

Présentation des propriétés ZFS

Lors de la création d'un instantané, l'espace disque correspondant est dans un premier temps
partagé entre cet instantané et le système de fichiers ainsi que les instantanés existants (le cas
échéant). Lorsque le système de fichiers est modifié, l'espace disque précédemment partagé
devient dédié à l'instantané. Il est alors comptabilisé dans l'espace utilisé par cet instantané.
L'espace disque utilisé par un instantané représente ses données uniques. La suppression
d'instantanés peut également augmenter l'espace disque dédié et utilisé par les autres
instantanés. Pour plus d'informations sur les instantanés et les questions d'espace,
reportez-vous à la section “Comportement d'espace saturé” à la page 41.
La quantité d'espace disque utilisé, disponible et référencé ne comprend pas les modifications
en attente. Ces modifications sont généralement prises en compte au bout de quelques
secondes. La modification d'un disque utilisant la fonction fsync(3c) ou O_SYNC ne garantit pas
la mise à jour immédiate des informations concernant l'utilisation de l'espace disque.
Les informations de propriété usedbychildren, usedbydataset , usedbyrefreservation et
usedbysnapshots peuvent être affichées à l'aide de la commande zfs list -o space. Ces
propriétés divisent la propriété used en espace disque utilisé par les descendants. Pour plus
d'informations, reportez-vous au Tableau 6–1.

Propriétés ZFS natives définies
Les propriétés natives définies sont les propriétés dont les valeurs peuvent être récupérées et
modifiées. La définition des propriétés natives s'effectue à l'aide de la commande zfs set, selon
la procédure décrite à la section “Définition des propriétés ZFS” à la page 163 ou à l'aide de la
commande zfs create, selon la procédure décrite à la section “Création d'un système de
fichiers ZFS” à la page 134. A l'exception des quotas et des réservations, les propriétés natives
définies sont héritées. Pour plus d'informations sur les quotas et les réservations, reportez-vous
à la section “Définition des quotas et réservations ZFS” à la page 182.
Certaines propriétés natives définies sont spécifiques à un type de jeu de données. Dans ce cas,
le type de jeu de données est mentionné dans la description figurant dans le Tableau 6–1. Sauf
indication contraire, les propriétés s'appliquent à tous les types de jeu de données : aux systèmes
de fichiers, aux volumes, aux clones et aux instantanés.
Les propriétés pouvant être définies sont répertoriées dans cette section et décrites dans le
Tableau 6–1.

■

■

■

■

■

aclinherit

Pour obtenir une description détaillée, reportez-vous à la section “Propriétés ACL”
à la page 228.

atime

canmount

casesensitivity

checksum

152

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

compression

copies

devices

dedup

encryption

exec

keysource

logbias

mlslabel

mountpoint

nbmand

normalization

primarycache

quota

readonly

recordsize

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section
“Propriété recordsize” à la page 157.

refquota

refreservation

reservation

rstchown

secondarycache

sharesmb

sharenfs

setuid

snapdir

version

vscan

utf8only

volsize

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section
“Propriété volsize” à la page 158.

volblocksize

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

153

Présentation des propriétés ZFS

■

■

zoned

xattr

Propriété canmount
Si la propriété canmount est désactivée (valeur off), le système de fichiers ne peut pas être
monté à l'aide de la commande zfs mount, ni de la commande zfs mount -a. Définir cette
propriété sur off équivaut à définir la propriété mountpoint sur none, à la différence près que le
système de fichiers conserve une propriété mountpoint normale pouvant être héritée. Vous
pouvez par exemple définir cette propriété sur la valeur off et définir des propriétés héritées
pour les systèmes de fichiers descendants. Toutefois, le système de fichiers parent à proprement
parler n'est jamais monté, ni accessible par les utilisateurs. Dans ce cas, le système de fichiers
parent sert de conteneur afin de pouvoir définir des propriétés sur le conteneur ; toutefois, le
conteneur à proprement parler n'est jamais accessible.

L'exemple suivant illustre la création du système de fichiers userpool avec la propriété
canmount désactivée (valeur off). Les points de montage des systèmes de fichiers utilisateur
descendants sont définis sur un emplacement commun, /export/home. Les systèmes de fichiers
descendants héritent des propriétés définies sur le système de fichiers parent, mais celui-ci n'est
jamais monté.

# zpool create userpool mirror c0t5d0 c1t6d0

# zfs set canmount=off userpool

# zfs set mountpoint=/export/home userpool

# zfs set compression=on userpool

# zfs create userpool/user1

# zfs create userpool/user2

# zfs mount

userpool/user1

userpool/user2

/export/home/user1

/export/home/user2

Lorsque la propriété canmount est définie sur noauto, le système de fichiers peut uniquement
être monté de façon explicite et pas automatique.

Propriété casesensitivity
Cette propriété indique si l'algorithme de correspondance de nom de fichiers utilisé par le
système de fichiers doit être casesensitive (respecter la casse), caseinsensitive (ne pas
tenir compte de la casse) ou autoriser une combinaison des deux styles de correspondance
(mixed).

Lorsqu'une demande de correspondance ne tenant pas compte de la casse est effectuée sur un
système de fichiers défini sur mixed, le comportement est généralement identique à ce qu'il
serait sur un système de fichiers ne tenant pas compte de la casse. Toutefois, un système de
fichiers doté d'une sensibilité à la casse "mixte" peut contenir des répertoires portant des noms
uniques en cas de respect de la casse, mais qui ne sont pas uniques lorsque la casse n'est pas prise
en compte.

154

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

Par exemple, un répertoire peut contenir des fichiers nommés foo, Foo et FOO. Si une demande
de correspondance ne tenant pas compte de la casse est effectuée pour l'une des formes possibles
de foo (par exemple foo, FOO, FoO, fOo, etc.), l'algorithme de correspondance sélectionne en
tant que correspondance l'un des trois fichiers existants. Il est impossible de prévoir avec
certitude quel fichier sera choisi comme correspondance par l'algorithme ; en revanche, il est
certain que le même fichier sera choisi comme correspondance pour toutes les formes de foo.
Le fichier choisi comme correspondance ne tenant pas compte de la casse pour foo, FOO , foO,
Foo, et ainsi de suite, est toujours le même, tant que le répertoire n'est pas modifié.

Les propriétés utf8only, normalization et casesensitivity fournissent également de
nouvelles autorisations qui peuvent être attribuées à des utilisateurs non privilégiés par le biais
de l'administration déléguée de ZFS. Pour plus d'informations, reportez-vous à la section
“Délégation d'autorisations ZFS” à la page 252.

Propriété copies
A des fins de fiabilité, les métadonnées d'un système de fichiers ZFS sont automatiquement
stockées plusieurs fois sur différents disques, lorsque cela est possible. Cette fonction est connue
sous le terme anglais de ditto blocks.

Cette version vous permet également de stocker plusieurs copies des données utilisateur par
système de fichiers à l'aide de la commande zfs set copies. Par exemple :

# zfs set copies=2 users/home

# zfs get copies users/home

NAME

PROPERTY VALUE

SOURCE

users/home copies

2

local

Les valeurs disponibles sont 1, 2 et 3. La valeur par défaut est 1. Ces copies constituent un ajout à
toute redondance de niveau pool, par exemple dans une configuration en miroir ou RAID-Z.

Stocker plusieurs copies des données utilisateur ZFS présente les avantages suivants :
■ Cela améliore la rétention des données en autorisant leur récupération à partir d'erreurs de
lecture de blocs irrécupérables, comme par exemple des défaillances de média (plus connues
sous le nom de bit rot) pour l'ensemble des configurations ZFS.

■ Cela garantit la sécurité des données, même lorsqu'un seul disque est disponible.
■ Cela permet de choisir les stratégies de protection des données par système de fichiers et de

dépasser les capacités du pool de stockage.

Remarque – Selon l'allocation des blocs "ditto" dans le pool de stockage, plusieurs copies peuvent
être placées sur un seul disque. La saturation ultérieure d'un disque peut engendrer
l'indisponibilité de tous les blocs "ditto".

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

155

Présentation des propriétés ZFS

Vous pouvez envisager l'utilisation des blocs "ditto" lorsque vous créez accidentellement un
pool non redondant et lorsque vous avez besoin de définir des stratégies de conservation de
données.

Propriété dedup
La propriété dedup détermine si les données en double sont supprimées d'un système de
fichiers. Si la propriété dedup est activée pour un système de fichiers, les blocs de données
dupliquées sont supprimés de façon synchrone. Par conséquent, seules les données uniques
sont stockées et les composants communs sont partagés entre les fichiers.

N'activez pas la propriété dedup sur des systèmes de fichiers résidant sur des systèmes de
production avant d'avoir passé en revue les points suivants :
1. Déterminez si vous pouvez économiser de l'espace grâce à la suppression des doublons. Si la
suppression des doublons ne s'appliquent pas à vos données, inutile d'activer cette propriété.
Par exemple :

# zdb -S tank

Simulated DDT histogram:

bucket

allocated

referenced

______

______________________________

______________________________

refcnt

blocks

LSIZE

PSIZE

DSIZE

blocks

LSIZE

PSIZE

DSIZE

------

------

-----

-----

-----

------

-----

-----

-----

1

2

4

8

16

32

64

128

256

512

2K

8K

2.27M

239G

188G

194G

2.27M

239G

188G

194G

327K

34.3G

27.8G

28.1G

698K

73.3G

59.2G

59.9G

30.1K

2.91G

2.10G

2.11G

152K

14.9G

10.6G

10.6G

7.73K

691M

529M

529M

74.5K

6.25G

4.79G

4.80G

673

43.7M

25.8M

25.9M

13.1K

822M

492M

494M

197

12.3M

7.02M

7.03M

7.66K

480M

269M

270M

47

22

7

4

1

1

1.27M

626K

626K

3.86K

103M

51.2M

51.2M

908K

250K

251K

3.71K

150M

40.3M

40.3M

302K

48K

53.7K

2.27K

88.6M

17.3M

19.5M

131K

7.50K

7.75K

2.74K

102M

5.62M

5.79M

2K

128K

2K

5K

2K

5K

3.23K

6.47M

6.47M

6.47M

13.9K

1.74G

69.5M

69.5M

Total

2.63M

277G

218G

225G

3.22M

337G

263G

270G

dedup = 1.20, compress = 1.28, copies = 1.03, dedup * compress / copies = 1.50

Si le ratio estimé est supérieur à 2, la suppression des doublons est susceptible de vous faire
gagner de la place.

Dans l'exemple ci-dessus, le ratio de suppression des doublons est inférieur à 2, si bien que
l'activation de dedup n'est pas recommandée.

2. Assurez-vous que votre système dispose de suffisamment de mémoire pour prendre en

charge dedup.
■ Chaque entrée de table dedup interne a une taille d'environ 320 octets.
■ Multipliez le nombre de blocs alloués par 320. Par exemple :

in-core DDT size = 2.63M x 320 = 841.60M

156

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

3. Les performances de la propriété dedup sont optimisées lorsque le tableau de suppression
des doublons tient en mémoire. Si ce tableau doit être écrit sur le disque, les performances
diminuent. Par exemple, la suppression d'un système de fichiers volumineux lorsque
l'option de dépuplication est activée entrave considérablement les performances du système
si les conditions relatives à la mémoire évoquées plus haut ne sont pas satisfaites.

Quand dedup est activé, l'algorithme de somme de contrôle dedup écrase la propriété checksum.
Définir la valeur de propriété sur verify équivaut à spécifier sha256,verify. Si la propriété est
définie sur verify et que deux blocs ont la même signature, ZFS effectue une vérification octet
par octet avec le bloc existant afin de garantir que les contenus sont identiques.

Cette propriété peut être activée pour chaque système de fichiers. Par exemple :

# zfs set dedup=on tank/home

Vous pouvez utiliser la commande zfs get pour déterminer si la propriété dedup est définie.

Bien que la suppression des doublons soit définie en tant que propriété du système de fichiers,
elle s'étend à l'échelle du pool. Par exemple, vous pouvez identifier le ratio de suppression des
doublons. Par exemple :

# zpool list tank

NAME

SIZE ALLOC

FREE

CAP DEDUP HEALTH ALTROOT

rpool

136G 55.2G 80.8G

40% 2.30x ONLINE -

La colonne DEDUP indique le nombre de suppressions de doublons effectuées. Si la propriété
dedup n'est activée sur aucun système de fichiers ou si la propriété dedup vient d'être activée sur
le système de fichiers, le ratio DEDUP est 1.00x.

Vous pouvez utiliser la commande zpool get pour déterminer la valeur de la propriété
dedupratio. Par exemple :

# zpool get dedupratio export

NAME

PROPERTY

VALUE SOURCE

rpool dedupratio 3.00x -

Cette propriété du pool illustre le nombre de suppressions de doublons de données effectuées
dans ce pool.

Propriété encryption
Vous pouvez utiliser la propriété encryption pour chiffrer les systèmes de fichiers ZFS. Pour
plus d'informations, reportez-vous à la section “Chiffrement des systèmes de fichiers ZFS”
à la page 188.

Propriété recordsize
La propriété recordsize spécifie une taille de bloc suggérée pour les fichiers du système de
fichiers.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

157

Présentation des propriétés ZFS

Cette propriété s'utilise uniquement pour les charges de travail de base de données accédant à
des fichiers résidant dans des enregistrements à taille fixe. Le système ZFS ajuste
automatiquement les tailles en fonction d'algorithmes internes optimisés pour les schémas
d'accès classiques. Pour les bases de données générant des fichiers volumineux mais accédant
uniquement à certains fragments de manière aléatoire, ces algorithmes peuvent se révéler
inadaptés. La définition d'une valeur recordsize supérieure ou égale à la taille d'enregistrement
de la base de données peut améliorer les performances du système de manière significative. Il est
vivement déconseillé d'utiliser cette propriété pour les systèmes de fichiers à usage générique.
En outre, elle peut affecter les performances du système. La taille spécifiée doit être une
puissance de 2 supérieure ou égale à 512 octets et inférieure ou égale à 128 Ko. La modification
de la valeur recordsize du système de fichiers affecte uniquement les fichiers créés
ultérieurement. Cette modification n'affecte pas les fichiers existants.

L'abréviation de la propriété est recsize.

Propriété sharesmb
Cette propriété permet de partager des systèmes de fichiers ZFS avec le service Oracle Solaris
SMB et d'identifier les options à utiliser.

Etant donné que le partage SMB requiert un nom de ressource, un nom de ressource unique est
construit à partir du nom du système de fichiers. Le nom construit est une copie du nom du
système de fichiers, à ceci près que les caractères du nom du système de fichiers qui ne sont pas
autorisés dans un nom de ressource sont remplacés par des traits de soulignement (_). Un
pseudo-propriété appelée name est également prise en charge et permet de remplacer le nom du
système de fichiers par un nom particulier. Ce nom particulier remplace ensuite le nom du
système de fichiers placé en préfixe en cas d'héritage.

Par exemple, si le système de fichiers, data/home/john est défini sur name=john, le nom de
ressource de data/home/john est john. S'il existe un système de fichiers enfant de
data/home/john/backups, son nom de ressource est john_backups. Lorsque la propriété
sharesmb est modifiée pour un système de fichiers, le système de fichiers et les éventuels enfants
héritant la propriété sont à nouveau partagés avec les nouvelles options, ce à condition que la
propriété ait été auparavant définie sur off ou que le système de fichiers et ses enfants aient été
partagés avant la modification de la propriété. Si la nouvelle propriété est définie sur off, le
partage des systèmes de fichiers est annulé.

Pour des exemples d'utilisation de la propriété sharesmb, reportez-vous à la section “Activation
et annulation du partage des systèmes de fichiers ZFS ” à la page 173.

Propriété volsize
La propriété volsize spécifie la taille logique du volume. Par défaut, la création d'un volume
définit une réservation de taille identique. Toute modification apportée à la valeur de la
propriété volsize se répercute dans des proportions identiques au niveau de la réservation. Ce
fonctionnement permet d'éviter les comportements inattendus lors de l'utilisation des volumes.

158

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des propriétés ZFS

L'utilisation de volumes contenant moins d'espace disponible que la valeur indiquée risque,
suivant le cas, d'entraîner des comportements non valides et des corruptions de données. Ces
symptômes peuvent également survenir lors de la modification et notamment de la réduction
de la taille du volume en cours d'utilisation. Faites preuve de prudence lorsque vous ajustez la
taille d'un volume.

Même s'il s'agit d'une opération déconseillée, vous avez la possibilité de créer des volumes
fragmentés. Pour ce faire, spécifiez l'étiquette -s dans la commande zfs create -V ou modifiez
la réservation, une fois le volume créé. Un volume fragmenté désigne un volume dont la
réservation est différente de la taille de volume. Les modifications apportées à la propriété
volsize des volumes fragmentés ne sont pas répercutées au niveau de la réservation.

Pour plus d'informations sur l'utilisation des volumes, reportez-vous à la section “Volumes
ZFS” à la page 265.

Propriétés ZFS définies par l'utilisateur
Outre les propriétés natives, le système ZFS prend en charge des propriétés définies par
l'utilisateur. Les propriétés définies par l'utilisateur n'ont aucune incidence sur le
comportement du système ZFS. En revanche, elles permettent d'annoter les jeux de données
avec des informations adaptées à votre environnement.

Les noms de propriétés définies par l'utilisateur doivent respecter les conventions suivantes :
■ Elles doivent contenir le caractère ":" (deux points) afin de les distinguer des propriétés

natives.

■ Elles doivent contenir des lettres en minuscule, des chiffres ou les signes de ponctuation

suivants : ':', '+','.', '_'.

■ La longueur maximale du nom d'une propriété définie par l'utilisateur est de 256 caractères.

La syntaxe attendue des noms de propriétés consiste à regrouper les deux composants suivants
(cet espace de noms n'est toutefois pas appliqué par les systèmes ZFS) :

module:property
Si vous utilisez des propriétés définies par l'utilisateur dans un contexte de programmation,
spécifiez un nom de domaine DNS inversé pour le composant module des noms de propriétés,
afin de réduire la probabilité que deux packages développés séparément n'utilisent un nom de
propriété identique à des fins différentes. Les noms de propriété commençant par com.oracle.
sont réservés à l'usage d'Oracle Corporation.

Les valeurs des propriétés définies par l'utilisateur doivent respecter les conventions suivantes :
■ Elles doivent être constituées de chaînes arbitraires systématiquement héritées et elle ne

doivent jamais être validées.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

159

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

■ La longueur maximale de la valeur d'une propriété définie par l'utilisateur est de

1024 caractères.

Par exemple :

# zfs set dept:users=finance userpool/user1

# zfs set dept:users=general userpool/user2

# zfs set dept:users=itops userpool/user3

Toutes les commandes fonctionnant avec des propriétés (par exemple, les commandes zfs
list, zfs get, zfs set, etc.) permettent d'utiliser des propriétés natives et des propriétés
définies par l'utilisateur.

Par exemple :

zfs get -r dept:users userpool

NAME

PROPERTY

VALUE

SOURCE

userpool

dept:users all

userpool/user1 dept:users finance

userpool/user2 dept:users general

userpool/user3 dept:users itops

local

local

local

local

Pour supprimer une propriété définie par l'utilisateur, utilisez la commande zfs inherit. Par
exemple :

# zfs inherit -r dept:users userpool

Si cette propriété n'est définie dans aucun jeu de données parent, elle est définitivement
supprimée.

Envoi de requêtes sur les informations des systèmes de
fichiers ZFS

La commande zfs list contient un mécanisme extensible permettant d'afficher et d'envoyer
des requêtes sur les informations des systèmes de fichiers. Cette section décrit les requêtes de
base ainsi que les requêtes plus complexes.

Affichage des informations de base des systèmes ZFS
La commande zfs list spécifiée sans option permet de répertorier les informations de base
sur les jeux de données. Cette commande affiche le nom de tous les jeux de données définis sur
le système ainsi que les valeurs used, available, referenced et mountpoint correspondantes.
Pour plus d'informations sur ces propriétés, reportez-vous à la section “Présentation des
propriétés ZFS” à la page 137.

Par exemple :

160

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

# zfs list

users

2.00G 64.9G

32K /users

users/home

2.00G 64.9G

35K /users/home

users/home/cindy

548K 64.9G

548K /users/home/cindy

users/home/mark

1.00G 64.9G 1.00G /users/home/mark

users/home/neil

1.00G 64.9G 1.00G /users/home/neil

Cette commande permet d'afficher des jeux de données spécifiques. Pour cela, spécifiez le nom
du ou des jeux de données à afficher sur la ligne de commande. Vous pouvez également spécifier
l'option -r pour afficher de manière récursive tous les descendants des jeux de données. Par
exemple :

# zfs list -t all -r users/home/mark

NAME

USED AVAIL REFER MOUNTPOINT

users/home/mark

1.00G 64.9G 1.00G /users/home/mark

users/home/mark@yesterday

users/home/mark@today

0

0

- 1.00G -

- 1.00G -

Vous pouvez utiliser la commande zfs list avec le point de montage d'un système de fichiers.
Par exemple :

# zfs list /user/home/mark

NAME

USED AVAIL REFER MOUNTPOINT

users/home/mark 1.00G 64.9G 1.00G /users/home/mark

L'exemple suivant montre comment afficher des informations de base sur tank/home/gina et
l'ensemble de ses systèmes de fichiers descendants :

# zfs list -r users/home/gina

NAME

USED AVAIL REFER MOUNTPOINT

users/home/gina

2.00G 62.9G

32K /users/home/gina

users/home/gina/projects

2.00G 62.9G

33K /users/home/gina/projects

users/home/gina/projects/fs1 1.00G 62.9G 1.00G /users/home/gina/projects/fs1

users/home/gina/projects/fs2 1.00G 62.9G 1.00G /users/home/gina/projects/fs2

Pour plus d'informations sur la commande zfs list, reportez-vous à la page de manuel
zfs(1M).

Création de requêtes ZFS complexes
Les options o, -t et -H permettent de personnaliser la sortie de la commande -zfs list.

Vous pouvez également personnaliser la sortie des valeurs de propriété en spécifiant l'option -o
ainsi que la liste des propriétés souhaitées séparées par une virgule. Toute propriété de jeu de
données peut être utilisée en tant qu'argument valide. Pour consulter la liste de toutes les
propriétés de jeu de données prises en charge, reportez-vous à la section “Présentation des
propriétés ZFS” à la page 137. Outre les propriétés répertoriées dans cette section, la liste de
l'option -o peut également contenir la valeur littérale name afin de définir l'inclusion du nom de
jeu de données dans la sortie.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

161

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

Les exemples suivants illustrent l'utilisation de la commande zfs list pour afficher le nom de
jeu de données et des valeurs sharenfs et mountpoint.

# zfs list -r -o name,sharenfs,mountpoint users/home

NAME

users/home

users/home/cindy

users/home/gina

users/home/gina/projects

SHARENFS MOUNTPOINT

on

on

on

on

/users/home

/users/home/cindy

/users/home/gina

/users/home/gina/projects

users/home/gina/projects/fs1 on

/users/home/gina/projects/fs1

users/home/gina/projects/fs2 on

/users/home/gina/projects/fs2

users/home/mark

users/home/neil

on

on

/users/home/mark

/users/home/neil

L'option -t permet de spécifier le type de jeu de données à afficher. Le tableau suivant décrit les
différents types valides.

TABLEAU 6–2 Types de jeux de données ZFS

Type

filesystem

volume

snapshot

Description

Systèmes de fichiers et clones

Volumes

Instantanés

L'option -t permet de spécifier la liste des types de jeux de données à afficher, séparés par une
virgule. L'exemple suivant illustre l'affichage du nom et de la propriété -used de l'ensemble des
systèmes de fichiers via l'utilisation simultanée des options -t et o :

# zfs list -r -t filesystem -o name,used users/home

NAME

users/home

users/home/cindy

users/home/gina

users/home/gina/projects

USED

4.00G

548K

2.00G

2.00G

users/home/gina/projects/fs1 1.00G

users/home/gina/projects/fs2 1.00G

users/home/mark

users/home/neil

1.00G

1.00G

L'option -H permet d'exclure l'en-tête de la commande zfs list lors de la génération de la
sortie. L'option -H permet de remplacer les espaces par un caractère de tabulation. Cette option
permet notamment d'effectuer des analyses sur les sorties (par exemple, des scripts). L'exemple
suivant illustre la sortie de la commande zfs list spécifiée avec l'option -H :

# zfs list -r -H -o name users/home

users/home

users/home/cindy

users/home/gina

users/home/gina/projects

162

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion des propriétés ZFS

users/home/gina/projects/fs1

users/home/gina/projects/fs2

users/home/mark

users/home/neil

Gestion des propriétés ZFS

La gestion des propriétés de jeu de données s'effectue à l'aide des sous-commandes set,
inherit et get de la commande zfs.
■ “Définition des propriétés ZFS” à la page 163
■ “Héritage des propriétés ZFS” à la page 164
■ “Envoi de requêtes sur les propriétés ZFS” à la page 165

Définition des propriétés ZFS
La commande zfs set permet de modifier les propriétés de jeu de données pouvant être
définies. Vous pouvez également définir les propriétés lors de la création des jeux de données à
l'aide de la commande zfs create. Pour consulter la listes des propriétés de jeu de données
définies, reportez-vous à la section “Propriétés ZFS natives définies” à la page 152.
La commande zfs set permet d'indiquer une séquence propriété/valeur au format
property=value, suivie du nom du jeu de données. Lors de chaque appel de la commande zfs
set, vous ne pouvez définir ou modifier qu'une propriété à la fois.
L'exemple suivant illustre la définition de la propriété atime sur la valeur off pour tank/home.

# zfs set atime=off tank/home

Vous pouvez également définir les propriétés des systèmes de fichiers une fois ces derniers
créés. Par exemple :

# zfs create -o atime=off tank/home

Vous pouvez spécifier des valeurs de propriété numériques en utilisant les suffixes faciles à
utiliser suivants (par taille croissante) : BKMGTPEZ. Ces suffixes peuvent être suivis de la lettre b
(signifiant "byte", octet) à l'exception du suffixe B, qui fait déjà référence à cette unité de mesure.
Les quatre invocations suivantes de zfs set sont des expressions numériques équivalentes qui
définissent la propriété quota sur la valeur de 20 Go sur le système de fichiers
users/home/mark :

# zfs set quota=20G users/home/mark

# zfs set quota=20g users/home/mark

# zfs set quota=20GB users/home/mark

# zfs set quota=20gb users/home/mark

Si vous tentez de définir une propriété sur un système de fichiers à 100% de sa capacité, un
message semblable à celui-ci s'affichera :

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

163

Gestion des propriétés ZFS

# zfs set quota=20gb users/home/mark

cannot set property for ’/users/home/mark’: out of space

Les valeurs des propriétés non numériques respectent la casse et doivent être en lettres
minuscules, à l'exception de mountpoint. Les valeurs de cette propriété peuvent contenir à la
fois des majuscules et des minuscules.

Pour plus d'informations sur la commande zfs set, reportez-vous à la page de manuel
zfs(1M).

Héritage des propriétés ZFS
A l'exception des quotas et des réservations, toutes les propriétés pouvant être définies héritent
de la valeur du système de fichiers parent (sauf si un quota ou une réservation est explicitement
défini sur le système de fichiers descendant). Si aucune valeur explicite n'est définie pour une
propriété d'un système ascendant, la valeur par défaut de cette propriété est appliquée. Vous
pouvez utiliser la commande zfs inherit pour effacer la valeur d'une propriété et faire ainsi
hériter la valeur du système de fichiers parent.

L'exemple suivant utilise la commande zfs set pour activer la compression du système de
fichiers tank/home/jeff. La commande zfs inherit est ensuite exécutée afin de supprimer la
valeur de la propriété compression, entraînant ainsi l'héritage de la valeur par défaut off. En
effet, la propriété compression n'est définie localement ni pour home, ni pour tank ; la valeur par
défaut est donc appliquée. Si la compression avait été activée pour ces deux systèmes, la valeur
définie pour le système ascendant direct aurait été utilisée (en l'occurrence, home).

# zfs set compression=on tank/home/jeff

# zfs get -r compression tank/home

NAME

PROPERTY

VALUE

SOURCE

tank/home

compression off

default

tank/home/eric

compression off

default

tank/home/eric@today compression -

-

tank/home/jeff

compression on

local

# zfs inherit compression tank/home/jeff

# zfs get -r compression tank/home

NAME

PROPERTY

VALUE

SOURCE

tank/home

compression off

default

tank/home/eric

compression off

default

tank/home/eric@today compression -

-

tank/home/jeff

compression off

default

La sous-commande inherit est appliquée de manière récursive lorsque l'option -r est spécifiée.
Dans l'exemple suivant, la commande entraîne l'héritage de la valeur de la propriété
compression pour tank/home ainsi que pour ses éventuels descendants :

# zfs inherit -r compression tank/home

164

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion des propriétés ZFS

Remarque – L'utilisation de l'option -r supprime la valeur de propriété actuelle pour l'ensemble
des systèmes de fichiers descendants.

Pour plus d'informations sur la commande zfs inherit, reportez-vous à la page de manuel
zfs(1M).

Envoi de requêtes sur les propriétés ZFS
Le moyen le plus simple pour envoyer une requête sur les valeurs de propriété consiste à
exécuter la commande zfs list. Pour plus d'informations, reportez-vous à la section
“Affichage des informations de base des systèmes ZFS” à la page 160. Cependant, dans le cadre
de requêtes complexes et pour les scripts, utilisez la commande zfs get afin de fournir des
informations plus détaillées dans un format personnalisé.

La commande zfs get permet de récupérer les propriétés de jeu de données. L'exemple suivant
illustre la récupération d'une seule valeur de propriété au sein d'un jeu de données :

# zfs get checksum tank/ws

NAME

PROPERTY

VALUE

tank/ws

checksum

on

SOURCE

default

La quatrième colonne SOURCE indique l'origine de la valeur de cette propriété. Le tableau
ci-dessous définit les valeurs possibles de la source.

TABLEAU 6–3 Valeurs possibles de la colonne SOURCE (commande zfs get)

Valeur

default

inherited from dataset-name

local

temporary

Description

Cette propriété n'a jamais été définie de manière explicite pour ce jeu
de données ni pour ses systèmes ascendants. La valeur par défaut est
utilisée.

La valeur de propriété est héritée du jeu de données parent spécifié par
la chaîne dataset-name.

La valeur de propriété a été définie de manière explicite pour ce jeu de
données à l'aide de la commande zfs set.

Cette valeur de propriété a été définie à l'aide la commande zfs mount
spécifiée avec l'option - o et n'est valide que pour la durée du montage.
Pour plus d'informations sur les propriétés de point de montage
temporaires, reportez-vous à la section “Utilisation de propriétés de
montage temporaires” à la page 172.

- (none)

Cette propriété est en lecture seule. Sa valeur est générée par ZFS.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

165

Gestion des propriétés ZFS

Le mot-clé all permet de récupérer toutes les valeurs de propriétés du jeu de données. Les
exemples suivants utilisent le mot-clé all :

# zfs get all tank/home

NAME

PROPERTY

VALUE

tank/home type

filesystem

SOURCE

-

tank/home creation

Fri Apr 22 10:42 2011 -

tank/home used

tank/home available

tank/home referenced

tank/home compressratio

tank/home mounted

tank/home quota

tank/home reservation

tank/home recordsize

611K

66.9G

33K

1.00x

yes

none

none

128K

tank/home mountpoint

/tank/home

tank/home sharenfs

tank/home checksum

tank/home compression

tank/home atime

tank/home devices

tank/home exec

tank/home setuid

tank/home readonly

tank/home zoned

off

on

off

on

on

on

on

off

off

tank/home snapdir

hidden

tank/home aclinherit

restricted

tank/home canmount

tank/home xattr

tank/home copies

tank/home version

tank/home utf8only

tank/home normalization

on

on

1

5

off

none

tank/home casesensitivity

sensitive

tank/home vscan

tank/home nbmand

tank/home sharesmb

tank/home refquota

tank/home refreservation

tank/home primarycache

tank/home secondarycache

tank/home usedbysnapshots

tank/home usedbydataset

off

off

off

none

none

all

all

0

33K

tank/home usedbychildren

578K

tank/home usedbyrefreservation 0

tank/home logbias

latency

tank/home dedup

tank/home mlslabel

off

none

tank/home sync

standard

tank/home encryption

tank/home keysource

tank/home keystatus

tank/home rekeydate

tank/home rstchown

tank/home shadow

off

none

none

-

on

none

-

-

-

-

-

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

-

-

-

-

default

default

default

default

default

default

default

-

-

-

-

default

default

default

default

-

default

-

default

default

-

L'option -s spécifiée avec la commande zfs get permet de spécifier, en fonction du type de
source, les propriétés à afficher. Cette option permet d'indiquer la liste des types de sources

166

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Gestion des propriétés ZFS

souhaités, séparés par une virgule. Seules les propriétés associées au type de source spécifié sont
affichées. Les types de source valides sont local, default, inherited, temporary et none.
L'exemple suivant indique l'ensemble des propriétés définies localement sur tank/ws.

# zfs get -s local all tank/ws

NAME

PROPERTY

tank/ws compression

VALUE

on

SOURCE

local

Les options décrites ci-dessus peuvent être associées à l'option -r afin d'afficher de manière
récursive les propriétés spécifiées sur les systèmes enfant du système de fichiers concerné. Dans
l'exemple suivant, toutes les propriétés temporaires de l'ensemble des systèmes de fichiers de
tank/home sont affichées de façon récursive :

# zfs get -r -s temporary all tank/home

NAME

PROPERTY

VALUE

tank/home

atime

tank/home/jeff

atime

tank/home/mark

quota

off

off

20G

SOURCE

temporary

temporary

temporary

Vous pouvez interroger les valeurs d'une propriété à l'aide de la commande zfs get sans
spécifier le système de fichiers cible (la commande fonctionne sur tous les systèmes de fichiers et
les pools). Par exemple :

# zfs get -s local all

tank/home

tank/home/jeff

tank/home/mark

atime

atime

quota

off

off

20G

local

local

local

Pour plus d'informations sur la commande zfs get, reportez-vous à la page de manuel
zfs(1M).

Envoi de requête sur les propriétés ZFS pour l'exécution de scripts
La commande zfs get prend en charge les options -H et -o, qui permettent l'exécution de
scripts. Vous pouvez utiliser l'option -H pour omettre les informations d'en-tête et pour
remplacer un espace par un caractère de tabulation. L'uniformisation des espaces permet de
faciliter l'analyse des données. Vous pouvez utiliser l'option -o pour personnaliser la sortie de
l'une des façons suivantes :
■ Le littéral nom peut être utilisé avec une liste séparée par des virgules de propriétés comme

l'explique la section “Présentation des propriétés ZFS” à la page 137.

■ Une liste de champs littéraux séparés par des virgules (name, value, property et source)

suivi d'un espace et d'un argument. En d'autres termes, il s'agit d'une liste de propriétés
séparées par des virgules.

L'exemple suivant illustre la commande permettant de récupérer une seule valeur en spécifiant
les options -H et -o de la commande zfs get:

# zfs get -H -o value compression tank/home

on

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

167

Montage de système de fichiers ZFS

L'option -p indique les valeurs numériques sous leur forme exacte. Par exemple, 1 Mo serait
signalé sous la forme 1000000. Cette option peut être utilisée comme suit :

# zfs get -H -o value -p used tank/home

182983742

L'option -r permet de récupérer de manière récursive les valeurs demandées pour l'ensemble
des descendants et peut s'utiliser avec toutes les options mentionnées précédemment. Dans
l'exemple suivant, les options -H, -o et - r sont spécifiées afin de récupérer le nom du système
de fichiers et la valeur de la propriété used pour export/home et ses descendants, tout en
excluant les en-têtes dans la sortie :

# zfs get -H -o name,value -r used export/home

Montage de système de fichiers ZFS

Cette section décrit le processus de montage des systèmes de fichiers ZFS
■ “Gestion des points de montage ZFS” à la page 168
■ “Montage de système de fichiers ZFS” à la page 170
■ “Utilisation de propriétés de montage temporaires” à la page 172
■ “Démontage des systèmes de fichiers ZFS” à la page 172

Gestion des points de montage ZFS
Par défaut, un système de fichiers ZFS est automatiquement monté lors de sa création. Vous
pouvez déterminer un comportement de point de montage spécifique pour un système de
fichiers comme décrit dans cette section.
Vous pouvez également définir le point de montage par défaut du système de fichiers d'un pool
lors de l'exécution de la commande de création zpool create en spécifiant l'option -m. Pour
plus d'informations sur la création de pools, reportez-vous à la section “Création de pools de
stockage ZFS” à la page 51.
Tous les systèmes de fichiers ZFS sont montés lors de l'initialisation à l'aide du service
svc://system/filesystem/local SMF (Service Management Facility). Les systèmes de
fichiers sont montés sous /path, où path correspond au nom du système de fichiers. Système de
fichiers ZFS
Vous pouvez remplacer le point de montage par défaut à l'aide de la commande zfs set pour
définir la propriété mountpoint sur un chemin spécifique. ZFS crée automatiquement le point
de montage spécifié, si nécessaire, et monte automatiquement le système de fichiers
correspondant.
Les systèmes de fichiers ZFS sont automatiquement montés au moment de l'initialisation sans
qu'il soit nécessaire d'éditer le fichier /etc/vfstab.

168

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Montage de système de fichiers ZFS

La propriété mountpoint est héritée. Par exemple, si le fichier pool/home est doté d'une
propriété mountpoint définie sur /export/stuff, alors pool/home/user hérite de la valeur
/export/stuff/user pour sa propriété mountpoint.
Pour éviter le montage d'un système de fichiers, définissez la propriété mountpoint sur none. En
outre, la propriété canmount peut être utilisée pour contrôler le montage d'un système de
fichiers. Pour plus d'informations sur la propriété canmount, reportez-vous à la section
“Propriété canmount” à la page 154.
Les systèmes de fichiers peuvent également être gérés de manière explicite à l'aide d'interfaces
de montage héritées en utilisant la commande zfs set pour définir la propriété mountpoint sur
legacy. Dans ce cas, le montage et la gestion d'un système de fichiers ne sont pas gérés
automatiquement par ZFS. Ces opérations s'effectuent alors à l'aide des outils hérités, comme
les commandes mount et umount et le fichier /etc/vfstab. Pour plus d'informations sur les
montages hérités, reportez-vous à la section “Points de montage hérités” à la page 170.

Points de montage automatiques
■ Lorsque vous modifiez la propriété mountpoint de legacy à none sur un chemin spécifique,

le système de fichiers ZFS est automatiquement monté.
Si le système de fichiers ZFS est géré automatiquement sans être monté et si la propriété
mountpoint est modifiée, le système de fichiers reste démonté.

■

Les systèmes de fichiers dont la propriété mountpoint n'est pas définie sur legacy sont gérés par
le système ZFS. L'exemple suivant illustre la création d'un système de fichiers dont le point de
montage est automatiquement géré par le système ZFS :

# zfs create pool/filesystem

# zfs get mountpoint pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mountpoint

/pool/filesystem

# zfs get mounted pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mounted

yes

SOURCE

default

SOURCE

-

Vous pouvez également définir la propriété mountpoint de manière explicite, comme dans
l'exemple suivant :

# zfs set mountpoint=/mnt pool/filesystem

# zfs get mountpoint pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mountpoint

/mnt

# zfs get mounted pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mounted

yes

SOURCE

local

SOURCE

-

Si la propriété mountpoint est modifiée, le système de fichiers est automatiquement démonté de
l'ancien point de montage et remonté sur le nouveau. Les répertoires de point de montage sont
créés, le cas échéant. Si ZFS n'est pas en mesure de démonter un système de fichiers parce qu'il
est actif, une erreur est signalée et un démontage manuel forcé doit être effectué.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

169

Montage de système de fichiers ZFS

Points de montage hérités
La gestion des systèmes de fichiers ZFS peut s'effectuer à l'aide d'outils hérités. Pour cela, la
propriété mountpoint doit être définie sur legacy. Les systèmes de fichiers hérités sont alors
gérés à l'aide des commandes mount et umount et du fichier /etc/vfstab. Lors de
l'initialisation, le système de fichiers ZFS ne monte pas automatiquement les systèmes de
fichiers hérités et les commandes ZFS mount et umount ne fonctionnent pas sur ces types de
systèmes de fichiers. Les exemples suivants illustrent la définition et la gestion d'un système de
fichiers ZFS hérité :

# zfs set mountpoint=legacy tank/home/eric

# mount -F zfs tank/home/eschrock /mnt

Pour monter automatiquement un système de fichiers hérité lors de l'initialisation, vous devez
ajouter une entrée au fichier /etc/vfstab. L'exemple suivant montre l'entrée telle qu'elle peut
apparaître dans le fichier /etc/vfstab :

#device

device

#to mount

to fsck

mount

point

FS

fsck

mount

mount

type

pass

at boot options

#

tank/home/eric -

/mnt

zfs

-

yes

-

Les entrées device to fsck et fsck pass sont définies sur - car la commande fsck ne
s'applique pas aux systèmes de fichiers ZFS. Pour plus d'informations sur l'intégrité des données
ZFS, reportez-vous à la section “Sémantique transactionnelle” à la page 25.

Montage de système de fichiers ZFS
Le montage des systèmes de fichiers ZFS s'effectue automatiquement lors du processus de
création ou lors de l'initialisation du système. Vous ne devez utiliser la commande zfs mount
que lorsque vous devez modifier les options de montage ou monter/démonter explicitement les
systèmes de fichiers.

Spécifiée sans argument, la commande zfs mount répertorie tous les systèmes de fichiers
actuellement montés gérés par ZFS. Les points de montage hérités ne sont pas inclus. Par
exemple :

# zfs mount | grep tank/home

zfs mount | grep tank/home

tank/home

/tank/home

tank/home/jeff

/tank/home/jeff

L'option -a permet de monter tous les systèmes de fichiers ZFS. Les systèmes de fichiers hérités
ne sont pas montés. Par exemple :

# zfs mount -a

170

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Montage de système de fichiers ZFS

Par défaut, le système ZFS autorise uniquement le montage sur les répertoires vides. Par
exemple :

# zfs mount tank/home/lori

cannot mount ’tank/home/lori’: filesystem already mounted

La gestion des points de montage hérités doit s'effectuer à l'aide des outils hérités. Toute
tentative d'utilisation des outils ZFS génère une erreur. Par exemple :

# zfs mount tank/home/bill

cannot mount ’tank/home/bill’: legacy mountpoint

use mount(1M) to mount this filesystem

# mount -F zfs tank/home/billm

Le montage d'un système de fichiers requiert l'utilisation d'un ensemble d'options basées sur les
valeurs des propriétés associées au système de fichiers. Le tableau ci-dessous illustre la
corrélation entre les propriétés et les options de montage :

TABLEAU 6–4 Options et propriétés de montage ZFS

Propriétés

atime

devices

exec

nbmand

Option de montage

atime/noatime

devices/nodevices

exec/noexec

nbmand/nonbmand

readonly

ro/rw

setuid

xattr

setuid/nosetuid

xattr/noaxttr

L'option de montage nosuid représente un alias de nodevices,nosetuid.

Vous pouvez utiliser les fonctionnalités de montage en miroir NFSv4 pour faciliter la gestion
des répertoires personnels ZFS montés via NFS.

Lorsque les systèmes de fichiers sont créés sur le serveur NFS, le client NFS peut les détecter
automatiquement dans le montage existant d'un système de fichiers parent.

Par exemple, si le serveur neo partage déjà le système de fichiers tank et qu'il est monté sur le
client zee, /tank/baz est automatiquement visible sur le client après avoir été créé sur le
serveur.

zee# mount neo:/tank /mnt

zee# ls /mnt

baa

bar

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

171

Montage de système de fichiers ZFS

neo# zfs create tank/baz

zee% ls /mnt

baa

bar

baz

zee% ls /mnt/baz

file1

file2

Utilisation de propriétés de montage temporaires
Si les options de montage décrites à la section précédente sont définies de manière explicite en
spécifiant l'option -o avec la commande zfs mount, les valeurs des propriétés associées sont
remplacées de manière temporaire. Ces valeurs de propriété sont désignées par la chaîne
temporary dans la commande zfs get et reprennent leur valeur d'origine une fois le système
de fichiers démonté. Si une valeur de propriété est modifiée alors que le système de fichiers est
monté, la modification prend immédiatement effet et remplace toute valeur temporaire.

L'exemple suivant illustre la définition temporaire de l'option de montage en lecture seule sur le
système de fichiers tank/home/neil. Le système de fichiers est censé être démonté.

# zfs mount -o ro users/home/neil

Pour modifier temporairement une valeur de propriété sur un système de fichiers monté, vous
devez utiliser l'option spécifique remount. Dans l'exemple suivant, la propriété atime est
temporairement définie sur la valeur off pour un système de fichiers monté :

# zfs mount -o remount,noatime users/home/neil

NAME

PROPERTY VALUE SOURCE

users/home/neil atime

off

temporary

# zfs get atime users/home/perrin

Pour plus d'informations sur la commande zfs mount, reportez-vous à la page de manuel
zfs(1M).

Démontage des systèmes de fichiers ZFS
Le démontage des systèmes de fichiers ZFS peut s'effectuer à l'aide de la commande zfs
unmount. La commande unmount peut utiliser le point de montage ou le nom du système de
fichiers comme argument.

L'exemple suivant illustre le démontage d'un système de fichiers avec l'argument de nom de
système de fichiers :

# zfs unmount users/home/mark

L'exemple suivant illustre le démontage d'un système de fichiers avec l'argument de point de
montage :

172

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Activation et annulation du partage des systèmes de fichiers ZFS

# zfs unmount /users/home/mark

Si le système de fichiers est occupé, la commande unmount échoue. L'option -f permet de forcer
le démontage d'un système de fichiers. Le démontage forcé d'un système de fichiers requiert une
attention particulière si le contenu de ce système est en cours d'utilisation. Ce type d'opération
peut entraîner des comportements d'application imprévisibles.

# zfs unmount tank/home/eric

cannot unmount ’/tank/home/eric’: Device busy

# zfs unmount -f tank/home/eric

Pour garantir la compatibilité ascendante, vous pouvez démonter les systèmes de fichiers ZFS à
l'aide de la commande héritée umount. Par exemple :

# umount /tank/home/bob

Pour plus d'informations sur la commande zfs unmount, reportez-vous à la page de manuel
zfs(1M).

Activation et annulation du partage des systèmes de fichiers
ZFS

Dans cette version de Solaris, procédez comme suit pour partager un système de fichiers ZFS et
publier le partage :
■ Créez le partage du système de fichiers et définissez les propriétés SMB ou NFS du partage à

l'aide de la commande zfs share.

L'utilisation d'une commande distincte pour la création d'un partage donne accès aux
fonctionnalités suivantes :
■ Définition des options utilisées pour partager un chemin donné dans un système de fichiers.
■ Possibilité de définir plusieurs partages par système de fichiers, mais un nom de partage est

utilisé pour identifier de façon univoque chaque partage.

■ Un partage peut définir des options pour le partage NFS et SMB.
■ Plusieurs chemins d'accès SMB peuvent être définis pour un même chemin d'accès de

répertoire.

■ Le partage est stocké dans un fichier dans le répertoire .zfs/share portant le nom du

partage.

L'interaction entre les partages définis et les propriétés sharenfs et sharesmb est la suivante :
■ La propriété sharenfs existante contrôle si le système de fichiers est publié sur NFS. La

valeur est activée ou désactivée. Cette propriété peut être héritée par les systèmes de fichiers
descendants.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

173

Activation et annulation du partage des systèmes de fichiers ZFS

■ La propriété sharesmb existante contrôle si le système de fichiers est publié sur SMB. La

valeur est activée ou désactivée. Cette propriété peut être héritée par les systèmes de fichiers
descendants.

■ Lorsque la propriété sharenfs ou la propriété sharesmb est activée, tous les partages définis

pour le système de fichiers (et tous les systèmes de fichiers descendants héritant de la
propriété) sont publiés pour le protocole approprié. Tous les partages définis sont également
publiés lorsque la commande zfs share est émise.
1. Si aucun partage n'est défini, le système de fichiers n'est pas partagé.
2. Si des partages sont définis pour le système de fichiers, seuls ces partages sont publiés. Le

point de montage du système de fichiers n'est partagé que s'il existe un partage qui le
partage explicitement.

■ Lorsque la propriété sharenfs ou la propriété sharesmb est désactivée, la publication de

tous les partages publiés du système de fichiers (et de tous les systèmes de fichiers
descendants qui héritent de la propriété) est annulée pour le protocole approprié. Ces
partages ne sont partagés qu'à partir du moment où la propriété sharenfs ou sharesmb est
activée.
Aucun des partages définis n'est supprimé lorsque la propriété est désactivée et ils sont à
nouveau partagés lorsque la propriété sharenfs ou la propriété sharesmb est réactivée.

■ Lorsque la commande zfs unshare est émise, la publication de tous les partages publiés du
système de fichiers est annulée. Ces partages ne sont pas publiés tant que la commande zfs
share n'est pas émise pour le système de fichiers.
Aucun des partages définis n'est supprimé lorsque la commande zfs unshare est émise et ils
sont à nouveau partagés à l'exécution suivante de la commande zfs share.

Cette section détaille quelques différences entre la nouvelle syntaxe de partage et la syntaxe de
partage héritée.

Les principales différences sont les suivantes :
■ La commande zfs set share remplace l'interface sharemgr pour le partage des systèmes de

fichiers ZFS.

■ L'interface sharemgr n'est plus disponible. La commande share héritée et la propriété

sharenfs sont toujours disponibles. Voir les exemples ci-dessous.

■ Le fichier /etc/dfs/dfstab existe toujours mais les modifications sont ignorées. SMF gère
les informations de partage ZFS ou UFS de manière à ce que les systèmes de fichiers soient
automatiquement partagés lorsque le système est réinitialisé ; ces informations sont donc
gérées de façon similaire aux informations de montage et de partage ZFS.

■ La commande share -a est semblable à la commande share -ap et permet un partage

persistent du système de fichiers.

■ Les systèmes de fichiers descendants n'héritent pas des propriétés de partage. Si un système
de fichiers descendant est créé avec une propriété sharenfs héritée activée, un partage est
créé pour ce nouveau système de fichiers descendant.

174

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Activation et annulation du partage des systèmes de fichiers ZFS

Syntaxe de partage ZFS héritée
L'ancienne syntaxe de partage est toujours prise en charge.
1. Utilisez la commande share pour partager un système de fichiers.

Par exemple, pour partager un système de fichiers ZFS :

# share -F nfs /tank/zfsfs

# cat /etc/dfs/sharetab

/tank/zfsfs

-

nfs

rw

La syntaxe ci-dessus équivaut à partager un système de fichiers UFS :

# share -F nfs /ufsfs

# cat /etc/dfs/sharetab

/ufsfs -

nfs

/tank/zfsfs

-

rw

nfs

rw

2. La commande zfs share ne peut pas être utilisée pour partager un système de fichiers tant

que la propriété sharenfs n'est pas définie.

# zfs share rpool/data

cannot share ’rpool/data’: legacy share

use share(1M) to share this filesystem, or

set the ’share’ property and set [sharenfs|sharesmb] property on

# zfs set sharenfs=on rpool/data

# cat /etc/dfs/sharetab

/rpool/data

-

nfs

rw

Toutes les méthodes publient immédiatement les partages de systèmes de fichiers.

Nouvelle syntaxe de partage ZFS
La nouvelle commande zfs set share permet de partager un système de fichiers ZFS via les
protocoles NFS ou SMB. Le partage n'est pas publié tant que la propriété sharenfs n'est pas
également définie sur le système de fichiers.

A l'aide de la commande zfs set share, créez un partage NFS ou SMB de système de fichiers
ZFS, puis définissez la propriété sharenfs.

# zfs create rpool/fs1

# zfs set share=name=fs1,path=/rpool/fs1,prot=nfs rpool/fs1

name=fs1,path=/rpool/fs1,prot=nfs

Le partage n'est pas publié tant que la propriété sharenfs ou la propriété sharesmb n'est pas
activée. Par exemple :

# zfs set sharenfs=on rpool/fs1

# cat /etc/dfs/sharetab

/rpool/fs1

fs1

nfs

sec=sys,rw

Un partage NFS public peut être créé comme suit :

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

175

Activation et annulation du partage des systèmes de fichiers ZFS

# zfs set share=name=pp,path=/pub,prot=nfs,sec=sys,rw=*,public rpool/public

name=pp,path=/pub,prot=nfs,public=true,sec=sys,rw=*

# zfs set sharenfs=on rpool/public

# cat /etc/dfs/sharetab

/pub

pp

nfs

public,sec=sys,rw

Vous pouvez également créer un partage d'un système de fichiers ZFS récemment créé à l'aide
d'une syntaxe semblable à la suivante :

# zfs create -o mountpoint=/ds -o sharenfs=on rpool/ds

Lorsque vous créez un partage NFS d'un système de fichiers ZFS, vous devez fournir les
composants de partage suivants :

share=name

path=pathname

prot=nfs ou smb
pool/filesystem

Indiquez un nom pour le partage. Le nom du partage ne peut pas excéder
80 caractères.
Indiquez un chemin d'accès pour votre partage NFS qui doit exister au
sein du système de fichiers ou du répertoire à partager.
Indiquez le protocole (NFS ou SMB).
Représente le système de fichiers ZFS à partager.

Options de partage supplémentaires :
description=string

rw= ou ro=

root=

sec=

Fournit un texte pouvant aider à identifier le partage. Les espaces ou les
virgules doivent être placés entre guillemets (" ") dans la description.
Indique si le partage est disponible en lecture/écriture ou en lecture
seule pour tous les clients. Vous pouvez également spécifier une liste
séparée par des deux-points comprenant des noms d'hôte, des adresses
IP ou un groupe réseau.
Identifie un utilisateur root à partir d'un hôte spécifié ou d'une liste
d'hôtes disposant d'un accès root. Par défaut, aucun hôte ne dispose
d'un accès root.
Identifie un mode de sécurité pour le serveur NFS, tel que sys, dh,
krb5, etc. Pour plus d'informations sur les modes de sécurité pris en
charge, reportez-vous à nfssec(5).

Les propriétés NFS suivantes doivent être spécifiées après prot=nfs, mais avant toute propriété
sec=.

■

■

■

■

■

■

anon=user-name| uid

nosub=true|false

nosuid=true|false

aclok=true|false

public=true|false

index=filename

176

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Activation et annulation du partage des systèmes de fichiers ZFS

■

■

log=TYPE_LOGTAG
cksum=TYPE_STRINGSET

Les propriétés SMB facultatives suivantes doivent être spécifiées après la propriété prot=smb :

■

■

■

■

■

■

■

■

ad-container=string
abe=[true|false]
csc=[disabled|manual|auto|vdo]
catia=[true|false]
guestok=[true|false]
ro=access-list
rw=access-list
none=liste d'accès

Reportez-vous aux pages de manuel share_nfs(1M) et share_smb(1M) pour une description
détaillée des propriétés de partages NFS et SMB.

Affichage ZFS partager des informations
Comme dans les versions précédentes, vous pouvez afficher la valeur de la propriété sharenfs à
l'aide de la propriété zfs get sharenfs ou de la syntaxe de commande zfs get all.

# zfs get sharenfs rpool/fs1

NAME

PROPERTY VALUE

SOURCE

rpool/fs1 sharenfs on

local

Les nouvelles informations de partage sont disponibles à l'aide de la commande zfs get share.

# zfs get share rpool/fs1

NAME

PROPERTY VALUE SOURCE

rpool/fs1 share

name=rpool_fs1,path=/rpool/fs1,prot=nfs local

Les nouvelles informations de partage ne sont pas disponibles dans la syntaxe de commande
zfs get all.

Si vous créez un partage pour un système de fichiers ZFS récemment créé, utilisez la commande
zfs get share pour identifier le nom share-name ou le nom share-path. Par exemple :

# zfs create -o mountpoint=/data -o sharenfs=on rpool/data

# zfs get share rpool/data

NAME

rpool/data

PROPERTY VALUE SOURCE

share

name=data,path=/data,prot=nfs local

Héritage de partage ZFS
L'héritage des propriétés zfs share, sharenfs ou sharesmb fonctionne de la manière
suivante :

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

177

Activation et annulation du partage des systèmes de fichiers ZFS

■ La propriété zfs share n'est pas transmise d'un système de fichiers parent à un système de
fichiers enfant. En outre, la commande zfs set share ne prend pas en charge l'option -r
permettant de définir une propriété ZFS sur les systèmes de fichiers descendants.
Si les propriétés sharenfs ou sharesmb sont définies sur un système de fichiers parent, les
propriétés sharenfs ou sharesmb sont également définies sur les systèmes de fichiers
descendants. Par exemple :

■

# zfs create -o mountpoint=/ds rpool/ds

# zfs set share=name=ds,path=/ds,prot=nfs rpool/ds

name=ds,path=/ds,prot=nfs

# zfs set sharenfs=on rpool/ds

# cat /etc/dfs/sharetab

/ds rpool_ds

nfs

sec=sys,rw

# zfs create rpool/ds/ds1

# zfs get sharenfs rpool/ds/ds1

NAME

PROPERTY VALUE

SOURCE

rpool/ds/ds1 sharenfs on

inherited from rpool/ds

Tout système de fichiers enfant existant hérite également de la valeur des propriétés
sharenfs ou sharesmb.

Si les propriétés sharenfs ou sharesmb sont désactivées sur le système de fichiers parent, les
propriétés sharenfs ou sharesmb sont désactivées sur les systèmes de fichiers descendants.
Par exemple :

# zfs set sharenfs=off rpool/ds

$ zfs get -r sharenfs rpool/ds

NAME

PROPERTY VALUE

SOURCE

rpool/ds

sharenfs off

local

rpool/ds/ds1

sharenfs off

inherited from rpool/ds

rpool/ds/ds2

sharenfs off

inherited from rpool/ds

rpool/ds/ds3

sharenfs off

inherited from rpool/ds

Modification d'un partage ZFS
Les propriétés de nom et de protocole doivent être spécifiées lorsque vous modifiez les valeurs
de propriétés d'un partage.

Par exemple, créez un partage NFS comme suit :

# zfs create -o mountpoint=/ds -o sharenfs=on rpool/ds

# zfs set share=name=ds,path=/ds,prot=nfs rpool/ds

name=ds,path=/ds,prot=nfs

Ajoutez ensuite le protocole SMB :

# zfs set share=name=ds,prot=nfs,prot=smb rpool/ds

name=ds,path=/ds,prot=nfs,prot=smb

Supprimez le protocole SMB :

178

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Activation et annulation du partage des systèmes de fichiers ZFS

# zfs set -c share=name=ds,prot=smb rpool/ds

name=ds,path=/ds,prot=nfs

Suppression d'un partage ZFS
Vous pouvez supprimer un partage existant à l'aide de la commande zfs set - c. Par exemple,
identifiez le nom du partage.

# zfs get share

NAME

PROPERTY VALUE SOURCE

rpool/ds

share

name=ds,path=/ds,prot=nfs local

Supprimez ensuite le partage en identifiant le nom share-name. Par exemple :

# zfs set -c share=name=ds rpool/ds

share ’ds’ was removed.

Lorsqu'un partage a été établi au moyen de la création d'un partage par défaut au moment de la
création du système de fichiers, le partage peut être supprimé par le biais du nom share-name ou
du nom share-path. Par exemple, un nom share-name par défaut, data et un nom share-path
par défaut, /data sont attribués au partage suivant.

# zfs create -o mountpoint=/data -o sharenfs=on rpool/data

# zfs get share rpool/data

NAME

PROPERTY VALUE SOURCE

rpool/data share

name=data,path=/data,prot=nfs local

Supprimez le partage en identifiant le nom share-name. Par exemple :

# zfs set -c share=name=data rpool/data

share ’data’ was removed.

Supprimez le partage en identifiant le nom share-path. Par exemple :

# zfs set -c share=path=/data rpool/data

share ’data’ was removed.

Partage de fichiers ZFS au sein d'une zone non globale
Dans les versions précédentes de Solaris, la création ou la publication de partages NFS ou SMB
dans une zone non globale Oracle Solaris n'était pas prise en charge. Dans cette version de
Solaris, vous pouvez créer et publier des partages NFS à l'aide de la commande zfs set share
et de la commande héritée share utilisée avec une zone non globale.
■ Lorsqu'un système de fichiers ZFS est monté et disponible dans une zone non globale, il peut

être partagé dans cette zone.

■ Un système de fichiers peut être partagé dans la zone globale à condition de ne pas être
monté dans une zone non globale et de ne pas être partagé dans une zone non globale.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

179

Activation et annulation du partage des systèmes de fichiers ZFS

■

Si la propriété mountpoint d'un système de fichiers ZFS est définie sur legacy, ce système de
fichiers peut être partagé à l'aide de la commande héritée share.

Par exemple, les systèmes de fichiers /export/home/data et export/home/data1 sont
disponibles dans zfszone.

zfszone# share -F nfs /export/home/data

zfszone#

cat /etc/dfs/sharetab

/export/home/data

export_home_data

nfs

sec=sys,rw

zfszone# zfs set share=name=data1,path=/export/home/data1,prot=nfs

tank/zones/export/home/data1

zfszone# zfs set sharenfs=on tank/zones/export/home/data1

zfszone# cat /etc/dfs/sharetab

/export/home/data1

data1

nfs

sec=sys,rw

Récapitulatif des commandes de partage ZFS nouvelles et héritées
Le tableau ci-dessous décrit la nouvelle syntaxe de partage de systèmes de fichiers ZFS et la
syntaxe de partage héritée.

TABLEAU 6–5 Récapitulatif des commandes de partage ZFS nouvelles et héritées

Tâche de partage ZFS

Syntaxe de partage héritée

Nouvelle syntaxe de partage

Partage d'un système de
fichiers ZFS sur NFS.

Activation de la propriété sharenfs.

1. Création du partage NFS.

# zfs set sharenfs=on tank/fs1

# zfs set share=name=fs1,path=/fs1,

prot=nfs tank/fs1

2. Activation de la propriété sharenfs.

# zfs set sharenfs=on tank/fs1

Partage d'un système de
fichiers ZFS sur SMB.

Activation de la propriété sharesmb.

1. Création du partage SMB.

# zfs set sharesmb=on tank/fs2

# zfs set share=name=fs2,path=/fs2,

prot=smb tank/fs2

2. Activation de la propriété sharesmb.

# zfs set sharesmb=on tank/fs2

Annulation du partage du
système de fichiers ZFS

Désactivation de la propriété
sharenfs.

Désactivation de la propriété sharenfs.

# zfs set sharenfs=off tank/fs1

# zfs set sharenfs=off tank/fs1

180

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Activation et annulation du partage des systèmes de fichiers ZFS

TABLEAU 6–5 Récapitulatif des commandes de partage ZFS nouvelles et héritées
Tâche de partage ZFS

Syntaxe de partage héritée

(Suite)

Nouvelle syntaxe de partage

Désactivation de la propriété
sharesmb.

Désactivation de la propriété sharesmb.

# zfs set sharesmb=off tank/fs2

# zfs set sharesmb=off tank/fs2

Ajout d'options de partage
à un partage existant.

Réinitialisation de la propriété
sharenfs.

Réinitialisation du partage avec la
propriété supplémentaire.

# zfs set sharenfs=nosuid tank/fs1

# zfs set share=name=fs1,prot=nfs,

nosuid rpool/fs1

name=fs1,path=/rpool/fs1,prot=nfs,

nosuid=true

Création d'un partage NFS
permanent.

Activation de la propriété sharenfs.

Activation de la propriété sharenfs.

# zfs set sharenfs=on tank/fs1

# zfs set sharenfs=on tank/fs1

Avec la syntaxe de commande share
héritée, il fallait éditer le fichier
/etc/dfs/dfstab pour créer un
partage permanent.

Le fichier /etc/dfs/dfstab n'est pas
disponible dans cette version de Solaris.

Création d'un partage
SMB permanent.

Activation de la propriété sharesmb.

Activation de la propriété sharesmb.

# zfs set sharesmb=on tank/fs2

# zfs set sharesmb=on tank/fs2

Ou, création du partage SMB à l'aide
de sharemgr.

La fonctionnalité sharemgr n'est pas
disponible dans cette version de Solaris.

# sharemgr create -P smb fssmb

# sharemgr add-share -r fs-smb -s /tank/fs2 fssmb

Dépannage des problèmes de partage ZFS
■ Vous ne pouvez pas partager un système de fichiers parent si un sous-répertoire ou un

système de fichiers descendant est déjà partagé.

# share -F nfs /rpool/fs2/dir1

# share -F nfs /rpool/fs2/dir2

# share -F nfs /rpool/fs2

share: NFS: descendant of path is shared: /rpool/fs2/dir1 in rpool_fs2_dir2

■ L'attribution d'un nouveau nom à un partage créé à l'aide de la commande zfs set share

n'est pas pris en charge.

■ Vous pouvez créer un partage de système de fichiers avec les protocoles NFS et SMB en

utilisant la commande zfs set share. Par exemple :

# zfs set share=name=ds,path=/ds,prot=nfs,prot=smb rpool/ds

name=ds,path=/ds,prot=nfs,prot=smb

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

181

Définition des quotas et réservations ZFS

Si vous souhaitez créer un partage de système de fichiers avec les protocoles NFS et SMB en
utilisant la commande héritée share, vous devez spécifier la commande à deux reprises. Par
exemple :

# share -F nfs /rpool/ds

# share -F smb /rpool/ds

# zfs get share rpool/df

name=rpool_ds,path=/rpool/ds,prot=nfs,prot=smb

■ Un chemin d'accès ou une description de partage comprenant une virgule (,) doit être placé

entre guillemets.

Problèmes de migration/transition de partage ZFS
Cette section présente tous les problèmes de transition.
■ Mise à niveau de votre système : les partages ZFS seront incorrects si revenez à un
environnement d'initialisation antérieur en raison de modifications apportées aux
propriétés dans cette version. Les partages non ZFS ne sont pas concernés. Si vous avez
l'intention de revenir à un environnement d'initialisation antérieur, enregistrez tout d'abord
une copie de la configuration du partage existante avant l'opération pkg update afin de
pouvoir restaurer la configuration du partage sur les systèmes de fichiers ZFS.
■ Dans l'environnement d'initialisation antérieur, utilisez la commande sharemgr show

-vp pour répertorier tous les partages et leur configuration.
Servez-vous des commandes zfs get sharenfs filesystem et zfs sharesmb filesystem
pour obtenir les valeurs des propriétés de partage.
Si vous revenez à un environnement d'initialisation antérieur, restaurez les valeurs
d'origine des propriétés sharenfs et sharesmb.

■

■

■ Comportement hérité d'annulation de partage : les commandes unshare -a ou

unshareall permettent d'annuler la publication d'un partage, mais ne mettent pas à jour le
référentiel de partages SMF. Si vous tentez de republier le partage existant, les conflits sont
recherchés dans le référentiel de partages et un message d'erreur s'affiche.

Définition des quotas et réservations ZFS

La propriété quota permet de limiter la quantité d'espace disque disponible pour un système de
fichiers. La propriété reservation permet quant à elle de garantir la disponibilité d'une certaine
quantité d'espace disque pour un système de fichiers. Ces deux propriétés s'appliquent au
système de fichiers sur lequel elles sont définies ainsi qu'à ses descendants.
Par exemple, si un quota est défini pour le système de fichiers tank/home, la quantité totale
d'espace disque utilisée par tank/home et l'ensemble de ses descendants ne peut pas excéder le
quota défini. De même, si une réservation est définie pour le jeu de données tank/home, cette
réservation s'applique à tank/home et à tous ses descendants. La quantité d'espace disque utilisée
par un système de fichiers et par tous ses descendants est indiquée par la propriété used.

182

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Définition des quotas et réservations ZFS

Les propriétés refquota et refreservation vous permettent de gérer l'espace d'un système de
fichiers sans prendre en compte l'espace disque utilisé par les descendants, notamment les
instantanés et les clones.

Dans cette version de Solaris, vous pouvez définir un quota d'utilisateur (user) ou de groupe sur
la quantité d'espace disque utilisée par les fichiers appartenant à un utilisateur ou à un groupe
spécifique. Les propriétés de quota d'utilisateur et de groupe ne peuvent pas être définies sur un
volume, sur un système de fichiers antérieur à la version 4, ou sur un pool antérieur à la version
15.

Considérez les points suivants pour déterminer quelles fonctions de quota et de réservation
conviennent le mieux à la gestion de vos systèmes de fichiers :
■ Les propriétés quota et reservation conviennent à la gestion de l'espace disque utilisé par

les systèmes de fichiers et leurs descendants.

■ Les propriétés refquota et refreservation conviennent à la gestion de l'espace disque

utilisé par les systèmes de fichiers.

■ La définition d'une propriété refquota ou refreservation supérieure à une la propriété

quota ou reservation n'a aucun effet. Lorsque vous définissez la propriété quota ou
refquota, les opérations qui tentent de dépasser l'une de ces valeurs échouent. Il est possible
de dépasser une valeur quota supérieure à une valeur refquota. Par exemple, si certains
blocs d'instantanés sont modifiés, la valeur quota risque d'être dépassée avant la valeur
refquota.

■ Les quotas d'utilisateurs et de groupes permettent d'augmenter plus facilement l'espace
disque contenant de nombreux comptes d'utilisateur, par exemple dans une université.

Pour plus d'informations sur la définition de quotas et réservations, reportez-vous aux sections
“Définitions de quotas sur les systèmes de fichiers ZFS” à la page 183 et “Définition de
réservations sur les systèmes de fichiers ZFS” à la page 187.

Définitions de quotas sur les systèmes de fichiers ZFS
Les quotas des systèmes de fichiers ZFS peuvent être définis et affichés à l'aide des commandes
zfs set et zfs get. Dans l'exemple suivant, un quota de 10 Go est défini pour
tank/home/jeff :

# zfs set quota=10G tank/home/jeff

# zfs get quota tank/home/jeff

NAME

PROPERTY VALUE SOURCE

tank/home/jeff quota

10G

local

Les quotas affectent également la sortie des commandes zfs list et df. Par exemple :

# zfs list -r tank/home

NAME

USED AVAIL REFER MOUNTPOINT

tank/home

1.45M 66.9G

36K /tank/home

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

183

Définition des quotas et réservations ZFS

tank/home/eric

547K 66.9G

547K /tank/home/eric

tank/home/jeff

322K 10.0G

291K /tank/home/jeff

tank/home/jeff/ws

31K 10.0G

31K /tank/home/jeff/ws

tank/home/lori

547K 66.9G

547K /tank/home/lori

tank/home/mark

31K 66.9G

31K /tank/home/mark

# df -h /tank/home/jeff

Filesystem

Size Used Avail Use% Mounted on

tank/home/jeff

10G 306K

10G

1% /tank/home/jeff

tank/home dispose de 66,9 Go d'espace disque disponible. Toutefois, tank/home/jeff et
tank/home/jeff/ws disposent uniquement de 10 Go d'espace disponible, respectivement, en
raison du quota défini pour tank/home/jeff.

Vous ne pouvez pas définir un quota sur une valeur inférieure à la quantité d'espace
actuellement utilisée par un système de fichiers. Par exemple :

# zfs set quota=10K tank/home/jeff

cannot set property for ’tank/home/jeff’:

size is less than current used or reserved space

Vous pouvez définir une propriété refquota sur un système de fichiers pour limiter l'espace
disque occupé par le système de fichiers. Cette limite fixe ne comprend pas l'espace disque
utilisé par les descendants. Par exemple, le quota de 10 Go de studentA n'est pas affecté par
l'espace utilisé par les instantanés.

# zfs set refquota=10g students/studentA

# zfs list -t all -r students

NAME

students

USED AVAIL REFER MOUNTPOINT

150M 66.8G

32K /students

students/studentA

150M 9.85G

150M /students/studentA

students/studentA@yesterday

0

-

150M -

# zfs snapshot students/studentA@today

# zfs list -t all -r students

students

150M 66.8G

32K /students

students/studentA

150M 9.90G

100M /students/studentA

students/studentA@yesterday 50.0M

students/studentA@today

0

-

-

150M -

100M -

Par souci de commodité, vous pouvez définir un autre quota pour un système de fichiers afin de
vous aider à gérer l'espace disque utilisé par les instantanés. Par exemple :

# zfs set quota=20g students/studentA

# zfs list -t all -r students

NAME

students

USED AVAIL REFER MOUNTPOINT

150M 66.8G

32K /students

students/studentA

150M 9.90G

100M /students/studentA

students/studentA@yesterday 50.0M

students/studentA@today

0

-

-

150M -

100M -

Dans ce scénario, studentA peut atteindre la limite maximale de refquota (10 Go), mais
studentA peut supprimer des fichiers pour libérer de l'espace même en présence d'instantanés.

184

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Définition des quotas et réservations ZFS

Dans l'exemple précédent, le plus petit des deux quotas (10 Go par rapport à 20 Go) s'affiche
dans la sortie zfs list. Pour afficher la valeur des deux quotas, utilisez la commande zfs get.
Par exemple :

# zfs get refquota,quota students/studentA

NAME

PROPERTY VALUE

students/studentA refquota 10G

students/studentA quota

20G

SOURCE

local

local

Définition de quotas d'utilisateurs et de groupes sur un système de
fichiers ZFS
Vous pouvez définir un quota d'utilisateurs ou de groupes en utilisant respectivement les
commandes zfs userquota et zfs groupquota. Par exemple :

# zfs create students/compsci

# zfs set userquota@student1=10G students/compsci

# zfs create students/labstaff

# zfs set groupquota@labstaff=20GB students/labstaff

Affichez le quota d'utilisateurs ou de groupes actuel comme suit :

# zfs get userquota@student1 students/compsci

NAME

PROPERTY

VALUE

students/compsci userquota@student1 10G

# zfs get groupquota@labstaff students/labstaff

NAME

PROPERTY

VALUE

students/labstaff groupquota@labstaff 20G

SOURCE

local

SOURCE

local

Vous pouvez afficher l'utilisation générale de l'espace disque par les utilisateurs et les groupes en
interrogeant les propriétés suivantes :

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

350M

none

POSIX User student1 426M

10G

# zfs groupspace students/labstaff

TYPE

NAME

USED QUOTA

POSIX Group labstaff 250M

20G

POSIX Group root

350M

none

Pour identifier l'utilisation de l'espace disque d'un groupe ou d'un utilisateur, vous devez
interroger les propriétés suivantes :

# zfs get userused@student1 students/compsci

NAME

PROPERTY

VALUE

students/compsci userused@student1 550M

# zfs get groupused@labstaff students/labstaff

NAME

PROPERTY

VALUE

students/labstaff groupused@labstaff 250

SOURCE

local

SOURCE

local

Les propriétés de quota d'utilisateurs et de groupes ne sont pas affichées à l'aide de la commande
zfs get all dataset, qui affiche une liste de toutes les autres propriétés du système de fichiers.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

185

Définition des quotas et réservations ZFS

Vous pouvez supprimer un quota d'utilisateurs ou de groupes comme suit :

# zfs set userquota@student1=none students/compsci

# zfs set groupquota@labstaff=none students/labstaff

Les quotas d'utilisateurs et de groupes sur les systèmes de fichiers ZFS offrent les fonctionnalités
suivantes :
■ Un quota d'utilisateurs ou de groupes défini sur un système de fichiers parent n'est pas

automatiquement hérité par un système de fichiers descendant.

■ Cependant, le quota d'utilisateurs ou de groupes est appliqué lorsqu'un clone ou un
instantané est créé à partir d'un système de fichiers lié à un quota d'utilisateurs ou de
groupes. De même, un quota d'utilisateurs ou de groupes est inclus avec le système de
fichiers lorsqu'un flux est créé à l'aide de la commande zfs send, même sans l'option -R.

■ Les utilisateurs dénués de privilèges peuvent uniquement disposer de leur propre utilisation
d'espace disque. L'utilisateur root ou l'utilisateur qui s'est vu accorder le privilège userused
ou groupused peut accéder aux informations de comptabilité de l'espace disque utilisateur
ou groupe de tout le monde.

■ Les propriétés userquota et groupquota ne peuvent pas être définies sur les volumes ZFS,
sur un système de fichiers antérieur à la version 4, ou sur un pool antérieur à la version 15.

L'application des quotas d'utilisateurs et de groupes peut être différée de quelques secondes. Ce
délai signifie que les utilisateurs peuvent dépasser leurs quotas avant que le système ne le
remarque et refuse d'autres écritures en affichant le message d'erreur EDQUOT .

Vous pouvez utiliser la commande quota héritée pour examiner les quotas d'utilisateurs dans
un environnement NFS où un système de fichiers ZFS est monté, par exemple. Sans aucune
option, la commande quota affiche uniquement la sortie en cas de dépassement du quota de
l'utilisateur. Par exemple :

# zfs set userquota@student1=10m students/compsci

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

350M

none

POSIX User student1 550M

10M

# quota student1

Block limit reached on /students/compsci

Si vous réinitialisez le quota d'utilisateurs et que la limite du quota n'est plus dépassée, vous
devez utiliser la commande quota -v pour examiner le quota de l'utilisateur. Par exemple :

# zfs set userquota@student1=10GB students/compsci

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

350M

none

POSIX User student1 550M

10G

# quota student1

# quota -v student1

Disk quotas for student1 (uid 102):

186

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Définition des quotas et réservations ZFS

Filesystem

usage quota limit

timeleft files quota limit

timeleft

/students/compsci

563287 10485760 10485760

-

-

-

-

-

Définition de réservations sur les systèmes de fichiers
ZFS
Une réservation ZFS désigne une quantité d'espace disque du pool garantie pour un jeu de
données. Dès lors, pour réserver une quantité d'espace disque pour un jeu de données, cette
quantité doit être actuellement disponible sur le pool. La quantité totale d'espace non utilisé des
réservations ne peut pas dépasser la quantité d'espace disque non utilisé du pool. La définition
et l'affichage des réservations ZFS s'effectuent respectivement à l'aide des commandes zfs set
et zfs get. Par exemple :

# zfs set reservation=5G tank/home/bill

# zfs get reservation tank/home/bill

NAME

PROPERTY

VALUE

SOURCE

tank/home/bill reservation 5G

local

Les réservations peuvent affecter la sortie de la commande zfs list. Par exemple :

# zfs list -r tank/home

NAME

USED AVAIL REFER MOUNTPOINT

tank/home

5.00G 61.9G

37K /tank/home

tank/home/bill

31K 66.9G

31K /tank/home/bill

tank/home/jeff

337K 10.0G

306K /tank/home/jeff

tank/home/lori

547K 61.9G

547K /tank/home/lori

tank/home/mark

31K 61.9G

31K /tank/home/mark

Notez que tank/home utilise 5 Go d'espace bien que la quantité totale d'espace à laquelle
tank/home et ses descendants font référence est bien inférieure à 5 Go. L'espace utilisé reflète
l'espace réservé pour tank/home/bill. Les réservations sont prises en compte dans le calcul de
l'espace disque utilisé du système de fichiers parent et non dans le quota, la réservation ou les
deux.

# zfs set quota=5G pool/filesystem

# zfs set reservation=10G pool/filesystem/user1

cannot set reservation for ’pool/filesystem/user1’: size is greater than

available space

Un jeu de données peut utiliser davantage d'espace disque que sa réservation, du moment que le
pool dispose d'un espace non réservé et disponible et que l'utilisation actuelle du jeu de données
se trouve en dessous des quotas. Un jeu de données ne peut pas utiliser un espace disque réservé
à un autre jeu de données.
Les réservations ne sont pas cumulatives. En d'autres termes, l'exécution d'une nouvelle
commande zfs set pour un jeu de données déjà associé à une réservation n'entraîne pas l'ajout
de la nouvelle réservation à la réservation existante. La seconde réservation remplace la
première. Par exemple :

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

187

Chiffrement des systèmes de fichiers ZFS

# zfs set reservation=10G tank/home/bill

# zfs set reservation=5G tank/home/bill

# zfs get reservation tank/home/bill

NAME

PROPERTY

VALUE

SOURCE

tank/home/bill reservation 5G

local

Vous pouvez définir une réservation refreservation pour garantir un espace disque ne
contenant aucun instantané ou clone au jeu de données. Cette valeur est prise en compte dans le
calcul de l'espace utilisé des jeux de données parent et vient en déduction des quotas et
réservations des jeux de données parent. Par exemple :

# zfs set refreservation=10g profs/prof1

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

10.0G 23.2G

19K /profs

profs/prof1

10G 33.2G

18K /profs/prof1

Vous pouvez également définir une valeur de réservation pour le même jeu de données afin de
garantir l'espace du jeu de données et pas de l'espace des instantanés. Par exemple :

# zfs set reservation=20g profs/prof1

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

20.0G 13.2G

19K /profs

profs/prof1

10G 33.2G

18K /profs/prof1

Les réservations régulières sont prises en compte dans le calcul de l'espace utilisé du parent.

Dans l'exemple précédent, le plus petit des deux quotas (10 Go par rapport à 20 Go) s'affiche
dans la sortie zfs list. Pour afficher la valeur des deux quotas, utilisez la commande zfs get.
Par exemple :

# zfs get reservation,refreserv profs/prof1

NAME

PROPERTY

VALUE

SOURCE

profs/prof1 reservation

20G

profs/prof1 refreservation 10G

local

local

Lorsque la propriété refreservation est définie, un instantané n'est autorisé que si
suffisamment d'espace non réservé est disponible dans le pool au-delà de cette réservation afin
de pouvoir contenir le nombre actuel d'octets référencés dans le jeu de données.

Chiffrement des systèmes de fichiers ZFS

Le chiffrement se définit comme le processus de codage de données à des fins de confidentialité ;
le propriétaire des données nécessite une clé pour pouvoir accéder aux données codées. Les
avantages de l'utilisation du chiffrement ZFS sont les suivants :
■ Le chiffrement ZFS est intégré à l'ensemble des commandes ZFS. A l'instar d'autres

opérations ZFS, les opérations de chiffrement telles que la modification et le renouvellement
de clés sont effectuées en ligne.

188

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Chiffrement des systèmes de fichiers ZFS

■ Vous pouvez utiliser vos pools de stockage existants pour autant qu'ils aient été mis à

niveau. Vous avez la possibilité de chiffrer des systèmes de fichiers spécifiques.

■ Le chiffrement ZFS peut être transmis aux systèmes de fichiers descendants. La gestion des

clés peut être déléguée par le biais de l'administration déléguée de ZFS.

■ Les données sont chiffrées à l'aide de la norme AES (Advanced Encryption Standard, Norme

de chiffrement avancé) avec des longueurs de clé de 128, 192 et 256 dans les modes de
fonctionnement CCM et GCM.

■ Le chiffrement ZFS utilise la structure cryptographique Oracle Solaris, ce qui lui donne

automatiquement accès aux éventuelles accélérations matérielles et implémentations
logicielles optimisées disponibles des algorithmes de chiffrement.

Vous pouvez définir une stratégie de chiffrement lors de la création d'un système de fichiers
ZFS, mais cette stratégie ne peut pas être modifiée. Par exemple, la propriété de chiffrement est
activée lors de la création du système de fichiers tank/home/darren. La stratégie de chiffrement
par défaut consiste en une invite à saisir une phrase de passe comportant 8 caractères au
minimum.

# zfs create -o encryption=on tank/home/darren

Enter passphrase for ’tank/home/darren’: xxxxxxx

Enter again: xxxxxxxx

Assurez-vous que le chiffrement est activé sur le système de fichiers. Par exemple :

# zfs get encryption tank/home/darren

NAME

PROPERTY

VALUE

tank/home/darren encryption on

SOURCE

local

L'algorithme de chiffrement par défaut est aes-128-ccm lorsque la valeur de chiffrement d'un
système de fichiers est on.

Une clé d'encapsulation est utilisée pour chiffrer les clés de chiffrement effectives des données.
La clé d'encapsulation est transmise de la commande zfs au noyau, comme dans l'exemple
ci-dessus au moment de la création du système de fichiers chiffré. Une clé d'encapsulation peut
se trouver dans un fichier (au format raw ou hexadécimal) ou être dérivée d'une phrase de passe.

Le format et l'emplacement de la clé d'encapsulation sont spécifiés dans la propriété keysource
de la manière suivante :

keysource=format,location

■ Formats possibles :

■

■

■

raw : octets bruts de la clé
hex : chaîne de clé hexadécimale
passphrase : chaîne de caractères générant une clé

■ Emplacements possibles :

■

prompt : vous êtes invité à saisir une clé lorsque le système de fichiers est créé ou monté

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

189

Chiffrement des systèmes de fichiers ZFS

■

■

■

,file:///filename : emplacement du fichier de clé dans un système de fichiers
pkcs11 :URI décrivant l'emplacement d'une clé dans un jeton PKCS#11
https://location : emplacement du fichier de clé sur un serveur sécurisé

Si le format spécifié par keysource est passphrase, la clé d'encapsulation est dérivée de la passe
de phrase. Dans le cas contraire, la valeur de la propriété keysource pointe vers la clé
d'encapsulation effective, sous forme d'octets bruts ou au format hexadécimal. Vous pouvez
indiquer que la phrase de passe doit être stockée dans un fichier ou dans un flux d'octets bruts
que l'utilisateur est invité à saisir, ce qui n'est probablement adapté qu'à l'écriture de scripts.

Lorsque les valeurs de la propriété keysource d'un système de fichiers correspondent à
passphrase, la clé d'encapsulation est dérivée de la phrase de passe à l'aide de PKCS#5 PBKD2 et
d'un salt généré de façon aléatoire pour chaque système de fichiers. Cela signifie que la même
phrase de passe génère une clé d'encapsulation différente lorsqu'elle est utilisée sur des systèmes
de fichiers descendants.

Les systèmes de fichiers descendants héritent de la stratégie de chiffrement du système de
fichiers parent, et celle-ci ne peut pas être supprimée. Par exemple :

# zfs snapshot tank/home/darren@now

# zfs clone tank/home/darren@now tank/home/darren-new

Enter passphrase for ’tank/home/darren-new’: xxxxxxx

Enter again: xxxxxxxx

# zfs set encryption=off tank/home/darren-new

cannot set property for ’tank/home/darren-new’: ’encryption’ is readonly

Si vous devez copier ou migrer des systèmes de fichiers ZFS chiffrés ou non chiffrés, tenez
compte des points suivants :
■ A l'heure actuelle, vous ne pouvez pas envoyer un flux de jeu de données non chiffré et le
recevoir en tant que flux chiffré, même si le chiffrement est activé sur le jeu de données du
pool de réception.

■ Vous pouvez utiliser les commandes suivantes pour migrer des données non chiffrées vers

un pool ou système de données où le chiffrement est activé :

■

■

■

■

cp -r

find | cpio

tar

rsync

■ Un flux de système de fichiers chiffré répliqué peut être reçu sur un système de fichiers

chiffré et les données restent chiffrées. Pour plus d'informations, reportez-vous à
l'Exemple 6–4.

190

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Chiffrement des systèmes de fichiers ZFS

Modification des clés d'un système de fichiers ZFS
chiffré
Vous pouvez modifier la clé d'encapsulation d'un système de fichiers chiffré à l'aide de la
commande zfs key -c. La clé d'encapsulation existante doit avoir été chargée auparavant : soit
lors de l'initialisation, soit par chargement explicite de la clé du système de fichiers (zfs key -l),
soit par montage du système de fichiers (zfs mount filesystem). Par exemple :

# zfs key -c tank/home/darren

Enter new passphrase for ’tank/home/darren’: xxxxxxxx

Enter again: xxxxxxxx

Dans l'exemple suivant, la clé d'encapsulation est modifiée et la valeur de la propriété
keysource est modifiée pour indiquer que la clé d'encapsulation provient d'un fichier.

# zfs key -c -o keysource=raw,file:///media/stick/key tank/home/darren

La clé de chiffrement des données d'un système de fichiers chiffré peut être modifiée à l'aide de
la commande zfs key -K, mais la nouvelle clé de chiffrement est uniquement utilisée pour les
nouvelles données écrites. Cette fonctionnalité peut être utilisée pour assurer la conformité avec
les directives NIST 800-57 relatives à la limitation dans le temps de la clé de chiffrement de
données. Par exemple :

# zfs key -K tank/home/darren

Dans l'exemple ci-dessus, la clé de chiffrement des données n'est ni visible ni directement gérée
par vous. En outre, la délégation keychange est requise pour effectuer une opération de
modification de clé.
Les algorithmes de chiffrement suivants sont disponibles :

■

■

aes-128-ccm, aes-192-ccm, aes-256-ccm
aes-128-gcm, aes-192-gcm, aes-256-gcm

La propriété keysource ZFS identifie le format et l'emplacement de la clé qui encapsule les clés
de chiffrement des données du système de fichiers. Par exemple :

# zfs get keysource tank/home/darren

NAME

PROPERTY

VALUE

SOURCE

tank/home/darren keysource passphrase,prompt local

La propriété rekeydate ZFS identifie la date de la dernière opération zfs key -K. Par exemple :

# zfs get rekeydate tank/home/darren

NAME

PROPERTY

VALUE

SOURCE

tank/home/darren rekeydate Tue Oct 12 15:36 2010 local

Si les propriétés creation et rekeydate d'un système de fichiers chiffré possèdent la même
valeur, cela signifie que le système de fichiers n'a jamais fait l'objet d'un renouvellement de clés
via une opération zfs key -K.

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

191

Chiffrement des systèmes de fichiers ZFS

Autorisations de délégation d'opérations sur les clés ZFS
Passez en revue les descriptions d'autorisations relatives à la délégation d'opérations sur les clés
suivantes :
■ Le chargement et le déchargement d'un système de fichiers à l'aide des commandes zfs key

-l et zfs key -u requièrent l'autorisation key. Dans la plupart des cas, l'autorisation de
montage est également requise.

■ La modification de la clé d'un système de fichiers à l'aide des commandes zfs key -c et zfs

key -K requiert l'autorisation keychange.

Envisagez de déléguer des autorisations distinctes pour l'utilisation (chargement ou
déchargement) et la modification de clés, de manière à mettre en place un modèle de gestion des
clés à deux personnes. Par exemple, déterminez les utilisateurs qui pourront utiliser les clés et
ceux qui seront autorisés à les modifier. Vous pouvez aussi spécifier que toute modification de
clé requiert la présence des deux utilisateurs. Ce modèle vous permet également de bâtir un
système de dépôt de clé (key escrow).

Montage d'un système de fichiers ZFS chiffré
Tenez compte des points suivants lorsque vous tentez de monter un système de fichiers ZFS
chiffré :

■

■

■

Si une clé de système de fichiers chiffré n'est pas disponible lors de l'initialisation, le système
de fichiers n'est pas monté automatiquement. Par exemple, un système de fichiers dont la
stratégie de chiffrement est définie sur passphrase,prompt ne sera pas monté lors de
l'initialisation car le processus d'initialisation ne s'interrompra pas pour afficher une invite
de saisie de phrase de passe.
Si vous souhaitez monter un système de fichiers avec une stratégie de chiffrement définie sur
passphrase,prompt lors de l'initialisation, vous devez explicitement le monter à l'aide de la
commande zfs mount et spécifier la phrase de passe ou utiliser la commande zfs key -l
pour être invité à saisir la clé après l'initialisation du système.
Par exemple :

# zfs mount -a

Enter passphrase for ’tank/home/darren’: xxxxxxxx

Enter passphrase for ’tank/home/ws’: xxxxxxxx

Enter passphrase for ’tank/home/mark’: xxxxxxxx

Si la propriété keysource d'un système de fichiers chiffré pointe vers un fichier appartenant
à un autre système de fichiers, l'ordre de montage des systèmes de fichiers peut déterminer si
le système de fichiers est monté lors de l'initialisation ou non, en particulier si le fichier est
placé sur un média amovible.

192

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Chiffrement des systèmes de fichiers ZFS

Interactions entre les propriétés de compression, de
suppression des doublons et de chiffrement ZFS
Tenez compte des points suivants lorsque vous utilisez les propriétés de compression, de
suppression des doublons et de chiffrement ZFS.
■ Lorsqu'un fichier est écrit, les données sont compressées, chiffrées et la somme de contrôle

est vérifiée. Lorsque cela est possible, les données sont ensuite dédupliquées.

■ Lorsqu'un fichier est lu, la somme de contrôle est vérifiée et les données sont déchiffrées. Si

nécessaire, les données sont ensuite décompressées.
Si la propriété dedup est activée sur un système de fichiers chiffré qui est également cloné et
si les commandes zfs key -K ou zfs clone -K n'ont pas été utilisées sur les clones, les
données de tous les clones sont dédupliquées, lorsque cela est possible.

■

Exemples de chiffrement de systèmes de fichiers ZFS

EXEMPLE 6–1 Chiffrement d'un système de fichiers ZFS à l'aide d'une clé raw
Dans l'exemple suivant, une clé de chiffrement aes-256-ccm est générée à l'aide de la
commande pktool et écrite dans un fichier, /cindykey.file.

# pktool genkey keystore=file outkey=/cindykey.file keytype=aes keylen=256

Le fichier /cindykey.file est ensuite spécifié lorsque le système de fichiers tank/home/cindy
est créé.

# zfs create -o encryption=aes-256-ccm -o keysource=raw,file:///cindykey.file

tank/home/cindy

EXEMPLE 6–2 Chiffrement d'un système de fichiers ZFS à l'aide d'un autre algorithme de chiffrement
Vous pouvez créer un pool de stockage ZFS et faire en sorte que tous les systèmes de fichiers du
pool de stockage héritent d'un algorithme de chiffrement. Dans l'exemple qui suit, le pool users
est créé et le système de fichiers users/home est créé et chiffré à l'aide d'une phrase de passe.
L'algorithme de chiffrement par défaut est aes-128-ccm.

Le système de fichiers users/home/mark est ensuite créé et chiffré à l'aide de l'algorithme de
chiffrement aes-256-ccm.

# zpool create -O encryption=on users mirror c0t1d0 c1t1d0 mirror c2t1d0 c3t1d0

Enter passphrase for ’users’: xxxxxxxx

Enter again: xxxxxxxx

# zfs create users/home

# zfs get encryption users/home

NAME

PROPERTY

VALUE

SOURCE

users/home encryption on

inherited from users

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

193

Chiffrement des systèmes de fichiers ZFS

EXEMPLE 6–2 Chiffrement d'un système de fichiers ZFS à l'aide d'un autre algorithme de chiffrement
(Suite)

# zfs create -o encryption=aes-256-ccm users/home/mark

# zfs get encryption users/home/mark

NAME

PROPERTY

VALUE

SOURCE

users/home/mark

encryption aes-256-ccm local

EXEMPLE 6–3 Clonage d'un système de fichiers ZFS chiffré
Si le système de fichiers clone hérite de la propriété keysource du même système de fichiers que
son instantané d'origine, une nouvelle propriété keysource n'est pas nécessaire, et vous n'êtes
pas invité à saisir une nouvelle phrase de passe lorsque keysource=passphrase,prompt . La
même propriété keysource est utilisée pour le clone. Par exemple :

Par défaut, vous n'êtes pas invité à saisir une clé lors du clonage d'un descendant d'un système
de fichiers chiffré.

# zfs create -o encryption=on tank/ws

Enter passphrase for ’tank/ws’: xxxxxxxx

Enter again: xxxxxxxx

# zfs create tank/ws/fs1

# zfs snapshot tank/ws/fs1@snap1

# zfs clone tank/ws/fs1@snap1 tank/ws/fs1clone

Si vous souhaitez créer une nouvelle clé pour le système de fichiers clone, utilisez la commande
zfs clone -K.

Si vous clonez un système de fichiers chiffré et non un système de fichiers chiffré descendant,
vous êtes invité à fournir une nouvelle clé. Par exemple :

# zfs create -o encryption=on tank/ws

Enter passphrase for ’tank/ws’: xxxxxxxx

Enter again: xxxxxxxx

# zfs snapshot tank/ws@1

# zfs clone tank/ws@1 tank/ws1clone

Enter passphrase for ’tank/ws1clone’: xxxxxxxx

Enter again: xxxxxxxx

EXEMPLE 6–4 Envoi et réception d'un système de fichiers ZFS chiffré
Dans l'exemple suivant, l'instantané tank/home/darren@snap1 est créé à partir du système de
fichiers chiffré /tank/home/darren. Ensuite, l'instantané est envoyé vers bpool/snaps avec la
propriété de chiffrement activée, si bien que les données résultantes reçues sont chiffrées.
Toutefois, le flux tank/home/darren@snap1 n'est pas chiffré pendant le processus d'envoi.

# zfs get encryption tank/home/darren

NAME

PROPERTY

VALUE

tank/home/darren encryption on

SOURCE

local

# zfs snapshot tank/home/darren@snap1

# zfs get encryption bpool/snaps

NAME

PROPERTY

VALUE

SOURCE

194

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de systèmes de fichiers ZFS

EXEMPLE 6–4 Envoi et réception d'un système de fichiers ZFS chiffré

(Suite)

bpool/snaps encryption on

inherited from bpool

# zfs send tank/home/darren@snap1 | zfs receive bpool/snaps/darren1012

# zfs get encryption bpool/snaps/darren1012

NAME

PROPERTY

VALUE

SOURCE

bpool/snaps/darren1012 encryption on

inherited from bpool

Dans ce cas, une nouvelle clé est automatiquement générée pour le système de fichiers chiffré
reçu.

Migration de systèmes de fichiers ZFS

Vous pouvez utiliser la fonctionnalité de migration shadow pour migrer des systèmes de
fichiers comme suit :

■

■

Système de fichiers ZFS local ou distant vers système de fichiers ZFS cible
Système de fichiers UFS local ou distant vers système de fichiers ZFS cible

La migration shadow est un processus permettant d'extraire les données à migrer :
■ Créez un système de fichiers ZFS vide.
■ Définissez la propriété shadow sur un système de fichiers ZFS vide, qui constitue le système

de fichiers cible (ou shadow), de manière à ce qu'elle pointe vers le système de fichiers à
migrer.

■ Les données du système de fichiers à migrer sont copiées vers le système de fichiers shadow.

Vous pouvez identifier le système de fichiers à migrer à l'aide de l'URI de la propriété shadow de
l'une des manières suivantes :

■

■

shadow=file:///path : utilisez cette syntaxe pour migrer un système de fichiers local
shadow=nfs://host:path : utilisez cette syntaxe pour migrer un système de fichiers NFS

Tenez compte des points suivants lors de la migration de systèmes de fichiers :
■ Le système de fichiers à migrer doit être défini sur lecture seule. Si le système de fichiers n'est

pas défini sur lecture seule, les modifications en cours risquent de ne pas être migrées.

■ Le système de fichiers cible doit être totalement vide.

■

Si le système est réinitialisé pendant la migration, la migration se poursuit une fois
l'initialisation terminée.

■ Le contenu d'un répertoire ou d'un fichier dont la migration n'est pas terminée est

■

inaccessible jusqu'à ce que l'ensemble du contenu ait été migré.
Si vous souhaitez migrer les informations relatives aux UID, GID et ACL vers le système de
fichiers shadow d'une migration NFS, assurez-vous que les informations du service de noms
sont accessibles entre le système local et le système distant. Vous pouvez envisager de copier

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

195

Migration de systèmes de fichiers ZFS

un sous-ensemble des données du système de fichiers à migrer afin de tester la migration et
de vous assurer que toutes les informations sont correctement migrées avant d'effectuer une
migration de grande envergure via NFS.

■ La migration des données d'un système de fichiers via NFS peut être lente, selon la bande

passante du réseau. Soyez patient.

■ La commande shadowstat permet de surveiller la migration d'un système de fichiers et

fournit les données suivantes :
■ La colonne BYTES XFRD indique le nombre d'octets transférés au système de fichiers

shadow.

■ La colonne BYTES LEFT est sans cesse mise à jour jusqu'à ce que la migration soit presque

terminée. ZFS n'identifie pas la quantité de données à migrer au début de la migration
car ce processus peut être trop long.

■ Pensez à utiliser les informations BYTES XFRD et ELAPSED TIME pour estimer la durée du

processus de migration.

▼ Migration d'un système de fichiers vers un système de

fichiers ZFS

1

2

3

Si vous migrez des données à partir d'un serveur NFS distant, assurez-vous que les informations
du service de noms sont accessibles sur les deux systèmes.
Pour des migrations de grande envergure à l'aide de NFS, envisagez d'effectuer une migration
test d'un sous ensemble de données afin de vous assurer que les informations relatives aux UID,
GUID, et ACL sont correctement migrées.

Si nécessaire, installez le package de migration shadow et activez le service shadowd pour vous
aider lors du processus de migration.

# pkg install shadow-migration

# svcadm enable shadowd

Si vous n'activez pas le processus shadowd, vous devrez restaurer le réglage none de la propriété
shadow à l'issue du processus de migration.

Définissez le système de fichiers local ou distant à migrer sur lecture seule.
Si vous migrez un système de fichiers ZFS local, définissez-le sur lecture seule. Par exemple :

# zfs set readonly=on tank/home/data

Si vous migrez un système de fichiers distant, partagez-le en lecture seule. Par exemple,

# share -F nfs -o ro /export/home/ufsdata

# share

-

/export/home/ufsdata

ro

""

196

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Migration de systèmes de fichiers ZFS

4

Créez un nouveau système de fichiers ZFS et définissez la propriété shadow de celui-ci sur le
système de fichiers à migrer.
Par exemple, si vous migrez un système de fichiers ZFS local, rpool/old, vers un nouveau
système de fichiers ZFS, users/home/shadow, définissez la propriété shadow sur rpool/old lors
de la création du système de fichiers users/home/shadow.

# zfs create -o shadow=file:///rpool/old users/home/shadow

Par exemple, pour migrer /export/home/ufsdata à partir d'un serveur distant, définissez la
propriété shadow lors de la création du système de fichiers ZFS.

# zfs create -o shadow=nfs://v120-brm-02/export/home/ufsdata users/home/shadow2

5

Vérifiez la progression de la migration.
Par exemple :

# shadowstat

EST

BYTES

BYTES

ELAPSED

DATASET

XFRD

LEFT

ERRORS TIME

users/home/shadow

45.5M

2.75M

users/home/shadow

users/home/shadow

55.8M

69.7M

-

-

No migrations in progress

-

-

-

00:02:31

00:02:41

00:02:51

Lorsque la migration est terminée, la propriété shadow est définie sur none.

# zfs get -r shadow users/home/shadow*

NAME

PROPERTY VALUE

SOURCE

users/home/shadow

shadow

none

users/home/shadow2 shadow

none

-

-

Dépannage des migrations de systèmes de fichiers
ZFS
Tenez compte des points suivants lors du dépannage des problèmes de migration ZFS :

■

■

■

■

Si le système de fichiers à migrer n'est pas défini sur lecture seule, certaines données ne sont
pas migrées.
Si le système de fichiers cible n'est pas vide lorsque la propriété shadow est définie, la
migration des données ne se lance pas.
Si vous ajoutez ou supprimez des données du système de fichiers à migrer alors que la
migration est en cours, ces modifications risquent de ne pas être migrées.
Si vous tentez de modifier le montage du système de fichiers shadow alors que la migration
est en cours, le message suivant s'affiche :

# zfs set mountpoint=/users/home/data users/home/shadow3

cannot unmount ’/users/home/shadow3’: Device busy

Chapitre 6 • Gestion des systèmes de fichiers Oracle Solaris ZFS

197

Mise à niveau des systèmes de fichiers ZFS

Mise à niveau des systèmes de fichiers ZFS

Si vous possédez des systèmes de fichiers ZFS d'une version antérieure de Solaris, vous pouvez
procéder à la mise à niveau de vos systèmes de fichiers à l'aide de la commande zfs upgrade
afin de tirer parti des fonctions du système de fichiers dans la version actuelle. De plus, cette
commande vous avertit lorsque vos systèmes de fichiers exécutent des versions antérieures.

Par exemple, la version de ce système de fichiers est la version 5 actuelle.

# zfs upgrade

This system is currently running ZFS filesystem version 5.

All filesystems are formatted with the current version.

Utilisez cette commande pour identifier les fonctions disponibles pour chaque version des
systèmes de fichiers.

# zfs upgrade -v

The following filesystem versions are supported:

VER DESCRIPTION

--- --------------------------------------------------------

1

2

3

4

5

Initial ZFS filesystem version

Enhanced directory entries

Case insensitive and File system unique identifier (FUID)

userquota, groupquota properties

System attributes

For more information on a particular version, including supported releases,

see the ZFS Administration Guide.

198

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

7C H A P I T R E

7

Utilisation des instantanés et des clones ZFS
Oracle Solaris

Ce chapitre fournit des informations sur la création et la gestion d'instantanés et de clones ZFS
Oracle Solaris. Des informations concernant l'enregistrement des instantanés sont également
fournies.
Ce chapitre contient les sections suivantes :
■ “Présentation des instantanés ZFS” à la page 199
■ “Création et destruction d'instantanés ZFS” à la page 200
■ “Affichage et accès des instantanés ZFS” à la page 203
■ “Restauration d'un instantané ZFS” à la page 205
■ “Présentation des clones ZFS” à la page 206
■ “Création d'un clone ZFS” à la page 207
■ “Destruction d'un clone ZFS” à la page 208
■ “Remplacement d'un système de fichiers ZFS par un clone ZFS” à la page 208
■ “Envoi et réception de données ZFS” à la page 209

Présentation des instantanés ZFS

Un instantané est une copie en lecture seule d'un système de fichiers ou d'un volume. La
création des instantanés est quasiment immédiate. Initialement, elle ne consomme pas d'espace
disque supplémentaire au sein du pool. Toutefois, à mesure que les données contenues dans le
jeu de données actif changent, l'instantané consomme de l'espace disque en continuant à faire
référence aux anciennes données et empêche donc la libération de l'espace disque.
Les instantanés ZFS présentent les caractéristiques suivantes :
■ Persistance au cours des réinitialisations de système.
■ Théoriquement, le nombre maximal d'instantanés est de 264 instantanés.
■ Les instantanés n'utilisent aucune sauvegarde de secours distincte. Les instantanés
consomment de l'espace disque provenant directement du pool de stockage auquel
appartient le système de fichiers ou le volume à partir duquel ils ont été créés.

199

Présentation des instantanés ZFS

■ Une seule opération, dite atomique, permet de créer rapidement des instantanés récursifs.

Ceux-ci sont tous créés simultanément ou ne sont pas créés du tout. Grâce à ce type
d'opération d'instantané atomique, une certaine cohérence des données d'instantané est
assurée, y compris pour les systèmes de fichiers descendants.

Il n'est pas possible d'accéder directement aux instantanés de volumes, mais ils peuvent être
clonés, sauvegardés, restaurés, etc. Pour plus d'informations sur la sauvegarde d'un instantané
ZFS, reportez-vous à la section “Envoi et réception de données ZFS” à la page 209.
■ “Création et destruction d'instantanés ZFS” à la page 200
■ “Affichage et accès des instantanés ZFS” à la page 203
■ “Restauration d'un instantané ZFS” à la page 205

Création et destruction d'instantanés ZFS
Les instantanés sont créés à l'aide de la commande zfs snapshot ou zfs snap, qui accepte
comme unique argument le nom de l'instantané à créer. Le nom de l'instantané est spécifié
comme suit :

filesystem@snapname
volume@snapname
Ce nom doit respecter les conventions d'attribution de nom définies à la section “Exigences
d'attribution de noms de composants ZFS” à la page 29.

Dans l'exemple suivant, un instantané de tank/home/matt nommé friday est créé.

# zfs snapshot tank/home/matt@friday

Vous pouvez créer des instantanés pour tous les systèmes de fichiers descendants à l'aide de
l'option -r. Par exemple :

# zfs snapshot -r tank/home@snap1

# zfs list -t snapshot -r tank/home

zfs list -t snapshot -r tank/home

NAME

USED AVAIL REFER MOUNTPOINT

tank/home@snap1

tank/home/mark@snap1

tank/home/matt@snap1

tank/home/tom@snap1

0

0

0

0

-

34K -

- 2.00G -

- 1.00G -

- 2.00G -

Les propriétés des instantanés ne sont pas modifiables. Les propriétés des jeux de données ne
peuvent pas être appliquées à un instantané. Par exemple :

# zfs set compression=on tank/home/matt@friday

cannot set property for ’tank/home/matt@friday’:

this property can not be modified for snapshots

La commande zfs destroy permet de détruire les instantanés. Par exemple :

200

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des instantanés ZFS

# zfs destroy tank/home/matt@friday

La destruction d'un jeu de données est impossible s'il existe des instantanés du jeu de données.
Par exemple :

# zfs destroy tank/home/matt

cannot destroy ’tank/home/matt’: filesystem has children

use ’-r’ to destroy the following datasets:

tank/home/matt@tuesday

tank/home/matt@wednesday

tank/home/matt@thursday

En outre, si des clones ont été créés à partir d'un instantané, ils doivent être détruits avant que
l'instantané ne puisse être détruit.
Pour de plus amples informations sur la sous-commande destroy, reportez-vous à la section
“Destruction d'un système de fichiers ZFS” à la page 135.

Conservation des clichés ZFS
Si vous disposez de plusieurs stratégies automatiques pour les instantanés pour que l'instantané
le plus ancien soit par exemple détruit par la commande zfs receive car il n'existe plus du côté
de l'envoi, vous pouvez utiliser la fonction de conservation des instantanés.
La conservation d'un instantané empêche sa destruction. En outre, cette fonction permet de
supprimer un instantané contenant des clones en attendant la suppression du dernier clone à
l'aide de la commande zfs destroy -d. Chaque instantané est associé à un décompte de
référence utilisateur initialisé sur 0 (zéro). Ce nombre augmente de 1 à chaque fois qu'un
instantané est conservé et diminue de 1 à chaque fois qu'un instantané conservé est libéré.
Dans la version précédente d'Oracle Solaris, les instantanés pouvaient uniquement être détruits
à l'aide de la commande zfs destroy s'ils ne contenaient aucun clone. Dans cette version
d'Oracle Solaris, les instantanés doivent également renvoyer un décompte de référence
utilisateur égal à 0 (zéro).
Vous pouvez conserver un instantané ou un jeu d'instantanés. Par exemple, la syntaxe suivante
insère une balise de conservation keep sur citerne/home/cindys/snap@1 :

# zfs hold keep tank/home/cindy@snap1

Vous pouvez utiliser l'option -r pour conserver récursivement les instantanés de tous les
systèmes de fichiers descendants. Par exemple :

# zfs snapshot -r tank/home@now

# zfs hold -r keep tank/home@now

Cette syntaxe permet d'ajouter une référence keep unique à cet instantané ou à ce jeu
d'instantanés. Chaque instantané possède son propre espace de noms de balise dans lequel
chaque balise de conservation doit être unique. Si un instantané est conservé, les tentatives de
destruction de ce dernier à l'aide de la commande zfs destroy échoueront. Par exemple :

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

201

Présentation des instantanés ZFS

# zfs destroy tank/home/cindy@snap1

cannot destroy ’tank/home/cindy@snap1’: dataset is busy

Pour détruire un instantané conservé, utilisez l'option -d. Par exemple :

# zfs destroy -d tank/home/cindy@snap1

Utilisez la commande zfs holds pour afficher la liste des instantanés conservés. Par exemple :

# zfs holds tank/home@now

NAME

TAG

TIMESTAMP

tank/home@now keep Fri May 6 06:34:03 2011

# zfs holds -r tank/home@now

NAME

TAG

TIMESTAMP

tank/home/cindy@now keep Fri May 6 06:34:03 2011

tank/home/mark@now

keep Fri May 6 06:34:03 2011

tank/home/matt@now

keep Fri May 6 06:34:03 2011

tank/home/tom@now

keep Fri May 6 06:34:03 2011

tank/home@now

keep Fri May 6 06:34:03 2011

Vous pouvez utiliser la commande zfs release pour libérer un instantané ou un jeu
d'instantanés conservé. Par exemple :

# zfs release -r keep tank/home@now

Si l'instantané est libéré, l'instantané peut être détruit à l'aide de la commande zfs destroy. Par
exemple :

# zfs destroy -r tank/home@now

Deux nouvelles propriétés permettent d'identifier les informations de conservation d'un
instantané :
■ La propriété defer_destroy est définie sur on si l'instantané a été marqué en vue d'une
destruction différée à l'aide de la commande zfs destroy -d. Dans le cas contraire, la
propriété est définie sur off.

■ La propriété userrefs également appelée décompte de référence utilisateur, est définie sur le

nombre de conservations pour cet instantané.

Renommage d'instantanés ZFS
Vous pouvez renommer les instantanés. Cependant, ils doivent rester dans le même pool et
dans le même jeu de données dans lequel il ont été créés. Par exemple :

# zfs rename tank/home/cindy@snap1 tank/home/cindy@today

En outre, la syntaxe de raccourci suivante est équivalente à la syntaxe précédente :

# zfs rename tank/home/cindy@snap1 today

202

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des instantanés ZFS

L'opération de renommage (rename) d'instantané n'est pas prise en charge, car le nom du pool
cible et celui du système de fichiers ne correspondent pas au pool et au système de fichiers dans
lesquels l'instantané a été créé :

# zfs rename tank/home/cindy@today pool/home/cindy@saturday

cannot rename to ’pool/home/cindy@today’: snapshots must be part of same

dataset

Vous pouvez renommer de manière récursive les instantanés à l'aide de la commande zfs
rename - r. Par exemple :

# zfs list -t snapshot -r users/home

NAME

USED AVAIL REFER MOUNTPOINT

users/home@now

23.5K

- 35.5K -

users/home@yesterday

users/home/lori@yesterday

users/home/mark@yesterday

users/home/neil@yesterday

0

0

0

0

-

38K -

- 2.00G -

- 1.00G -

- 2.00G -

# zfs rename -r users/home@yesterday @2daysago

# zfs list -t snapshot -r users/home

NAME

USED AVAIL REFER MOUNTPOINT

users/home@now

23.5K

- 35.5K -

users/home@2daysago

users/home/lori@2daysago

users/home/mark@2daysago

users/home/neil@2daysago

0

0

0

0

-

38K -

- 2.00G -

- 1.00G -

- 2.00G -

Affichage et accès des instantanés ZFS
Par défaut, les instantanés ne sont plus affichés dans la sortie zfs list. Vous devez utiliser la
commande zfs list -t snapshot pour afficher les informations relatives aux instantanés. Ou
activez la propriété de pool listsnapshots. Par exemple :

# zpool get listsnapshots tank

NAME PROPERTY

VALUE

SOURCE

tank listsnapshots off

default

# zpool set listsnapshots=on tank

# zpool get listsnapshots tank

NAME PROPERTY

VALUE

SOURCE

tank listsnapshots on

local

Les instantanés des systèmes de fichiers sont accessibles dans le répertoire .zfs/snapshot au
sein de la racine du système de fichiers. Par exemple, si tank/home/matt est monté sur
/home/matt, les données de l'instantané de tank/home/matt@thursday sont accessibles dans le
répertoire /home/matt/.zfs/snapshot/thursday.

# ls /tank/home/matt/.zfs/snapshot

tuesday wednesday thursday

Vous pouvez répertorier les instantanés comme suit :

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

203

Présentation des instantanés ZFS

# zfs list -t snapshot -r tank/home

NAME

USED AVAIL REFER MOUNTPOINT

tank/home/cindy@today

tank/home/mark@today

tank/home/matt@tuesday

tank/home/matt@wednesday

tank/home/matt@thursday

0

0

20K

20K

0

- 2.00G -

- 2.00G -

- 1.00G -

- 1.00G -

- 1.00G -

Vous pouvez répertorier les instantanés qui ont été créés pour un système de fichiers particulier
comme suit :

# zfs list -r -t snapshot -o name,creation tank/home

NAME

CREATION

tank/home/cindy@today

Fri May 6 6:32 2011

tank/home/mark@today

Fri May 6 6:22 2011

tank/home/matt@tuesday

Tue May 3 6:27 2011

tank/home/matt@wednesday Wed May 4 6:28 2011

tank/home/matt@thursday

Thu May 5 6:28 2011

Comptabilisation de l'espace disque des instantanés ZFS
Lors de la création d'un instantané, son espace disque est initialement partagé entre l'instantané
et le système de fichiers et éventuellement avec des instantanés précédents. Lorsque le système
de fichiers change, l'espace disque précédemment partagé devient dédié à l'instantané, et il est
compté dans la propriété used de l'instantané. De plus, la suppression d'instantanés peut
augmenter la quantité d'espace disque dédié à d'autres instantanés (et, par conséquent, utilisé
par ceux-ci).

La valeur de la propriété referenced de l'espace d'un instantané est la même que lors de la
création de l'instantané sur le système de fichiers.

Vous pouvez identifier des informations supplémentaires sur la façon dont les valeurs de la
propriété used sont utilisées. Les nouvelles propriétés de système de fichiers en lecture seule
décrivent l'utilisation de l'espace disque pour les clones, les systèmes de fichiers et les volumes.
Par exemple :

$ zfs list -o space -r rpool

AVAIL

USED USEDSNAP USEDDS USEDREFRESERV USEDCHILD

NAME

rpool

rpool/ROOT

60.0G 6.92G

60.0G 3.89G

0

0

40.5K

31K

rpool/ROOT/solaris

60.0G 3.49G

40.4M

3.16G

rpool/ROOT/solaris-1

60.0G

403M

rpool/ROOT/solaris-1/var 60.0G 92.7M

0

0

310M

92.7M

rpool/ROOT/solaris/var

60.0G

306M

89.9M

216M

rpool/dump

60.1G 2.00G

rpool/export

60.0G 96.5K

rpool/export/home

60.0G 64.5K

rpool/export/home/admin

60.0G 32.5K

rpool/swap

60.0G 1.03G

0

0

0

0

0

1.94G

62.7M

32K

32K

32.5K

1.00G

0

0

0

32.5M

Pour une description de ces propriétés, reportez-vous au Tableau 6–1.

204

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

0

0

0

0

0

0

6.92G

3.89G

306M

92.7M

0

0

0

64.5K

32.5K

0

0

Présentation des instantanés ZFS

Restauration d'un instantané ZFS
Vous pouvez utiliser la commande zfs rollback pour abandonner toutes les modifications
apportées à un système de fichiers depuis la création d'un instantané spécifique. Le système de
fichiers revient à l'état dans lequel il était lors de la prise de l'instantané. Par défaut, la
commande ne permet pas de restaurer un instantané autre que le plus récent.

Pour restaurer un instantané précédent, tous les instantanés intermédiaires doivent être
détruits. Vous pouvez détruire les instantanés précédents en spécifiant l'option -r.

S'il existe des clones d'un instantané intermédiaire, vous devez spécifier l'option -R pour
détruire également les clones.

Remarque – Si le système de fichiers que vous souhaitez restaurer est actuellement monté, il doit
être démonté, puis remonté. Si le système de fichiers ne peut pas être démonté, la restauration
échoue. L'option -f force le démontage du système de fichiers, le cas échéant.

Dans l'exemple suivant, l'état du système de fichiers tank/home/matt correspondant à
l'instantané tuesday est restauré.

# zfs rollback tank/home/matt@tuesday

cannot rollback to ’tank/home/matt@tuesday’: more recent snapshots exist

use ’-r’ to force deletion of the following snapshots:

tank/home/matt@wednesday

tank/home/matt@thursday

# zfs rollback -r tank/home/matt@tuesday

Dans cet exemple, les instantanés wednesday et thursday sont détruits en raison de la
restauration de l'instantané tuesday précédent.

# zfs list -r -t snapshot -o name,creation tank/home/matt

NAME

CREATION

tank/home/matt@tuesday Tue May 3 6:27 2011

Identification des différences entre des instantanés
ZFS (zfs diff)
Vous pouvez déterminer les différences entre des instantanés ZFS en utilisant la commande zfs
diff.

Supposons par exemple que les deux instantanés suivants sont créés :

$ ls /tank/home/tim

fileA

$ zfs snapshot tank/home/tim@snap1

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

205

Présentation des clones ZFS

$ ls /tank/home/tim

fileA fileB

$ zfs snapshot tank/home/tim@snap2

Par exemple, afin d'identifier les différences entre deux instantanés, utilisez une syntaxe
semblable à la suivante :

$ zfs diff tank/home/tim@snap1 tank/home/tim@snap2

M

+

/tank/home/tim/

/tank/home/tim/fileB

Dans la sortie, M indique que le répertoire a été modifié. Le + indique que fileB existe dans
l'instantané le plus récent.

Dans la sortie suivante, le M indique qu'un fichier dans un instantané a été renommé.

$ mv /tank/cindy/fileB /tank/cindy/fileC

$ zfs snapshot tank/cindy@snap2

$ zfs diff tank/cindy@snap1 tank/cindy@snap2

M

R

/tank/cindy/

/tank/cindy/fileB -> /tank/cindy/fileC

Le tableau suivant résume les modifications apportées au fichier ou au répertoire identifiées par
la commande zfs diff.

Modification de répertoire ou de fichier

Identificateur

Le fichier ou le répertoire a été modifié ou le lien d'un
répertoire ou d'un fichier a changé

Le fichier ou le répertoire est présent dans l'ancien instantané
mais pas dans le plus récent

Le fichier ou le répertoire est présent dans l'instantané le plus
récent mais pas dans le plus ancien.

Le fichier ou le répertoire a été renommé

M

—

+

R

Pour de plus amples informations, reportez-vous à la page de manuel zfs(1M).

Présentation des clones ZFS

Un clone est un volume ou un système de fichiers accessible en écriture et dont le contenu initial
est similaire à celui du jeu de données à partir duquel il a été créé. Tout comme pour les
instantanés, la création d'un clone est quasiment instantanée et ne consomme initialement
aucun espace disque supplémentaire. En outre, vous pouvez prendre un instantané d'un clone.

Les clones se créent uniquement à partir d'un instantané. Lors du clonage d'un instantané, une
dépendance implicite se crée entre le clone et l'instantané. Bien que le clone soit créé à une autre

206

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Présentation des clones ZFS

emplacement dans la hiérarchie de système de fichiers, l'instantané d'origine ne peut pas être
supprimé tant que le clone existe. La propriété origin indique cette dépendance et la
commande zfs destroy répertorie ces dépendances, le cas échéant.

Un clone n'hérite pas des propriétés du jeu de données à partir duquel il a été créé. Les
commandes zfs get et zfs set permettent d'afficher et de modifier les propriétés d'un jeu de
données cloné. Pour de plus amples informations sur la configuration des propriétés de jeux de
données ZFS, reportez-vous à la section “Définition des propriétés ZFS” à la page 163.

Dans la mesure où un clone partage initialement son espace disque avec l'instantané d'origine,
la valeur de la propriété used est initialement égale à zéro. A mesure que le clone est modifié, il
utilise de plus en plus d'espace disque. La propriété used de l'instantané d'origine ne tient pas
compte de l'espace disque consommé par le clone.
■ “Création d'un clone ZFS” à la page 207
■ “Destruction d'un clone ZFS” à la page 208
■ “Remplacement d'un système de fichiers ZFS par un clone ZFS” à la page 208

Création d'un clone ZFS
Pour créer un clone, utilisez la commande zfs clone en spécifiant l'instantané à partir duquel
créer le clone, ainsi que le nom du nouveau volume ou système de fichiers. Le nouveau volume
ou système de fichiers peut se trouver à tout emplacement de la hiérarchie ZFS. Le nouveau jeu
de données est du même type (un système de fichiers ou un volume, par exemple) que celui de
l'instantané à partir duquel le clone a été créé. Vous ne pouvez pas créer le clone d'un système de
fichiers dans un autre pool que celui de l'instantané du système de fichiers d'origine.

Dans l'exemple suivant, un nouveau clone appelé tank/home/matt/bug123 possédant le même
contenu initial que l'instantané tank/ws/gate@yesterday est créé :

# zfs snapshot tank/ws/gate@yesterday

# zfs clone tank/ws/gate@yesterday tank/home/matt/bug123

Dans l'exemple suivant, un espace de travail est créé à partir de l'instantané
projects/newproject@today pour un utilisateur temporaire, sous le nom
projects/teamA/tempuser. Ensuite, les propriétés sont configurées dans l'espace de travail
cloné.

# zfs snapshot projects/newproject@today

# zfs clone projects/newproject@today projects/teamA/tempuser

# zfs set share=name=projectA,path=/projects/teamA/tempuser,prot=nfs

projects/teamA/tempuser

name=projectA,path=/projects/teamA/tempuser,prot=nfs

# zfs set sharenfs=on projects/teamA/tempuser

# zfs set quota=5G projects/teamA/tempuser

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

207

Présentation des clones ZFS

Destruction d'un clone ZFS
La commande zfs destroy permet de détruire les clones ZFS. Par exemple : Destruction

# zfs destroy tank/home/matt/bug123

Les clones doivent être détruits préalablement à la destruction de l'instantané parent.

Remplacement d'un système de fichiers ZFS par un
clone ZFS
La commande zfs promote permet de remplacer un système de fichiers ZFS actif par un clone
de ce système de fichiers. Cette fonction facilite le clonage et le remplacement des systèmes de
fichiers pour que le système de fichiers original devienne le clone du système de fichiers spécifié.
En outre, cette fonction permet de détruire le système de fichiers à partir duquel le clone a été
créé. Il est impossible de détruire un système de fichiers d'origine possédant des clones actifs,
sans le remplacer par l'un de ses clones. Pour plus d'informations sur la destruction des clones,
reportez-vous à la section “Destruction d'un clone ZFS” à la page 208.
Dans l'exemple suivant, le système de fichiers tank/test/productA est cloné, puis le clone du
système de fichiers (tank/test/productAbeta) devient le système de fichiers
tank/test/productA d'origine.

# zfs create tank/test

# zfs create tank/test/productA

# zfs snapshot tank/test/productA@today

# zfs clone tank/test/productA@today tank/test/productAbeta

# zfs list -r tank/test

NAME

tank/test

USED AVAIL REFER MOUNTPOINT

104M 66.2G

23K /tank/test

tank/test/productA

104M 66.2G

104M /tank/test/productA

tank/test/productA@today

0

-

104M -

tank/test/productAbeta

0 66.2G

104M /tank/test/productAbeta

# zfs promote tank/test/productAbeta

# zfs list -r tank/test

NAME

tank/test

USED AVAIL REFER MOUNTPOINT

104M 66.2G

24K /tank/test

tank/test/productA

0 66.2G

104M /tank/test/productA

tank/test/productAbeta

104M 66.2G

104M /tank/test/productAbeta

tank/test/productAbeta@today

0

-

104M -

Dans la sortie zfs list, les informations de comptabilisation de l'espace disque du système de
fichiers d'origine productA ont été remplacées par celles du système de fichiers productAbeta
Pour terminer le processus de remplacement de clone, renommez les systèmes de fichiers. Par
exemple :

# zfs rename tank/test/productA tank/test/productAlegacy

# zfs rename tank/test/productAbeta tank/test/productA

# zfs list -r tank/test

208

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

Vous pouvez également supprimer l'ancien système de fichiers si vous le souhaitez. Par
exemple :

# zfs destroy tank/test/productAlegacy

Envoi et réception de données ZFS

La commande zfs send crée une représentation de flux d'un instantané qui est écrite dans la
sortie standard. Un flux complet est généré par défaut. Vous pouvez rediriger la sortie vers un
fichier ou un système fichier. La commande zfs receive crée un instantané dont le contenu est
spécifié dans le flux fourni dans l'entrée standard. En cas de réception d'un flux complet, un
système de fichiers est également créé. Ces commandes permettent d'envoyer les données
d'instantané ZFS et de recevoir les systèmes de fichiers et les données d'instantané ZFS.
Reportez-vous aux exemples de la section suivante.
■ “Enregistrement de données ZFS à l'aide d'autres produits de sauvegarde” à la page 210
■ “Envoi d'un instantané ZFS” à la page 212
■ “Réception d'un instantané ZFS” à la page 213
■ “Application de différentes valeurs de propriété à un flux d'instantané ZFS” à la page 214
■ “Envoi et réception de flux d'instantanés ZFS complexes” à la page 216
■ “Réplication distante de données ZFS” à la page 219

Les solutions de sauvegarde suivantes sont disponibles pour enregistrer les données ZFS :
■ Produits de sauvegarde d'entreprise : si vous souhaitez disposer des fonctions suivantes,

considérez une solution de sauvegarde d'entreprise :
■ Restauration fichier par fichier
■ Vérification des médias de sauvegarde
■ Gestion des médias
Instantanés de systèmes de fichiers et restauration d'instantanés : exécutez les
commandes zfs snapshot et zfs rollback pour créer facilement une copie d'un système
de fichiers et restaurer une version précédente d'un système de fichier, le cas échéant. Par
exemple, vous pouvez utiliser cette solution pour restaurer un ou plusieurs fichiers issus
d'une version précédente d'un système de fichiers.
Pour de plus amples informations sur la création et la restauration d'instantané,
reportez-vous à la section “Présentation des instantanés ZFS” à la page 199.

■

■ Enregistrement d'instantanés : utilisez les commandes zfs send et zfs receive pour

envoyer et recevoir un instantané ZFS. Vous pouvez enregistrer les modifications
incrémentielles entre instantanés, mais la restauration individuelle de fichiers est
impossible. L'instantané du système doit être restauré dans son intégralité. Ces commandes
ne constituent pas une solution de sauvegarde complète pour l'enregistrement de vos
données ZFS.

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

209

Envoi et réception de données ZFS

■ Réplication distante : utilisez les commandes zfs send et zfs receive pour copier un

système de fichiers d'un système vers un autre. Ce processus diffère d'un produit de gestion
de volume classique qui pourrait mettre les périphériques en miroir dans un WAN. Aucune
configuration ni aucun matériel spécifique n'est requis. La réplication de systèmes de
fichiers ZFS a ceci d'avantageux qu'elle permet de recréer un système de fichiers dans un
pool de stockage et de spécifier différents niveaux de configuration pour le nouveau pool,
comme RAID-Z, mais avec des données de système de fichiers identiques.

■ Utilitaires d'archivage : enregistrez les données ZFS à l'aide d'utilitaires d'archivage tels que
tar, cpio et pax, ou des produits de sauvegarde tiers. Actuellement, les deux utilitaires tar
et cpio traduisent correctement les ACL de type NFSv4, contrairement à l'utilitaire pax.

Enregistrement de données ZFS à l'aide d'autres
produits de sauvegarde
Outre les commandes zfs send et zfs receive, vous pouvez utiliser des utilitaires d'archivage,
tels que les commandes tar et cpio pour enregistrer des fichiers ZFS. Ces utilitaires
enregistrent et restaurent les attributs de fichiers et les ACL ZFS. Vérifiez les options adéquates
des commandes tar et cpio.

Pour les informations les plus récentes sur les problèmes relatifs aux produits de sauvegarde
ZFS et tiers, reportez-vous aux notes de version d'Oracle Solaris 11.

Reconnaissance des flux d'instantané ZFS
Un instantané d'un système de fichiers ou volume ZFS est converti en flux d'instantané à l'aide
de la commande zfs send. Ensuite, vous pouvez utiliser le flux d'instantané pour recréer un
système de fichiers ou volume ZFS à l'aide de la commande zfs receive.

Selon les options zfs send utilisées pour créer le flux d'instantané, différents formats de flux
sont générés.
■ Flux complet : se compose du contenu intégral du jeu de données, depuis sa création jusqu'à

la prise de l'instantané spécifié.
Le flux par défaut généré par la commande zfs send est un flux complet. Il contient un
système de fichiers ou un volume, jusqu'à et y compris l'instantané spécifié. Le flux ne
contient pas d'autre instantané que celui spécifié sur la ligne de commande.

■ Flux incrémentiel : se compose des différences entre deux instantanés.

Un package de flux est un type de flux contenant un ou plusieurs flux incrémentiels. Il existe
trois types de packages de flux :

210

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

■ Package de flux de réplication : se compose du jeu de données spécifié et de ses descendants.
Il inclut tous les instantanés intermédiaires. Si l'origine d'un jeu de données cloné n'est pas
un descendant de l'instantané spécifié sur la ligne de commande, le jeu de données d'origine
n'est pas inclus dans le package de flux. Pour recevoir le flux, le jeu de données d'origine doit
exister dans le pool de stockage de destination.
Examinez la liste de jeux de données suivis de leur origine suivante. Nous supposerons qu'ils
ont été créés dans l'ordre dans lequel ils apparaissent ci-dessous :

NAME

pool/a

pool/a/1

pool/a/1@clone

pool/b

pool/b/1

ORIGIN

-

-

-

-

pool/a/1@clone

pool/b/1@clone2

-

pool/b/2

pool/b/1@clone2

pool/b@pre-send

pool/b/1@pre-send

pool/b/2@pre-send

pool/b@send

pool/b/1@send

pool/b/2@send

-

-

-

-

-

-

Un package de flux de réplication créé en respectant la syntaxe suivante :

# zfs send -R pool/b@send ....

Se compose des flux complets et incrémentiels suivants :

TYPE

SNAPSHOT

INCREMENTAL FROM

full

pool/b@pre-send

-

incr

pool/b@send

pool/b@pre-send

incr

pool/b/1@clone2

pool/a/1@clone

incr

pool/b/1@pre-send

pool/b/1@clone2

incr

pool/b/1@send

pool/b/1@send

incr

pool/b/2@pre-send

pool/b/1@clone2

incr

pool/b/2@send

pool/b/2@pre-send

Dans la sortie qui précède, l'instantané pool/a/1@clone n'est pas inclus dans le package de
flux de réplication. En l'état, ce package de flux de réplication peut uniquement être reçu
dans un pool possédant déjà l'instantané pool/a/1@clone .

■ Package de flux récursif : se compose du jeu de données spécifié et de ses descendants. A la

différence des packages de flux de réplication, les instantanés intermédiaires ne sont pas
inclus, sauf s'ils constituent l'origine d'un jeu de données cloné inclus dans le flux. Par
défaut, si l'origine d'un jeu de données n'est pas un descendant de l'instantané spécifié sur la
ligne de commande, le comportement est le même que pour les flux de réplication.
Néanmoins, un flux récursif autonome, comme décrit ci-après, est créé de manière à ce qu'il
n'y ait aucune dépendance externe.
Un package de flux récursif créé en respectant la syntaxe suivante :

# zfs send -r pool/b@send ...

Se compose des flux complets et incrémentiels suivants :

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

211

Envoi et réception de données ZFS

TYPE

SNAPSHOT

INCREMENTAL FROM

full

pool/b@send

-

incr

pool/b/1@clone2

pool/a/1@clone

incr

pool/b/1@send

pool/b/1@clone2

incr

pool/b/2@send

pool/b/1@clone2

Dans la sortie qui précède, l'instantané pool/a/1@clone n'est pas inclus dans le package de
flux récursif. En l'état, ce package de flux récursif peut uniquement être reçu dans un pool
qui possède déjà l'instantané pool/a/1@clone . Ce comportement est similaire au scénario
du package de flux de réplication décrit plus haut.

■ Package de flux récursif autonome : ne dépend d'aucun jeu de données non inclus dans le

package de flux. Le package de flux récursif créé en respectant la syntaxe suivante :

# zfs send -rc pool/b@send ...

Se compose des flux complets et incrémentiels suivants :

TYPE

SNAPSHOT

INCREMENTAL FROM

full

pool/b@send

-

full

pool/b/1@clone2

incr

pool/b/1@send

pool/b/1@clone2

incr

pool/b/2@send

pool/b/1@clone2

Notez que le flux récursif autonome possède un flux complet de l'instantané
pool/b/1@clone2, ce qui permet la réception de l'instantané pool/b/1 sans dépendance
externe.

Envoi d'un instantané ZFS
Vous pouvez utiliser la commande zfs send pour envoyer une copie d'un flux d'instantané et
recevoir ce flux dans un autre pool du même système ou dans un autre pool d'un système
différent utilisé pour stocker les données de sauvegarde. Par exemple, pour envoyer le flux
d'instantané à un pool différent du même système, employez une syntaxe du type suivant :

# zfs send tank/dana@snap1 | zfs recv spool/ds01

Vous pouvez utiliser zfs recv en tant qu'alias pour la commande zfs receive.
Si vous envoyez le flux de l'instantané à un système différent, envoyez la sortie de la commande
zfs send à la commande ssh. Par exemple :

sys1# zfs send tank/dana@snap1 | ssh sys2 zfs recv newtank/dana

Lors de l'envoi d'un flux complet, le système de fichiers de destination ne doit pas exister.
Vous pouvez envoyer les données incrémentielles à l'aide de l'option zfs send - i. Par
exemple :

sys1# zfs send -i tank/dana@snap1 tank/dana@snap2 | ssh sys2 zfs recv newtank/dana

212

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

Le premier argument (snap1) correspond à l'instantané le plus ancien, le second (snap2) à
l'instantané le plus récent. Dans ce cas, le système de fichiers newtank/dana doit déjà exister
pour que la réception incrémentielle s'effectue correctement.
La source de l'instantané1 incrémentiel peut être spécifiée comme étant le dernier composant
du nom de l'instantané. Grâce à ce raccourci, il suffit de spécifier le nom après le signe @ pour
l'instantané1, qui est considéré comme provenant du même système de fichiers que
l'instantané2. Par exemple :

sys1# zfs send -i snap1 tank/dana@snap2 | ssh sys2 zfs recv newtank/dana

Cette syntaxe de raccourci est équivalente à la syntaxe incrémentielle de l'exemple précédent.
Le message s'affiche en cas de tentative de génération d'un flux incrémentiel à partir d'un
instantané1 provenant d'un autre système de fichiers :

cannot send ’pool/fs@name’: not an earlier snapshot from the same fs

Si vous devez stocker de nombreuses copies, envisagez de compresser une représentation de
flux d'instantané ZFS à l'aide de la commande gzip. Par exemple :

# zfs send pool/fs@snap | gzip > backupfile.gz

Réception d'un instantané ZFS
Gardez les points suivants à l'esprit lorsque vous recevez un instantané d'un système de fichiers :
■ L'instantané et le système de fichiers sont reçus.
■ Le système de fichiers et tous les systèmes de fichiers descendants sont démontés.
■ Les systèmes de fichiers sont inaccessibles tant qu'ils sont en cours de réception.
■ Le système de fichiers d'origine à recevoir ne doit pas exister tant qu'il est en cours de

transfert.
Si ce nom existe déjà, vous pouvez utiliser la commande zfs rename pour renommer le
système de fichiers.

■

Par exemple :

# zfs send tank/gozer@0830 > /bkups/gozer.083006

# zfs receive tank/gozer2@today < /bkups/gozer.083006

# zfs rename tank/gozer tank/gozer.old

# zfs rename tank/gozer2 tank/gozer

Si vous apportez des modifications au système de fichiers de destination et souhaitez effectuer
un autre envoi incrémentiel d'instantané, vous devez au préalable restaurer le système de
fichiers destinataire.
Voyez l'exemple suivant. Modifiez tout d'abord le système de fichiers comme suit :

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

213

Envoi et réception de données ZFS

sys2# rm newtank/dana/file.1

Effectuez ensuite un envoi incrémentiel de char/dana@snap3. Cependant, vous devez d'abord
annuler (roll back) le système de fichiers destinataire pour permettre la réception du nouvel
instantané incrémentiel. Vous pouvez aussi utiliser l'option -F pour éviter l'étape de
restauration. Par exemple :

sys1# zfs send -i tank/dana@snap2 tank/dana@snap3 | ssh sys2 zfs recv -F newtank/dana

Lors de la réception d'un instantané incrémentiel, le système de fichiers de destination doit déjà
exister.
Si vous apportez des modifications au système de fichiers sans restaurer le système de fichiers
destinataire pour permettre la réception du nouvel instantané incrémentiel, ou si vous ne
spécifiez pas l'option -F, un message similaire au message suivant s'affiche :

sys1# zfs send -i tank/dana@snap4 tank/dana@snap5 | ssh sys2 zfs recv newtank/dana

cannot receive: destination has been modified since most recent snapshot

Les vérifications suivantes sont requises pour assurer l'exécution de l'option -F :

■

■

Si l'instantané le plus récent ne correspond pas à la source incrémentielle, la restauration et
la réception ne s'effectuent pas intégralement et un message d'erreur s'affiche.
Si vous avez fourni accidentellement le nom d'un système de fichiers qui ne correspond pas
à la source incrémentielle dans la commande zfs receive, la restauration et la réception ne
s'effectuent pas correctement et le message d'erreur suivant s'affiche :

cannot send ’pool/fs@name’: not an earlier snapshot from the same fs

Application de différentes valeurs de propriété à un
flux d'instantané ZFS
Vous pouvez envoyer un flux d'instantané ZFS avec une certaine valeur de propriété de système
de fichiers, mais vous pouvez spécifier une valeur de propriété locale différente lorsque le flux de
l'instantané est reçu. Vous pouvez également indiquer que la valeur de propriété d'origine doit
être utilisée lorsque le flux d'instantané est reçu pour recréer le système de fichiers d'origine. En
outre, vous pouvez désactiver une propriété de système de fichiers lorsque le flux d'instantané
est reçu.
■ Utilisez la commande zfs inherit -S pour rétablir la valeurs de propriété locale reçue, le

cas échéant. Si une propriété ne reçoit aucune valeur, le comportement de la commande zfs
inherit -S est le même que la commande zfs inherit sans l'option -S. Si la propriété ne
reçoit aucune valeur, la commande zfs inherit masque la valeur reçue par la valeur héritée
jusqu'à ce que l'émission d'une commande zfs inherit -S rétablisse la valeur reçue.

■ Vous pouvez utiliser la commande zfs get -o pour prendre en compte la nouvelle colonne

RECEIVED ajoutée. Vous pouvez également utiliser la commande zfs get -o all pour
ajouter toutes les colonnes, y compris la colonne RECEIVED.

214

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

■ Vous pouvez utiliser l'option zfs send -p pour ajouter des propriétés dans le flux d'envoi

sans l'option -R.

■ L'option zfs send -e permet d'utiliser le dernier élément du nom de l'instantané envoyé

pour définir le nom du nouvel instantané. L'exemple suivant envoie l'instantané
poola/bee/cee@1 au système de fichiers poold/eee et utilise uniquement le dernier
élément (cee@1) du nom de l'instantané pour créer le système de fichiers et l'instantané
reçus.

# zfs list -rt all poola

NAME

poola

USED AVAIL REFER MOUNTPOINT

134K

134G

23K /poola

poola/bee

44K

134G

23K /poola/bee

poola/bee/cee

21K

134G

21K /poola/bee/cee

poola/bee/cee@1

0

-

21K -

# zfs send -R poola/bee/cee@1 | zfs receive -e poold/eee

# zfs list -rt all poold

NAME

poold

USED AVAIL REFER MOUNTPOINT

134K

134G

23K /poold

poold/eee

44K

134G

23K /poold/eee

poold/eee/cee

21K

134G

21K /poold/eee/cee

poold/eee/cee@1

0

-

21K -

Dans certains cas, les propriétés du système de fichiers dans un flux envoyé ne peuvent pas
s'appliquer au système de fichiers récepteur ou aux propriétés du système de fichiers local,
comme la valeur de propriété mountpoint, et risquent d'interférer avec une restauration.

Par exemple, dans le système de fichiers tank/données, la propriété compression est désactivée.
Un instantané du système de fichiers tank/data est envoyé avec des propriétés (option -p) à un
pool de sauvegarde et est reçu avec la propriété compression activée.

# zfs get compression tank/data

NAME

PROPERTY

VALUE

SOURCE

tank/data compression off

default

# zfs snapshot tank/data@snap1

# zfs send -p tank/data@snap1 | zfs recv -o compression=on -d bpool

# zfs get -o all compression bpool/data

NAME

PROPERTY

VALUE

RECEIVED SOURCE

bpool/data compression on

off

local

Dans l'exemple, la propriété compression est activée lorsque l'instantané est reçu dans bpool.
Par conséquent, pour bpool/data, la valeur compression est activée.

Si ce flux d'instantané est envoyé à un nouveau pool, restorepool, à des fins de récupération,
vous pouvez être amené à conserver toutes les propriétés de l'instantané d'origine. Dans ce cas,
vous devez utiliser la commande zfs send -b pour restaurer les propriétés de l'instantané
d'origine. Par exemple :

# zfs send -b bpool/data@snap1 | zfs recv -d restorepool

# zfs get -o all compression restorepool/data

NAME

PROPERTY

VALUE

RECEIVED SOURCE

restorepool/data compression off

off

received

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

215

Envoi et réception de données ZFS

Dans l'exemple, la valeur de compression est off, elle représente la valeur de compression de
l'instantané du système de fichiers tank/data d'origine.

Si vous disposez d'une valeur de propriété de système de fichiers local dans un flux d'instantané
et que vous souhaitez désactiver la propriété lors de sa réception, utilisez la commande zfs
receive -x. Par exemple, la commande suivante envoie un flux d'instantané récursif des
systèmes de fichiers du répertoire personnel avec toutes les propriétés de système de fichiers
réservées à un pool de sauvegarde, mais sans les valeurs de propriété du quota.

# zfs send -R tank/home@snap1 | zfs recv -x quota bpool/home

# zfs get -r quota bpool/home

NAME

PROPERTY VALUE SOURCE

bpool/home

quota

none

local

bpool/home@snap1

quota

-

-

bpool/home/lori

quota

none

default

bpool/home/lori@snap1 quota

-

-

bpool/home/mark

quota

none

default

bpool/home/mark@snap1 quota

-

-

Si l'instantané récursif n'a pas été reçu avec l'option -x, la propriété de quota doit être définie
dans les systèmes de fichiers reçus.

# zfs send -R tank/home@snap1 | zfs recv bpool/home

# zfs get -r quota bpool/home

NAME

PROPERTY VALUE SOURCE

bpool/home

quota

none

received

bpool/home@snap1

quota

-

-

bpool/home/lori

quota

10G

received

bpool/home/lori@snap1 quota

-

-

bpool/home/mark

quota

10G

received

bpool/home/mark@snap1 quota

-

-

Envoi et réception de flux d'instantanés ZFS
complexes
Cette section décrit l'utilisation des options zfs send -I et -R pour envoyer et recevoir des flux
d'instantanés plus complexes.

Gardez les points suivants à l'esprit lors de l'envoi et de la réception de flux d'instantanés ZFS
complexes :
■ Utilisez l'option zfs send -I pour envoyer tous les flux incrémentiels d'un instantané à un

instantané cumulé. Vous pouvez également utiliser cette option pour envoyer un flux
incrémentiel de l'instantané d'origine pour créer un clone. L'instantané d'origine doit déjà
exister sur le côté récepteur afin d'accepter le flux incrémentiel.

■ Utilisez l'option zfs send -R pour envoyer un flux de réplication de tous les systèmes de

fichiers descendants. Une fois le flux de réplication reçu, les propriétés, instantanés,
systèmes de fichiers descendants et clones sont conservés.

216

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

■ Lorsque l'option zfs send -r est utilisée sans l'option -c et lorsque l'option zfs send -R est

utilisée, les packages de flux omettent dans certains cas l'origin des clones. Pour plus
d'informations, reportez-vous à la section “Reconnaissance des flux d'instantané ZFS”
à la page 210.

■ Vous pouvez utiliser les deux options pour envoyer un flux de réplication incrémentiel.

■ Les modifications des propriétés sont conservées, tout comme les opérations rename et

destroy des instantanés et des systèmes de fichiers.
Si l'option zfs recv -F n'est pas spécifiée lors de la réception du flux de réplication, les
opérations destroy du jeu de données sont ignorées. La syntaxe de zfs recv -F dans ce
cas peut conserve également sa signification de récupération le cas échéant.

■

■ Tout comme dans les autres cas - i ou -I (autres que zfs send -R), si l'option -I est
utilisée, tous les instantanés créés entre snapA et snapD sont envoyés. Si l'option -i est
utilisée, seul snapD (pour tous les descendants) est envoyé.

■ Pour recevoir ces nouveaux types de flux zfs send, le système récepteur doit exécuter une

version du logiciel capable de les envoyer. La version des flux est incrémentée.
Vous pouvez cependant accéder à des flux d'anciennes versions de pool en utilisant une
version plus récente du logiciel. Vous pouvez par exemple envoyer et recevoir des flux créés
à l'aide des nouvelles options à partir d'un pool de la version 3. Vous devez par contre
exécuter un logiciel récent pour recevoir un flux envoyé avec les nouvelles options.

EXEMPLE 7–1 Envoi et réception de flux d'instantanés ZFS complexes
Plusieurs instantanés incrémentiels peuvent être regroupés en un seul instantané à l'aide de
l'option zfs send -I. Par exemple :

# zfs send -I pool/fs@snapA pool/fs@snapD > /snaps/fs@all-I

Vous pouvez ensuite supprimer snapB, snapC et snapD.

# zfs destroy pool/fs@snapB

# zfs destroy pool/fs@snapC

# zfs destroy pool/fs@snapD

Pour recevoir les instantanés combinés, vous devez utiliser la commande suivante :

# zfs receive -d -F pool/fs < /snaps/fs@all-I

# zfs list

NAME

pool

pool/fs

pool/fs@snapA

pool/fs@snapB

pool/fs@snapC

pool/fs@snapD

USED AVAIL REFER MOUNTPOINT

428K 16.5G

20K /pool

71K 16.5G

21K /pool/fs

16K

17K

17K

0

- 18.5K -

-

20K -

- 20.5K -

-

21K -

Vous pouvez également utiliser la commande zfs send -I pour regrouper un instantané et un
clone d'instantané en un nouveau jeu de données. Par exemple :

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

217

Envoi et réception de données ZFS

EXEMPLE 7–1 Envoi et réception de flux d'instantanés ZFS complexes

(Suite)

# zfs create pool/fs

# zfs snapshot pool/fs@snap1

# zfs clone pool/fs@snap1 pool/clone

# zfs snapshot pool/clone@snapA

# zfs send -I pool/fs@snap1 pool/clone@snapA > /snaps/fsclonesnap-I

# zfs destroy pool/clone@snapA

# zfs destroy pool/clone

# zfs receive -F pool/clone < /snaps/fsclonesnap-I

Vous pouvez utiliser la commande zfs send -R pour répliquer un système de fichiers ZFS et
tous ses systèmes de fichiers descendants, jusqu'à l'instantané nommé. Une fois ce flux reçu, les
propriétés, instantanés, systèmes de fichiers descendants et clones sont conservés.

Dans l'exemple suivant, des instantanés des systèmes de fichiers utilisateur sont créés. Un flux
de réplication de tous les instantanés utilisateur est créé. Les systèmes de fichiers et instantanés
d'origine sont ensuite détruits et récupérés.

# zfs snapshot -r users@today

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

187K 33.2G

22K /users

users@today

0

-

22K -

users/user1

18K 33.2G

18K /users/user1

users/user1@today

0

-

18K -

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

# zfs send -R users@today > /snaps/users-R

# zfs destroy -r users

# zfs receive -F -d users < /snaps/users-R

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

196K 33.2G

22K /users

users@today

0

-

22K -

users/user1

18K 33.2G

18K /users/user1

users/user1@today

0

-

18K -

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

Dans l'exemple suivant, la commande zfs send -R a été utilisée pour répliquer le système de
fichiers users et ses descendants et pour envoyer le flux répliqué vers un autre pool, users2.

# zfs create users2 mirror c0t1d0 c1t1d0

# zfs receive -F -d users2 < /snaps/users-R

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

224K 33.2G

22K /users

users@today

0

-

22K -

users/user1

33K 33.2G

18K /users/user1

users/user1@today

15K

-

18K -

218

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Envoi et réception de données ZFS

EXEMPLE 7–1 Envoi et réception de flux d'instantanés ZFS complexes

(Suite)

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

users2

188K 16.5G

22K /users2

users2@today

0

-

22K -

users2/user1

18K 16.5G

18K /users2/user1

users2/user1@today

0

-

18K -

users2/user2

18K 16.5G

18K /users2/user2

users2/user2@today

0

-

18K -

users2/user3

18K 16.5G

18K /users2/user3

users2/user3@today

0

-

18K -

Réplication distante de données ZFS
Les commandes zfs send et zfs recv permettent d'effectuer une copie distante d'une
représentation de flux d'instantané d'un système vers un autre. Par exemple :

# zfs send tank/cindy@today | ssh newsys zfs recv sandbox/restfs@today

Cette commande envoie les données de l'instantané tank/cindy@today et les reçoit dans le
système de fichiers sandbox/restfs. La commande suivante crée également un instantané
restfs@aujourd’hui sur le système newsys. Dans cet exemple, l'utilisateur a été configuré pour
utiliser ssh dans le système distant.

Chapitre 7 • Utilisation des instantanés et des clones ZFS Oracle Solaris

219

220

8C H A P I T R E

8

Utilisation des ACL et des attributs pour
protéger les fichiers Oracle Solaris ZFS

Ce chapitre décrit l'utilisation des listes de contrôle d'accès (ACL, Access Control List) pour
protéger les fichiers ZFS en accordant des autorisations à un niveau de granularité plus fin que
les autorisations UNIX standard.

Ce chapitre contient les sections suivantes :
■ “Nouveau modèle ACL Solaris” à la page 221
■ “Configuration d'ACL dans des fichiers ZFS” à la page 229
■ “Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé” à la page 231
■ “Configuration et affichage d'ACL dans des fichiers ZFS en format compact” à la page 242
■ “Application d'attributs spéciaux aux fichiers ZFS” à la page 248

Nouveau modèle ACL Solaris

Les versions précédentes de Solaris assuraient la prise en charge d'une implémentation ACL
reposant principalement sur la spécification POSIX-draft ACL. Les ACL basées sur
POSIX-draft sont utilisées pour protéger les fichiers UFS et sont traduites par les versions de
NFS antérieures à NFSv4.

Grâce à l'introduction de NFSv4, un nouveau modèle d'ACL assure entièrement la prise en
charge de l'interopérabilité qu'offre NFSv4 entre les clients UNIX et non UNIX. La nouvelle
implémentation d'ACL, telle que définie dans les spécifications NFSv4, fournit des sémantiques
bien plus riches, basées sur des ACL NT.

Les différences principales du nouveau modèle d'ACL sont les suivantes :
■ Modèle basé sur la spécification NFSv4 et similaire aux ACL de type NT.

■

Jeu de privilèges d'accès bien plus granulaire. Pour plus d'informations, reportez-vous au
Tableau 8–2.

■ Configuration et affichage avec les commandes chmod et ls, et non les commandes setfacl

et getfacl.

221

Nouveau modèle ACL Solaris

■

Sémantique d'héritage bien plus riche pour déterminer comment les privilèges d'accès sont
appliqués d'un répertoire à un sous-répertoire, et ainsi de suite. Pour plus d'informations,
reportez-vous à la section “Héritage d'ACL” à la page 227.

Les deux modèles d'ACL assurent un contrôle d'accès à un niveau de granularité plus fin que
celui disponible avec les autorisations de fichier standard. De façon similaire aux listes de
contrôle d'accès POSIX-draft, les nouvelles ACL se composent de plusieurs ACE (Access
Control Entry, entrées de contrôle d'accès).

Les ACL POSIX-draft utilisent une seule entrée pour définir quelles autorisations sont
accordées et lesquelles sont refusées. Le nouveau modèle d'ACL dispose de deux types d'ACE
qui affectent la vérification d'accès : ALLOW et DENY. Il est en soi impossible de déduire de toute
entrée de contrôle d'accès (ACE) définissant un groupe d'autorisations si les autorisations qui
n'ont pas été définies dans cette ACE sont ou non accordées.

La conversion entre les ACL NFSv4 et les ACL POSIX-draft s'effectue comme suit :

■

Si vous employez un utilitaire compatible avec les ACL (les commandes cp, mv, tar, cpio ou
rcp, par exemple) pour transférer des fichiers UFS avec des ACL vers un système de fichiers
ZFS, les ACL POSIX-draft sont converties en ACL NFSv4 équivalentes.

■ Les ACL NFSv4 sont converties en ACL POSIX-draft. Un message tel que le suivant s'affiche

si une ACL NFSv4 n'est pas convertie en ACL POSIX-draft :

# cp -p filea /var/tmp

cp: failed to set acl entries on /var/tmp/filea

■

Si vous créez une archive cpio ou tar UFS avec l'option de conservation des ACL (tar -p ou
cpio -P) dans un système exécutant la version actuelle de Solaris, les ACL sont perdues en
cas d'extraction de l'archive sur un système exécutant une version précédente de Solaris.
Tous les fichiers sont extraits avec les modes de fichier corrects, mais les entrées d'ACL sont
ignorées.

■ Vous pouvez utiliser la commande ufsrestore pour restaurer des données dans un système
de fichiers ZFS. Si les données d'origine incluent des ACL POSIX-style, elles sont converties
en ACL NFSv4-style.

■ En cas de tentative de configuration d'une ACL SFSv4 dans un fichier UFS, un message tel

que le suivant s'affiche :

chmod: ERROR: ACL type’s are different

■ En cas de tentative de configuration d'une ACL POSIX dans un fichier ZFS, un message tel

que le suivant s'affiche :

# getfacl filea

File system doesn’t support aclent_t style ACL’s.

See acl(5) for more information on Solaris ACL support.

Pour obtenir des informations sur les autres limitations des ACL et des produits de sauvegarde,
reportez-vous à la section “Enregistrement de données ZFS à l'aide d'autres produits de
sauvegarde” à la page 210.

222

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveau modèle ACL Solaris

Descriptions de syntaxe pour la configuration des ACL
Deux formats d'ACL de base sont fournis comme suit :
Syntaxe pour la configuration d'ACL triviales

chmod [options] A[index]{+|=}owner@ |group@ |everyone@: autorisations
d'accès/...[:indicateurs d'héritage]: deny | allow fichier

chmod [options] A-owner@, group@, everyone@:autorisations d'accès /...[:indicateurs
d'héritage]:deny | allow fichier ...

chmod [options] A[index]- fichier
Syntaxe pour la configuration d'ACL non triviales

chmod [options] A[index]{+|=}user|group:name:autorisations d'accès /...[:indicateurs
d'héritage]:deny | allow fichier

chmod [options] A-user|group:name:autorisations d'accès /...[:indicateurs d'héritage]:deny
| allow fichier ...

chmod [options] A[index]- fichier
owner@, group@, everyone@

Identifie le type d'entrée d'ACL pour la syntaxe d'ACL triviale. Pour obtenir une description
des types d'entrées d'ACL, reportez-vous au Tableau 8–1.

utilisateur ou groupe :ID-entrée-ACL=nomutilisateur ou nomgroupe

Identifie le type d'entrée d'ACL pour la syntaxe d'ACL explicite. Le type d'entrée d'ACL pour
l'utilisateur et le groupe doit également contenir l'ID d'entrée d'ACL, le nom d'utilisateur ou
le nom de groupe. Pour obtenir une description des types d'entrées d'ACL, reportez-vous au
Tableau 8–1.

autorisations-d'accès/.../

Identifie les autorisations d'accès accordées ou refusées. Pour obtenir une description des
privilèges d'accès d'ACL, reportez-vous au Tableau 8–2.

indicateurs-héritage

Identifie une liste optionnelle d'indicateurs d'héritage d'ACL. Pour une description des
indicateurs d'héritage d'ACL, reportez-vous au Tableau 8–4.

deny | allow

Détermine si les autorisations d'accès sont accordées ou refusées.

Dans l'exemple suivant, aucune valeur d'ID d'entrée d'ACL n'existe pour owner@, group@ ou
everyone@.

group@:write_data/append_data/execute:deny

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

223

Nouveau modèle ACL Solaris

L'exemple suivant inclut un ID d'entrée d'ACL car un utilisateur spécifique (type d'entrée d'ACL)
est inclus dans la liste.

0:user:gozer:list_directory/read_data/execute:allow

Lorsqu'une entrée d'ACL s'affiche, elle est similaire à celle-ci :

2:group@:write_data/append_data/execute:deny

La désignation 2 ou ID d'index dans cet exemple identifie l'entrée d'ACL dans la plus grande
ACL, qui peut présenter plusieurs entrées pour le propriétaire, des UID spécifiques, un groupe
et pour tous. Vous pouvez spécifier l'ID d'index avec la commande chmod pour identifier la
partie de l'ACL que vous souhaitez modifier. Par exemple, vous pouvez identifier l'ID d'index 3
par A3 dans la commande chmod comme ci-dessous :

chmod A3=user:venkman:read_acl:allow filename

Les types d'entrées d'ACL (qui sont les représentations d'ACL du propriétaire, du groupe et
autres) sont décrits dans le tableau suivant.

TABLEAU 8–1 Types d'entrées d'ACL

Type d'entrée d'ACL

Description

owner@

group@

everyone@

user

group

Spécifie l'accès accordé au propriétaire de l'objet.

Spécifie l'accès accordé au groupe propriétaire de l'objet.

Spécifie l'accès accordé à tout utilisateur ou groupe ne correspondant à aucune
autre entrée d'ACL.

Avec un nom d'utilisateur, spécifie l'accès accordé à un utilisateur supplémentaire
de l'objet. Doit inclure l'ID d'entrée d'ACL qui contient un nom d'utilisateur ou un
ID utilisateur. Le type d'entrée d'ACL est incorrect si la valeur n'est ni un UID
numérique, ni un nom d'utilisateur.

Avec un nom de groupe, spécifie l'accès accordé à un utilisateur supplémentaire de
l'objet. Doit inclure l'ID d'entrée d'ACL qui contient un nom de groupe ou un ID de
groupe. Le type d'entrée d'ACL est incorrect si la valeur n'est ni un GID numérique,
ni un nom de groupe.

Les privilèges d'accès sont décrits dans le tableau suivant.

TABLEAU 8–2 Privilèges d'accès d'ACL

Privilège d'accès

Privilège d'accès
compact

Description

add_file

w

add_subdirectory p

Autorisation d'ajouter un fichier à un répertoire.

Dans un répertoire, autorisation de créer un sous-répertoire.

224

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

TABLEAU 8–2 Privilèges d'accès d'ACL

(Suite)

Nouveau modèle ACL Solaris

Privilège d'accès
compact

Description

Privilège d'accès

append_data

delete

p

d

delete_child

D

execute

x

Non implémentée actuellement.

Droit de supprimer un fichier. Pour plus d'informations sur le
comportement spécifique de l'autorisation delete, reportez-vous
au Tableau 8–3.

Droit de supprimer un fichier ou un répertoire au sein d'un
répertoire. Pour plus d'informations sur le comportement
spécifique de l'autorisation delete_child , reportez-vous au
Tableau 8–3.

Autorisation d'exécuter un fichier ou d'effectuer une recherche dans
le contenu d'un répertoire.

list_directory

r

Autorisation de dresser la liste du contenu d'un répertoire.

read_acl

read_attributes

read_data

read_xattr

synchronize

write_xattr

c

a

r

R

s

W

write_data

w

write_attributes A

write_acl

C

Autorisation de lire l'ACL (ls).

Autorisation de lire les attributs de base (non ACL) d'un fichier.
Considérez les attributs de base comme les attributs de niveau stat.
L'autorisation de ce bit de masque d'accès signifie que l'entité peut
exécuter ls(1) et stat(2).

Autorisation de lire le contenu du fichier.

Autorisation de lire les attributs étendus d'un fichier ou d'effectuer
une recherche dans le répertoire d'attributs étendus d'un fichier.

Non implémentée actuellement.

Autorisation de créer des attributs étendus ou d'écrire dans le
répertoire d'attributs étendus.
L'attribution de cette autorisation à un utilisateur signifie que ce
dernier peut créer un répertoire d'attributs étendus pour un fichier.
Les autorisations du fichier d'attributs contrôlent l'accès de
l'utilisateur à l'attribut.

Autorisation de modifier ou de remplacer le contenu d'un fichier.

Autorisation de remplacer les durées associées à un fichier ou un
répertoire par une valeur arbitraire.

Autorisation d'écriture sur l'ACL ou capacité de la modifier à l'aide
de la commande chmod.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

225

Nouveau modèle ACL Solaris

TABLEAU 8–2 Privilèges d'accès d'ACL

(Suite)

Privilège d'accès

Privilège d'accès
compact

Description

write_owner

o

Autorisation de modifier le propriétaire ou le groupe d'un fichier.
Ou capacité d'exécuter les commandes chown ou chgrp sur le
fichier.
Autorisation de devenir propriétaire d'un fichier ou autorisation de
définir la propriété de groupe du fichier sur un groupe dont fait
partie l'utilisateur. Le privilège PRIV_FILE_CHOWN est requis pour
définir la propriété de fichier ou de groupe sur un groupe ou un
utilisateur arbitraire.

Le tableau suivant fournit des détails supplémentaires sur les comportements delete et
delete_child d'ACL.

TABLEAU 8–3 Comportement des autorisations delete et delete_child d'ACL.

Droits d'accès au répertoire parent

Autorisations d'objet cible

L'ACL autorise la
suppression

L'ACL refuse la
suppression

L'ACL autorise delete_child Autorisation

Autorisation

L'ACL refuse delete_child

Autorisation

Refus

Autorisation de
suppression non
spécifiée

Autorisation

Refus

L'ACL autorise uniquement
write et execute

Autorisation

Autorisation

Autorisation

L'ACL refuse write et execute Autorisation

Refus

Refus

Jeux d'ACL ZFS
Au lieu de définir séparément des autorisations individuelles, il est possible d'appliquer les
combinaisons d'ACL suivantes par le biais d'un jeu d'ACL. Les jeux d'ACL suivants sont
disponibles :

Nom de jeu d'ACL

full_set

modify_set

read_set

write_set

Autorisations d'ACL incluses

Toutes les autorisations

Toutes les autorisations à l'exception de write_acl et

write_owner

read_data, read_attributes, read_xattr et read_acl

write_data, append_data, write_attributes et

write_xattr

226

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Nouveau modèle ACL Solaris

Ces jeux d'ACL sont prédéfinis et ne peuvent pas être modifiés

Héritage d'ACL
L'héritage d'ACL a pour finalité de permettre à un fichier ou répertoire récemment créé
d'hériter des ACL qui leur sont destinées, tout en tenant compte des bits d'autorisation existants
dans le répertoire parent.

Par défaut, les ACL ne sont pas propagées. Si vous configurez une ACL non triviale dans un
répertoire, aucun répertoire enfant n'en hérite. Vous devez spécifier l'héritage d'une ACL dans
un fichier ou un répertoire.

Les indicateurs d'héritage facultatifs sont décrits dans le tableau suivant.

TABLEAU 8–4

Indicateurs d'héritage d'ACL

Indicateur d'héritage

Indicateur d'héritage
compact

Description

file_inherit

dir_inherit

inherit_only

f

d

i

no_propagate

n

Hérite de l'ACL uniquement à partir du répertoire parent vers
les fichiers du répertoire.

Hérite de l'ACL uniquement à partir du répertoire parent vers
les sous-répertoires du répertoire.

Hérite de l'ACL à partir du répertoire parent mais ne s'applique
qu'aux fichiers et sous-répertoires récemment créés, pas au
répertoire lui-même. Cet indicateur requiert les indicateurs
file_inherit et/ou dir_inherit afin de spécifier ce qui doit
être hérité.

N'hérite que de l'ACL provenant du répertoire parent vers le
contenu de premier niveau du répertoire, et non les contenus
de second niveau et suivants. Cet indicateur requiert les
indicateurs file_inherit et/ou dir_inherit afin de spécifier
ce qui doit être hérité.

-

SO

Aucune autorisation n'est accordée.

Actuellement, les indicateurs suivants s'appliquent uniquement à un client ou serveur SMB.

successful_access

S

failed_access

F

Indique si une alarme ou un enregistrement d'audit doit être
initié lorsqu'un accès réussit. Cet indicateur est utilisé avec les
types d'ACE (entrées de contrôle d'accès) d'audit ou d'alarme.

Indique si une alarme ou un enregistrement d'audit doit être
lancé lorsqu'un accès échoue. Cet indicateur est utilisé avec les
types d'ACE (entrées de contrôle d'accès) d'audit ou d'alarme.

inherited

I

Indique qu'une ACE a été héritée.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

227

Nouveau modèle ACL Solaris

De plus, vous pouvez configurer une stratégie d'héritage d'ACL par défaut plus ou moins stricte
sur le système de fichiers à l'aide de la propriété de système de fichiers aclinherit. Pour de plus
amples informations, consultez la section suivante.

Propriétés ACL
Le système de fichiers ZFS inclut les propriétés d'ACL suivantes permettant de déterminer le
comportement spécifique de l'héritage d'ACL et des interactions d'ACL avec les opérations
chmod.

■

aclinherit : détermine le comportement d'héritage d'ACL. Les valeurs possibles sont les
suivantes :

■

■

■

■

■

discard : pour les nouveaux objets, aucune entrée d'ACL n'est héritée lors de la création
d'un fichier ou d'un répertoire. L'ACL dans le fichier ou le répertoire est égale au mode
d'autorisation du fichier ou répertoire.
noallow : pour les nouveaux objets, seules les entrées d'ACL héritables dont le type
d'accès est deny sont héritées.
restricted : pour les nouveaux objets, les autorisations write_owner et write_acl sont
supprimées lorsqu'une entrée d'ACL est héritée.
passthrough : lorsqu'une valeur de propriété est définie sur passthrough, les fichiers
sont créés dans un mode déterminé par les ACE héritées. Si aucune ACE pouvant être
héritée n'affecte le mode, ce mode est alors défini en fonction du mode demandé à partir
de l'application.
passthrough-x : a la même sémantique que passthrough, si ce n'est que lorsque
passthrough-x est activé, les fichiers sont créés avec l'autorisation d'exécution (x), mais
uniquement si l'autorisation d'exécution est définie en mode de création de fichier et
dans une entrée de contrôle d'accès (ACE) pouvant être héritée et qui affecte le mode.

■

Le mode par défaut de aclinherit est restricted.
aclmode : modifie le comportement des ACL lorsqu'un fichier est créé et contrôle la
modification des ACL au cours d'une opération chmod. Les valeurs possibles sont les
suivantes :

■

■

discard :un système de fichiers dont la valeur de la propriété aclmode est discard
supprime toutes les entrées d'ACL qui ne représentent pas le mode du fichier. Il s'agit de
la valeur par défaut.
mask : un système de fichiers dont la valeur de la propriété aclmode est mask restreint les
autorisations utilisateur ou groupe. Les autorisations sont réduites de manière à ne pas
excéder les bits d'autorisation du groupe, à moins qu'il ne s'agisse d'une entrée utilisateur
possédant le même UID que le propriétaire du fichier ou du répertoire. Dans ce cas, les
autorisations d'ACL sont réduites de manière à ne pas excéder les bits d'autorisation du

228

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration d'ACL dans des fichiers ZFS

propriétaire. La valeur de masque préserve en outre l'ACL lors des modifications de
mode successives, à condition qu'aucune opération de jeu d'ACL explicite n'ait été
effectuée.
passthrough : un système de fichiers avec une propriété aclmode de passthrough
indique qu'aucune modification n'est apportée à l'ACL en dehors de la génération des
entrées d'ACL nécessaires pour représenter le nouveau mode du fichier ou du répertoire.

■

Le mode par défaut pour aclmode est discard.

Pour plus d'informations sur l'utilisation de la propriété aclmode, reportez-vous à
l'Exemple 8–14.

Configuration d'ACL dans des fichiers ZFS

Dans la mesure où elles sont implémentées avec ZFS, les ACL se composent d'un tableau
d'entrées d'ACL. ZFS fournit un modèle d'ACL pur, dans lequel tous les fichiers présentent une
ACL. En règle générale, cette liste est triviale dans la mesure où elle ne représente que les entrées
propriétaire/groupe/autre UNIX classiques.

Les fichiers ZFS disposent toujours de bits d'autorisation et d'un mode, mais ces valeurs
constituent plutôt un cache de ce que représente une ACL. Par conséquent, si vous modifiez les
autorisations du fichier, son ACL est mise à jour en conséquence. En outre, si vous supprimez
une ACL non triviale qui accordait à un utilisateur l'accès à un fichier ou à un répertoire, il est
possible que cet utilisateur y ait toujours accès en raison des bits d'autorisation qui accordent
l'accès à un groupe ou à tous les utilisateurs. L'ensemble des décisions de contrôle d'accès est
régi par les autorisations représentées dans l'ACL d'un fichier ou d'un répertoire.

Les règles principales d'accès aux ACL dans un fichier ZFS sont comme suit :
■ ZFS traite les entrées d'ACL dans l'ordre dans lesquelles elles sont répertoriées dans l'ACL,

en partant du haut.
Seules les entrées d'ACL disposant d'un " who " correspondant au demandeur d'accès sont
traitées.

■

■ Une fois l'autorisation allow accordée, cette dernière ne peut plus être refusée par la suite par

une entrée d'ACL de refus dans le même jeu de d'autorisations d'ACL.

■ Le propriétaire du fichier dispose de l'autorisation write_acl de façon inconditionnelle,
même si celle-ci est explicitement refusée. Dans le cas contraire, toute autorisation non
spécifiée est refusée.
Dans les cas d'autorisations deny ou lorsqu'une autorisation d'accès est manquante, le
sous-système de privilèges détermine la demande d'accès accordée pour le propriétaire du
fichier ou pour le superutilisateur. Ce mécanisme évite que les propriétaires de fichiers
puissent accéder à leurs fichiers et permet aux superutilisateurs de modifier les fichiers à des
fins de récupération.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

229

Configuration d'ACL dans des fichiers ZFS

Si vous configurez une ACL non triviale dans un répertoire, les enfants du répertoire n'en
héritent pas automatiquement. Si vous configurez une ACL non triviale, et souhaitez qu'elle soit
héritée par les enfants du répertoire, vous devez utiliser les indicateurs d'héritage d'ACL. Pour
plus d'informations, reportez-vous au Tableau 8–4 et à la section “Configuration d'héritage
d'ACL dans des fichiers ZFS en format détaillé” à la page 236.

Lorsque vous créez un fichier, en fonction de la valeur umask, une ACL triviale par défaut,
similaire à la suivante, est appliquée :

$ ls -v file.1

-rw-r--r--

1 root

root

206663 Jun 23 15:06 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Chaque catégorie d'utilisateur (owner@, group@, everyone@) dispose d'une entrée d'ACL dans
cet exemple.

Voici une description de l'ACL de ce fichier :

0:owner@

1:group@

2:everyone@

Le propriétaire peut lire et modifier le contenu du fichier
(read_data/write_data/append_data). Il peut également modifier les
attributs du fichier tels que les horodatages, les attributs étendus et les ACL
(write_xattr/write_attributes /write_acl). Le propriétaire peut
également modifier la propriété du fichier (write_owner:allow).

L'autorisation d'accès synchronize n'est actuellement pas implémentée.
Les autorisations de lecture du fichier et de ses attributs sont attribuées au
groupe (read_data/read_xattr/read_attributes/read_acl:allow).
Les autorisations de lecture du fichier et de ses attributs sont attribués à toute
personne ne correspondant ni à un utilisateur ni à un groupe
(read_data/read_xattr/read_attributes/read_acl/
synchronize:allow ). L'autorisation d'accès synchronize n'est actuellement
pas implémentée.

Lorsqu'un répertoire est créé, en fonction de la valeur umask, l'ACL par défaut du répertoire est
similaire à l'exemple suivant :

$ ls -dv dir.1

drwxr-xr-x

2 root

root

2 Jul 20 13:44 dir.1

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

230

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

2:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Voici une description de l'ACL de ce répertoire :

0:owner@

1:group@

2:everyone@

Le propriétaire peut lire et modifier le contenu du répertoire
(list_directory/read_data/add_file/write_data/add_subdirectory
/append_data ) et lire et modifier les attributs du fichier tels que les
horodatages, les attributs étendus et les ACL
(/read_xattr/write_xattr/read_attributes/write_attributes/read_acl/
write_acl ). En outre, le propriétaire peut faire des recherches dans le
contenu (execute), supprimer un fichier ou un répertoire (delete_child) et
modifier la possession du répertoire (write_owner:allow).

L'autorisation d'accès synchronize n'est actuellement pas implémentée.
Le groupe peut répertorier et lire le contenu et les attributs du répertoire. De
plus, le groupe dispose d'autorisations d'exécution pour effectuer des
recherches dans le contenu du répertoire
(list_directory/read_data/read_xattr/execute/read_attributes
/read_acl/synchronize:allow).
Toute personne n'étant ni un utilisateur ni un groupe dispose d'autorisations
de lecture et d'exécution sur le contenu et les attributs du répertoire
(list_directory/read_data/read_xattr/execute/read_
attributes/read_acl/synchronize:allow ). L'autorisation d'accès
synchronize n'est actuellement pas implémentée.

Configuration et affichage d'ACL dans des fichiers ZFS en
format détaillé

Vous pouvez modifier les ACL dans des fichiers ZFS à l'aide de la commande chmod. La syntaxe
chmod suivante pour la modification de l'ACL utilise la spécification acl pour identifier le format
de la liste. Pour une description de la spécification ACL, reportez-vous à la section “Descriptions
de syntaxe pour la configuration des ACL” à la page 223.
■ Ajout d'entrées d'ACL

■ Ajout d'une entrée d'ACL pour un utilisateur

% chmod A+acl-specification filename

■ Ajout d'une entrée d'ACL par ID d'index

% chmod Aindex-ID+acl-specification filename

Cette syntaxe insère la nouvelle entrée d'ACL à l'emplacement d'ID d'index spécifié.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

231

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

■ Remplacement d'une entrée d'ACL

% chmod A=acl-specification filename

% chmod Aindex-ID=acl-specification filename

■

Suppression d'entrées d'ACL

■

■

■

Suppression d'une entrée d'ACL par l'ID d'index

% chmod Aindex-ID- filename

Suppression d'une entrée d'ACL par utilisateur

% chmod A-acl-specification filename

Suppression de la totalité des ACE non triviales d'un fichier

% chmod A- filename

Les informations détaillées de l'ACL s'affichent à l'aide de la commande ls - v. Par exemple :

# ls -v file.1

-rw-r--r--

1 root

root

206695 Jul 20 13:43 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Pour obtenir des informations sur l'utilisation du format d'ACL compact, consultez
“Configuration et affichage d'ACL dans des fichiers ZFS en format compact” à la page 242.

EXEMPLE 8–1 Modification des ACL triviales dans des fichiers ZFS
Cette section fournit des exemples de configuration et d'affichage d'ACL triviales.

Dans l'exemple suivant, une ACL triviale existe dans le fichier file.1 :

# ls -v file.1

-rw-r--r--

1 root

root

206695 Jul 20 13:43 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivants, les autorisations write_data sont accordées au groupe group@.

# chmod A1=group@:read_data/write_data:allow file.1

# ls -v file.1

-rw-rw-r--

1 root

root

206695 Jul 20 13:43 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

232

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–1 Modification des ACL triviales dans des fichiers ZFS

(Suite)

1:group@:read_data/write_data:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivant, les autorisations du fichier file.1 sont reconfigurées sur 644.

# chmod 644 file.1

# ls -v file.1

-rw-r--r--

1 root

root

206695 Jul 20 13:43 file.1

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

EXEMPLE 8–2 Configuration d'ACL non triviales dans des fichiers ZFS
Cette section fournit des exemples de configuration et d'affichage d'ACL non triviales.

Dans l'exemple suivant, les autorisations read_data/execute sont ajoutées à l'utilisateur gozer
dans le répertoire test.dir.

# chmod A+user:gozer:read_data/execute:allow test.dir

# ls -dv test.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:23 test.dir

0:user:gozer:list_directory/read_data/execute:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les autorisations read_data/execute sont retirées à l'utilisateur gozer.

# chmod A0- test.dir

# ls -dv test.dir

drwxr-xr-x

2 root

root

2 Jul 20 14:23 test.dir

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

2:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

233

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

Interactions entre les ACL et les autorisations dans les fichiers ZFS

EXEMPLE 8–3
Les exemples d'ACL suivants illustrent les interactions entre la configuration des ACL et la
modification successive des bits d'autorisation du répertoire ou du fichier.

Dans l'exemple suivant, une ACL triviale existe dans le fichier file.2:

# ls -v file.2

-rw-r--r--

1 root

root

2693 Jul 20 14:26 file.2

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

2:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivant, les autorisations d'ACL allow sont supprimées de everyone@.

# chmod A2- file.2

# ls -v file.2

-rw-r-----

1 root

root

2693 Jul 20 14:26 file.2

0:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

Dans cette sortie, les bits d'autorisation du fichier sont réinitialisés de 644 à 640. Les
autorisations de lecture de everyone@ ont été supprimées des bits d'autorisation du fichier
lorsque les autorisations "allow" des ACL ont été supprimées de everyone@.

Dans l'exemple suivant, l'ACL existante est remplacée par des autorisations
read_data/write_data pour everyone@.

# chmod A=everyone@:read_data/write_data:allow file.3

# ls -v file.3

-rw-rw-rw-

1 root

root

2440 Jul 20 14:28 file.3

0:everyone@:read_data/write_data:allow

Dans cette sortie, la syntaxe chmod remplace effectivement l'ACL existante par les autorisations
read_data/write_data:allow pour les autorisations de lecture/écriture pour le propriétaire, le
groupe et everyone@. Dans ce modèle, everyone@ spécifie l'accès à tout utilisateur ou groupe.
Dans la mesure où aucune entrée d'ACL owner@ ou group@ n'existe pour ignorer les
autorisations pour l'utilisateur ou le groupe, les bits d'autorisation sont définis sur 666.

Dans l'exemple suivant, l'ACL existante est remplacée par des autorisations de lecture pour
l'utilisateur gozer.

# chmod A=user:gozer:read_data:allow file.3

# ls -v file.3

----------+ 1 root

root

2440 Jul 20 14:28 file.3

0:user:gozer:read_data:allow

234

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–3

Interactions entre les ACL et les autorisations dans les fichiers ZFS

(Suite)

Dans cette sortie, les autorisations de fichier sont calculées pour être 000 car aucune entrée
d'ACL n'existe pour owner@, group@, ou everyone@, qui représentent les composant
d'autorisation classiques d'un fichier. Le propriétaire du fichier peut résoudre ce problème en
réinitialisant les autorisations (et l'ACL) comme suit :

# chmod 655 file.3

# ls -v file.3

-rw-r-xr-x

1 root

root

2440 Jul 20 14:28 file.3

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/execute/read_attributes/read_acl

/synchronize:allow

3:everyone@:read_data/read_xattr/execute/read_attributes/read_acl

/synchronize:allow

EXEMPLE 8–4 Restauration des ACL triviales dans des fichiers ZFS
Vous pouvez utiliser la commande chmod pour supprimer toutes les ACL non triviales d'un
fichier ou d'un répertoire.

Dans l'exemple suivant, deux ACE non triviales existent dans test5.dir.

# ls -dv test5.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:32 test5.dir

0:user:lp:read_data:file_inherit:deny

1:user:gozer:read_data:file_inherit:deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

3:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

4:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les ACL non triviales pour les utilisateurs gozer et lp sont supprimées.
L'ACL restante contient les valeurs par défaut de owner@, group@ et everyone@.

# chmod A- test5.dir

# ls -dv test5.dir

drwxr-xr-x

2 root

root

2 Jul 20 14:32 test5.dir

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

2:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

235

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–5 Application d'un jeu d'ACL à des fichiers ZFS
Des jeux d'ACL sont fournis pour vous éviter d'avoir à appliquer séparément les autorisations
d'ACL. Pour une description des jeux d'ACL, reportez-vous à la section “Jeux d'ACL ZFS”
à la page 226.

Vous pouvez par exemple appliquer le jeu read_set comme suit :

# chmod A+user:otto:read_set:allow file.1

# ls -v file.1

-r--r--r--+ 1 root

root

206695 Jul 20 13:43 file.1

0:user:otto:read_data/read_xattr/read_attributes/read_acl:allow

1:owner@:read_data/read_xattr/write_xattr/read_attributes

/write_attributes/read_acl/write_acl/write_owner/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Vous pouvez appliquer les jeux write_set et read_set comme suit :

# chmod A+user:otto:read_set/write_set:allow file.2

# ls -v file.2

-rw-r--r--+ 1 root

root

2693 Jul 20 14:26 file.2

0:user:otto:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl:allow

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Configuration d'héritage d'ACL dans des fichiers ZFS
en format détaillé
Vous pouvez déterminer comment les ACL sont héritées ou non dans les fichiers et répertoires.
Par défaut, les ACL ne sont pas propagées. Si vous configurez une ACL non triviale dans un
répertoire, aucun répertoire subséquent n'en hérite. Vous devez spécifier l'héritage d'une ACL
dans un fichier ou un répertoire.

La propriété aclinherit peut être définie de manière globale pour un système de fichiers. Par
défaut, aclinherit est défini sur restricted.

Pour plus d'informations, reportez-vous à la section “Héritage d'ACL” à la page 227.

EXEMPLE 8–6 Attribution d'héritage d'ACL par défaut
Par défaut, les ACL ne sont pas propagées par le biais d'une structure de répertoire.

236

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–6 Attribution d'héritage d'ACL par défaut

(Suite)

Dans l'exemple suivant, une ACE non triviale de read_data/write_data/execute est
appliquée pour l'utilisateur gozer dans le fichier test.dir.

# chmod A+user:gozer:read_data/write_data/execute:allow test.dir

# ls -dv test.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:53 test.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Si un sous-répertoire test.dir est créé, l'ACE pour l'utilisateur gozer n'est pas propagée.
L'utilisateur gozer n'aurait accès à sub.dir que si les autorisations de sub.dir lui accordaient
un accès en tant que propriétaire de fichier, membre de groupe ou everyone@.

# mkdir test.dir/sub.dir

# ls -dv test.dir/sub.dir

drwxr-xr-x

2 root

root

2 Jul 20 14:54 test.dir/sub.dir

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

2:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

EXEMPLE 8–7 Attribution d'héritage d'ACL dans les fichiers et les répertoires
Cette série d'exemples identifie les ACE du fichier et du répertoire qui sont appliquées lorsque
l'indicateur file_inherit est paramétré.

Dans cet exemple, les autorisations read_data/write_data sont ajoutées pour les fichiers dans
le répertoire test2.dir pour l'utilisateur gozer afin qu'il dispose de l'accès en lecture à tout
nouveau fichier :

# chmod A+user:gozer:read_data/write_data:file_inherit:allow test2.dir

# ls -dv test2.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:55 test2.dir

0:user:gozer:read_data/write_data:file_inherit:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

237

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–7 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les autorisations de l'utilisateur gozer sont appliquées au fichier
test2.dir/file.2 récemment créé. L'héritage d'ACL étant accordé
(read_data:file_inherit:allow), l'utilisateur gozer peut lire le contenu de tout nouveau
fichier.

# touch test2.dir/file.2

# ls -v test2.dir/file.2

-rw-r--r--+ 1 root

root

0 Jul 20 14:56 test2.dir/file.2

0:user:gozer:read_data:inherited:allow

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans la mesure où la propriété aclinherit pour ce système de fichiers est paramétrée sur le
mode par défaut, restricted, l'utilisateur gozer ne dispose pas de l'autorisation write_data
pour le fichier file.2 car l'autorisation de groupe du fichier ne le permet pas.

Notez que l'autorisation inherit_only appliquée lorsque les indicateurs file_inherit ou
dir_inherit sont définis, est utilisée pour propager l'ACL dans la structure du répertoire.
Ainsi, l'utilisateur gozer se voit uniquement accorder ou refuser l'autorisation des autorisations
everyone@, à moins qu'il ne soit le propriétaire du fichier ou membre du groupe propriétaire du
fichier. Par exemple :

# mkdir test2.dir/subdir.2

# ls -dv test2.dir/subdir.2

drwxr-xr-x+ 2 root

root

2 Jul 20 14:57 test2.dir/subdir.2

0:user:gozer:list_directory/read_data/add_file/write_data:file_inherit

/inherit_only/inherited:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

La série d'exemples suivants identifie les ACL du fichier et du répertoire appliquées lorsque les
indicateurs file_inherit et dir_inherit sont paramétrés.

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les droits de lecture, d'écriture et
d'exécution hérités des fichiers et répertoires récemment créés.

238

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–7 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

# chmod A+user:gozer:read_data/write_data/execute:file_inherit/dir_inherit:allow

test3.dir

# ls -dv test3.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 15:00 test3.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:file_inherit/dir_inherit:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Le texte inherited de la sortie ci-dessous est un message d'information qui indique que l'ACE
est héritée.

# touch test3.dir/file.3

# ls -v test3.dir/file.3

-rw-r--r--+ 1 root

root

0 Jul 20 15:01 test3.dir/file.3

0:user:gozer:read_data:inherited:allow

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans ces exemples, les bits d'autorisation du répertoire parent pour group@ et everyone@
n'accordent pas les autorisations. Par conséquent, l'utilisateur gozer se voit refuser ces
autorisations. La propriété par défaut de aclinherit est restricted, ce qui signifie que les
autorisations write_data et execute ne sont pas héritées.

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les autorisations de lecture,
d'écriture et d'exécution qui sont héritées pour les fichiers récemment créés, mais ne sont pas
propagées vers tout contenu subséquent du répertoire.

# chmod A+user:gozer:read_data/write_data/execute:file_inherit/no_propagate:allow

test4.dir

# ls -dv test4.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 15:05 test4.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:file_inherit/no_propagate:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

239

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–7 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'exemple suivant l'illustre, lors de la création d'un sous-répertoire, l'autorisation
read_data/write_data/execute de l'utilisateur gozer pour les fichiers n'est pas propagée au
nouveau répertoire sub4.dir.

# mkdir test4.dir/sub4.dir

# ls -dv test4.dir/sub4.dir

drwxr-xr-x

2 root

root

2 Jul 20 15:06 test4.dir/sub4.dir

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

2:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'exemple suivant l'illustre, les autorisations read_data/write_data/execute de
l'utilisateur gozer sont réduites en fonction des autorisations du groupe propriétaire.

# touch test4.dir/file.4

# ls -v test4.dir/file.4

-rw-r--r--+ 1 root

root

0 Jul 20 15:09 test4.dir/file.4

0:user:gozer:read_data:inherited:allow

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

EXEMPLE 8–8 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through
Si la propriété aclinherit du système de fichiers tank/cindy est définie sur passthrough,
l'utilisateur gozer hérite de l'ACL appliquée à test4.dir pour le nouveau fichier file.5 de la
manière suivante :

# zfs set aclinherit=passthrough tank/cindy

# touch test4.dir/file.5

# ls -v test4.dir/file.5

-rw-r--r--+ 1 root

root

0 Jul 20 14:16 test4.dir/file.5

0:user:gozer:read_data/write_data/execute:inherited:allow

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

240

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–9 Héritage d'ACL avec mode d'héritage ACL défini sur Discard
Si la propriété aclinherit d'un système de fichiers est définie sur discard, il est alors possible
de supprimer les ACL avec les bits d'autorisation lors d'un changement de répertoire. Par
exemple :

# zfs set aclinherit=discard tank/cindy

# chmod A+user:gozer:read_data/write_data/execute:dir_inherit:allow test5.dir

# ls -dv test5.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:18 test5.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:dir_inherit:allow

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

3:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Si vous décidez ultérieurement de renforcer les bits d'autorisation d'un répertoire, l'ACL non
triviale est supprimée. Par exemple :

# chmod 744 test5.dir

# ls -dv test5.dir

drwxr--r--

2 root

root

2 Jul 20 14:18 test5.dir

0:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

1:group@:list_directory/read_data/read_xattr/read_attributes/read_acl

/synchronize:allow

2:everyone@:list_directory/read_data/read_xattr/read_attributes/read_acl

/synchronize:allow

EXEMPLE 8–10 Héritage d'ACL avec mode d'héritage de liste défini sur Noallow
Dans l'exemple suivant, deux ACL non triviales avec héritage de fichier sont définies. Une ACL
accorde l'autorisation read_data, tandis qu'une autre refuse cette autorisation. Cet exemple
illustre également comment spécifier deux ACE dans la même commande chmod.

# zfs set aclinherit=noallow tank/cindy

# chmod A+user:gozer:read_data:file_inherit:deny,user:lp:read_data:file_inherit:allow

test6.dir

# ls -dv test6.dir

drwxr-xr-x+ 2 root

root

2 Jul 20 14:22 test6.dir

0:user:gozer:read_data:file_inherit:deny

1:user:lp:read_data:file_inherit:allow

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/read_xattr/write_xattr/execute/delete_child

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

3:group@:list_directory/read_data/read_xattr/execute/read_attributes

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

241

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–10 Héritage d'ACL avec mode d'héritage de liste défini sur Noallow

(Suite)

/read_acl/synchronize:allow

4:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'illustre l'exemple suivant, lors de la création d'un nouveau fichier, l'ACL qui accorde
l'autorisation read_data est supprimée.

# touch test6.dir/file.6

# ls -v test6.dir/file.6

-rw-r--r--+ 1 root

root

0 Jul 20 14:23 test6.dir/file.6

0:user:gozer:read_data:inherited:deny

1:owner@:read_data/write_data/append_data/read_xattr/write_xattr

/read_attributes/write_attributes/read_acl/write_acl/write_owner

/synchronize:allow

2:group@:read_data/read_xattr/read_attributes/read_acl/synchronize:allow

3:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Configuration et affichage d'ACL dans des fichiers ZFS en
format compact

Vous pouvez définir et afficher les autorisations relatives aux fichiers ZFS en format compact
utilisant 14 lettres uniques pour représenter les autorisations. Les lettres représentant les
autorisations compactes sont répertoriées dans le Tableau 8–2 et le Tableau 8–4.

Vous pouvez afficher les listes d'ACL compactes pour les fichiers et les répertoires à l'aide de la
commande ls -V. Par exemple :

# ls -V file.1

-rw-r--r--

1 root

root

206695 Jul 20 14:27 file.1

owner@:rw-p--aARWcCos:-------:allow

group@:r-----a-R-c--s:-------:allow

everyone@:r-----a-R-c--s:-------:allow

La sortie d'ACL compacte est décrite comme suit :

owner@

Le propriétaire peut lire et modifier le contenu du fichier (
rw=read_data/write_data), (p= append_data). Le propriétaire peut
également modifier les attributs du fichier tels que l'horodatage, les attributs
étendus et les listes de contrôle d'accès (ACL) (a=read_attributes ,
A=write_xattr, R= read_xattr, W=write_attributes, c=read_acl,
C=write_acl). De plus, le propriétaire peut modifier la propriété du fichier
(o=write_owner).

L'autorisation d'accès synchronize (s) n'est pas implémentée pour le moment.

242

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

group@

Les autorisations de lecture sur le fichier sont accordées au groupe sur le fichier
(r= read_data) et les attributs du fichier (a=read_attributes ,
R=read_xattr, c= read_acl).

everyone@

L'autorisation d'accès synchronize (s) n'est pas implémentée pour le moment.
Les autorisations de lecture sur le fichier et sur ses attributs sont accordés à toute
personne n'étant ni un utilisateur ni un groupe (r=read_data, a=append_data,
R=read_xattr , c=read_acl et s= synchronize).

L'autorisation d'accès synchronize (s) n'est pas implémentée pour le moment.

Le format d'ACL compact dispose des avantages suivants par rapport au format d'ACL détaillé :
■ Les autorisations peuvent être spécifiées en tant qu'arguments de position pour la

commande chmod.

■ Les tirets (-), qui n'identifient aucune autorisation, peuvent être supprimés. Seules les lettres

nécessaires doivent être spécifiées.

■ Les indicateurs d'autorisations et d'héritage sont configurés de la même manière.

Pour obtenir des informations sur l'utilisation du format d'ACL détaillé, consultez
“Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé” à la page 231.

EXEMPLE 8–11 Configuration et affichage des ACL en format compact
Dans l'exemple suivant, une ACL triviale existe dans le fichier file.1 :

# ls -V file.1

-rw-r--r--

1 root

root

206695 Jul 20 14:27 file.1

owner@:rw-p--aARWcCos:-------:allow

group@:r-----a-R-c--s:-------:allow

everyone@:r-----a-R-c--s:-------:allow

Dans cet exemple, les autorisations read_data/execute sont ajoutées à l'utilisateur gozer sur le
fichier file.1.

# chmod A+user:gozer:rx:allow file.1

# ls -V file.1

-rw-r--r--+ 1 root

root

206695 Jul 20 14:27 file.1

user:gozer:r-x-----------:-------:allow

owner@:rw-p--aARWcCos:-------:allow

group@:r-----a-R-c--s:-------:allow

everyone@:r-----a-R-c--s:-------:allow

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les droits de lecture, d'écriture et
d'exécution qui sont hérités des fichiers et répertoires récemment créés grâce à l'utilisation de
l'ACL compacte.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

243

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–11 Configuration et affichage des ACL en format compact

(Suite)

# chmod A+user:gozer:rwx:fd:allow dir.2

# ls -dV dir.2

drwxr-xr-x+ 2 root

root

2 Jul 20 14:33 dir.2

user:gozer:rwx-----------:fd-----:allow

owner@:rwxp-DaARWcCos:-------:allow

group@:r-x---a-R-c--s:-------:allow

everyone@:r-x---a-R-c--s:-------:allow

Vous pouvez également couper et coller les autorisations et les indicateurs d'héritage à partir de
la sortie ls -V en format chmod compact. Par exemple, pour dupliquer les autorisations et les
indicateurs d'héritage sur dir.2 pour l'utilisateur gozer vers l'utilisateur cindy sur dir.2,
copiez et collez l'autorisation et les indicateurs d'héritage (rwx-----------:fd-----:allow )
dans votre commande chmod. Par exemple :

# chmod A+user:cindy:rwx-----------:fd-----:allow dir.2

# ls -dV dir.2

drwxr-xr-x+ 2 root

root

2 Jul 20 14:33 dir.2

user:cindy:rwx-----------:fd-----:allow

user:gozer:rwx-----------:fd-----:allow

owner@:rwxp-DaARWcCos:-------:allow

group@:r-x---a-R-c--s:-------:allow

everyone@:r-x---a-R-c--s:-------:allow

EXEMPLE 8–12 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through
Un système de fichiers dont la propriété aclinherit est définie sur passthrough hérite de
toutes les entrées d'ACL pouvant être héritées, sans qu'aucune modification ne leur soit
apportée. Lorsque cette propriété est définie sur passthrough, les fichiers sont créés avec un
mode d'autorisation déterminé par les ACE pouvant être héritées. Si aucune ACE pouvant être
héritée n'affecte le mode d'autorisation, ce mode est alors défini en fonction du mode demandé
à partir de l'application.

Les exemples suivants respectent la syntaxe ACL compacte pour illustrer le processus d'héritage
des bits d'autorisation en définissant le mode aclinherit sur la valeur passthrough .

Dans cet exemple, une ACL est définie sur test1.dir pour forcer l'héritage. La syntaxe crée une
entrée d'ACL owner@, group@ et everyone@ pour les fichiers nouvellement créés. Les
répertoires nouvellement créés héritent d'une entrée d'ACL @owner, group@ et everyone@.

# zfs set aclinherit=passthrough tank/cindy

# pwd

/tank/cindy

# mkdir test1.dir

# chmod A=owner@:rwxpcCosRrWaAdD:fd:allow,group@:rwxp:fd:allow,

everyone@::fd:allow test1.dir

# ls -Vd test1.dir

drwxrwx---+ 2 root

root

2 Jul 20 14:42 test1.dir

244

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–12 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through

(Suite)

owner@:rwxpdDaARWcCos:fd-----:allow

group@:rwxp----------:fd-----:allow

everyone@:--------------:fd-----:allow

Dans cet exemple, un fichier nouvellement créé hérite de l'ACL dont les fichiers nouvellement
créés doivent hériter d'après ce qui a été spécifié.

# cd test1.dir

# touch file.1

# ls -V file.1

-rwxrwx---+ 1 root

root

0 Jul 20 14:44 file.1

owner@:rwxpdDaARWcCos:------I:allow

group@:rwxp----------:------I:allow

everyone@:--------------:------I:allow

Dans cet exemple, un répertoire nouvellement créé hérite à la fois des ACE contrôlant l'accès à
ce répertoire et des ACE à appliquer ultérieurement aux enfants de ce répertoire.

# mkdir subdir.1

# ls -dV subdir.1

drwxrwx---+ 2 root

root

2 Jul 20 14:45 subdir.1

owner@:rwxpdDaARWcCos:fd----I:allow

group@:rwxp----------:fd----I:allow

everyone@:--------------:fd----I:allow

Les entrées fd----I servent à propager l'héritage et ne sont pas prises en compte durant le
contrôle d'accès.

Dans l'exemple suivant, un fichier est créé à l'aide d'une ACL triviale dans un autre répertoire où
les ACE héritées ne sont pas présentes.

# cd /tank/cindy

# mkdir test2.dir

# cd test2.dir

# touch file.2

# ls -V file.2

-rw-r--r--

1 root

root

0 Jul 20 14:48 file.2

owner@:rw-p--aARWcCos:-------:allow

group@:r-----a-R-c--s:-------:allow

everyone@:r-----a-R-c--s:-------:allow

EXEMPLE 8–13 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through-X
Lorsque aclinherit=passthrough-x est activé, les fichiers sont créés avec l'autorisation
d'exécution (x) pour propriétaire@, groupe@ ou tous les utilisateurs@, mais seulement si
l'autorisation d'exécution est définie dans le mode de création de fichier et dans une ACE
héritable qui affecte le mode.

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

245

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–13 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through-X

(Suite)

L'exemple suivant montre comment hériter l'autorisation d'exécution en définissant le mode
aclinherit sur passthrough-x.

# zfs set aclinherit=passthrough-x tank/cindy

L'ACL suivante est définie sur /tank/cindy/test1.dir pour permettre l'héritage des ACL
exécutables pour les fichiers de owner@.

# chmod A=owner@:rwxpcCosRrWaAdD:fd:allow,group@:rwxp:fd:allow,

everyone@::fd:allow test1.dir

# ls -Vd test1.dir

drwxrwx---+ 2 root

root

2 Jul 20 14:50 test1.dir

owner@:rwxpdDaARWcCos:fd-----:allow

group@:rwxp----------:fd-----:allow

everyone@:--------------:fd-----:allow

Un fichier (file1) est créé avec les autorisations demandées 0666. Les autorisations obtenues
sont 0660. L'autorisation d'exécution n'était pas héritée car le mode de création ne le requérait
pas.

# touch test1.dir/file1

# ls -V test1.dir/file1

-rw-rw----+ 1 root

root

0 Jul 20 14:52 test1.dir/file1

owner@:rw-pdDaARWcCos:------I:allow

group@:rw-p----------:------I:allow

everyone@:--------------:------I:allow

Ensuite, un fichier exécutable appelé t est généré à l'aide du compilateur cc dans le répertoire
testdir.

# cc -o t t.c

# ls -V t

-rwxrwx---+ 1 root

root

7396 Dec 3 15:19 t

owner@:rwxpdDaARWcCos:------I:allow

group@:rwxp----------:------I:allow

everyone@:--------------:------I:allow

Les autorisations obtenues sont 0770 car cc a demandé des autorisations 0777, ce qui a entraîné
l'héritage de l'autorisation d'exécution à partir des entrées propriétaire@, groupe@ et tous les
utilisateurs@.

Interactions entre les ACL et les opérations chmod sur les fichiers ZFS

EXEMPLE 8–14
Les exemples suivants illustrent l'incidence de certaines valeurs des propriétés aclmode et
aclinherit sur l'interaction des ACL existantes avec une opération chmod modifiant les
autorisations de répertoire ou de fichier en vue de restreindre ou d'augmenter les autorisations
d'ACL existantes à des fins de conformité avec le groupe propriétaire.

246

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–14

Interactions entre les ACL et les opérations chmod sur les fichiers ZFS

(Suite)

Dans cet exemple, la propriété aclmode est définie sur mask et la propriété aclinherit sur
restricted. Les autorisations d'ACL sont affichées en mode compact dans cet exemple, ce qui
permet de mieux repérer les modifications apportées aux autorisations.

Paramètres de propriété du fichier et des groupes et autorisations d'ACL initiaux :

# zfs set aclmode=mask pond/whoville

# zfs set aclinherit=restricted pond/whoville

# ls -lV file.1

-rwxrwx---+ 1 root

root

206695 Aug 30 16:03 file.1

user:amy:r-----a-R-c---:-------:allow

user:rory:r-----a-R-c---:-------:allow

group:sysadmin:rw-p--aARWc---:-------:allow

group:staff:rw-p--aARWc---:-------:allow

owner@:rwxp--aARWcCos:-------:allow

group@:rwxp--aARWc--s:-------:allow

everyone@:------a-R-c--s:-------:allow

Une opération chown modifie la propriété du fichier file.1 et la sortie est visible par
l'utilisateur propriétaire, amy. Par exemple :

# chown amy:staff file.1

# su - amy

$ ls -lV file.1

-rwxrwx---+ 1 amy

staff

206695 Aug 30 16:03 file.1

user:amy:r-----a-R-c---:-------:allow

user:rory:r-----a-R-c---:-------:allow

group:sysadmin:rw-p--aARWc---:-------:allow

group:staff:rw-p--aARWc---:-------:allow

owner@:rwxp--aARWcCos:-------:allow

group@:rwxp--aARWc--s:-------:allow

everyone@:------a-R-c--s:-------:allow

Les opérations chmod suivantes font passer les autorisations à un mode plus restrictif. Dans cet
exemple, les autorisations d'ACL modifiées du groupe sysadmin et du groupe staff n'excèdent
pas les autorisations du groupe propriétaire.

$ chmod 640 file.1

$ ls -lV file.1

-rw-r-----+ 1 amy

staff

206695 Aug 30 16:03 file.1

user:amy:r-----a-R-c---:-------:allow

user:rory:r-----a-R-c---:-------:allow

group:sysadmin:r-----a-R-c---:-------:allow

group:staff:r-----a-R-c---:-------:allow

owner@:rw-p--aARWcCos:-------:allow

group@:r-----a-R-c--s:-------:allow

everyone@:------a-R-c--s:-------:allow

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

247

Application d'attributs spéciaux aux fichiers ZFS

EXEMPLE 8–14

Interactions entre les ACL et les opérations chmod sur les fichiers ZFS

(Suite)

L'opération chmod suivante fait passer les autorisations à un mode moins restrictif. Dans cet
exemple, les autorisations d'ACL modifiées du groupe sysadmin et du groupe staff sont
restaurées pour accorder les mêmes autorisations que celles du groupe propriétaire.

$ chmod 770 file.1

$ ls -lV file.1

-rwxrwx---+ 1 amy

staff

206695 Aug 30 16:03 file.1

user:amy:r-----a-R-c---:-------:allow

user:rory:r-----a-R-c---:-------:allow

group:sysadmin:rw-p--aARWc---:-------:allow

group:staff:rw-p--aARWc---:-------:allow

owner@:rwxp--aARWcCos:-------:allow

group@:rwxp--aARWc--s:-------:allow

everyone@:------a-R-c--s:-------:allow

Application d'attributs spéciaux aux fichiers ZFS

Les exemples suivants montrent comment appliquer et afficher des attributs spéciaux, tels que
l'immuabilité ou l'accès en lecture seule, à des fichiers ZFS.

Pour plus d'informations sur l'affichage et l'application d'attributs spéciaux, reportez-vous aux
pages de manuel ls(1) et chmod(1).

EXEMPLE 8–15 Application de l'immuabilité à un fichier ZFS
Respectez la syntaxe suivante pour rendre un fichier immuable :

# chmod S+ci file.1

# echo this >>file.1

-bash: file.1: Not owner

# rm file.1

rm: cannot remove ‘file.1’: Not owner
Vous pouvez afficher les attributs spéciaux qui s'appliquent à des fichiers ZFS en respectant la
syntaxe suivante :

# ls -l/c file.1

-rw-r--r--+ 1 root

root

206695 Jul 20 14:27 file.1

{A-----im----}

Respectez la syntaxe suivante pour annuler l'immuabilité d'un fichier :

# chmod S-ci file.1

# ls -l/c file.1

-rw-r--r--+ 1 root

root

206695 Jul 20 14:27 file.1

# rm file.1

{A------m----}

248

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Application d'attributs spéciaux aux fichiers ZFS

EXEMPLE 8–16 Application d'un accès en lecture seule à un fichier ZFS
L'exemple suivant indique comment appliquer l'accès en lecture seule à un fichier ZFS.

# chmod S+cR file.2

# echo this >>file.2

-bash: file.2: Not owner

EXEMPLE 8–17 Affichage et modification des attributs d'un fichier ZFS
Vous pouvez afficher et définir des attributs spéciaux en respectant la syntaxe suivante :

# ls -l/v file.3

-r--r--r--

1 root

root

206695 Jul 20 14:59 file.3

noimmutable,av modified,noav_quarantined,nonounlink,nooffline,nosparse}

{archive,nohidden,noreadonly,nosystem,noappendonly,nonodump,

# chmod S+cR file.3

# ls -l/v file.3

-r--r--r--

1 root

root

206695 Jul 20 14:59 file.3

{archive,nohidden,readonly,nosystem,noappendonly,nonodump,noimmutable,

av_modified,noav_quarantined,nonounlink,nooffline,nosparse}

Certains de ces attributs s'appliquent uniquement à un environnement Oracle Solaris SMB.

Vous pouvez effacer tous les attributs d'un fichier. Par exemple :

# chmod S-a file.3

# ls -l/v file.3

-r--r--r--

1 root

root

206695 Jul 20 14:59 file.3

noimmutable,noav_modified,noav_quarantined,nonounlink,nooffline,nosparse}

{noarchive,nohidden,noreadonly,nosystem,noappendonly,nonodump,

Chapitre 8 • Utilisation des ACL et des attributs pour protéger les fichiers Oracle Solaris ZFS

249

250

9C H A P I T R E

9

Administration déléguée de ZFS dans Oracle
Solaris

Ce chapitre explique comment utiliser les fonctions d'administration déléguée pour permettre
aux utilisateurs ne disposant pas des autorisations nécessaires d'effectuer des tâches
d'administration de ZFS.
Ce chapitre contient les sections suivantes :
■ “Présentation de l'administration déléguée de ZFS” à la page 251
■ “Délégation d'autorisations ZFS” à la page 252
■ “Affichage des autorisations ZFS déléguées (exemples)” à la page 260
■ “Délégation d'autorisations ZFS (exemples)” à la page 257
■ “Suppression des autorisations ZFS déléguées (exemples)” à la page 262

Présentation de l'administration déléguée de ZFS

L'administration déléguée de ZFS vous permet de distribuer des autorisations précises à des
utilisateurs ou à des groupes spécifiques, voire à tous les utilisateurs. Deux types d'autorisations
déléguées sont prises en charge :
■ Les autorisations individuelles suivantes peuvent être explicitement déléguées : autorisation

de création (create), autorisation de destruction (destroy), autorisation de montage
(mount), autorisation de créer des instantanés (snapshot), etc.

■ Des groupes d'autorisations appelés jeux d'autorisations peuvent être définis. Si un jeu

d'autorisations est modifié, tout utilisateur de ce jeu de d'autorisations est automatiquement
affecté par ces modifications. Les jeux d'autorisations commencent par le symbole @ et sont
limités à 64 caractères. Les caractères suivant le symbole @ dans le nom de jeu ont les mêmes
restrictions que ceux des noms de systèmes de fichiers ZFS standard.

L'administration déléguée de ZFS offre des fonctions similaires au modèle de sécurité RBAC. La
délégation ZFS offre les avantages suivants pour la gestion des pools de stockage et systèmes de
fichiers ZFS :
■ Les autorisations sont transférées avec le pool de stockage ZFS lorsqu'un pool est migré.

251

Délégation d'autorisations ZFS

■ L'héritage dynamique vous permet de contrôler la propagation des autorisations dans les

systèmes de fichiers.
Il est possible de définir une configuration de manière à ce que seul le créateur d'un système
de fichiers puisse détruire celui-ci.

■

■ Les autorisations peuvent être déléguées à des systèmes de fichiers spécifiques. Tout

nouveau système de fichiers peut automatiquement récupérer des autorisations.
Simplifie l'administration de systèmes de fichiers en réseau (NFS, Network File System). Un
utilisateur disposant d'autorisations explicites peut par exemple créer un instantané sur un
système NFS dans le répertoire .zfs/snapshot approprié.

■

Considérez l'utilisation de l'administration déléguée pour la répartition des tâches ZFS. Pour
plus d'informations sur l'utilisation de RBAC pour gérer les tâches d'administration générales
dans Oracle Solaris, reportez-vous à la section Partie III, “Rôles, profils de droits et privilèges”
du manuel Administration d’Oracle Solaris : services de sécurité.

Désactivation des droits délégués de ZFS
La propriété delegation du pool permet de contrôler les fonctions d'administration déléguées.
Par exemple :

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation on

default

# zpool set delegation=off users

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation off

local

Par défaut, la propriété delegation est activée.

Délégation d'autorisations ZFS

Vous pouvez utiliser la commande zfs allow pour déléguer les autorisations sur les systèmes
de fichiers ZFS vers des utilisateurs non root en utilisant l'une des méthodes suivantes :
■ Vous pouvez déléguer des autorisations individuelles à un utilisateur, à un groupe, voire à

tous les utilisateurs.

■ Vous pouvez déléguer des groupes d'autorisations individuelles sous forme de jeu

d'autorisations à un utilisateur, à un groupe, voire à tous les utilisateurs.

■ Vous pouvez déléguer des autorisations localement, soit uniquement au système de fichiers

actuel, soit à l'ensemble de ses descendants.

Le tableau suivant décrit les opérations pouvant être déléguées et toute autorisation dépendante
requise pour réaliser ces opérations déléguées.

252

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Autorisation
(sous-commande)

Description

Autorisation d'accorder des
autorisations qui vous ont été octroyées à
un autre utilisateur.

Autorisation de cloner tout instantané
du jeu de données.

allow

clone

create

destroy

diff

hold

mount

promote

receive

release

rename

Délégation d'autorisations ZFS

Dépendances

Nécessité de disposer de l'autorisation en
cours d'octroi.

Nécessité de disposer des autorisations
create et mount sur le système de fichiers
d'origine.

Autorisation de créer des jeux de
données descendants.

Nécessité de disposer de l'autorisation
mount.

Autorisation de détruire un jeu de
données.

Nécessité de disposer de l'autorisation
mount.

Autorisation d'identifier les chemins
d'accès à l'intérieur d'un jeu de données.

Les utilisateurs non root ont besoin de
cette autorisation pour utiliser la
commande zfs diff.

Autorisation de conservation d'un
instantané.

Autorisation de monter et démonter un
système de fichiers, et de créer et détruire
les liens vers des périphériques de
volume.

Autorisation de promouvoir le clonage
d'un jeu de données.

Autorisation de créer des systèmes de
fichiers descendants à l'aide de la
commande zfs receive.

Autorisation de libérer un instantané
conservé, ce qui peut détruire
l'instantané.

Autorisation de renommer un jeu de
données.

Nécessité de disposer également des
autorisations mount et promote sur le
système de fichiers d'origine.

Nécessité de disposer également des
autorisations mount et create.

Nécessité de disposer également des
autorisations create et mount sur le
nouveau parent.

restauration

Autorisation de restaurer un instantané.

send

share

Autorisation d'envoyer un flux
d'instantané.

Autorisation de partager et de départager
un système de fichiers

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

253

Délégation d'autorisations ZFS

Autorisation
(sous-commande)

Description

Dépendances

snapshot

Autorisation de créer un instantané
d'un•jeu de données.

Vous pouvez déléguer le jeu d'autorisations suivant mais une autorisation peut être limitée à
l'accès, à la lecture ou à la modification :

■

■

■

■

■

■

■

groupquota

groupused

key

keychange

userprop

userquota

userused

Vous pouvez en outre déléguer l'administration des propriétés ZFS suivantes à des utilisateurs
non root :

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

aclinherit

aclmode

atime

canmount

casesensitivity

checksum

compression

copies

dedup

devices

encryption

exec

keysource

logbias

mountpoint

nbmand

normalization

primarycache

quota

readonly

recordsize

refquota

refreservation

reservation

rstchown

secondarycache

254

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Délégation d'autorisations ZFS

■

■

■

■

■

■

■

■

■

■

■

■

■

setuid

shadow

sharenfs

sharesmb

snapdir

sync

utf8only

version

volblocksize

volsize

vscan

xattr

zoned

Certaines de ces propriétés ne peuvent être définies qu'à la création d'un jeu de données. Pour
une description de ces propriétés, reportez-vous à la section “Présentation des propriétés ZFS”
à la page 137.

Délégation des autorisations ZFS (zfs allow)
La syntaxe de zfs allow est la suivante :

zfs allow -[ldugecs] everyone|user|group[,...] perm|@setname,...] filesystem| volume
La syntaxe de zfs allow suivante (en gras) identifie les utilisateurs auxquels les autorisations
sont déléguées :

zfs allow [-uge]|user|group|everyone [,...] filesystem | volume
Vous pouvez spécifier plusieurs entrées sous forme de liste séparée par des virgules. Si aucune
option -uge n'est spécifiée, l'argument est interprété en premier comme le mot-clé everyone,
puis comme un nom d'utilisateur et enfin, comme un nom de groupe. Pour spécifier un
utilisateur ou un groupe nommé "everyone", utilisez l'option -u ou l'option -g. Pour spécifier
un groupe portant le même nom qu'un utilisateur, utilisez l'option -g. L'option -c délègue des
autorisations create-time.

La syntaxe de zfs allow suivante (en gras) identifie la méthode de spécification des
autorisations et jeux d'autorisations :

zfs allow [-s] ... perm|@setname [,...] filesystem | volume
Vous pouvez spécifier plusieurs autorisations sous forme de liste séparée par des virgules. Les
noms d'autorisations sont identiques aux sous-commandes et propriétés ZFS. Pour plus
d'informations, reportez-vous à la section précédente.

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

255

Délégation d'autorisations ZFS

Les autorisations peuvent être regroupées en jeux d'autorisations et sont identifiées par l'option
-s. Les jeux d'autorisations peuvent être utilisés par d'autres commandes zfs allow pour le
système de fichiers spécifié et ses descendants. Les jeux d'autorisations sont évalués
dynamiquement et de ce fait, toute modification apportée à un jeu est immédiatement mise à
jour. Les jeux d'autorisations doivent se conformer aux mêmes critères d'attribution de noms
que les systèmes de fichiers ZFS, à ceci près que leurs noms doivent commencer par le caractère
arobase (@) et ne pas dépasser 64 caractères.

La syntaxe de zfs allow suivante (en gras) identifie la méthode de délégation des
autorisations :

zfs allow [-ld] ... ... filesystem | volume
L'option -l indique que les autorisations sont accordées au système de fichiers spécifié mais pas
à ses descendants, à moins de spécifier également l'option -d. L'option -d indique que les
autorisations sont accordées aux systèmes de fichiers descendants mais pas à ce système de
fichiers, à moins de spécifier également l'option - l. Si aucune option n'est spécifiée, les
autorisations sont accordées au système de fichiers ou au volume ainsi qu'à ses descendants.

Suppression des autorisations déléguées de ZFS (zfs
unallow)
Vous pouvez supprimer des autorisations précédemment déléguées à l'aide de la commande
zfs unallow.

Supposons par exemple que vous déléguiez les autorisations create, destroy, mount et
snapshot de la manière suivante :

# zfs allow cindy create,destroy,mount,snapshot tank/home/cindy

# zfs allow tank/home/cindy

---- Permissions on tank/home/cindy ----------------------------------

Local+Descendent permissions:

user cindy create,destroy,mount,snapshot

Pour supprimer ces autorisations, vous devez respecter la syntaxe suivante :

# zfs unallow cindy tank/home/cindy

# zfs allow tank/home/cindy

256

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Délégation d'autorisations ZFS (exemples)

Délégation d'autorisations ZFS (exemples)

EXEMPLE 9–1 Délégation d'autorisations à un utilisateur individuel
Lorsque vous déléguez les autorisations create et mount à un utilisateur individuel, vous devez
vous assurer que cet utilisateur dispose d'autorisations sur le point de montage sous-jacent.

Par exemple, pour déléguer à l'utilisateur mark les autorisations create et mount sur le système
de fichiers tank, définissez au préalable ces autorisations :

# chmod A+user:mark:add_subdirectory:fd:allow /tank/home

Utilisez ensuite la commande zfs allow pour déléguer les autorisations create, destroy et
mount. Par exemple :

# zfs allow mark create,destroy,mount tank/home

L'utilisateur mark peut dorénavant créer ses propres systèmes de fichiers dans le système de
fichiers tank/home. Par exemple :

# su mark

mark$ zfs create tank/home/mark

mark$ ^D

# su lp

$ zfs create tank/home/lp

cannot create ’tank/home/lp’: permission denied

EXEMPLE 9–2 Délégation des autorisations de création (create) et de destruction (destroy) à un groupe
L'exemple suivant décrit comment configurer un système de fichiers de manière à ce que tout
membre du groupe staff puisse créer et monter des systèmes de fichiers dans le système de
fichiers tank/home, et détruire ses propres systèmes de fichiers. Toutefois, les membres du
groupe staff ne sont pas autorisés à détruire les systèmes de fichiers des autres utilisateurs.

# zfs allow staff create,mount tank/home

# zfs allow -c create,destroy tank/home

# zfs allow tank/home

---- Permissions on tank/home ----------------------------------------

Create time permissions:

create,destroy

Local+Descendent permissions:

group staff create,mount

# su cindy

cindy% zfs create tank/home/cindy/files

cindy% exit

# su mark

mark% zfs create tank/home/mark/data

mark% exit

cindy% zfs destroy tank/home/mark/data

cannot destroy ’tank/home/mark/data’: permission denied

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

257

Délégation d'autorisations ZFS (exemples)

EXEMPLE 9–3 Délégation d'autorisations au niveau approprié d'un système de fichiers
Assurez-vous de déléguer les autorisations aux utilisateurs au niveau approprié du système de
fichiers. Par exemple, les autorisations create, destroy et mount pour les systèmes de fichiers
locaux et descendants sont déléguées à l'utilisateur mark. L'autorisation locale de créer un
instantané du système de fichiers tank/home a été délégué à l'utilisateur mark, mais pas celle de
créer un instantané de son propre système de fichiers. L'autorisation snapshot ne lui a donc pas
été déléguée au niveau approprié du système de fichiers.

# zfs allow -l mark snapshot tank/home

# zfs allow tank/home

---- Permissions on tank/home ----------------------------------------

Create time permissions:

create,destroy

Local permissions:

user mark snapshot

Local+Descendent permissions:

group staff create,mount

# su mark

mark$ zfs snapshot tank/home@snap1

mark$ zfs snapshot tank/home/mark@snap1

cannot create snapshot ’tank/home/mark@snap1’: permission denied

Pour déléguer à l'utilisateur mark cette autorisation au niveau du système de fichiers
descendants, utilisez l'option zfs allow -d. Par exemple :

# zfs unallow -l mark snapshot tank/home

# zfs allow -d mark snapshot tank/home

# zfs allow tank/home

---- Permissions on tank/home ----------------------------------------

Create time permissions:

create,destroy

Descendent permissions:

user mark snapshot

Local+Descendent permissions:

group staff create,mount

# su mark

$ zfs snapshot tank/home@snap2

cannot create snapshot ’tank/home@snap2’: permission denied

$ zfs snapshot tank/home/mark@snappy

L'utilisateur mark ne peut maintenant créer un instantané qu'à un niveau inférieur du système
de fichiers tank/home.

EXEMPLE 9–4 Définition et utilisation d'autorisations déléguées complexes
Vous pouvez déléguer des autorisations spécifiques à des utilisateurs ou des groupes. Par
exemple, la commande zfs allow suivante délègue des autorisations spécifiques au groupe
staff. En outre, les autorisations destroy et snapshot sont déléguées après la création de
systèmes de fichiers tank/home.

# zfs allow staff create,mount tank/home

# zfs allow -c destroy,snapshot tank/home

# zfs allow tank/home

258

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Délégation d'autorisations ZFS (exemples)

EXEMPLE 9–4 Définition et utilisation d'autorisations déléguées complexes

(Suite)

---- Permissions on tank/home ----------------------------------------

Create time permissions:

create,destroy,snapshot

Local+Descendent permissions:

group staff create,mount

Etant donné que l'utilisateur mark est membre du groupe staff, il peut créer des systèmes de
fichiers dans tank/home. En outre, l'utilisateur mark peut créer un instantané de
tank/home/mark2 parce qu'il dispose des autorisations spécifiques pour le faire. Par exemple :

# su mark

$ zfs create tank/home/mark2

$ zfs allow tank/home/mark2

---- Permissions on tank/home/mark2 ----------------------------------

Local permissions:

user mark create,destroy,snapshot

---- Permissions on tank/home ----------------------------------------

Create time permissions:

create,destroy,snapshot

Local+Descendent permissions:

group staff create,mount

L'utilisateur mark ne peut pas créer d'instantané dans tank/home/mark parce qu'il ne dispose
pas des autorisations spécifiques pour le faire. Par exemple :

$ zfs snapshot tank/home/mark@snap1

cannot create snapshot ’tank/home/mark@snap1’: permission denied

Dans cet exemple, l'utilisateur mark possède l'autorisation create dans son répertoire
personnel, ce qui signifie qu'il peut créer des instantanés. Ce scénario s'avère utile lorsque votre
système de fichiers est monté sur un système NFS.

$ cd /tank/home/mark2

$ ls

$ cd .zfs

$ ls

shares snapshot

$ cd snapshot

$ ls -l

total 3

drwxr-xr-x

2 mark

staff

2 Sep 27 15:55 snap1

$ pwd

/tank/home/mark2/.zfs/snapshot

$ mkdir snap2

$ zfs list

# zfs list -r tank/home

NAME

USED AVAIL REFER MOUNTPOINT

tank/home/mark

63K 62.3G

32K /tank/home/mark

tank/home/mark2

49K 62.3G

31K /tank/home/mark2

tank/home/mark2@snap1

18K

tank/home/mark2@snap2

0

-

-

31K -

31K -

$ ls

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

259

Affichage des autorisations ZFS déléguées (exemples)

EXEMPLE 9–4 Définition et utilisation d'autorisations déléguées complexes

(Suite)

snap1 snap2

$ rmdir snap2

$ ls

snap1

EXEMPLE 9–5 Définition et utilisation d'un jeu d'autorisations délégué ZFS
L'exemple suivant décrit comment créer un jeu d'autorisations intitulé @myset et délègue ce jeu
d'autorisations ainsi que l'autorisation de renommage au groupe staff pour le système de
fichiers tank. L'utilisateur cindy, membre du groupe staff, a l'autorisation de créer un système
de fichiers dans tank. Par contre, l'utilisateur lp ne dispose pas de cette autorisation de création
de systèmes de fichiers dans tank.

# zfs allow -s @myset create,destroy,mount,snapshot,promote,clone,readonly tank

# zfs allow tank

---- Permissions on tank ---------------------------------------------

Permission sets:

@myset clone,create,destroy,mount,promote,readonly,snapshot

# zfs allow staff @myset,rename tank

# zfs allow tank

---- Permissions on tank ---------------------------------------------

Permission sets:

@myset clone,create,destroy,mount,promote,readonly,snapshot

Local+Descendent permissions:

group staff @myset,rename

# chmod A+group:staff:add_subdirectory:fd:allow tank

# su cindy

cindy% zfs create tank/data

cindy% zfs allow tank

---- Permissions on tank ---------------------------------------------

Permission sets:

@myset clone,create,destroy,mount,promote,readonly,snapshot

Local+Descendent permissions:

group staff @myset,rename

cindy% ls -l /tank

total 15

drwxr-xr-x

2 cindy

staff

2 Jun 24 10:55 data

cindy% exit

# su lp

$ zfs create tank/lp

cannot create ’tank/lp’: permission denied

Affichage des autorisations ZFS déléguées (exemples)
Vous pouvez vous servir de la commande suivante pour afficher les autorisations :

# zfs allow dataset
Cette commande affiche les autorisations définies ou accordées au jeu de données spécifié. La
sortie contient les composants suivants :

260

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Affichage des autorisations ZFS déléguées (exemples)

Jeux d'autorisations

■ Autorisations individuelles ou autorisations à la création

■

■

■

■

Jeu de données local
Jeux de données locaux et descendants
Jeux de données descendants uniquement

EXEMPLE 9–6 Affichage des autorisations d'administration déléguées de base
La sortie suivante indique que l'utilisateur cindy dispose des autorisations create, destroy,
mount et snapshot sur le système de fichiers tank/cindy.

# zfs allow tank/cindy

-------------------------------------------------------------

Local+Descendent permissions on (tank/cindy)

user cindy create,destroy,mount,snapshot

EXEMPLE 9–7 Affichage des autorisations d'administration déléguée complexes
La sortie de cet exemple indique les autorisations suivantes sur les systèmes de fichiers
pool/fred et pool.

Pour le système de fichiers pool/fred :
■ Deux jeux d'autorisations sont définis :

■

■

@eng (create, destroy , snapshot, mount, clone , promote, rename)
@simple (create, mount )

■ Les autorisations à la création sont définies pour le jeu d'autorisations @eng et la propriété
mountpoint. "A la création" signifie qu'une fois qu'un jeu de systèmes de fichiers est créé, le
jeu d'autorisations @eng et l'autorisation de définir la propriété mountpoint sont déléguées.
■ Le jeu d'autorisations @eng est délégué à l'utilisateur tom et les autorisations create, destroy

et mount pour les systèmes de fichiers locaux sont déléguées à l'utilisateur joe.

■ Le jeu d'autorisations @basic ainsi que les autorisations share et rename pour les systèmes

de fichiers locaux et descendants sont délégués à l'utilisateur fred.

■ Le jeu d'autorisations @basic pour les systèmes de fichiers descendants uniquement est

délégué à l'utilisateur barney et au groupe staff.

Pour le système de fichiers pool :
■ Le jeu d'autorisations @simple (create, destroy, mount) est défini.
■ Le jeu d'autorisations sur le système de fichiers local @simple est accordé au groupe staff.

La sortie de cet exemple est la suivante :

$ zfs allow pool/fred

---- Permissions on pool/fred ----------------------------------------

Permission sets:

@eng create,destroy,snapshot,mount,clone,promote,rename

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

261

Suppression des autorisations ZFS déléguées (exemples)

EXEMPLE 9–7 Affichage des autorisations d'administration déléguée complexes

(Suite)

@simple create,mount

Create time permissions:

@eng,mountpoint

Local permissions:

user tom @eng

user joe create,destroy,mount

Local+Descendent permissions:

user fred @basic,share,rename

user barney @basic

group staff @basic

---- Permissions on pool ---------------------------------------------

Permission sets:

@simple create,destroy,mount

Local permissions:

group staff @simple

Suppression des autorisations ZFS déléguées (exemples)

Vous pouvez utiliser la commande zfs unallow pour supprimer des autorisations déléguées.
Par exemple, l'utilisateur cindy possède les autorisations create, destroy, mount et snapshot
sur le système de fichiers tank/cindy.

# zfs allow cindy create,destroy,mount,snapshot tank/home/cindy

# zfs allow tank/home/cindy

---- Permissions on tank/home/cindy ----------------------------------

Local+Descendent permissions:

user cindy create,destroy,mount,snapshot

La syntaxe suivante de la commande zfs unallow supprime l'autorisation de réaliser des
instantanés (snapshot) du système de fichiers tank/home/cindy accordée à l'utilisateur cindy :

# zfs unallow cindy snapshot tank/home/cindy

# zfs allow tank/home/cindy

---- Permissions on tank/home/cindy ----------------------------------

Local+Descendent permissions:

user cindy create,destroy,mount

cindy% zfs create tank/home/cindy/data

cindy% zfs snapshot tank/home/cindy@today

cannot create snapshot ’tank/home/cindy@today’: permission denied

Autre exemple : l'utilisateur mark possède les autorisations suivantes sur le système de fichiers
tank/home/mark :

# zfs allow tank/home/mark

---- Permissions on tank/home/mark ----------------------------------

Local+Descendent permissions:

user mark create,destroy,mount

-------------------------------------------------------------

La syntaxe suivante de la commande zfs unallow supprime toutes les autorisations accordées à
l'utilisateur mark pour le système de fichiers tank/home/mark :

262

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Suppression des autorisations ZFS déléguées (exemples)

# zfs unallow mark tank/home/mark

La syntaxe suivante de la commande zfs unallow supprime un jeu d'autorisations sur le
système de fichiers tank.

# zfs allow tank

---- Permissions on tank ---------------------------------------------

Permission sets:

@myset clone,create,destroy,mount,promote,readonly,snapshot

Create time permissions:

create,destroy,mount

Local+Descendent permissions:

group staff create,mount

# zfs unallow -s @myset tank

# zfs allow tank

---- Permissions on tank ---------------------------------------------

Create time permissions:

create,destroy,mount

Local+Descendent permissions:

group staff create,mount

Chapitre 9 • Administration déléguée de ZFS dans Oracle Solaris

263

264

10C H A P I T R E

1 0

Rubriques avancées Oracle Solaris ZFS

Ce chapitre décrit les volumes ZFS, l'utilisation de ZFS dans un système Solaris avec zones
installées, les pools racine de remplacement ZFS et les profils de droits ZFS.

Ce chapitre contient les sections suivantes :
■ “Volumes ZFS” à la page 265
■ “Utilisation de ZFS dans un système Solaris avec zones installées” à la page 268
■ “Utilisation de pools racine ZFS de remplacement” à la page 274

Volumes ZFS

Un volume ZFS est un jeu de données qui représente un périphérique en mode bloc. Les
volumes ZFS sont identifiés en tant que périphériques dans le répertoire
/dev/zvol/{dsk,rdsk}/pool.

Dans l'exemple suivant, un volume ZFS de 5 GO portant le nom tank/vol est créé :

# zfs create -V 5gb tank/vol

Lors de la création d'un volume, une réservation est automatiquement définie sur la taille
initiale du volume pour éviter tout comportement inattendu. Si, par exemple, la taille du
volume diminue, les données risquent d'être corrompues. Vous devez faire preuve de prudence
lors de la modification de la taille du volume.

En outre, si vous créez un instantané pour un volume modifiant la taille de ce dernier, cela peut
provoquer des incohérences lorsque vous tentez d'annuler (roll back) l'instantané ou de créer
un clone à partir de l'instantané.

Pour de plus amples informations concernant les propriétés de systèmes de fichiers applicables
aux volumes, reportez-vous au Tableau 6–1.

265

Volumes ZFS

En cas d'utilisation d'un système Solaris avec zones installées, la création ou le clonage d'un
volume ZFS dans une zone non globale est impossible. Si vous tentez d'effectuer cette action,
cette dernière échouera. Pour obtenir des informations relatives à l'utilisation de volumes ZFS
dans une zone globale, reportez-vous à la section “Ajout de volumes ZFS à une zone non
globale” à la page 270.

Utilisation d'un volume ZFS en tant que périphérique
de swap ou de vidage
Lors de l'installation d'un système de fichiers racine ZFS ou d'une migration à partir d'un
système de fichiers racine UFS, un périphérique de swap est créé sur un volume ZFS du pool
racine ZFS. Par exemple :

# swap -l

swapfile

dev

swaplo

blocks

free

/dev/zvol/dsk/rpool/swap 253,3

16 8257520 8257520

Lors de l'installation d'un système de fichiers racine ZFS ou d'une migration à partir d'un
système de fichiers racine UFS, un périphérique de vidage est créé sur un volume ZFS du pool
racine ZFS. Le périphérique de vidage ne nécessite aucune administration une fois configuré.
Par exemple :

# dumpadm

Dump content: kernel pages

Dump device: /dev/zvol/dsk/rpool/dump (dedicated)

Savecore directory: /var/crash/

Savecore enabled: yes

Si vous devez modifier votre zone de swap ou votre périphérique de vidage après l'installation
du système, utilisez les commandes swap et dumpadm de la même manière que dans les versions
précédentes de Solaris. Si vous tentez de créer un autre volume de swap, créez un volume ZFS
d'une taille spécifique et activez le swap sur le périphérique. Par exemple :

# zfs create -V 2G rpool/swap2

# swap -a /dev/zvol/dsk/rpool/swap2

# swap -l

swapfile

dev swaplo blocks

free

/dev/zvol/dsk/rpool/swap 256,1

16 2097136 2097136

/dev/zvol/dsk/rpool/swap2 256,5

16 4194288 4194288

N'effectuez pas de swap vers un fichier dans un système de fichiers ZFS. La configuration de
fichier swap ZFS n'est pas prise en charge.

Pour plus d'informations sur l'ajustement de la taille des volumes de swap et de vidage,
reportez-vous à la section “Ajustement de la taille de vos périphériques de swap et de vidage
ZFS” à la page 123.

266

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Volumes ZFS

Utilisation d'un volume ZFS en tant qu'unité logique
de stockage iSCSI
Le logiciel COMSTAR (Common Multiprotocol SCSI Target) permet de convertir n'importe
quel hôte Oracle Solaris en périphérique cible SCSI accessible à des hôtes initiateurs via un
réseau de stockage. Vous pouvez créer et configurer un volume ZFS en vue de le partager en tant
qu'unité logique de stockage (LUN) iSCSI.

Commencez par installer le package COMSTAR.

# pkg install group/feature/storage-server

Créez ensuite un volume ZFS qui sera utilisé en tant que cible iSCSI, puis créez le LUN basé sur
un périphérique en mode bloc SCSI. Par exemple :

# zfs create -V 2g tank/volumes/v2

# sbdadm create-lu /dev/zvol/rdsk/tank/volumes/v2

Created the following LU:

-------------------------------- ------------------- ----------------

GUID

DATA SIZE

SOURCE

600144f000144f1dafaa4c0faff20001 2147483648

/dev/zvol/rdsk/tank/volumes/v2

# sbdadm list-lu

Found 1 LU(s)

-------------------------------- ------------------- ----------------

GUID

DATA SIZE

SOURCE

600144f000144f1dafaa4c0faff20001 2147483648

/dev/zvol/rdsk/tank/volumes/v2

Vous pouvez exposer les vues du LUN à tous les clients ou à des clients sélectionnés. Identifiez le
GUID du LUN, puis partagez la vue du LUN. Dans l'exemple suivant, la vue du LUN est
partagée avec tous les clients.

# stmfadm list-lu

LU Name: 600144F000144F1DAFAA4C0FAFF20001

# stmfadm add-view 600144F000144F1DAFAA4C0FAFF20001

# stmfadm list-view -l 600144F000144F1DAFAA4C0FAFF20001

View Entry: 0

Host group

: All

Target group : All

LUN

: 0

L'étape suivante consiste à créer les iSCSI cibles. Pour plus d'informations sur la création des
cibles iSCSI, reportez-vous au Chapitre 14, “Configuration des périphériques de stockage avec
COMSTAR” du manuel Administration d’Oracle Solaris : Périphériques et systèmes de fichiers.

Un volume ZFS en tant que cible iSCSI est géré comme n'importe quel autre jeu de données
ZFS, à l'exception du fait que vous ne pouvez pas renommer l'ensemble de données, annuler
une capture d'écran de volume, ou de l'exportation du pool pendant que les volumes ZFS sont
partagés en tant que iSCSI LUN. Des messages similaires au message suivant s'afficheront :

Chapitre 10 • Rubriques avancées Oracle Solaris ZFS

267

Utilisation de ZFS dans un système Solaris avec zones installées

# zfs rename tank/volumes/v2 tank/volumes/v1

cannot rename ’tank/volumes/v2’: dataset is busy

# zpool export tank

cannot export ’tank’: pool is busy

L'ensemble des informations de configuration de cible iSCSI est stocké dans le jeu de données.
Tout comme un système de fichiers NFS partagé, une cible iSCSI importée dans un système
différent est partagée adéquatement.

Utilisation de ZFS dans un système Solaris avec zones
installées

Les sections suivantes décrivent l'utilisation d'un système de fichiers ZFS sur un système avec
des zones Oracle Solaris :
■ “Ajout de systèmes de fichiers ZFS à une zone non globale” à la page 269
■ “Délégation de jeux de données à une zone non globale” à la page 270
■ “Ajout de volumes ZFS à une zone non globale” à la page 270
■ “Utilisation de pools de stockage ZFS au sein d'une zone” à la page 271
■ “Gestion de propriétés ZFS au sein d'une zone” à la page 271
■ “Explication de la propriété zoned” à la page 272

Tenez compte des points suivants lors de l'association de jeux de données à des zones :

■

Il est possible d'ajouter un système de fichiers ou un clone ZFS à une zone non globale en
déléguant ou non le contrôle administratif.

■ Vous pouvez ajouter un volume ZFS en tant que périphérique à des zones non globales.
■ L'association d'instantanés ZFS à des zones est impossible à l'heure actuelle.

Dans les sections suivantes, le terme jeu de données ZFS fait référence à un système de fichier ou
à un clone.

L'ajout d'un jeu de données permet à la zone non globale de partager l'espace avec la zone
globale, mais l'administrateur de zone ne peut pas contrôler les propriétés ou créer de nouveaux
systèmes de fichiers dans la hiérarchie de systèmes de fichiers sous-jacents. Cette opération est
identique à l'ajout de tout autre type de système de fichiers à une zone. Effectuez-la lorsque vous
souhaitez simplement partager de l'espace commun.

ZFS autorise également la délégation de jeux de données à une zone non globale, ce qui permet
à l'administrateur de zone de contrôler parfaitement le jeu de données et ses enfants.
L'administrateur de zone peut créer et détruire les systèmes de fichiers ou les clones au sein de
ce jeu de données et modifier les propriétés des jeux de données. L'administrateur de zone ne
peut pas affecter des jeux de données qui n'ont pas été ajoutés à la zone, y compris ceux qui
dépassent les quotas de niveau supérieur du jeu de données délégué.

268

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Utilisation de ZFS dans un système Solaris avec zones installées

Tenez compte des points suivants lorsque vous utilisez ZFS sur un système sur lequel des zones
Oracle Solaris sont installées :
■ La propriété mountpoint d'un système de fichiers ZFS ajouté à une zone non globale doit

être définie sur legacy.

■ Lorsqu'un emplacement source zonepath et l'emplacement cible zonepath résident tous

deux dans un système de fichiers ZFS et se trouvent dans le même pool, la commande
zoneadm clone utilise dorénavant automatiquement le clone ZFS pour cloner une zone. La
commande zoneadm clone crée un instantané ZFS de la source de l'emplacement zonepath
et configure l'emplacement zonepath cible. Vous ne pouvez pas utiliser la commande zfs
clone pour cloner une zone. Pour plus d'informations, reportez-vous à la Partie II, “Oracle
Solaris Zones” du manuel Administration Oracle Solaris : Oracle Solaris Zones, Oracle
Solaris 10 Zones et gestion des ressources.

Ajout de systèmes de fichiers ZFS à une zone non
globale
Vous pouvez ajouter un système de fichiers ZFS en tant que système de fichiers générique
lorsqu'il s'agit simplement de partager de l'espace avec la zone globale. La propriété mountpoint
d'un système de fichiers ZFS ajouté à une zone non globale doit être définie sur legacy. Par
exemple, si le système de fichiers tank/zone/zion doit être ajouté à une zone non globale,
définissez comme suit la propriété mountpoint dans la zone globale :

# zfs set mountpoint=legacy tank/zone/zion

La sous-commande add fs de la commande zonecfg permet d'ajouter un système de fichiers
ZFS à une zone non globale.

Dans l'exemple suivant, un système de fichiers ZFS est ajouté à une zone non globale par un
administrateur global de la zone globale :

# zonecfg -z zion

zonecfg:zion> add fs

zonecfg:zion:fs> set type=zfs

zonecfg:zion:fs> set special=tank/zone/zion

zonecfg:zion:fs> set dir=/export/shared

zonecfg:zion:fs> end

Cette syntaxe permet d'ajouter le système de fichiers ZFS tank/zone/zion à la zone zion déjà
configurée et montée sur /export/shared. La propriété mountpoint du système de fichiers doit
être définie sur legacy et le système de fichiers ne peut pas être déjà monté à un autre
emplacement. L'administrateur de zone peut créer et détruire des fichiers au sein du système de
fichiers. Le système de fichiers ne peut pas être remonté à un autre emplacement, tout comme
l'administrateur ne peut pas modifier les propriétés suivantes du système de fichiers : atime,
readonly, compression, etc. L'administrateur de zone globale est chargé de la configuration et
du contrôle des propriétés du système de fichiers.

Chapitre 10 • Rubriques avancées Oracle Solaris ZFS

269

Utilisation de ZFS dans un système Solaris avec zones installées

Pour plus d'informations sur la commande zonecfg et sur la configuration des types de
ressources à l'aide de zonecfg, reportez-vous à la Partie II, “Oracle Solaris Zones” du manuel
Administration Oracle Solaris : Oracle Solaris Zones, Oracle Solaris 10 Zones et gestion des
ressources

Délégation de jeux de données à une zone non
globale
Si l'objectif principal est de déléguer l'administration du stockage d'une zone, le système de
fichiers ZFS prend en charge l'ajout de jeux de données à une zone non globale à l'aide de la
sous-commande add dataset de la commande zonecfg.

Dans l'exemple suivant, un système de fichiers ZFS est délégué à une zone non globale par un
administrateur global dans la zone globale.

# zonecfg -z zion

zonecfg:zion> add dataset

zonecfg:zion:dataset> set name=tank/zone/zion

zonecfg:zion:dataset> set alias=tank

zonecfg:zion:dataset> end

Contrairement à l'ajout d'un système de fichiers, cette syntaxe entraîne la visibilité du système
de fichiers ZFS tank/zone/zion dans la zone zion déjà configurée. Dans la zone zion, ce
système de fichiers n'est pas accessible en tant que tank/zone/zion, mais en tant que virtual
pool nommé tank. L'alias du système de fichiers délégué fournit à la zone en tant que pool
virtuel une vue du pool d'origine. La propriété d'alias indique le nom du pool virtuel. Si aucun
alias n'est précisé, un alias par défaut correspondant au dernier composant du nom du système
de fichiers est utilisé. Si aucun alias n'avait été indiqué, l'alias par défaut aurait été zion dans
l'exemple ci-dessus.

Dans les jeux de données délégués, l'administrateur des zones peut définir les propriétés des
systèmes de fichiers et créer des systèmes de fichiers descendants. En outre, l'administrateur des
zones peut créer des instantanés ainsi que des clones, et contrôler la totalité de la hiérarchie du
système de fichiers. Si des volumes ZFS sont créés au sein de systèmes de fichier délégués, ils
risquent d'entrer en conflit avec les volumes ZFS ajoutés en tant que ressources de
périphériques. Pour plus d'informations, reportez-vous à la section suivante et à dev(7FS).

Ajout de volumes ZFS à une zone non globale
Il est possible d'ajouter ou de créer un volume ZFS dans une zone non globale ou d'ajouter
l'accès aux données d'un volume dans une zone non globale de l'une des manières suivantes :
■ Dans une zone non globale, un administrateur de zone possédant des privilèges peut créer

un volume ZFS en tant que descendant d'un système de fichiers précédemment délégué. Par
exemple :

270

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Utilisation de ZFS dans un système Solaris avec zones installées

# zfs create -V 2g tank/zone/zion/vol1

La syntaxe ci-dessus signifie que l'administrateur des zones peut gérer les propriétés du
volume et des données dans la zone non globale.

■ Dans une zone globale, utilisez la sous-commande zonecfg add dataset et indiquez un

volume ZFS à ajouter à une zone non globale. Par exemple :

# zonecfg -z zion

zonecfg:zion> add dataset

zonecfg:zion:dataset> set name=tank/volumes/vol1

zonecfg:zion:dataset> end

La syntaxe ci-dessus signifie que l'administrateur des zones peut gérer les propriétés du
volume et des données dans la zone non globale.

■ Dans une zone globale, utilisez la sous-commande zonecfg add device et indiquez un
volume ZFS dont les données sont accessibles dans une zone non globale. Par exemple :

# zonecfg -z zion

zonecfg:zion> add device

zonecfg:zion:device> set match=/dev/zvol/dsk/tank/volumes/vol2

zonecfg:zion:device> end

La syntaxe précédente signifie que seules les données du volume peuvent être consultées
dans la zone non globale.

Utilisation de pools de stockage ZFS au sein d'une
zone
Il est impossible de créer ou de modifier des pools de stockage ZFS au sein d'une zone. Le
modèle d'administration délégué centralise le contrôle de périphériques de stockage physique
au sein de la zone globale et le contrôle du stockage virtuel dans les zones non globales. Bien
qu'un jeu de données au niveau du pool puisse être ajouté à une zone, toute commande
modifiant les caractéristiques physiques du pool, comme la création, l'ajout ou la suppression
de périphériques est interdite au sein de la zone. Même si les périphériques physiques sont
ajoutés à une zone à l'aide de la sous-commande add device de la commande zonecfg, ou si les
fichiers sont utilisés, la commande zpool n'autorise pas la création de nouveaux pools au sein de
la zone.

Gestion de propriétés ZFS au sein d'une zone
Après avoir délégué un jeu de données à une zone, l'administrateur de zone peut contrôler les
propriétés spécifiques au jeu de données. Lorsqu'un jeu de données est délégué à une zone, tous
les ancêtres s'affichent en tant que jeux de données en lecture seule, alors que le jeu de données
lui-même, ainsi que tous ses descendants, est accessible en écriture. Considérez par exemple la
configuration suivante :

Chapitre 10 • Rubriques avancées Oracle Solaris ZFS

271

Utilisation de ZFS dans un système Solaris avec zones installées

global# zfs list -Ho name

tank

tank/home

tank/data

tank/data/matrix

tank/data/zion

tank/data/zion/home

En cas d'ajout de tank/data/zion à une zone ayant l'alias zion par défaut, chaque jeu de
données possède les propriétés suivantes.

Jeu de données

Visible

Accessible en écriture

Propriétés immuables

tank

tank/home

tank/data

tank/data/zion

tank/data/zion/home

Non

Non

Non

Oui

Oui

-

-

-

Oui

Oui

-

-

-

zoned, quota,

reservation

zoned

Notez que tous les parents de tank/zone/zion sont invisibles et que tous ses descendants sont
accessibles en écriture. L'administrateur de zone ne peut pas modifier la propriété zoned car cela
entraînerait un risque de sécurité, comme décrit dans la section suivante.
Les utilisateurs privilégiés dans la zone peuvent modifier toute autre propriété paramétrable, à
l'exception des propriétés quota et reservation. Ce comportement permet à un
administrateur de zone globale de contrôler l'espace disque occupé par tous les jeux de données
utilisés par la zone non globale.
En outre, l'administrateur de zone globale ne peut pas modifier les propriétés sharenfs et
mountpoint après la délégation d'un jeu de données à une zone non globale.

Explication de la propriété zoned
Lors qu'un jeu de données est délégué à une zone non globale, il doit être marqué spécialement
pour que certaines propriétés ne soient pas interprétées dans le contexte de la zone globale.
Lorsqu'un jeu de données est délégué à une zone non globale sous le contrôle d'un
administrateur de zone, son contenu n'est plus fiable. Comme dans tous les systèmes de fichiers,
cela peut entraîner la présence de binaires setuid, de liens symboliques ou d'autres contenus
douteux qui pourraient compromettre la sécurité de la zone globale. De plus, l'interprétation de
la propriété mountpoint est impossible dans le contexte de la zone globale. Dans le cas
contraire, l'administrateur de zone pourrait affecter l'espace de noms de la zone globale. Afin de
résoudre ceci, ZFS utilise la propriété zoned pour indiquer qu'un jeu de données a été délégué à
une zone non globale à un moment donné.

272

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Utilisation de ZFS dans un système Solaris avec zones installées

La propriété zoned est une valeur booléenne automatiquement activée lors de la première
initialisation d'une zone contenant un jeu de données ZFS. L'activation manuelle de cette
propriété par un administrateur de zone n'est pas nécessaire. Si la propriété zoned est définie, le
montage ou le partage du jeu de données est impossible dans la zone globale. Dans l'exemple
suivant, le fichier tank/zone/zion a été délégué à une zone, alors que le fichier
tank/zone/global ne l'a pas été :

# zfs list -o name,zoned,mountpoint -r tank/zone

NAME

ZONED MOUNTPOINT

tank/zone/global

off /tank/zone/global

tank/zone/zion

on /tank/zone/zion

# zfs mount

tank/zone/global

/tank/zone/global

tank/zone/zion

/export/zone/zion/root/tank/zone/zion

Notez la différence entre la propriété mountpoint et le répertoire dans lequel le jeu de données
tank/zone/zion est actuellement monté. La propriété mountpoint correspond à la propriété
telle qu'elle est stockée dans le disque et non à l'emplacement auquel est monté le jeu de données
sur le système.

Lors de la suppression d'un jeu de données d'une zone ou de la destruction d'une zone, la
propriété zoned n'est pas effacée automatiquement. Ce comportement est dû aux risques de
sécurité inhérents associés à ces tâches. Dans la mesure où un utilisateur qui n'est pas fiable
dispose de l'accès complet au jeu de données et à ses enfants, la propriété mountpoint risque
d'être configurée sur des valeurs erronées ou des binaires setuid peuvent exister dans les
systèmes de fichiers.

Afin d'éviter tout risque de sécurité, l'administrateur global doit effacer manuellement la
propriété zoned pour que le jeu de données puisse être utilisé à nouveau. Avant de configurer la
propriété zoned sur off, assurez-vous que la propriété mountpoint du jeu de données et de tous
ses enfants est configurée sur des valeurs raisonnables et qu'il n'existe aucun binaire setuid, ou
désactivez la propriété setuid.

Après avoir vérifié qu'aucune vulnérabilité n'existe au niveau de la sécurité, vous pouvez
désactiver la propriété zoned à l'aide de la commande zfs set ou zfs inherit. Si la propriété
zoned est désactivée alors que le jeu de données est en cours d'utilisation au sein d'une zone, le
système peut se comporter de façon imprévue. Ne modifiez la propriété que si vous êtes sûr que
le jeu de données n'est plus en cours d'utilisation dans une zone non globale.

Copie de zones vers d'autres systèmes
Si vous devez migrer une ou plusieurs zones vers un autre système, pensez à utiliser les
commandes zfs send et zfs receive. Selon le scénario, il peut être préférable d'utiliser des
flux de réplication ou des flux récursifs.

Chapitre 10 • Rubriques avancées Oracle Solaris ZFS

273

Utilisation de pools racine ZFS de remplacement

Les exemples de cette section décrivent la copie de données de zone d'un système à un autre.
Des étapes supplémentaires sont nécessaires pour transférer la configuration de chaque zone et
rattacher chaque zone au nouveau système. Pour plus d'informations, reportez-vous à la
Partie II, “Oracle Solaris Zones” du manuel Administration Oracle Solaris : Oracle Solaris Zones,
Oracle Solaris 10 Zones et gestion des ressources.

Si toutes les zones d'un système doivent migrer vers un autre système, envisagez d'utiliser un
flux de réplication car il permet de conserver les instantanés et les clones. Les instantanés et les
clones sont largement utilisés par les commandes pkg update, beadm create et zoneadm
clone.

Dans l'exemple suivant, les zones de sysA sont installées dans le système de fichiers
rpool/zones et doivent être copiées dans le système de fichiers tank/zones sur sys. Les
commandes suivantes créent un instantané et copient les données vers sysb à l'aide d'un flux de
réplication :

sysA# zfs snapshot -r rpool/zones@send-to-sysB

sysA# zfs send -R rpool/zones@send-to-sysB | ssh sysB zfs receive -d tank

Dans l'exemple ci-dessous, l'une des zones est copiée de sysC vers sysD. Supposons que la
commande ssh ne soit pas disponible mais qu'une instance de serveur NFS le soit. Les
commandes suivantes peuvent être utilisées pour générer un flux zfs send récursif sans se
soucier de savoir si la zone est le clone d'une autre zone ou non.

sysC# zfs snapshot -r rpool/zones/zone1@send-to-nfs

sysC# zfs send -rc rpool/zones/zone1@send-to-nfs > /net/nfssrv/export/scratch/zone1.zfs

sysD# zfs create tank/zones

sysD# zfs receive -d tank/zones < /net/nfssrv/export/scratch/zone1.zfs

Utilisation de pools racine ZFS de remplacement

Lors de sa création, un pool est intrinsèquement lié au système hôte. Le système hôte gère les
informations du pool. Cela lui permet de détecter l'indisponibilité de ce dernier, le cas échéant.
Même si elles sont utiles dans des conditions normales d'utilisation, ces informations peuvent
causer des interférences lors de l'initialisation à partir d'autres médias ou lors de la création d'un
pool sur un média amovible. La fonction de pool racine de remplacement de ZFS permet de
résoudre ce problème. Un pool racine de remplacement n'est pas conservé d'une réinitialisation
système à une autre et tous les points de montage sont modifiés de sorte à être relatifs à la racine
du pool.

Création de pools racine de remplacement ZFS
La création d'un pool racine de remplacement s'effectue le plus souvent en vue d'une utilisation
avec un média amovible. Dans ces circonstances, les utilisateurs souhaitent employer un

274

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Utilisation de pools racine ZFS de remplacement

système de fichiers unique et le monter à l'emplacement de leur choix dans le système cible.
Lorsqu'un pool racine de remplacement est créé à l'aide de l'option zpool create-R, le point de
montage du système de fichiers racine est automatiquement défini sur /, qui est l'équivalent de
la racine de remplacement elle-même.

Dans l'exemple suivant, un pool nommé morpheus est créé à l'aide /mnt en tant que chemin de
racine de remplacement :

# zpool create -R /mnt morpheus c0t0d0

# zfs list morpheus

NAME

morpheus

USED AVAIL REFER MOUNTPOINT

32.5K 33.5G

8K /mnt

Notez le système de fichiers morpheus dont le point de montage est la racine de remplacement
du pool, /mnt. Le point de montage stocké sur le disque est / et le chemin complet de /mnt n'est
interprété que dans le contexte du pool racine de remplacement. Ce système de fichiers peut
ensuite être exporté ou importé sous un pool racine de remplacement arbitraire d'un autre
système en respectant la syntaxe de valeur de la racine secondaire -R.

# zpool export morpheus

# zpool import morpheus

cannot mount ’/’: directory is not empty

# zpool export morpheus

# zpool import -R /mnt morpheus

# zfs list morpheus

NAME

morpheus

USED AVAIL REFER MOUNTPOINT

32.5K 33.5G

8K /mnt

Importation de pools racine de remplacement
L'importation de pool s'effectue également à l'aide d'une racine de remplacement. Cette
fonction permet de récupérer les données, le cas échéant, lorsque les points de montage ne
doivent pas être interprétés dans le contexte de la racine actuelle, mais sous un répertoire
temporaire où pourront s'effectuer les réparations. Vous pouvez également utiliser cette
fonction lors du montage de médias amovibles comme décrit dans la section précédente.

Dans l'exemple suivant, un pool nommé morpheus est importé à l'aide de /mnt en tant que
chemin racine de remplacement : Cet exemple part du principe que morpheus a été
précédemment exporté.

# zpool import -R /a pool

# zpool list morpheus

NAME

SIZE

ALLOC FREE

CAP HEALTH ALTROOT

pool 44.8G

78K 44.7G

0% ONLINE /a

# zfs list pool

NAME

USED AVAIL REFER MOUNTPOINT

pool 73.5K 44.1G

21K /a/pool

Chapitre 10 • Rubriques avancées Oracle Solaris ZFS

275

276

11C H A P I T R E

1 1

Dépannage d'Oracle Solaris ZFS et récupération
de pool

Ce chapitre décrit les méthodes d'identification et de résolution des pannes de ZFS. Des
informations relatives à la prévention des pannes sont également fournies.

Ce chapitre contient les sections suivantes :
■ “Identification des défaillances ZFS” à la page 277
■ “Contrôle de l'intégrité d'un système de fichiers ZFS” à la page 279
■ “Résolution de problèmes avec le système de fichiers ZFS” à la page 281
■ “Réparation d'un configuration ZFS endommagée” à la page 287
■ “Réparation d'un périphérique manquant” à la page 287
■ “Remplacement ou réparation d'un périphérique endommagé ” à la page 289
■ “Réparation de données endommagées” à la page 300
■ “Réparation d'un système impossible à réinitialiser” à la page 304

Identification des défaillances ZFS

En tant que système de fichiers et gestionnaire de volumes combinés, ZFS peut rencontrer
différentes pannes. Ce chapitre commence par une description des différentes pannes, puis
explique comment les identifier sur un système en cours d'exécution. Il se conclut en expliquant
comment résoudre les problèmes. Le système de fichiers ZFS peut rencontrer trois types
d'erreurs de base :
■ “Périphériques manquants dans un pool de stockage ZFS” à la page 278
■ “Périphériques endommagés dans un pool de stockage ZFS” à la page 278
■ “Données ZFS corrompues” à la page 278

Notez que les trois types d'erreurs peuvent se produire dans un même pool. Une procédure de
réparation complète implique de trouver et de corriger une erreur, de passer à la suivante et
ainsi de suite.

277

Identification des défaillances ZFS

Périphériques manquants dans un pool de stockage
ZFS
Si un périphérique est entièrement supprimé du système, ZFS s'assure que le périphérique ne
peut pas être ouvert et il le place dans l'état REMOVED. En fonction du niveau de réplication des
données du pool, ce retrait peut résulter ou non en une indisponibilité de la totalité du pool. Le
pool reste accessible en cas de suppression d'un périphérique mis en miroir ou RAID-Z. Un
pool peut renvoyer l'état FAULTED. Cela signifie qu'aucune donnée n'est accessible jusqu'à ce que
le périphérique soit reconnecté selon les conditions suivantes :

■

■

■

Si tous les composants d'un miroir sont supprimés
Si plusieurs périphériques d'un périphérique RAID-Z (raidz1) sont supprimés
Si le périphérique de niveau supérieur est supprimé dans une configuration contenant un
seul disque

Périphériques endommagés dans un pool de stockage
ZFS
Le terme " endommagé " fait référence à une grande variété d'erreurs possibles. Les exemples
incluent les éléments suivants :
■ Erreurs d'E/S transitoires causées par un disque ou un contrôleur défaillant
■ Corruption de données sur disque causée par les rayons cosmiques
■ Bogues de pilotes entraînant des transferts de données vers ou à partir d'un emplacement

erroné

■ Ecrasement accidentel de parties du périphérique physique par un utilisateur
Certaines erreurs sont transitoires, par exemple une erreur d'E/S aléatoire alors que le
contrôleur rencontre des problèmes. Dans d'autres cas, les dommages sont permanents, par
exemple lors de la corruption sur disque. En outre, même si les dommages sont permanents,
cela ne signifie pas que l'erreur est susceptible de se reproduire. Par exemple, si un utilisateur
écrase une partie d'un disque par accident, aucune panne matérielle ne s'est produite et il est
inutile de remplacer le périphérique. L'identification du problème exact dans un périphérique
n'est pas une tâche aisée. Elle est abordée plus en détail dans une section ultérieure.

Données ZFS corrompues
La corruption de données se produit lorsqu'une ou plusieurs erreurs de périphériques
(indiquant un ou plusieurs périphériques manquants ou endommagés) affectent un
périphérique virtuel de niveau supérieur. Par exemple, la moitié d'un miroir peut subir des
milliers d'erreurs sans jamais causer de corruption de données. Si une erreur se produit sur
l'autre côté du miroir au même emplacement, les données sont endommagées.

278

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Contrôle de l'intégrité d'un système de fichiers ZFS

La corruption de données est toujours permanente et nécessite une soin particulier lors de la
réparation. Même en cas de réparation ou de remplacement des périphériques sous-jacents, les
données d'origine sont irrémédiablement perdues. La plupart du temps, ce scénario requiert la
restauration des données à partir de sauvegardes. Les erreurs de données sont enregistrées à
mesure qu'elles sont détectées et peuvent être contrôlées à l'aide de nettoyages de pools de
routine, comme expliqué dans la section suivante. Lorsqu'un bloc corrompu est supprimé, le
nettoyage de disque suivant reconnaît que la corruption n'est plus présente et supprime toute
trace de l'erreur dans le système.

Contrôle de l'intégrité d'un système de fichiers ZFS

Il n'existe pas d'utilitaire fsck équivalent pour ZFS. Cet utilitaire remplissait deux fonctions :
réparer et valider le système de fichiers.

Réparation du système de fichiers
Avec les systèmes de fichiers classiques, la méthode d'écriture des données est affectée par les
pannes inattendues entraînant des incohérences de systèmes de fichiers. Un système de fichiers
classique n'étant pas transactionnel, les blocs non référencés, les comptes de liens défectueux ou
autres structures de systèmes de fichiers incohérentes sont possibles. L'ajout de la journalisation
résout certains de ces problèmes, mais peut entraîner des problèmes supplémentaires lorsque la
restauration du journal est impossible. Une incohérence des données sur disque dans une
configuration ZFS ne se produit qu'à la suite d'une panne de matérielle (auquel cas le pool aurait
dû être redondant) ou en présence d'un bogue dans le logiciel ZFS.

L'utilitaire fsck répare les problèmes connus spécifiques aux systèmes de fichiers UFS. La
plupart des problèmes au niveau des pools de stockage ZFS sont généralement liés à un matériel
défaillant ou à des pannes de courant. En utilisant des pools redondants, vous pouvez éviter de
nombreux problèmes. Si le pool est endommagé suite à une défaillance de matériel ou à une
coupure de courant, reportez-vous à la section “Réparation de dommages présents dans
l'ensemble du pool de stockage ZFS” à la page 303.

Si le pool n'est pas redondant, le risque qu'une corruption de système de fichiers puisse rendre
tout ou partie de vos données inaccessibles est toujours présent.

Validation du système de fichiers
Outre la réparation du système de fichiers, l'utilitaire fsck valide l'absence de problème relatif
aux données sur le disque. Cette tâche requiert habituellement le démontage du système de
fichiers et en l'exécution de l'utilitaire fsck, éventuellement en mettant le système en mode
utilisateur unique lors du processus. Ce scénario entraîne une indisponibilité proportionnelle à

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

279

Contrôle de l'intégrité d'un système de fichiers ZFS

la taille du système de fichiers en cours de vérification. Plutôt que de requérir un utilitaire
explicite pour effectuer la vérification nécessaire, ZFS fournit un mécanisme pour effectuer une
vérification de routine des incohérences. Cette fonctionnalité, appelée nettoyage, est
fréquemment utilisée dans les systèmes de mémoire et autres systèmes comme méthode de
détection et de prévention d'erreurs pour éviter qu'elles entraînent des pannes matérielles ou
logicielles.

Contrôle du nettoyage de données ZFS
Si ZFS rencontre une erreur, soit via le nettoyage ou lors de l'accès à un fichier à la demande,
l'erreur est journalisée en interne pour vous donner une vue d'ensemble rapide de toutes les
erreurs connues au sein du pool.

Nettoyage explicite de données ZFS
La façon la plus simple de vérifier l'intégrité des données est de lancer un nettoyage explicite de
toutes les données au sein du pool. Cette opération traverse toutes les données dans le pool une
fois et vérifie que tous les blocs sont lisibles. Le nettoyage va aussi vite que le permettent les
périphériques, mais la priorité de toute E/S reste inférieure à celle de toute opération normale.
Cette opération peut affecter les performances, bien que les données du pool restent utilisables
et leur réactivité quasiment la même lors du nettoyage. La commande zpool scrubpermet de
lancer un nettoyage explicite. Par exemple :

# zpool scrub tank

La commande zpool status ne permet pas d'afficher l'état de l'opération de nettoyage actuelle.
Par exemple :

# zpool status -v tank

pool: tank

state: ONLINE

scan: scrub in progress since Mon Jun 7 12:07:52 2010

201M scanned out of 222M at 9.55M/s, 0h0m to go

0 repaired, 90.44% done

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Une seule opération de nettoyage actif par pool peut se produire à la fois.

L'option -s permet d'interrompre une opération de nettoyage en cours. Par exemple :

# zpool scrub -s tank

280

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Résolution de problèmes avec le système de fichiers ZFS

Dans la plupart des cas, une opération de nettoyage pour assurer l'intégrité des données doit
être menée à son terme. Vous pouvez cependant interrompre une telle opération si les
performances du système sont affectées.

Un nettoyage de routine garantit des E/S continues pour l'ensemble des disques du système. Cet
opération a cependant pour effet secondaire d'empêcher la gestion de l'alimentation de placer
des disques inactifs en mode basse consommation. Si le système réalise en général des E/S en
permanence, ou si la consommation n'est pas une préoccupation, ce problème peut être ignoré.

Pour de plus amples informations sur l'interprétation de la sortie de zpool status,
reportez-vous à la section “Requête d'état de pool de stockage ZFS” à la page 87.

Nettoyage et réargenture de données ZFS
Lors du remplacement d'un périphérique, une opération de réargenture est amorcée pour
déplacer les données des copies correctes vers le nouveau périphérique. Cette action est une
forme de nettoyage de disque. Par conséquent, une seule action de ce type peut être effectuée à
un moment donné dans le pool. Lorsqu'une opération de nettoyage est en cours, toute
opération de resynchronisation suspend le nettoyage ; le nettoyage reprend une fois que la
resynchronisation est terminée.

Pour de plus amples informations sur la resynchronisation, reportez-vous à la section
“Affichage de l'état de réargenture” à la page 298.

Résolution de problèmes avec le système de fichiers ZFS

Les sections suivantes décrivent l'identification et la résolution des problèmes dans les systèmes
de fichiers ZFS ou les pools de stockage :
■ “Recherche de problèmes éventuels dans un pool de stockage ZFS” à la page 283
■ “Consultation de la sortie de zpool status” à la page 283
■ “Rapport système de messages d'erreur ZFS” à la page 286

Les fonctions suivantes permettent d'identifier les problèmes au sein de la configuration ZFS :
■ La commande zpool status permet d'afficher les informations détaillées des pools de

stockage ZFS.

■ Les défaillances de pool et de périphérique sont rapportées par le biais de messages de

diagnostics ZFS/FMA.

■ La commande zpool history permet d'afficher les commandes ZFS précédentes qui ont

modifié les informations d'état de pool.

La commande zpool status permet de résoudre la plupart des problèmes au niveau de ZFS.
Cette commande analyse les différentes erreurs système et identifie les problèmes les plus
sévères. En outre, elle propose des actions à effectuer et un lien vers un article de connaissances

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

281

Résolution de problèmes avec le système de fichiers ZFS

pour de plus amples informations. Notez que cette commande n'identifie qu'un seul problème
dans le pool, même si plusieurs problèmes existent. Par exemple, les erreurs de corruption de
données sont généralement provoquées par la panne d'un périphérique, mais le remplacement
d'un périphérique défaillant peut ne pas résoudre tous les problèmes de corruption de données.

En outre, un moteur de diagnostic ZFS diagnostique et signale les défaillances au niveau du pool
et des périphériques. Les erreurs liées aux sommes de contrôle, aux E/S, aux périphériques et
aux pools font également l'objet d'un rapport lorsqu'elles sont liées à ces défaillances. Les
défaillances ZFS telles que rapportées par fmd s'affichent sur la console ainsi que les dans le
fichier de messages système. Dans la plupart des cas, le message fmd vous dirige vers la
commande zpool status pour obtenir des instructions supplémentaires de récupération.

Le processus de récupération est comme décrit ci-après :
■ Le cas échéant, la commande zpool history permet d'identifier les commandes ZFS ayant

précédé le scénario d'erreur. Par exemple :

# zpool history tank

History for ’tank’:

2010-07-15.12:06:50 zpool create tank mirror c0t1d0 c0t2d0 c0t3d0

2010-07-15.12:06:58 zfs create tank/eric

2010-07-15.12:07:01 zfs set checksum=off tank/eric

Dans cette sortie, notez que les sommes de contrôle sont désactivées pour le système de
fichiers tank/eric. Cette configuration est déconseillée.
Identifiez les erreurs à l'aide des messages fmd affichés sur la console système ou dans le
fichier /var/adm/messages.

■

■ Obtenez des instructions de réparation supplémentaires grâce à la commande zpool

status -x.

■ Réparez les pannes. Pour ce faire, suivez les étapes ci-après :

■ Remplacez le périphérique défaillant ou manquant et mettez-le en ligne.
■ Restaurez la configuration défaillante ou les données corrompues à partir d'une

sauvegarde.

■ Vérifiez la récupération à l'aide de la commande zpool status - x.

■

Sauvegardez la configuration restaurée, le cas échéant.

Cette section explique comment interpréter la sortie zpool status afin de diagnostiquer le type
de défaillances pouvant survenir. Même si la commande effectue automatiquement le travail,
vous devez comprendre exactement les problèmes identifiés pour diagnostiquer la panne. Les
sections suivantes expliquent comment corriger les différents types de problèmes que vous
risquez de rencontrer.

282

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Résolution de problèmes avec le système de fichiers ZFS

Recherche de problèmes éventuels dans un pool de
stockage ZFS
La méthode la plus simple pour déterminer s'il existe des problèmes connus sur le système
consiste à exécuter la commande zpool status -x. Cette commande décrit uniquement les
pools présentant des problèmes. Si tous les pools du système fonctionnent correctement, la
commande affiche les éléments suivants :

# zpool status -x

all pools are healthy

Sans l'indicateur -x, la commande affiche l'état complet de tous les pools (ou du pool demandé
s'il est spécifié sur la ligne de commande), même si les pools sont autrement fonctionnels.
Pour de plus amples informations sur les options de ligne de commande de la commande zpool
status, reportez-vous à la section “Requête d'état de pool de stockage ZFS” à la page 87.

Consultation de la sortie de zpool status
La sortie complète de zpool status est similaire à ce qui suit :

# zpool status tank

# zpool status tank

pool: tank

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: none requested

config:

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0 cannot open

NAME

tank

DEGRADED

mirror-0 DEGRADED

c1t0d0 ONLINE

c1t1d0 UNAVAIL

errors: No known data errors

Cette sortie est décrite ci-dessous :

Informations globales d'état des pools
Cette section de la sortie zpool status se compose des champs suivants, certains d'entre eux
n'étant affichés que pour les pools présentant des problèmes :

pool

state

Identifie le nom du pool.
Indique l'état de maintenance actuel du pool. Ces informations concernent
uniquement la capacité de pool à fournir le niveau de réplication requis.

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

283

Résolution de problèmes avec le système de fichiers ZFS

status

action

see

scrub

errors

Décrit les problèmes du pool. Ce champ est absent si aucune erreur n'est détectée.
Action recommandée pour la réparation des erreurs. Ce champ est absent si aucune
erreur n'est détectée.
Fait référence à un article de connaissances contenant des informations de
réparation détaillées. Les articles en ligne sont mis à jour plus régulièrement que ce
guide. Par conséquent, vous devez vous y reporter pour obtenir les procédures de
réparation les plus récentes. Ce champ est absent si aucune erreur n'est détectée.
Identifie l'état actuel d'une opération de nettoyage. Ce champ peut indiquer la date
et l'heure du dernier nettoyage, un nettoyage en cours ou l'absence de demande de
nettoyage.
Identifie les erreurs de données ou l'absence d'erreurs de données connues.

Informations de configuration de pool
Le champ config de la sortie zpool status décrit la configuration des périphériques inclus
dans le pool, ainsi que leur état et toute erreur générée à partir des périphériques. L'état peut être
l'un des suivants : ONLINE, FAULTED, DEGRADED, UNAVAILABLE ou OFFLINE. Si l'état n'est pas
ONLINE, la tolérance de pannes du pool a été compromise.

La deuxième section de la sortie de configuration affiche des statistiques d'erreurs. Ces erreurs
se divisent en trois catégories :

■

■

■

READ : erreurs d'E/S qui se sont produites lors de l'envoi d'une demande de lecture
WRITE : erreurs d'E/S qui se sont produites lors de l'envoi d'une demande d'écriture
CKSUM : erreurs de somme de contrôle signifiant que le périphérique a renvoyé des données
corrompues en réponse à une demande de lecture.

Il est possible d'utiliser ces erreurs pour déterminer si les dommages sont permanents. Des
erreurs d'E/S peu nombreuses peuvent indiquer une interruption de service temporaire. Si elles
sont nombreuses, il est possible que le périphérique présente un problème permanent. Ces
erreurs ne correspondent pas nécessairement à la corruption de données telle qu'interprétée par
les applications. Si la configuration du périphérique est redondante, les périphériques peuvent
présenter des erreurs impossibles à corriger, même si aucune erreur ne s'affiche au niveau du
périphérique RAID-Z ou du miroir. Dans ce cas, ZFS a récupéré les données correctes et a
réussi à réparer les données endommagées à partir des répliques existantes.

Pour de plus amples informations sur l'interprétation de ces erreurs, reportez-vous à la section
“Détermination du type de panne de périphérique” à la page 290.

Enfin, les informations auxiliaires supplémentaire sont affichées dans la dernière colonne de la
sortie de zpool status. Ces informations s'étendent dans le champ state et facilitent le
diagnostic des pannes. Si l'état d'un périphérique est FAULTED, ce champ indique si périphérique

284

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Résolution de problèmes avec le système de fichiers ZFS

est inaccessible ou si les données du périphérique sont corrompues. Si le périphérique est en
cours de réargenture, ce champ affiche la progression du processus.

Pour de plus amples informations sur le contrôle de la progression de la resynchronisation,
reportez-vous à la section “Affichage de l'état de réargenture” à la page 298.

Etat du nettoyage
La section sur le nettoyage de la sortie zpool status décrit l'état actuel de toute opération de
nettoyage explicite. Ces informations sont distinctes de la détection d'erreurs dans le système,
mais il est possible de les utiliser pour déterminer l'exactitude du rapport d'erreurs de
corruption de données. Si le dernier nettoyage s'est récemment terminé, toute corruption de
données existante aura probablement déjà été détecté.

Les messages d'état du nettoyage zpool status suivants sont fournis :
■ Rapport de progression du nettoyage. Par exemple :

scan: scrub in progress since Mon Jun 7 08:56:04 2010

1.90G scanned out of 16.2G at 9.33M/s, 0h26m to go

0 repaired, 11.69% done

■ Message de fin du nettoyage. Par exemple :

scrub repaired 0 in 0h12m with 0 errors on Mon Jun 7 09:08:48 2010

■ Message d'annulation du nettoyage en cours. Par exemple :

scan: scrub canceled on Thu Jun 3 09:39:39 2010

Les messages de fin de nettoyage subsistent après plusieurs réinitialisations du système.

Pour de plus amples informations sur le nettoyage de données et l'interprétation de ces
informations, reportez-vous à la section “Contrôle de l'intégrité d'un système de fichiers ZFS”
à la page 279.

Erreurs de corruption de données
La commande zpool status indique également si des erreurs connues sont associées au pool.
La détection de ces erreurs a pu s'effectuer lors du nettoyage des données ou lors des opérations
normales. Le système de fichiers ZFS gère un journal persistant de toutes les erreurs de données
associées à un pool. Ce journal tourne à chaque fois qu'un nettoyage complet du système est
terminé.

Les erreurs de corruption de données constituent toujours des erreurs fatales. Elles indiquent
une erreur d'E/S dans au moins une application, en raison de la présence de données
corrompues au sein du pool. Les erreurs de périphérique dans un pool redondant n'entraînent
pas de corruption de données et ne sont pas enregistrées en tant que partie de ce journal. Par
défaut, seul le nombre d'erreurs trouvées s'affiche. Vous pouvez obtenir la liste complète des
erreurs et de leurs spécificités à l'aide de l'option zpool status -v. Par exemple :

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

285

Résolution de problèmes avec le système de fichiers ZFS

# zpool status -v

pool: tank

state: UNAVAIL

status: One or more devices are faulted in response to IO failures.

action: Make sure the affected devices are connected, then run ’zpool clear’.

see: http://www.sun.com/msg/ZFS-8000-HC

scrub: scrub completed after 0h0m with 0 errors on Tue Feb 2 13:08:42 2010

config:

NAME

tank

UNAVAIL

c1t0d0

ONLINE

c1t1d0

UNAVAIL

STATE

READ WRITE CKSUM

0

0

4

0

0

1

0 insufficient replicas

0

0 cannot open

errors: Permanent errors have been detected in the following files:

/tank/data/aaa

/tank/data/bbb

/tank/data/ccc

La commande fmd affiche également un message similaire dans la console système et le fichier
/var/adm/messages. La commande fmdump permet également de réaliser le suivi de ces
messages.

Pour de plus amples informations sur l'interprétation d'erreurs de corruption de données,
reportez-vous à la section “Identification du type de corruption de données” à la page 300.

Rapport système de messages d'erreur ZFS
Outre le suivi permanent des erreur au sein du pool, ZFS affiche également des messages syslog
lorsque des événements intéressants se produisent. Les scénarios suivants donnent lieu à des
événements de notification :
■ Transition d'état de périphérique – Si l'état d'un périphérique devient FAULTED, ZFS

consigne un message indiquant que la tolérance de pannes du pool risque d'être
compromise. Un message similaire est envoyé si le périphérique est mis en ligne
ultérieurement, restaurant la maintenance du pool.

■ Corruption de données : en cas de détection de corruption de données, ZFS consigne un

message indiquant où et quand s'est produit la détection. Ce message n'est consigné que lors
de la première détection. Les accès ultérieurs ne génèrent pas de message.

■ Défaillances de pool et de périphérique : en cas de défaillance d'un pool ou d'un

périphérique, le démon du gestionnaire de pannes rapporte ces erreurs par le biais de
messages syslog et de la commande fmdump.

Si ZFS détecte un erreur de périphérique et la corrige automatiquement, aucune notification
n'est générée. De telles erreurs ne constituent pas une défaillance de redondance de pool ou de
l'intégrité des données. En outre, de telles erreurs sont typiquement dues à un problème de
pilote accompagné de son propre jeu de messages d'erreur.

286

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Réparation d'un configuration ZFS endommagée

Réparation d'un périphérique manquant

ZFS gère un cache de pools actifs et les configurations correspondantes dans le système de
fichiers racine. Si ce fichier de cache est corrompu ou n'est plus synchronisé avec les
informations de configuration stockées dans le disque, l'ouverture du pool n'est plus possible.
Le système de fichiers ZFS tente d'éviter cette situation, même si des corruptions arbitraires
peuvent toujours survenir en raison des caractéristiques du système de fichiers sous-jacent et du
stockage. En général, cette situation est due à la disparition d'un pool du système alors qu'il
devrait être disponible. Parfois, elle correspond à une configuration partielle, dans laquelle il
manque un nombre inconnu de périphériques virtuels de niveau supérieur. Quel que soit le cas,
la configuration peut être récupérée en exportant le pool (s'il est visible à tous) et en le
réimportant.

Pour de plus amples informations sur l'importation et l'exportation de pools, reportez-vous à la
section “Migration de pools de stockage ZFS” à la page 99.

Réparation d'un périphérique manquant

Si l'ouverture d'un périphérique est impossible, ce dernier s'affiche dans l'état UNAVAIL dans la
sortie de zpool status. Cet état indique que ZFS n'a pas pu ouvrir le périphérique lors du
premier accès au pool ou que le périphérique est devenu indisponible par la suite. Si le
périphérique rend un périphérique de niveau supérieur indisponible, l'intégralité du pool
devient inaccessible. Dans le cas contraire, la tolérance de pannes du pool risque d'être
compromise. Quel que soit le cas, le périphérique doit simplement être reconnecté au système
pour fonctionner à nouveau normalement.

Par exemple, après une panne de périphérique, fmd peut afficher un message similaire au
suivant :

SUNW-MSG-ID: ZFS-8000-FD, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Thu Jun 24 10:42:36 PDT 2010

PLATFORM: SUNW,Sun-Fire-T200, CSN: -, HOSTNAME: daleks

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: a1fb66d0-cc51-cd14-a835-961c15696fcb

DESC: The number of I/O errors associated with a ZFS device exceeded

acceptable levels. Refer to http://sun.com/msg/ZFS-8000-FD for more information.

AUTO-RESPONSE: The device has been offlined and marked as faulted. An attempt

will be made to activate a hot spare if available.

IMPACT: Fault tolerance of the pool may be compromised.

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Pour afficher des informations détaillées sur le problème du périphérique et sa résolution,
utilisez la commande zpool status -x. Par exemple :

# zpool status -x

pool: tank

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

287

Réparation d'un périphérique manquant

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scan: scrub repaired 0 in 0h0m with 0 errors on Tue Sep 27 16:59:07 2011

config:

NAME

tank

mirror-0 DEGRADED

c2t2d0 ONLINE

c2t1d0 UNAVAIL

errors: No known data errors

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0

0 cannot open

Cette sortie indique que le périphérique c2t1d0 manquant ne fonctionne pas. Si vous estimez
que le périphérique est défectueux, remplacez-le.

Si nécessaire, exécutez ensuite la commande zpool online pour mettre le périphérique
remplacé en ligne. Par exemple :

# zpool online tank c2t1d0

Signalez à FMA que le périphérique a été remplacé si la sortie de fmadm faulty identifie l'erreur
de périphérique. Par exemple :

# fmadm faulty

--------------- ------------------------------------ -------------- ---------

TIME

EVENT-ID

MSG-ID

SEVERITY

--------------- ------------------------------------ -------------- ---------

Sep 27 16:58:50 e6bb52c3-5fe0-41a1-9ccc-c2f8a6b56100 ZFS-8000-D3

Major

Host

: t2k-brm-10

Platform

: SUNW,Sun-Fire-T200

Chassis_id :

Product_sn :

Fault class : fault.fs.zfs.device

Affects

: zfs://pool=tank/vdev=c75a8336cda03110

faulted and taken out of service

Problem in : zfs://pool=tank/vdev=c75a8336cda03110

faulted and taken out of service

Description : A ZFS device failed. Refer to http://sun.com/msg/ZFS-8000-D3 for

more information.

Response

: No automated response will occur.

Impact

: Fault tolerance of the pool may be compromised.

Action

: Run ’zpool status -x’ and replace the bad device.

# fmadm repair zfs://pool=tank/vdev=c75a8336cda03110

288

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

Confirmez ensuite que le pool dont le périphérique a été remplacé fonctionne correctement. Par
exemple :

# zpool status -x tank

pool ’tank’ is healthy

Reconnexion physique d'un périphérique
La reconnexion d'un périphérique dépend du périphérique en question. S'il s'agit d'un disque
connecté au réseau, la connectivité au réseau doit être restaurée. S'il s'agit d'un périphérique
USB ou autre média amovible, il doit être reconnecté au système. S'il s'agit d'un disque local, un
contrôleur est peut-être tombé en panne, rendant le périphérique invisible au système. Dans ce
cas, il faut remplacer le contrôleur pour que les disques soient à nouveau disponibles. D'autres
problèmes existent et dépendent du type de matériel et de sa configuration. Si un disque tombe
en panne et n'est plus visible pour le système, le périphérique doit être traité comme un
périphérique endommagé. Suivez les procédures décrites dans la section “Remplacement ou
réparation d'un périphérique endommagé ” à la page 289.

Notification relative à la disponibilité de
périphériques dans ZFS
Une fois le périphérique reconnecté au système, sa disponibilité peut être détectée
automatiquement ou non dans ZFS. Si le pool était précédemment défaillant ou si le system a
été réinitialisé en tant que partie de la procédure attach, alors ZFS rebalaye automatiquement
tous les périphériques lors de la tentative d'ouverture du pool. Si le pool était endommagé et que
le périphérique a été remplacé alors que le système était en cours d'exécution, vous devez
indiquer à ZFS que le périphérique est dorénavant disponible et qu'il est prêt à être rouvert à
l'aide de la commande zpool online. Par exemple :

# zpool online tank c0t1d0

Pour de plus amples informations sur la remise en ligne de périphériques, reportez-vous à la
section “Mise en ligne d'un périphérique” à la page 75.

Remplacement ou réparation d'un périphérique endommagé

Cette section explique comment déterminer les types de panne de périphériques, effacer les
erreurs transitoires et remplacer un périphérique.

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

289

Remplacement ou réparation d'un périphérique endommagé

Détermination du type de panne de périphérique
L'expression périphérique endommagé peut décrire un grand nombre de situations :
■ Bit rot : sur la durée, des événements aléatoires, tels que les influences magnétiques et les

rayons cosmiques, peuvent entraîner une inversion des bits stockés dans le disque. Ces
événements sont relativement rares mais, cependant, assez courants pour entraîner des
corruptions de données potentielles dans des systèmes de grande taille ou de longue durée.

■ Lectures ou écritures mal dirigées : les bogues de microprogrammes ou les pannes de

matériel peuvent entraîner un référencement incorrect de l'emplacement du disque par des
lectures ou écritures de blocs entiers. Ces erreurs sont généralement transitoires, mais un
grand nombre d'entre elles peut indiquer un disque défectueux.

■ Erreur d'administrateur : les administrateurs peuvent écraser par erreur des parties du
disque avec des données erronées (la copie de /dev/zero sur des parties du disque, par
exemple) qui entraînent la corruption permanente du disque. Ces erreurs sont toujours
transitoires.
Interruption temporaire de service : un disque peut être temporairement indisponible,
entraînant l'échec des E/S. En général, cette situation est associée aux périphériques
connectés au réseau, mais les disques locaux peuvent également connaître des interruptions
temporaires de service. Ces erreurs peuvent être transitoires ou non.

■

■ Matériel défectueux ou peu fiable : cette situation englobe tous les problèmes liés à un
matériel défectueux, y compris les erreurs d'E/S cohérentes, les transports défectueux
entraînant des corruptions aléatoires ou des pannes. Ces erreurs sont typiquement
permanentes.

■ Périphérique mis hors ligne : si un périphérique est hors ligne, il est considéré comme

ayant été mis hors ligne par l'administrateur, parce qu'il était défectueux. L'administrateur
qui a mis ce dispositif hors ligne peut déterminer si cette hypothèse est exacte.

Il est parfois difficile de déterminer la nature exacte de la panne du dispositif. La première étape
consiste à examiner le décompte d'erreurs dans la sortie de zpool status. Par exemple :

# zpool status -v tank

pool: tank

state: ONLINE

status: One or more devices has experienced an error resulting in data

corruption. Applications may be affected.

action: Restore the file in question if possible. Otherwise restore the

entire pool from backup.

see: http://www.sun.com/msg/ZFS-8000-8A

scan: scrub in progress since Tue Sep 27 17:12:40 2011

63.9M scanned out of 528M at 10.7M/s, 0h0m to go

0 repaired, 12.11% done

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

2

0

0

290

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

mirror-0 ONLINE

c2t2d0 ONLINE

c2t1d0 ONLINE

2

2

2

0

0

0

0

0

0

errors: Permanent errors have been detected in the following files:

/tank/words

Les erreurs sont divisées en erreurs d'E/S et en erreurs de sommes de contrôle. Ces deux
catégories peuvent indiquer le type de panne possible. Une opération typique renvoie un très
petit nombre d'erreurs (quelques-unes sur une longue période). Si les erreurs sont nombreuses,
un périphérique est probablement en panne ou sur le point de tomber en panne. Cependant,
une erreur provoquée par un administrateur peut également entraîner un grand nombre
d'erreurs. Le journal système syslog constitue une autre source d'informations. Si le journal
présente un grand nombre de messages SCSI ou de pilote Fibre Channel, il existe probablement
de graves problèmes matériels. L'absence de messages syslog indique que les dommages sont
probablement transitoires.

L'objectif est de répondre à la question suivante :

Est-il possible qu'une autre erreur se produise dans ce périphérique ?

Les erreurs qui ne se produisent qu'une fois sont considérées transitoires et n'indiquent pas une
panne potentielle. Les erreurs suffisamment persistantes ou sévères pour indiquer une panne
matérielle potentielle sont considérées comme étant des erreurs fatales. Aucun logiciel
automatisé actuellement disponible avec ZFS ne permet de déterminer le type d'erreur. Par
conséquent, l'administrateur doit procéder manuellement. Une fois l'erreur déterminée, vous
pouvez réaliser l'action adéquate. En cas d'erreurs fatales, effacez les erreurs transitoires ou
remplacez le périphérique. Ces procédures de réparation sont décrites dans les sections
suivantes.

Même si les erreurs de périphériques sont considérées comme étant transitoires, elles peuvent
tout de même entraîner des erreurs de données impossibles à corriger au sein du pool. Ces
erreurs requièrent des procédures de réparation spéciales, même si le périphérique sous-jacent
est considéré comme étant fonctionnel ou réparé. Pour de plus amples informations sur la
réparation d'erreurs de données, reportez-vous à la section “Réparation de données
endommagées” à la page 300.

Suppression des erreurs transitoires
Si les erreurs de périphérique sont considérées comme étant transitoires, dans la mesure où il
est peu probable qu'elles affectent la maintenance du périphérique, elles peuvent être effacées en
toute sécurité pour indiquer qu'aucune erreur fatale ne s'est produite. Pour effacer les
compteurs d'erreurs pour les périphériques mis en miroir ou RAID-Z, utilisez la commande
zpool clear. Par exemple :

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

291

Remplacement ou réparation d'un périphérique endommagé

# zpool clear tank c1t1d0

Cette syntaxe efface toutes les erreurs du périphérique et tout décompte d'erreurs de données
associées au périphérique.

Pour effacer toutes les erreurs associées aux périphériques virtuels du pool et tout décompte
d'erreurs de données associées au pool, respectez la syntaxe suivante :

# zpool clear tank

Pour de plus amples informations relatives à la suppression des erreurs de pool, reportez-vous à
la section “Effacement des erreurs de périphérique de pool de stockage” à la page 76.

Remplacement d'un périphérique dans un pool de
stockage ZFS
Si le périphérique présente ou risque de présenter une panne permanente, il doit être remplacé.
Le remplacement du périphérique dépend de la configuration.
■ “Détermination de la possibilité de remplacement du périphérique” à la page 292
■ “Périphériques impossibles à remplacer” à la page 293
■ “Remplacement d'un périphérique dans un pool de stockage ZFS” à la page 293
■ “Affichage de l'état de réargenture” à la page 298

Détermination de la possibilité de remplacement du périphérique
Si le périphérique à remplacer fait partie d'une configuration redondante, il doit exister
suffisamment de répliques pour permettre la récupération des données correctes. Si deux
disques d'un miroir à quatre directions sont défaillants, chaque disque peut être remplacé car
des répliques saines sont disponibles. Cependant, en cas de panne de deux disques dans un
périphérique virtuel RAID-Z à quatre directions (raidz1), aucun disque ne peut être remplacé
en l'absence de répliques suffisantes permettant de récupérer les données. Si le périphérique est
endommagé mais en ligne, il peut être remplacé tant que l'état du pool n'est pas FAULTED.
Toutefois, toute donnée endommagée sur le périphérique est copiée sur le nouveau
périphérique, à moins que le nombre de copies des données non endommagées soit déjà
suffisant.

Dans la configuration suivante, le disque c1t1d0 peut être remplacé et toute donnée du pool est
copiée à partir de la réplique saine, c1t0d0.

mirror

c1t0d0

c1t1d0

DEGRADED

ONLINE

FAULTED

292

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

Le disque c1t0d0 peut également être remplacé, mais un autorétablissement des données est
impossible, car il n'existe aucune réplique correcte.

Dans la configuration suivante, aucun des disques défaillants ne peut être remplacé. Les disques
ONLINE ne peuvent pas l'être non plus, car le pool lui-même est défaillant.

raidz

c1t0d0

c2t0d0

c3t0d0

c4t0d0

FAULTED

ONLINE

FAULTED

FAULTED

ONLINE

Dans la configuration suivante, chacun des disques de niveau supérieur peut être remplacé.
Cependant, les données incorrectes seront également copiées dans le nouveau disque, le cas
échéant.

c1t0d0

c1t1d0

ONLINE

ONLINE

Si les deux disques sont défectueux, alors tout remplacement est impossible car le pool
lui-même est défectueux.

Périphériques impossibles à remplacer
Si la perte d'un périphérique entraîne une défaillance du pool ou si le périphérique contient trop
d'erreurs de données dans une configuration non redondante, alors le remplacement du
périphérique en toute sécurité est impossible. En l'absence de redondance suffisante, il n'existe
pas de données correctes avec lesquelles réparer le périphérique défectueux. Dans ce cas, la
seule option est de détruire le pool, recréer la configuration et restaurer les données à partir
d'une copie de sauvegarde.

Pour de plus amples informations sur la restauration d'un pool entier, reportez-vous à la section
“Réparation de dommages présents dans l'ensemble du pool de stockage ZFS” à la page 303.

Remplacement d'un périphérique dans un pool de stockage ZFS
Après avoir déterminé qu'il est possible de remplacer un périphérique, exécutez la commande
zpool replace pour le remplacer effectivement. Exécutez la commande suivante si vous
remplacez le périphérique endommagé par un autre périphérique différent :

# zpool replace tank c1t1d0 c2t0d0

Cette commande lance la migration de données vers le nouveau périphérique, soit à partir du
périphérique endommagé, soit à partir d'autres périphériques du pool s'il s'agit d'une
configuration redondante. Une fois l'exécution de la commande terminée, le périphérique
endommagé est séparé de la configuration. Il peut dorénavant être retiré du système. Si vous

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

293

Remplacement ou réparation d'un périphérique endommagé

avez déjà retiré le périphérique et que vous l'avez remplacé par un autre dans le même
emplacement, utilisez la forme "périphérique unique" de la commande. Par exemple :

# zpool replace tank c1t1d0

Cette commande formate adéquatement un disque non formaté et resynchronise ensuite les
données à partir du reste de la configuration.

Pour de plus amples informations sur la commande zpool replace reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 76.

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS
L'exemple suivant illustre le remplacement d'un périphérique (c1t3d0) du pool de stockage mis
en miroir tank sur un système équipé de périphériques SATA. Pour remplacer le disque c1t3d0
par un nouveau au même emplacement (c1t3d0), annulez la configuration du disque avant de
procéder au remplacement. Voici les principales étapes à suivre :
■ Déconnectez le disque (c1t3d0) à remplacer. Vous ne pouvez pas annuler la configuration

d'un disque utilisé.

■ Utilisez la commande cfgadm pour identifier le disque (c1t3d0) dont la configuration doit
être annulée et annulez-la. Dans cette configuration en miroir, le pool est endommagé et le
disque est hors ligne, mais le pool reste disponible.

■ Remplacez le disque (c1t3d0). Vérifiez que la DEL bleue Ready to Remove, indiquant que le

périphérique est prêt à être retiré, est allumée avant de retirer le lecteur défaillant.

■ Reconfigurez le disque (c1t3d0).
■ Mettez le nouveau disque (c1t3d0) en ligne.
■ Exécutez la commande zpool replace pour remplacer le disque (c1t3d0).

Remarque – Si vous avez précédemment défini la propriété de pool autoreplace sur on, tout
nouveau périphérique détecté au même emplacement physique qu'un périphérique
appartenant précédemment au pool est automatiquement formaté et remplacé sans recourir
à la commande zpool replace. Cette fonction n'est pas prise en charge sur tous les types de
matériel.

■

Si un disque défectueux est automatiquement remplacé par un disque hot spare, vous devrez
peut-être déconnecter le disque hot spare une fois le disque défectueux remplacé. Par
exemple, si c2t4d0 reste actif comme disque hot spare actif une fois le disque défectueux
remplacé, déconnectez-le.

# zpool detach tank c2t4d0

■

Si FMA signale le périphérique défaillant, effacez la panne de périphérique.

294

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS

(Suite)

# fmadm faulty

# fmadm repair zfs://pool=name/vdev=guid

L'exemple suivant explique étape par étape comment remplacer un disque dans un pool de
stockage ZFS.

# zpool offline tank c1t3d0

# cfgadm | grep c1t3d0

sata1/3::dsk/c1t3d0

disk

connected

configured

ok

# cfgadm -c unconfigure sata1/3

Unconfigure the device at: /devices/pci@0,0/pci1022,7458@2/pci11ab,11ab@1:3

This operation will suspend activity on the SATA device

Continue (yes/no)? yes

# cfgadm | grep sata1/3

sata1/3

disk

connected

unconfigured ok

<Physically replace the failed disk c1t3d0>

# cfgadm -c configure sata1/3

# cfgadm | grep sata1/3

sata1/3::dsk/c1t3d0

disk

connected

configured

ok

# zpool online tank c1t3d0

# zpool replace tank c1t3d0

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Tue Feb 2 13:17:32 2010

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0 ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror-1 ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror-2 ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Notez que la commande zpool output affiche parfois l'ancien disque et le nouveau sous
l'en-tête de remplacement. Par exemple :

replacing

DEGRADED

c1t3d0s0/o FAULTED

c1t3d0

ONLINE

0

0

0

0

0

0

0

0

0

Ce texte signifie que la procédure de remplacement et la resynchronisation du nouveau disque
sont en cours.

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

295

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS

(Suite)

Pour remplacer un disque (c1t3d0) par un autre disque (c4t3d0), il suffit d'exécuter la
commande zpool replace. Par exemple :

# zpool replace tank c1t3d0 c4t3d0

# zpool status

pool: tank

state: DEGRADED

scrub: resilver completed after 0h0m with 0 errors on Tue Feb 2 13:35:41 2010

config:

NAME

tank

STATE

READ WRITE CKSUM

DEGRADED

mirror-0

ONLINE

c0t1d0

ONLINE

c1t1d0

ONLINE

mirror-1

ONLINE

c0t2d0

ONLINE

c1t2d0

ONLINE

mirror-2

DEGRADED

c0t3d0

ONLINE

replacing

DEGRADED

c1t3d0

OFFLINE

c4t3d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

La commande zpool status doit parfois être exécutée plusieurs fois jusqu'à la fin du
remplacement du disque.

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Tue Feb 2 13:35:41 2010

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror-0

ONLINE

c0t1d0

ONLINE

c1t1d0

ONLINE

mirror-1

ONLINE

c0t2d0

ONLINE

c1t2d0

ONLINE

mirror-2

ONLINE

c0t3d0

ONLINE

c4t3d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

EXEMPLE 11–2 Remplacement d'un périphérique de journalisation défaillant
ZFS identifie les défaillances de journal d'intention dans la sortie de commande zpool status.
Le composant FMA (Fault Management Architecture) signale également ces erreurs. ZFS et
FMA décrivent comment récupérer les données en cas de défaillance du journal d'intention.

296

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–2 Remplacement d'un périphérique de journalisation défaillant

(Suite)

L'exemple suivant montre comment récupérer les données d'un périphérique de journalisation
défaillant (c0t5d0 ) dans le pool de stockage (pool). Voici les principales étapes à suivre :
■ Vérifiez la sortie zpool status -x et le message de diagnostic FMA, décrits ici :

https://support.oracle.com/

CSP/main/

article?cmd=show&type=NOT&doctype=REFERENCE&alias=EVENT:ZFS-8000-K4

■ Remplacez physiquement le périphérique de journalisation défaillant.
■ Mettez le nouveau périphérique de journalisation en ligne.
■ Effacez la condition d'erreur du pool.
■ Effacez l'erreur FMA.

Par exemple, si le système s'arrête soudainement avant que les opérations d'écriture synchrone
ne soient affectées à un pool disposant d'un périphérique de journalisation distinct, un message
tel que le suivant s'affiche :

# zpool status -x

pool: pool

state: FAULTED

status: One or more of the intent logs could not be read.

Waiting for adminstrator intervention to fix the faulted pool.

action: Either restore the affected device(s) and run ’zpool online’,

or ignore the intent log records by running ’zpool clear’.

scrub: none requested

config:

NAME

pool

STATE

READ WRITE CKSUM

FAULTED

mirror-0

ONLINE

c0t1d0

ONLINE

c0t4d0

ONLINE

logs

FAULTED

c0t5d0

UNAVAIL

0

0

0

0

0

0

0

0

0

0

0

0

0 bad intent log

0

0

0

0 bad intent log

0 cannot open

<Physically replace the failed log device>

# zpool online pool c0t5d0

# zpool clear pool

# fmadm faulty

# fmadm repair zfs://pool=name/vdev=guid
Vous pouvez résoudre la panne du périphérique de journalisation de l'une des façons suivantes :
■ Remplacez ou récupérez le périphérique de journalisation. Dans cet exemple, le

périphérique de journalisation est c0t5d0.

■ Mettez le périphérique de journalisation en ligne.

# zpool online pool c0t5d0

■ Réinitialisez la condition d'erreur de périphérique de journalisation défaillante.

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

297

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–2 Remplacement d'un périphérique de journalisation défaillant

(Suite)

# zpool clear pool

Pour effectuer une récupération suite à cette erreur sans remplacer le périphérique de
journalisation défaillant, vous pouvez effacer l'erreur à l'aide de la commande zpool clear.
Dans ce scénario, le pool fonctionnera en mode dégradé et les enregistrements de journal seront
enregistrés dans le pool principal jusqu'à ce que le périphérique de journalisation distinct soit
remplacé.

Envisagez d'utiliser des périphériques de journalisation mis en miroir afin d'éviter un scénario
de défaillance de périphérique de journalisation.

Affichage de l'état de réargenture
Le processus de remplacement d'un périphérique peut prendre beaucoup de temps, selon la
taille du périphérique et la quantité de données dans le pool. Le processus de déplacement de
données d'un périphérique à un autre s'appelle la resynchronisation. Vous pouvez la contrôler à
l'aide de la commande zpool status.

Les messages d'état de la réargenture zpool status suivants sont fournis :
■ Rapport de progression de la réargenture. Par exemple :

scan: resilver in progress since Mon Jun 7 09:17:27 2010

13.3G scanned out of 16.2G at 18.5M/s, 0h2m to go

13.3G resilvered, 82.34% done

■ Message de fin de la réargenture. Par exemple :

resilvered 16.2G in 0h16m with 0 errors on Mon Jun 7 09:34:21 2010

Les messages de fin de réargenture subsistent après plusieurs réinitialisations du système.

Les systèmes de fichiers traditionnels effectuent la réargenture de données au niveau du bloc.
Dans la mesure où ZFS élimine la séparation en couches artificielles du gestionnaire de volume,
il peut effectuer la resynchronisation de façon bien plus puissante et contrôlée. Les deux
avantages de cette fonction sont comme suite :
■ ZFS n'effectue la réargenture que de la quantité minimale de données requises. Dans le cas

d'une brève interruption de service (par rapport à un remplacement complet d'un
périphérique), vous pouvez effectuer la resynchronisation du disque en quelques minutes
ou quelques secondes. Lors du remplacement d'un disque entier, la durée du processus de
resynchronisation est proportionnelle à la quantité de données utilisées dans le disque. Le
remplacement d'un disque de 500 Go ne dure que quelques secondes si le pool ne contient
que quelques giga-octets d'espace utilisé.

298

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Remplacement ou réparation d'un périphérique endommagé

■ La réargenture est un processus fiable qui peut être interrompu, le cas échéant. En cas de
mise hors-tension ou de réinitialisation du système, le processus de réargenture reprend
exactement là où il s'est arrêté, sans requérir une intervention manuelle.

La commande zpool status permet de visualiser le processus de réargenture. Par exemple :

# zpool status tank

pool: tank

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scan: resilver in progress since Mon Jun 7 10:49:20 2010

54.6M scanned out of 222M at 5.46M/s, 0h0m to go

54.5M resilvered, 24.64% done

config:

STATE

READ WRITE CKSUM

NAME

tank

ONLINE

mirror-0

ONLINE

replacing-0 ONLINE

c1t0d0

ONLINE

c2t0d0

ONLINE

c1t1d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 (resilvering)

0

Dans cet exemple, le disque c1t0d0 est remplacé par c2t0d0. Cet événement est observé dans la
sortie d'état par la présence du périphérique virtuel replacing de la configuration. Ce
périphérique n'est pas réel et ne permet pas de créer un pool. L'objectif de ce périphérique
consiste uniquement à afficher le processus de resynchronisation et à identifier le périphérique
en cours de remplacement.

Notez que tout pool en cours de resynchronisation se voit attribuer l'état ONLINE ou DEGRADED
car il ne peut pas fournir le niveau souhaité de redondance tant que le processus n'est pas
terminé. La resynchronisation s'effectue aussi rapidement que possible, mais les E/S sont
toujours programmées avec une priorité inférieure à celle des E/S requises par l'utilisateur afin
de minimiser l'impact sur le système. Une fois la resynchronisation terminée, la nouvelle
configuration complète s'applique, remplaçant l'ancienne configuration. Par exemple :

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h1m with 0 errors on Tue Feb 2 13:54:30 2010

config:

NAME

tank

mirror-0 ONLINE

c2t0d0 ONLINE

c1t1d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0 377M resilvered

0

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

299

Réparation de données endommagées

L'état du pool est à nouveau ONLINE et le disque défectueux d'origine (c1t0d0) a été supprimé de
la configuration.

Réparation de données endommagées

Les sections suivantes décrivent comment identifier le type de corruption de données et
comment réparer les données le cas échéant.
■ “Identification du type de corruption de données” à la page 300
■ “Réparation d'un fichier ou répertoire corrompu” à la page 301
■ “Réparation de dommages présents dans l'ensemble du pool de stockage ZFS” à la page 303

ZFS utilise les données des sommes de contrôles, de redondance et d'auto-rétablissement pour
minimiser le risque de corruption de données. Cependant, la corruption de données peut se
produire si le pool n'est pas redondant, si la corruption s'est produite alors que le pool était
endommagé ou si une série d'événements improbables a corrompu plusieurs copies d'un
élément de données. Quelle que soit la source, le résultat est le même : les données sont
corrompues et par conséquent inaccessibles. Les actions à effectuer dépendent du type de
données corrompue et de leurs valeurs relatives. Deux types de données peuvent être
corrompus :
■ Métadonnées de pool : ZFS requiert une certaine quantité de données à analyser afin

d'ouvrir un pool et d'accéder aux jeux de données. Si ces données sont corrompues, le pool
entier ou des parties de la hiérarchie du jeu de données sont indisponibles.

■ Données d'objet : dans ce cas, la corruption se produit au sein d'un fichier ou périphérique

spécifique. Ce problème peut rendre une partie du fichier ou répertoire inaccessible ou
endommager l'objet.

Les données sont vérifiées lors des opérations normales et lors du nettoyage. Pour de plus
amples informations sur la vérification de l'intégrité des données du pool, reportez-vous à la
section “Contrôle de l'intégrité d'un système de fichiers ZFS” à la page 279.

Identification du type de corruption de données
Par défaut, la commande zpool status indique qu'une corruption s'est produite, mais
n'indique pas à quel endroit. Par exemple :

# zpool status monkey

pool: monkey

state: ONLINE

status: One or more devices has experienced an error resulting in data

corruption. Applications may be affected.

action: Restore the file in question if possible. Otherwise restore the

entire pool from backup.

300

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Réparation de données endommagées

see: http://www.sun.com/msg/ZFS-8000-8A

scrub: scrub completed after 0h0m with 8 errors on Tue Jul 13 13:17:32 2010

config:

NAME

STATE

READ WRITE CKSUM

monkey

ONLINE

c1t1d0

ONLINE

c2t5d0

ONLINE

8

2

6

0

0

0

0

0

0

errors: 8 data errors, use ’-v’ for a list

Toute erreur indique seulement qu'une erreur s'est produite à un moment donné. Il est possible
que certaines erreurs ne soient plus présentes dans le système. Dans le cadre d'une utilisation
normale, elles le sont. Certaines interruptions de service temporaires peuvent entraîner une
corruption de données qui est automatiquement réparée une fois l'interruption de service
terminée. Un nettoyage complet du pool examine chaque bloc actif dans le pool. Ainsi, le
journal d'erreur est réinitialisé à la fin de chaque nettoyage. Si vous déterminez que les erreurs
ne sont plus présentes et ne souhaitez pas attendre la fin du nettoyage, la commande zpool
online permet de réinitialiser toutes les erreurs du pool.

Si la corruption de données se produit dans des métadonnées au niveau du pool, la sortie est
légèrement différente. Par exemple :

# zpool status -v morpheus

pool: morpheus

id: 1422736890544688191

state: FAULTED

status: The pool metadata is corrupted.

action: The pool cannot be imported due to damaged devices or data.

see: http://www.sun.com/msg/ZFS-8000-72

config:

morpheus

FAULTED

corrupted data

c1t10d0

ONLINE

Dans le cas d'une corruption au niveau du pool, ce dernier se voit attribuer l'état FAULTED, car le
pool ne peut pas fournir le niveau de redondance requis.

Réparation d'un fichier ou répertoire corrompu
En cas de corruption d'un fichier ou d'un répertoire, le système peut tout de même continuer à
fonctionner, selon le type de corruption. Tout dommage est irréversible, à moins que des copies
correctes des données n'existent sur le système. Si les données sont importantes, vous devez
restaurer les données affectées à partir d'une sauvegarde. Quand bien même, vous devriez
pouvoir réparer les données corrompues sans restaurer la totalité du pool.

En cas de dommages au sein d'un bloc de données de fichiers, le fichier peut être supprimé en
toute sécurité. L'erreur est alors effacée du système. Utilisez la commande zpool status -v
pour afficher la liste des noms de fichier contenant des erreurs persistantes. Par exemple :

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

301

Réparation de données endommagées

# zpool status -v

pool: monkey

state: ONLINE

status: One or more devices has experienced an error resulting in data

corruption. Applications may be affected.

action: Restore the file in question if possible. Otherwise restore the

entire pool from backup.

see: http://www.sun.com/msg/ZFS-8000-8A

scrub: scrub completed after 0h0m with 8 errors on Tue Jul 13 13:17:32 2010

config:

NAME

STATE

READ WRITE CKSUM

monkey

ONLINE

c1t1d0

ONLINE

c2t5d0

ONLINE

8

2

6

0

0

0

0

0

0

errors: Permanent errors have been detected in the following files:

/monkey/a.txt

/monkey/bananas/b.txt

/monkey/sub/dir/d.txt

monkey/ghost/e.txt

/monkey/ghost/boo/f.txt

La liste des noms de fichiers comportant des erreurs persistantes peut être décrite comme suit :

■

■

■

Si le chemin complet du fichier est trouvé et si le jeu de données est monté, le chemin
complet du fichier s'affiche. Par exemple :

/monkey/a.txt

Si chemin complet du fichier est trouvé mais que le jeu de données n'est pas monté, le nom
du jeu de données non précédé d'un slash (/) s'affiche, suivi du chemin du fichier au sein du
jeu de données. Par exemple :

monkey/ghost/e.txt

Si le nombre d'objet vers un chemin de fichiers ne peut pas être converti, soit en raison d'une
erreur soit parce qu'aucun chemin de fichiers réel n'est associé à l'objet, tel que c'est le cas
pour dnode_t, alors le nom du jeu de données s'affiche, suivi du numéro de l'objet. Par
exemple :

monkey/dnode:<0x0>

■ En cas de corruption d'un MOS (Meta-Object Set, jeu de méta-objet), la balise spéciale

<metadata> s'affiche, suivie du numéro de l'objet.

Si la corruption se situe au sein des métadonnées d'un répertoire ou d'un fichier, vous devez
déplacer le fichier vers un autre emplacement. Vous pouvez déplacer en toute sécurité les
fichiers ou les répertoires vers un autre emplacement. Cela permet de restaurer l'objet d'origine
à son emplacement.

302

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Réparation de données endommagées

Réparation de dommages présents dans l'ensemble
du pool de stockage ZFS
Si des dommages sont présents dans les métadonnées du pool et que cela empêche l'ouverture
ou l'importation du pool, vous pouvez utiliser les options suivantes :
■ Tentez de récupérer le pool à l'aide de la commande zpool clear -F ou zpool import -F.
Ces commandes tentent d'annuler (roll back) les dernières transactions restantes du pool
pour qu'elles reviennent à un fonctionnement normal. Vous pouvez utiliser la commande
zpool status pour vérifier le pool endommagé et les mesures de récupération
recommandées. Par exemple :

# zpool status

pool: tpool

state: FAULTED

status: The pool metadata is corrupted and the pool cannot be opened.

action: Recovery is possible, but will result in some data loss.

Returning the pool to its state as of Wed Jul 14 11:44:10 2010

should correct the problem. Approximately 5 seconds of data

must be discarded, irreversibly. Recovery can be attempted

by executing ’zpool clear -F tpool’. A scrub of the pool

is strongly recommended after recovery.

see: http://www.sun.com/msg/ZFS-8000-72

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

tpool

FAULTED

c1t1d0

ONLINE

c1t3d0

ONLINE

0

0

0

0

0

0

1 corrupted data

2

4

Le processus de récupération comme décrit dans la sortie ci-dessus consiste à utiliser la
commande suivante :

# zpool clear -F tpool

Si vous tentez d'importer un pool de stockage endommagé, des messages semblables aux
messages suivants s'affichent :

# zpool import tpool

cannot import ’tpool’: I/O error

Recovery is possible, but will result in some data loss.

Returning the pool to its state as of Wed Jul 14 11:44:10 2010

should correct the problem. Approximately 5 seconds of data

must be discarded, irreversibly. Recovery can be attempted

by executing ’zpool import -F tpool’. A scrub of the pool

is strongly recommended after recovery.

Le processus de récupération comme décrit dans la sortie ci-dessus consiste à utiliser la
commande suivante :

# zpool import -F tpool

Pool tpool returned to its state as of Wed Jul 14 11:44:10 2010.

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

303

Réparation d'un système impossible à réinitialiser

Discarded approximately 5 seconds of transactions

Si le pool endommagé se trouve dans le fichier zpool.cache, le problème est détecté lors de
l'initialisation du système. Le pool endommagé est consigné dans la commande zpool
status. Si le pool ne se trouve pas dans le fichier zpool.cache, il n'est pas importé ou ouvert
et des messages indiquant que le pool est endommagé s'affichent lorsque vous tentez de
l'importer.

■ Vous pouvez importer un pool endommagé en mode lecture seule. Cette méthode permet

d'importer le pool, ce qui vous permet d'accéder aux données. Par exemple :

# zpool import -o readonly=on tpool

Pour plus d'informations sur l'importation d'un pool en lecture seule, reportez-vous à la
section “Importation d'un pool en mode lecture seule” à la page 106.

■

■ Vous pouvez importer un pool avec un périphérique de journalisation manquant à l'aide de

la commande zpool import -m. Pour plus d'informations, reportez-vous à la section
“Importation d'un pool avec un périphérique de journalisation manquant” à la page 105.
Si le pool ne peut pas être récupéré par le biais de l'une des méthodes de récupération de
pool, vous devez restaurer le pool et l'ensemble de ses données à partir d'une copie de
sauvegarde. Le mécanisme utilisé varie énormément selon la configuration du pool et la
stratégie de sauvegarde. Tout d'abord, enregistrez la configuration telle qu'elle s'affiche dans
la commande zpool status pour pouvoir la recréer après la destruction du pool. Ensuite,
détruisez le pool à l'aide de la commande zpool destroy -f.
Conservez également un fichier décrivant la disposition des jeux de données et les diverses
propriétés définies localement dans un emplacement sûr, car ces informations deviennent
inaccessibles lorsque le pool est lui-même inaccessible. Avec la configuration du pool et la
disposition des jeux de données, vous pouvez reconstruire la configuration complète après
destruction du pool. Les données peuvent ensuite être renseignées par la stratégie de
sauvegarde ou de restauration de votre choix.

Réparation d'un système impossible à réinitialiser

ZFS a été conçu pour être robuste et stable malgré les erreurs. Cependant, les bogues de logiciels
ou certains problèmes inattendus peuvent entraîner la panique du système lors de l'accès à un
pool. Dans le cadre du processus d'initialisation, chaque pool doit être ouvert. En raison de ces
défaillances, le système effectue des réinitialisations en boucle. Pour pouvoir reprendre les
opérations dans cette situation, vous devez indiquer à ZFS de ne pas rechercher de pool au
démarrage.

ZFS conserve un cache interne de pools disponibles et de leurs configurations dans
/etc/zfs/zpool.cache. L'emplacement et le contenu de ce fichier sont privés et sujets à
modification. Si le système devient impossible à initialiser, redémarrez au jalon none à l'aide de
l'option d'initialisation -m milestone=none. Une fois le système rétabli, remontez le système de

304

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Réparation d'un système impossible à réinitialiser

fichiers racine en tant que système accessible en écriture, puis renommez ou placez le fichier
/etc/zfs/zpool.cache à un autre emplacement. En raison de ces actions, ZFS oublie
l'existence de pools dans le système, ce qui l'empêche d'accéder au pool défectueux à l'origine du
problème. Vous pouvez ensuite passer à un état normal de système en exécutant la commande
svcadm milestone all. Vous pouvez utiliser un processus similaire lors de l'initialisation à
partir d'une racine de remplacement pour effectuer des réparations.

Une fois le système démarré, vous pouvez tenter d'importer le pool à l'aide de la commande
zpool import. Cependant, dans ce cas, l'erreur qui s'est produite lors de l'initialisation risque de
se reproduire car la commande utilise le même mécanisme d'accès aux pools. Si le système
contient plusieurs pools, procédez comme suit :
■ Renommez ou déplacez le fichier zpool.cache vers un autre emplacement comme décrit

dans le paragraphe ci-dessus.

■ Utilisez la commande fmdump -eV pour afficher les pools présentant des erreurs fatales et

déterminer ainsi quel pool pose des problèmes.
Importez les pools un à un en ignorant ceux qui posent problème, comme décrit dans la
sortie de la commande fmdump.

■

Chapitre 11 • Dépannage d'Oracle Solaris ZFS et récupération de pool

305

306

12C H A P I T R E

1 2

Archivage des instantanés et récupération du
pool racine

Ce chapitre décrit la procédure d'archivage des instantanés qui peuvent être utilisés pour migrer
ou restaurer un système Oracle Solaris 11 en cas de panne système. Les étapes présentées
peuvent servir à constituer un plan de récupération après sinistre de base ou à migrer la
configuration d'un système vers un nouveau périphérique d'initialisation.

Ce chapitre contient les sections suivantes :
■ “Présentation du processus de récupération ZFS” à la page 307
■ “Création d'une archive d'instantanés ZFS pour la récupération” à la page 308
■ “Recréation du pool racine et récupération des instantanés de pool racine” à la page 310

Présentation du processus de récupération ZFS

L'ensemble des données du système de fichiers doivent au minimum être sauvegardées
régulièrement afin de réduire le temps d'inactivité dû à des pannes système. En cas de panne
majeure du système, cette précaution vous permet de restaurer les instantanés du pool racine
ZFS plutôt que de devoir réinstaller le système d'exploitation et recréer la configuration du
système. Restaurez ensuite toutes les données de pool non racine.

Tout système exécutant Oracle Solaris 11 doit faire l'objet d'une sauvegarde et d'un archivage.
Le processus global comprend les étapes suivantes :
■ Création d'une archive d'instantanés ZFS pour les systèmes de fichiers du pool racine et

pour les pools non racine qui doivent être migrés ou récupérés.
Il est recommandé de réitérer l'archivage des instantanés du pool racine après la mise à jour
du système d'exploitation.

■ Enregistrement de l'archive d'instantanés sur un média amovible local, tel qu'un lecteur
USB, ou envoi des instantanés vers un système distant pour une éventuelle extraction.

■ Remplacement des disques ou autres composants système défectueux.

307

Création d'une archive d'instantanés ZFS pour la récupération

■

Initialisation du système cible à partir du média d'installation d'Oracle Solaris 11, création
de nouveaux pools de stockage et récupération des systèmes de fichiers.

■ Configuration minimale de l'initialisation, suite à quoi le système est opérationnel et offre

tous les services en cours d'exécution au moment de l'archivage.

Conditions pour la récupération de pools ZFS
■ Le système archivé et le système de récupération doivent avoir la même architecture et

doivent posséder la configuration minimale requise pour Oracle Solaris 11 en fonction des
plates-formes prises en charge.

■ Les disques de remplacement qui contiendront le nouveau pool de stockage ZFS doivent
avoir une capacité au moins égale au volume des données utilisées dans les pools archivés
(voir ci-après).

■ Un accès root est nécessaire sur les deux systèmes contenant les instantanés archivés et le

système de récupération. Si vous utilisez ssh pour accéder au système distant, vous devrez le
configurer pour l'accès privilégié.

Création d'une archive d'instantanés ZFS pour la récupération

Avant de créer l'instantané du pool racine ZFS, envisagez d'enregistrer les informations
suivantes :
■ Capturez les propriétés du pool racine.

sysA# zpool get all rpool

■

Identifiez la taille et la capacité actuelle du disque du pool racine.

sysA# zpool list

NAME

SIZE ALLOC

FREE CAP DEDUP HEALTH ALTROOT

rpool

74G 5.42G 68.6G

7% 1.00x ONLINE -

■

Identifiez les composants du pool racine.

sysA# zfs list -r rpool

NAME

rpool

USED AVAIL REFER MOUNTPOINT

5.48G 67.4G 75.5K /rpool

rpool/ROOT

3.44G 67.4G

31K legacy

rpool/ROOT/solaris

3.44G 67.4G 3.14G /

rpool/ROOT/solaris/var

303M 67.4G

214M /var

rpool/dump

1.01G 67.4G 1000M -

rpool/export

97.5K 67.4G

32K /rpool/export

rpool/export/home

65.5K 67.4G

32K /rpool/export/home

rpool/export/home/admin 33.5K 67.4G 33.5K /rpool/export/home/admin

rpool/swap

1.03G 67.4G 1.00G -

308

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Création d'une archive d'instantanés ZFS pour la récupération

▼ Création d'une archive d'instantanés ZFS

Les étapes suivantes décrivent la procédure de création d'un instantané récursif du pool racine
incluant tous les systèmes de fichiers du pool racine. D'autres pools non racine peuvent être
archivés de la même manière.

Tenez compte des points suivants :
■ Pour une récupération complète d'un système, envoyez les instantanés vers un pool sur un

système distant.

■ Créez un partage NFS à partir du système distant et, si nécessaire, configurez ssh afin

d'autoriser l'accès privilégié.

■ L'instantané de pool racine récursif est envoyé sous la forme d'un unique fichier

d'instantanés de grande taille vers un système distant. Mais vous pouvez également envoyer
les instantanés récursifs de manière à ce qu'ils soient stockés individuellement sur un
système distant.

Dans les étapes suivantes, l'instantané récursif est nommé rpool@snap1. Le système local à
récupérer est sysA et le système distant est sysB. Notez que rpool est le nom par défaut du pool
racine et qu'il peut être différent sur votre système.

1

2

3

Connectez-vous en tant qu'administrateur.

Créez un instantané récursif du pool racine.

sysA# zfs snapshot -r rpool@rpool.snap1

Si vous le souhaitez, réduisez l'archive d'instantanés en supprimant les instantanés de swap et
de vidage.

sysA# zfs destroy rpool/dump@rpool.snap1

sysA# zfs destroy rpool/swap@rpool.snap1

Le volume de swap ne contient pas de données pertinentes pour une migration ou une
récupération de système. Ne supprimez pas l'instantané du volume de vidage si vous souhaitez
conserver les éventuels vidages sur incident.

4

Envoyez l'instantané de pool racine récursif vers un autre pool sur un autre système.

a. Partagez un système de fichiers sur un système distant en vue de la réception du ou des

instantané(s) :
Dans les étapes suivantes, le système de fichiers /tank/snaps est partagé en vue du stockage
de l'instantané de racine récursif.

sysB# zfs set share=name=snapf,path=/tank/snaps,prot=nfs,root=sysA tank/snaps

sysB# zfs set sharenfs=on tank/snaps

Chapitre 12 • Archivage des instantanés et récupération du pool racine

309

Recréation du pool racine et récupération des instantanés de pool racine

b. Envoyez l'instantané du pool racine récursif vers un système distant.

Envoyez l'instantané récursif vers le système de fichiers distant qui a été partagé à l'étape
précédente.

sysA# zfs send -Rv rpool@rpool.snap1 | gzip > /net/sysB/tank/snaps/

rpool.snap1.gz

sending from @ to rpool@rpool.snap1

sending from @ to rpool/export@rpool.snap1

sending from @ to rpool/export/home@rpool.snap1

sending from @ to rpool/export/home/admin@rpool.snap1

sending from @ to rpool/ROOT@rpool.snap1

sending from @ to rpool/ROOT/solaris@install

sending from @ to rpool/ROOT/solaris@install

sending from @install to rpool/ROOT/solaris@rpool.snap1

sending from @ to rpool/ROOT/solaris/var@install

sending from @install to rpool/ROOT/solaris/var@rpool.snap1

Recréation du pool racine et récupération des instantanés de
pool racine

Au cas où vous devriez recréer votre pool racine et récupérer les instantanés de pool racine, les
étapes générales à effectuer sont les suivantes :
■ Préparation du ou des disques de pool racine de remplacement et recréation du pool racine
■ Restauration des instantanés des systèmes de fichiers du pool racine

■

■

Sélection et activation d l'environnement d'initialisation souhaité
Initialisation du système

▼ Recréation du pool racine sur le système de

récupération
Tenez compte des points suivants lors de la récupération du pool racine.
■ En cas de panne d'un disque de pool racine non redondant, vous devez initialiser le système
à partir d'un média d'installation ou d'un serveur d'installation afin de réinstaller le système
d'exploitation ou de restaurer les instantanés de pool racine précédemment archivés.
Pour plus d'informations sur le remplacement d'un disque sur le système, reportez-vous à la
documentation fournie avec votre matériel.

■ En cas de panne d'un pool racine mis en miroir, vous pouvez remplacer le disque défectueux
pendant que le système est opérationnel. Pour plus d'informations sur le remplacement d'un
disque défectueux dans un pool racine en miroir, reportez-vous à la section “Remplacement
d'un disque dans un pool racine ZFS” à la page 119.

310

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Recréation du pool racine et récupération des instantanés de pool racine

1

2

Identifiez et remplacez le disque de pool racine ou composant système défectueux.
Ce disque est généralement le périphérique d'initialisation par défaut. Vous pouvez également
sélectionner un autre disque, puis redéfinir le périphérique d'initialisation par défaut.

Initialisez le système à partir du média d'installation d'Oracle Solaris 11 en sélectionnant l'une
des options suivantes.
■ Média d'installation DVD ou USB (SPARC ou x86) : insérez le média et sélectionnez le

périphérique approprié en tant que périphérique d'initialisation.
Si un média de texte est utilisé, sélectionnez l'option Shell dans le menu du programme
d'installation en mode texte.

■ Live Media (x86 uniquement) : la session du bureau GNOME peut être utilisée pendant la

procédure de récupération.

■ Programme d'installation automatisée ou copie locale du média AI (SPARC ou x86) : à

partir du menu du programme d'installation en mode texte, sélectionnez l'option Shell. Sur
un système SPARC, initialisez le média AI (localement ou sur le réseau), puis sélectionnez
l'option Shell :

ok boot net:dhcp

.

.

.Welcome to the Oracle Solaris 11 installation menu

1 Install Oracle Solaris

2 Install Additional Drivers

3 Shell

4 Terminal type (currently xterm)

5 Reboot

Please enter a number [1]: 3

3

Préparez le disque du pool racine.

a. Confirmez que le disque de pool racine de remplacement est visible dans l'utilitaire format.

# format

Searching for disks...done

AVAILABLE DISK SELECTIONS:

0. c2t0d0 <FUJITSU-MAY2073RCSUN72G-0401 cyl 14087 alt 2 hd 24 sec 424>

/pci@780/pci@0/pci@9/scsi@0/sd@0,0

1. c2t1d0 <FUJITSU-MAY2073RCSUN72G-0401 cyl 14087 alt 2 hd 24 sec 424>

/pci@780/pci@0/pci@9/scsi@0/sd@1,0

2. c2t2d0 <SEAGATE-ST973402SSUN72G-0400-68.37GB>

/pci@780/pci@0/pci@9/scsi@0/sd@2,0

3. c2t3d0 <SEAGATE-ST973401LSUN72G-0556-68.37GB>

/pci@780/pci@0/pci@9/scsi@0/sd@3,0

Specify disk (enter its number): 0

Chapitre 12 • Archivage des instantanés et récupération du pool racine

311

Recréation du pool racine et récupération des instantanés de pool racine

b. Confirmez que le disque du pool racine possède une étiquette SMI (VTOC) et une tranche 0

contenant la majeure partie de l'espace disque.
Consultez la table de partition pour vous assurer que le disque du pool racine possède une
étiquette SMI et une tranche 0.

selecting c2t0d0

[disk formatted]

format> partition

partition> print

c. Si nécessaire, attribuez une étiquette SMI (VTOC) au disque.

Utilisez les raccourcis de commande suivants pour modifier l'étiquette du disque.
Assurez-vous que vous modifier l'étiquette du bon disque, car ces commandes n'effectuent
aucune vérification d'erreurs.

■

SPARC :

sysA# format -L vtoc -d c2t0d0

Confirmez que la tranche 0 dispose d'un espace disque correctement alloué. La partition
par défaut est appliquée dans la commande ci-dessus, mais risque d'être trop petite pour
la tranche 0 du pool racine. Pour plus d'informations sur la modification de la table de
partition par défaut, reportez-vous à la section “Création d’une tranche de disque pour
un système de fichiers racine ZFS” du manuel Administration d’Oracle Solaris :
Périphériques et systèmes de fichiers.
x86 :

■

sysA# fdisk -B /dev/rdsk/c2t0d0p0

sysA# format -L vtoc -d c2t0d0

Confirmez que la tranche 0 dispose d'un espace disque correctement alloué. La partition
par défaut est appliquée dans la commande ci-dessus, mais risque d'être trop petite pour
la tranche 0 du pool racine. Pour plus d'informations sur la modification de la table de
partition par défaut, reportez-vous à la section “Création d’une tranche de disque pour
un système de fichiers racine ZFS” du manuel Administration d’Oracle Solaris :
Périphériques et systèmes de fichiers.

4

5

6

Recréez le pool racine.

sysA# zpool create rpool c2t0d0s0

Montez le système de fichiers contenant les instantanés depuis le système distant.

sysA# mount -F nfs sysB:/tank/snaps /mnt

Restaurez les instantanés du pool racine.

sysA# gzcat /mnt/rpool.snap1.gz | zfs receive -Fv rpool

receiving full stream of rpool@rpool.snap1 into rpool@rpool.snap1

received 92.7KB stream in 1 seconds (92.7KB/sec)

receiving full stream of rpool/export@rpool.snap1 into rpool/export@rpool.snap1

received 47.9KB stream in 1 seconds (47.9KB/sec)

312

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Recréation du pool racine et récupération des instantanés de pool racine

7

8

9

.

.

.

Définissez la propriété bootfs.

sysA# zpool set bootfs=rpool/ROOT/solaris rpool

Si nécessaire, recréez des périphériques de swap et de vidage.
Par exemple :

sysA# zfs create -V 4G rpool/swap

sysA# zfs create -V 4G rpool/dump

Pour plus d'informations sur le dimensionnement des volumes de swap et de vidage,
reportez-vous à la section “Planification de l’espace de swap” du manuel Administration
d’Oracle Solaris : Périphériques et systèmes de fichiers.

Montez l'environnement d'initialisation.
L'étape suivante requiert que l'environnement d'initialisation soit monté de manière à
permettre l'installation des blocs d'initialisation.

sysA# beadm mount solaris /tmp/mnt

10

Installez les blocs d'initialisation sur le nouveau disque.

■

SPARC :

sysA# installboot /tmp/mnt/usr/platform/‘uname -i‘/lib/fs/zfs/bootblk /dev/rdsk/c2t0d0s0

■

x86 :

sysA# installgrub /tmp/mnt/boot/grub/stage1 /tmp/mnt/boot/grub/stage2

/dev/rdsk/c2t0d0s0

11

Si ce ne sont pas les mêmes périphériques qui seront utilisés ou s'ils seront configurés
différemment sur le système d'origine, effacez les informations de périphérique existantes.
Ensuite, indiquez au système de reconfigurer les nouvelles informations de périphérique.

# devfsadm -Cn -r /tmp/mnt

# touch /tmp/mnt/reconfigure

12

Démontez l'environnement d'initialisation.

#beadm unmount solaris

13

Activez l'environnement d'initialisation, si nécessaire.
Par exemple :

sysA# beadm list

BE

--

Active Mountpoint Space Policy Created

------ ---------- ----- ------ -------

solaris-1 -

solaris

-

-

-

13.26M static 2011-09-28 15:23

3.87G static 2011-09-29 08:20

# beadm activate solaris

Chapitre 12 • Archivage des instantanés et récupération du pool racine

313

Recréation du pool racine et récupération des instantanés de pool racine

14

Assurez-vous que vous pouvez correctement initialiser le système à partir du disque de pool
racine de remplacement.
Si nécessaire, réinitialisez l'unité d'initialisation par défaut :

■

■

SPARC : configurez le système de manière à ce qu'il s'initialise automatiquement à partir du
nouveau disque, soit en utilisant la commande eeprom, soit en utilisant la commande
setenv de la PROM d'initialisation.
x86 : reconfigurez le BIOS du système.

314

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

13C H A P I T R E

1 3

Pratiques recommandées pour Oracle Solaris
ZFS

Ce chapitre décrit les pratiques recommandées pour la création, la surveillance et la gestion de
pools de stockage ZFS et de systèmes de fichiers.

Ce chapitre contient les sections suivantes :
■ “Pratiques recommandées pour les pools de stockage” à la page 315
■ “Pratiques recommandées pour les systèmes de fichiers” à la page 321

Pratiques recommandées pour les pools de stockage

Les sections suivantes décrivent les pratiques recommandées pour la création et la surveillance
de pools de stockage ZFS. Pour plus d'informations sur le dépannage des problèmes de pools de
stockage, reportez-vous au Chapitre 11, “Dépannage d'Oracle Solaris ZFS et récupération de
pool”.

Pratiques recommandées générales
■ Maintenez votre système à jour grâce aux dernières versions et aux correctifs Solaris.
■ Evaluez la mémoire requise en fonction de la charge de travail actuelle du système.

■ Avec une application dont l'encombrement mémoire est connu, telle qu'une application

de base de données par exemple, vous pouvez limiter la taille de l'ARC de manière à ce
que l'application n'ait pas besoin de récupérer la mémoire nécessaire à partir du cache
ZFS.

■ Tenez compte de la mémoire requise pour la suppression des doublons.

■

Identifiez l'utilisation de la mémoire par ZFS à l'aide de la commande suivante :

# mdb -k

> ::memstat

Page Summary

Pages

MB %Tot

315

Pratiques recommandées pour les pools de stockage

------------

---------------- ---------------- ----

Kernel

ZFS File Data

Anon

Exec and libs

Page cache

Free (cachelist)

388117

81321

29928

1359

4890

6030

1516

19%

317

116

5

19

23

4%

1%

0%

0%

0%

Free (freelist)

1581183

6176

76%

Total

Physical

> $q

2092828

2092827

8175

8175

■ Envisagez l'utilisation de mémoire ECC pour prévenir la corruption de mémoire. La

corruption de mémoire silencieuse peut endommager vos données.

■ Effectuez des sauvegardes régulières : bien qu'un pool créé avec redondance ZFS permette
de réduire le temps d'inactivité dû à des pannes matérielles, il n'est pas à l'abri des pannes
matérielles, des pannes d'alimentation ou des déconnexions de câbles. Veillez à sauvegarder
régulièrement vos données. Toutes les données importantes doivent être sauvegardées. Il
existe différentes méthodes de copie des données :
■ Prise quotidienne ou à intervalles réguliers d'instantanés ZFS.

■

Sauvegardes hebdomadaires des données du pool ZFS. Vous pouvez utiliser la
commande zpool split pour créer une copie exacte du pool de stockage ZFS mis en
miroir.
Sauvegardes mensuelles à l'aide d'un produit de sauvegarde mis en oeuvre à l'échelle de
l'entreprise.
■ RAID matériel

■

■ Envisagez l'utilisation du mode JBOD pour les baies de stockage plutôt que des baies

RAID matérielles, afin que ZFS puisse gérer le stockage et la redondance.

■ Utilisez la redondance matérielle RAID et/ou la redondance ZFS.
■ L'utilisation de la redondance ZFS présente de nombreux avantages : pour les

environnements de production, configurez ZFS de manière à lui permettre de réparer les
incohérences de données. Utilisez la redondance ZFS, telle que RAIDZ, RAIDZ-2,
RAIDZ-3, la mise en miroir, quel que soit le niveau RAID mis en oeuvre sur le
périphérique de stockage sous-jacent. Avec une telle redondance, ZFS est en mesure de
détecter et de réparer les défaillances du périphérique de stockage sous-jacent ou des
connexions à l'hôte de celui-ci.

■ Les vidages sur incident consomment davantage d'espace disque, généralement entre 1/2 et

3/4 de la taille de la mémoire physique.

Pratiques de création de pools de stockage ZFS
Les sections suivantes présentent des pratiques recommandées générales et plus spécifiques
pour les pools de stockage

316

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Pratiques recommandées pour les pools de stockage

Pratiques recommandées générales pour les pools de stockage
■ Utilisez des disques entiers pour activer la mise en cache et faciliter la maintenance. La
création de pools sur des tranches complique la gestion et la récupération des disques.

■ Utilisez la redondance ZFS pour permettre à ZFS de réparer les incohérences de données.

■ Le message suivant s'affiche lorsqu'un pool non redondant est créé :

# zpool create tank c4t1d0 c4t3d0

’tank’ successfully created, but with no redundancy; failure

of one device will cause loss of the pool

■ Pour des pools mis en miroir, utilisez des paires de disques mis en miroir
■ Pour des pools RAIDZ, regroupez 3 à 9 disques par VDEV

■ Utilisez des disques hot spare pour réduire le temps d'inactivité dû aux pannes matérielles.
■ Utilisez des disques de taille similaire afin que de répartir les E/S de façon équilibrée entre les

périphériques.
■ Des LUN de petite taille peuvent être étendus en LUN de grande taille
■ N'étendez pas des LUN de façon excessive, comme par exemple de 128 Mo à 2 To, de

manière à conserver des tailles de metaslabs optimales

■ Envisagez la création d'un petit pool racine et de pools de données plus volumineux pour

assurer une récupération plus rapide du système

Pratiques recommandées pour la création de pool racine
■ Créez des pools racine comportant des tranches à l'aide de l'identificateur s*. N'utilisez pas

l'identificateur p*. Le pool racine ZFS d'un système est généralement créé au moment de
l'installation du système. Si vous êtes en train de créer un second pool racine ou de recréer
un pool racine, utilisez une syntaxe semblable à la suivante :

# zpool create rpool c0t1d0s0

Sinon, créez un pool racine mis en miroir. Par exemple :

# zpool create rpool mirror c0t1d0s0 c0t2d0s0

■ Le pool racine doit être créé sous la forme d'une configuration en miroir ou d'une

configuration à disque unique. Les configurations RAID-Z ou entrelacées ne sont pas
prises en charge. Vous ne pouvez pas ajouter d'autres disques mis en miroir pour créer
plusieurs périphériques virtuels de niveau supérieur à l'aide de la commande zpool add.
Toutefois, vous pouvez étendre un périphérique virtuel mis en miroir à l'aide de la
commande zpool attach.

■ Un pool racine ne peut pas avoir de périphérique de journal distinct.
■ Les propriétés d'un pool peuvent être définies lors d'une installation AI, mais
l'algorithme de compression gzip n'est pas pris en charge sur les pools racine.

Chapitre 13 • Pratiques recommandées pour Oracle Solaris ZFS

317

Pratiques recommandées pour les pools de stockage

■ Ne renommez pas le pool racine une fois qu'il a été créé par une installation initiale. Si

vous renommez le pool racine, cela peut empêcher l'initialisation du système.

Pratiques recommandées pour la création de pools non racine
■ Créez des pools non racine avec des disques entiers à l'aide de l'identificateur d*. N'utilisez

pas l'identificateur p*.
■ ZFS fonctionne mieux sans logiciel de gestion de volumes supplémentaire.
■ Pour de meilleures performances, utilisez des disques individuels ou, tout au moins, des
LUN constitués d'un petit nombre de disques. En offrant à ZFS un meilleur aperçu de la
configuration des LUN, vous lui permettez de prendre de meilleures décisions de
planification d'E/S.

■ Créez des configurations de pools redondants dans plusieurs contrôleurs afin de réduire

le temps d'inactivité dû à une panne de contrôleur.

■ Pools de stockage mis en miroir : consomment davantage d'espace disque mais

présentent de meilleures performances pour les petites lectures aléatoires.

# zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0

■ Pools de stockage RAID-Z : ces pools peuvent être créés avec 3 stratégies de parité,
d'une parité égale à 1 (raidz), 2 (raidz2) ou 3 (raidz3). Une configuration RAID-Z
optimise l'espace disque et fournit généralement des performances satisfaisantes lorsque
les données sont écrites et lues en gros blocs (128 Ko ou plus).
■ Prenons l'exemple d'une configuration RAID-Z à parité simple (raidz) avec 2

périphériques virtuels à 3 disques (2+1) chacun.

# zpool create rzpool raidz1 c1t0d0 c2t0d0 c3t0d0 raidz1 c1t1d0 c2t1d0 c3t1d0

■ Une configuration RAIDZ-2 améliore la disponibilité des données et offre les mêmes

performances qu'une configuration RAID-Z. En outre, sa valeur de temps moyen
entre pertes de données MTTDL (Mean Time To Data Loss) est nettement meilleure
que celle d'une configuration RAID-Z ou de miroirs bidirectionnels. Créez une
configuration RAID-Z à double parité RAID-Z (raidz2) à 6 disques (4+2).

# zpool create rzpool raidz2 c0t1d0 c1t1d0 c4t1d0 c5t1d0 c6t1d0 c7t1d0

raidz2 c0t2d0 c1t2d0 c4t2d0 c5t2d0 c6t2d0 c7t2d

■ La configuration RAIDZ-3 optimise l'espace disque et offre une excellente

disponibilité car elle peut résister à 3 pannes de disque. Créez une configuration
RAID-Z à triple parité (raidz3) à 9 disques (6+3).

# zpool create rzpool raidz3 c0t0d0 c1t0d0 c2t0d0 c3t0d0 c4t0d0

c5t0d0 c6t0d0 c7t0d0 c8t0d0

318

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Pratiques recommandées pour les pools de stockage

Pratiques recommandées pour la création de pools pour une base de
données Oracle
Tenez compte des pratiques recommandées pour la création de pools de stockage suivantes
lorsque vous créez une base de données Oracle.
■ Utilisez un pool mis en miroir ou un RAID matériel pour plusieurs pools
■ Les pools RAID-Z ne sont généralement pas recommandés pour les charges de travail en

lecture aléatoire

■ Créez un petit pool distinct avec un périphérique de journalisation distinct pour les fichiers

de journalisation de la base de données

■ Créez un petit pool distinct pour le journal d'archivage

Pour plus d'informations, consultez le livre blanc suivant :

http://blogs.oracle.com/storage/entry/new_white_paper_configuring_oracle

Pratiques recommandées pour l'optimisation des
performances des pools de stockage
■ N'utilisez pas plus de 80 % de la capacité d'un pool pour des performances optimales.
■ Les pools mis en miroir sont à préférer aux pools RAID-Z pour des charges de travail en

lecture/écriture aléatoires

■ Périphériques de journalisation distincts

■ Recommandés pour améliorer les performances d'écriture synchrone
■ Avec une charge d'écriture synchrone élevée, empêche la fragmentation de l'écriture de

nombreux blocs de journal dans le pool principal

■ Des périphériques de cache distincts sont recommandés pour améliorer les performances de

lecture

■

■ Nettoyage/réargenture : un très grand pool RAID-Z comportant un grand nombre de

périphériques nécessite des temps de nettoyage et de réargenture plus long
Si les performances du pool sont ralenties : utilisez la commande zpool status pour
éliminer les problèmes matériels à l'origine des problèmes de performances du pool. Si la
commande zpool status ne fait apparaître aucun problème, utilisez la commande fmdump
pour afficher les pannes matérielles ou utilisez la commande fmdump -eV pour repérer les
éventuelles défaillances matérielles qui n'ont pas encore été signalées en tant qu'erreur.

Chapitre 13 • Pratiques recommandées pour Oracle Solaris ZFS

319

Pratiques recommandées pour les pools de stockage

Pratiques recommandées pour la maintenance et la
gestion d'un pool de stockage ZFS
■ Assurez-vous que la capacité d'un pool est inférieure à 80 %, pour obtenir de meilleures

performances.
Les performances d'un pool peuvent se dégrader lorsque le pool est très plein et que les
systèmes de fichiers sont fréquemment mis à jour, comme c'est le cas par exemple pour un
serveur de courrier très actif. Des pools pleins peuvent entraîner une baisse des
performances, mais aucun autre problème. Si la charge de travail principale consiste en des
fichiers immuables, maintenez un taux d'utilisation du pool de 95 à 96 %. Même avec un
contenu essentiellement statique et un taux d'utilisation de 95 à 96 %, les performances
d'écriture, de lecture et de réargenture risquent de se dégrader.
■ Contrôlez l'espace des pools et des systèmes de fichiers pour vous assurer qu'il n'est pas

entièrement utilisé.

■ Vous pouvez envisager d'utiliser des quotas et des réservations ZFS afin d'être sûr que

l'espace du système de fichiers ne dépasse pas 80 % de la capacité du pool.

■

Surveillez la santé du pool

■

■

Surveillez les pools redondants sur une base hebdomadaire à l'aide de zpool status et

fmdump

Surveillez les pools non redondants toutes les deux semaines à l'aide de zpool status et

fmdump

■

■ Exécutez régulièrement zpool scrub pour repérer les problèmes d'intégrité des données.
Si vous utilisez des lecteurs de qualité grand public, envisagez de planifier un nettoyage
hebdomadaire.
Si vous utilisez des lecteurs de qualité professionnelle, envisagez de planifier un
nettoyage hebdomadaire.

■

■ Vous devez également exécuter un nettoyage avant de remplacer des périphériques ou

de réduire temporairement la redondance d'un pool, afin d'assurer que tous les
périphériques sont alors opérationnels.

■

Surveillance des défaillances de pools ou de périphériques : utilisez zpool status comme
décrit ci-dessous. Utilisez également les commandes fmdump ou fmdump -eV pour vérifier
l'absence de défauts et d'erreurs au niveau des périphériques.

■

■

Surveillez la santé des pools redondants sur une base hebdomadaire à l'aide de zpool
status et fmdump
Surveillez la santé des pools non redondants toutes les deux semaines à l'aide de zpool
status et fmdump

■ Le périphérique de pool est UNAVAIL ou OFFLINE : si un périphérique de pool n'est pas

disponible, vérifiez si le périphérique est répertorié dans la sortie de la commande format. Si
le périphérique n'apparaît pas dans la sortie de format, il n'est pas visible sur ZFS.

320

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Pratiques recommandées pour les systèmes de fichiers

L'état de périphérique de pool UNAVAIL ou OFFLINE signifie généralement que le
périphérique est en panne, qu'un câble est déconnecté ou qu'un autre problème matériel, tel
qu'un câble ou un contrôleur défectueux, a rendu inaccessible le périphérique.

■ Envisagez de configurer le service smtp-notify de manière à ce qu'il vous informe lorsqu'un

composant matériel est diagnostiqué comme défectueux. Pour plus d'informations,
reportez-vous à la rubrique Paramètres de notification des pages de manuel smf(5) et
smtp-notify(1M).
Par défaut, certaines notifications sont configurées automatiquement pour être envoyées à
l'utilisateur root. Si vous ajoutez un alias pour votre compte utilisateur en tant qu'utilisateur
root dans le fichier /etc/aliases, vous recevrez par courrier électronique des notifications
semblables à la suivante :

-------- Original Message --------

Subject: Fault Management Event: tardis:SMF-8000-YX

Date: Wed, 21 Sep 2011 11:11:27 GMT

From: No Access User <noaccess@tardis.drwho.COM>

Reply-To: root@tardis.drwho.COM

To: root@tardis.drwho.COM

SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Wed Sep 21 11:11:27 GMT 2011

PLATFORM: Sun-Fire-X4140, CSN: 0904QAD02C, HOSTNAME: tardis

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: d9e3469f-8d84-4a03-b8a3-d0beb178c017

DESC: A ZFS device failed. Refer to http://sun.com/msg/ZFS-8000-D3

for more information.

AUTO-RESPONSE: No automated response will occur.

IMPACT: Fault tolerance of the pool may be compromised.

■

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Surveillez l'espace du pool de stockage : utilisez les commandes zpool list et zfs list
pour déterminer la quantité d'espace disque utilisée par les données des systèmes de fichiers.
Les instantanés ZFS peuvent consommer de l'espace disque et, lorsqu'ils ne sont pas
répertoriés par la commande zfs list, peuvent consommer de l'espace disque de manière
silencieuse. Utilisez la commande d'instantané zfs list - t pour identifier l'espace disque
consommé par des instantanés.

Pratiques recommandées pour les systèmes de fichiers

Les sections suivantes décrivent les pratiques recommandées pour les systèmes de fichiers.

Pratiques recommandées pour la création de
systèmes de fichiers
Les sections suivantes décrivent les pratiques recommandées pour la création de systèmes de
fichiers ZFS.
■ Créez un système de fichiers par utilisateur pour les répertoires personnels

Chapitre 13 • Pratiques recommandées pour Oracle Solaris ZFS

321

Pratiques recommandées pour les systèmes de fichiers

■ Envisagez d'utiliser des quotas et des réservations de systèmes de fichiers pour gérer et

réserver de l'espace disque pour les systèmes de fichiers importants.

■ Envisagez de définir des quotas par utilisateur ou par groupe pour gérer l'espace disque dans

un environnement comptant de nombreux utilisateurs.

■ Utilisez l'héritage des propriétés ZFS pour appliquer des propriétés à un grand nombre de

systèmes de fichiers descendants.

Pratiques recommandées pour la création de systèmes de fichiers pour
une base de données Oracle
Tenez compte des pratiques recommandées pour la création de systèmes de fichiers suivantes
lorsque vous créez une base de données Oracle.
■ Faites correspondre la propriété recordsize ZFS à la taille db_block_size Oracle.
■ Créez des systèmes de fichiers pour la table de base données et l'index dans le pool de base de
données principal, en utilisant une taille recordsize de 8 Ko et la valeur primarycache par
défaut.

■ Créez des systèmes de fichiers pour les données temporaires et l'espace de la table

d'annulation dans le pool de base de données principal, en utilisant les valeurs recordsize
et primarycache par défaut.

■ Créez un système de fichiers pour le journal d'archivage dans le pool d'archivage, en activant

la compression et la valeur recordsize par défaut et en définissant primarycache sur
metadata.

Pour plus d'informations, consultez le livre blanc suivant :

http://blogs.oracle.com/storage/entry/new_white_paper_configuring_oracle

Pratiques recommandées pour la surveillance de
systèmes de fichiers ZFS
Il est recommandé de surveiller les systèmes de fichiers ZFS pour s'assurer qu'ils sont
disponibles et pour identifier les problèmes de consommation d'espace.
■ Une fois par semaine, contrôlez l'espace disponible des systèmes de fichiers à l'aide des
commandes zpool list et zfs list au lieu des commandes du et df car les anciennes
commandes ne tiennent pas compte de l'espace consommé par les systèmes de fichiers
descendants ou les instantanés.

■ Affichez la consommation d'espace des systèmes de fichiers à l'aide de la commande zfs

list -o space.

■ L'espace des systèmes de fichiers peut être consommé par des instantanés à votre insu. Vous
pouvez afficher toutes les informations sur les systèmes de fichiers en respectant la syntaxe
suivante :

322

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Pratiques recommandées pour les systèmes de fichiers

# zfs list -t all

■ Un système de fichiers /var distinct est automatiquement créé lorsqu'un système est

installé, mais il est recommandé de définir un quota et une réservation sur ce système de
fichiers pour s'assurer qu'il ne consomme pas d'espace de pool root à votre insu.

■ En outre, vous pouvez utiliser la commande fsstat pour afficher les activités de traitement

de fichiers des systèmes de fichiers ZFS. Les activités peuvent être consignées par point de
montage ou par type de système de fichiers. L'exemple suivant illustre les activités générales
de système de fichiers ZFS :

# fsstat /

new name

name attr attr lookup rddir read read write write

file remov chng

get

set

ops

ops

ops bytes

ops bytes

832

589

286 837K 3.23K 2.62M 20.8K 1.15M 1.75G 62.5K 348M /

■

Sauvegardes
■ Conservez des instantanés des systèmes de fichiers
■ Envisagez d'effectuer des sauvegardes hebdomadaires et mensuelles à l'aide de logiciels

mis en oeuvre à l'échelle de l'entreprise
Stockez des instantanés des pools racine sur un système distant pour la récupération à
chaud

■

Chapitre 13 • Pratiques recommandées pour Oracle Solaris ZFS

323

324

AA N N E X E A

Descriptions des versions d'Oracle Solaris ZFS

Cette annexe décrit les versions ZFS disponibles, les fonctionnalités de chacune d'entre elles et
le SE Solaris correspondant.

L'annexe contient les sections suivantes :
■ “Présentation des versions ZFS” à la page 325
■ “Versions de pool ZFS” à la page 325
■ “Versions du système de fichiers ZFS” à la page 327

Présentation des versions ZFS

Les nouvelles fonctions de pool et de système de fichiers ZFS ont été introduites dans certaines
versions du système de fichiers ZFS disponibles avec Solaris. Vous pouvez utiliser la commande
zpool upgrade ou zfs upgrade pour déterminer si la version d'un pool ou un système de
fichiers est antérieure à la version Solaris en cours d'exécution. Vous pouvez également utiliser
ces commandes pour mettre à niveau la version de votre pool et de votre système de fichiers.

Pour plus d'informations sur l'utilisation des commandes zpool upgrade et zfs upgrade,
reportez-vous aux sections “Mise à niveau des systèmes de fichiers ZFS” à la page 198 et “Mise à
niveau de pools de stockage ZFS” à la page 109.

Versions de pool ZFS

Le tableau suivant présente une liste des versions de pool ZFS disponibles dans la version
d'Oracle Solaris.

Version

Oracle Solaris 11

Description

1

snv_36

Version ZFS initiale

325

Versions de pool ZFS

Version

Oracle Solaris 11

Description

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

snv_38

snv_42

snv_62

snv_62

snv_62

snv_68

snv_69

snv_77

snv_78

snv_94

snv_96

snv_98

snv_103

snv_114

snv_116

snv_120

snv_121

snv_125

snv_128

snv_128

snv_128

snv_135

snv_137

snv_140

snv_141

snv_145

snv_147

snv_148

Blocs ditto (métadonnées répliquées)

Disques hot spare et RAID-Z à double parité

zpool history

Algorithme de compression gzip

Propriété de pool bootfs

Périphérique de journalisation de tentatives distincts

Administration déléguée

Propriétés refquota et refreservation

Périphériques de cache

Performances de nettoyage améliorées

Propriétés d'instantané

Propriété snapused

Propriété Aclinherit passthrough-x

Comptabilisation des espaces de groupe et d'utilisateur

Propriété stmf

RAID-Z triple parité

Instantanés conservés par l'utilisateur

Suppression d'un périphérique de journalisation

Algorithme de compression zle (encodage de chaîne vide)

Suppression des doublons

Propriétés reçues

Slim ZIL

Attributs système

Statistiques de nettoyage améliorées

Performances de suppression des instantanés améliorées

Performances de création d'instantanés améliorées

Remplacements de vdev multiples

Programme d'allocation hybride de miroir/RAID-Z

326

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Versions du système de fichiers ZFS

Version

Oracle Solaris 11

30

31

32

33

snv_149

snv_150

snv_151

snv_163

Description

Chiffrement

Performances "zfs list" améliorées

Taille de bloc d'1 Mo

Prise en charge améliorée du partage

Versions du système de fichiers ZFS

Le tableau suivant répertorie les versions du système de fichiers ZFS disponibles dans la version
d'Oracle Solaris.

Version

Oracle Solaris 11

Description

1

2

3

4

5

snv_36

snv_69

snv_77

snv_114

snv_137

Version initiale du système de fichiers ZFS

Entrées de répertoire améliorées

Insensibilité à la casse et identificateur unique de système de
fichiers (FUID)

Propriétés userquota et groupquota

Attributs système

Annexe A • Descriptions des versions d'Oracle Solaris ZFS

327

328

Index

A
Accès

ACL

Instantané ZFS

Exemple, 203

ACL dans un fichier ZFS

Description détaillée, 230, 231

aclinherit, propriété, 228
Configuration d'ACL dans un fichier ZFS (mode

Configuration d'héritage d'ACL dans un fichier ZFS

compact)
Description, 242
Exemple, 243

(mode détaillé)
Exemple, 236

détaillé)
Description, 231

Configuration de fichiers ZFS

Description, 229

Définition des ACL sur un fichier ZFS (mode

Description, 221
Description de format, 223
Différences avec les ACL POSIX-draft, 222
Héritage d'ACL, 227
Indicateur d'héritage d'ACL, 227
Modification d'ACL triviale dans un fichier ZFS

(mode détaillé)
Exemple, 232

Privilèges d'accès, 224
Propriété d'ACL, 228
Restauration d'une ACL triviale sur un fichier ZFS

(mode détaillé)

ACL, Restauration d'une ACL triviale sur un fichier ZFS
(mode détaillé) (Suite)

Exemple, 235

Type d'entrée, 224

ACL, mode de propriété, aclinherit, 138
ACL NFSv4

Description de format, 223
Différences avec les ACL POSIX-draft, 222
Héritage d'ACL, 227
Indicateur d'héritage d'ACL, 227
Modèle

Description, 221
Propriété d'ACL, 228

ACL POSIX-draft, Description, 222
ACL Solaris

Description de format, 223
Différences avec les ACL POSIX-draft, 222
Héritage d'ACL, 227
Indicateur d'héritage d'ACL, 227
Nouveau modèle

Description, 221
Propriété d'ACL, 228

aclinherit, propriété, 228
Administration déléguée , présentation, 251
Administration déléguée de ZFS, présentation, 251
Administration simplifiée, Description, 27
Affichage

Autorisations déléguées (exemple), 260
Etat de maintenance d'un pool de stockage ZFS

Etat détaillé du fonctionnement du pool de stockage

Exemple, 96

ZFS

329

Index

Affichage, Etat détaillé du fonctionnement du pool de
stockage ZFS (Suite)
Exemple, 97

Etat fonctionnel d'un pool de stockage

canmount (propriété), Description détaillée, 154
casesensitivity, propriété, Description, 140
checksum, propriété, Description, 140
Clone

Rapport syslog de messages d'erreur ZFS

Description, 95

Description, 286

Statistiques d'E/S à l'échelle du pool de stockage ZFS

Création, exemple, 207
Définition, 27
Destruction, exemple, 208
Fonction, 206

Comportement d'espace saturé, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 41

Composant ZFS, Convention d'attribution de nom, 29
Composants, Pool de stockage ZFS, 43
compression, propriété, Description, 141
compressratio, propriété, Description, 141
Comptabilisation de l'espace ZFS, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 40

Configuration

ACL dans un fichier ZFS

Description, 229

Description, 242
Exemple, 243

ACL dans un fichier ZFS (mode compact)

Héritage d'ACL dans un fichier ZFS (mode détaillé)

Exemple, 236

Configuration en miroir

Description, 48
Fonction de redondance, 48
Vue conceptuelle, 48
Configuration RAID-Z

Double parité, description, 48
Exemple, 53
Fonction de redondance, 48
Parité simple, description, 48
Vue conceptuelle, 48

Configuration RAID-Z, ajout de disques, Exemple, 65
Connexion

Périphérique, à un pool de stockage ZFS (zpool

attach)
Exemple, 68

Contrôle

Intégrité des données ZFS, 279
Validation des données (nettoyage), 280

Exemple, 93

Exemple, 94

Statistiques d'E/S de pool de stockage vdev ZFS

Statistiques d'E/S de pools de stockage ZFS

Description, 92

Ajout

Disques, configuration RAID-Z (exemple), 65
Périphérique à un pool de stockage ZFS (zpool add)

Périphérique de journalisation mis en miroir

Exemple, 63

(exemple), 66

Périphériques de cache (exemple), 67
Système de fichiers ZFS à une zone non globale

Volume ZFS à une zone non globale

Exemple, 269

Exemple, 270

Ajustement, Tailles des périphériques de swap et de

vidage, 123

allocated (propriété), description, 84
altroot, propriété (description), 85
atime, Propriété, Description, 138
autoreplace (propriété), description, 85
Autorétablissement, Description, 50
available (propriété), Description, 139

B
Blocs d'initialisation, Installation avec installboot et

installgrub, 126

bootfs, propriété, description, 85

C
cachefile (propriété), description, 85
canmount, propriété, Description, 139

330

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Index

Convention d'attribution de nom, Composant ZFS, 29
copies, propriété, Description, 141
Création

dedupratio, propriété, description, 85
Définition

ACL sur un fichier ZFS (mode détaillé)

Clone ZFS, exemple, 207
Hiérarchie d'un système de fichiers ZFS, 35
Instantané ZFS

Nouveau pool de stockage mis en miroir (zpool

Description, 231
atime, propriété ZFS

exemple, 163

compression, propriété

Exemple, 36

mountpoint, propriété, 36
Point de montage ZFS (zfs set mountpoint)

Exemple, 169

Points de montage hérités

Exemple, 170

quota, propriété (exemple), 37
Quota d'un système de fichiers ZFS (zfs set quota)

Exemple, 183

Réservation de système de fichiers ZFS

Pool de stockage avec périphérique de cache

Pool de stockage RAID-Z à double parité (zpool

Pool de stockage RAID-Z à parité simple (zpool

Pool de stockage RAID-Z à triple parité (zpool

Exemple, 200

split)
Exemple, 70

(exemple), 56

create, commande)
Exemple, 53

create)
Exemple, 53

create, commande)
Exemple, 53

Pool de stockage ZFS
Description, 51

Pool de stockage ZFS (zpool create)

Exemple, 33, 51

Pool de stockage ZFS avec périphériques de

journalisation (exemple), 54

Pool de stockage ZFS mis en miroir (zpool create)

Exemple, 51

Pool racine de remplacement

Exemple, 275

Système de fichiers ZFS, 36

Description, 134
Exemple, 134

Exemple, 33

Volume ZFS

Exemple, 265

creation (propriété), description, 141

Système de fichiers ZFS de base (zpool create)

D
dedup, propriété, Description, 141
dedupditto propriété, description, 85

Exemple, 187

sharenfs, propriété

Exemple, 36

Définition de propriétés ZFS

aclinherit, 138
canmount, 139
checksum, 140
devices, 142
mountpoint, 143
recordsize, 145
reservation, 146
snapdir, 148
version, 149
zoned, 150

Délégation

Autorisations (exemple), 257
Jeu de données à une zone non globale

Exemple, 270

Délégation d'autorisations, zfs allow, 255
Délégation d'autorisations à un groupe, Exemple, 257
Délégation d'autorisations à un utilisateur individuel,

Exemple, 257

Démontage

Système de fichiers ZFS

Exemple, 172

331

Index

Dépannage

Détection de problèmes éventuels (zpool status

Déterminer si un périphérique peut être remplacé

-x), 283

Description, 292

Panne ZFS, 277
Périphérique endommagé, 278
Périphérique manquant (faulted), 278
Problème d'identification, 282
Remplacement d'un périphérique (zpool replace)

Réparation d'un système qui ne peut être initialisé

Exemple, 298

Description, 304

Réparation d'une configuration ZFS

endommagée, 287

Réparation de dommages au niveau d'un pool

Description, 304

Destruction

Clone ZFS, exemple, 208
Instantané ZFS

Exemple, 201

Pool de stockage ZFS
Description, 51

Exemple, 62

Système de fichiers ZFS

Exemple, 135

Pool de stockage ZFS (zpool destroy)

dépendants
Exemple, 136

Détection

Niveaux de réplication incohérents

Exemple, 60

Périphérique en cours d'utilisation

Exemple, 59

Détermination

Type de panne de périphérique

Description, 290

Déterminer

Remplacement d'un périphérique

Description, 292

devices, propriété, Description, 142

332

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Différences entre le système de fichiers ZFS et les
systèmes de fichiers standard, Nouveau modèle
d'ACL standard, 42

Différences entre un système de fichiers classique et

ZFS, Granularité d'un système de fichiers, 39
Différences existant entre les systèmes de fichiers

traditionnels et ZFS
Comportement d'espace saturé, 41
Comptabilisation de l'espace ZFS, 40
Gestion d'un volume traditionnel, 42
Montage d'un système de fichiers ZFS, 42

Disque, Composant de pool de stockage ZFS, 45
Disque entier, Composant de pool de stockage ZFS, 45
Disque hot spare

Création

Exemple, 79

Description

Exemple, 79

Données

Corruption, 278
Corruption identifiée (zpool status -v)

Exemple, 285

Nettoyage

Exemple, 280

Réargenture

Description, 281

Réparation, 279
Validation (nettoyage), 280

Périphérique d'un pool de stockage ZFS (zpool

E
Effacement

clear)
Description, 76

Enregistrement

Données d'un système de fichiers ZFS (zfs send)

Exemple, 212

Vidage mémoire sur incident
savecore, commande, 124

Entrelacement dynamique

Description, 50

Système de fichiers ZFS comportant des systèmes

vidage, 124

dumpadm, commande, Activation d'un périphérique de

Entrelacement dynamique (Suite)

Fonction de pool de stockage, 50

Envoi et réception

Données d'un système de fichiers ZFS

Description, 209

Etiquette EFI

Description, 44
Interaction avec ZFS, 44

exec, propriété, Description, 142
Exigences matérielles et logicielles, 32
Exportation

Pool de stockage ZFS

Exemple, 101

F
failmode (propriété), description, 86
Fichiers, Composants de pools de stockage ZFS, 46
Fonctionnalités des réplications ZFS, Mise en miroir ou

RAID-Z, 47

free (propriété), description, 86

G
Gestion d'un volume traditionnel, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 42

Granularité d'un système de fichiers, Différences entre

un système de fichiers classique et ZFS, 39

H
Héritage

Propriétés ZFS (zfs inherit)

Description, 164

Hiérarchie d'un système de fichiers, Création, 35

Index

I
Identification

Pool de stockage ZFS à importer (zpool import -a)

Exemple, 101

Stockage requis, 33
Type de corruption de données (zpool status -v)

Exemple, 300

Importation

Pool de stockage ZFS

Exemple, 104

Pool de stockage ZFS, à partir de répertoires

alternatifs (zpool import -d)
Exemple, 103

Pool racine de remplacement

Exemple, 275

Initialisation

Environnement d'initialisation ZFS avec boot -L et

boot -Z sur un système SPARC, 127

Système de fichiers racine, 125
Installation des blocs d'initialisation

installboot et installgrup

Exemple, 126

Instantané

Accès

Exemple, 203

Comptabilisation d'espace, 204
Création

Exemple, 200

Définition, 29
Destruction

Exemple, 201

Fonction, 199
Renommer

Exemple, 202

Restauration

Exemple, 205

J
Jeu d'autorisations défini, 251
Jeu de données

Définition, 28
Description, 134

Jeux de données, types, Description, 162

333

Index

Journal d'intention ZFS (ZIL), Description, 54

L
Lecture seule, propriétés ZFS

compression, 141
Description, 151
origin, 144
referenced, 145
type, 148

Liste

Descendants des systèmes de fichier ZFS

(Exemple de), 161

Informations sur le pool ZFS, 34
Pool de stockage ZFS
Description, 87

Pools de stockage ZFS

Exemple, 88

Propriétés ZFS (zfs list)

Exemple, 165

Propriétés ZFS par valeur de source

Exemple, 167

Propriétés ZFS pour l'exécution de scripts

Exemple, 167

Système de fichiers ZFS

Exemple, 160

Système de fichiers ZFS (zfs list)

Système de fichiers ZFS sans l'en-tête

Types de systèmes de fichiers ZFS

Eexemple, 37

Exemple, 162

Exemple, 162

listsnapshots, propriété, description, 86
logbias, propriété, Description, 142

Mise à niveau (Suite)

Systèmes de fichiers ZFS

Description, 198

Mise en ligne d'un périphérique

Pool de stockage ZFS (zpool online)

Exemple, 75

Mise en ligne et hors ligne de périphérique

Mise hors ligne d'un périphérique (zpool offline)

Pool de stockage ZFS
Description, 73

Pool de stockage ZFS

Exemple, 74

mlslabel, propriété, Description, 143
Mode de panne

Données corrompues, 278
Périphérique endommagé, 278
Périphérique manquant (faulted), 278
Mode de propriétés d'ACL, aclmode, 138
Modèle d'ACL Solaris, Différences entre le système de
fichiers ZFS et les systèmes de fichiers standard, 42

ACL triviale dans un fichier ZFS (mode détaillé)

Modification

Exemple, 232
Modification du nom

Système de fichiers ZFS

Exemple, 136

Montage

Système de fichiers ZFS

Exemple, 171

Montage d'un système de fichiers ZFS, Différences

existant entre les systèmes de fichiers traditionnels et
ZFS, 42

mounted (propriété), Description, 143
mountpoint, propriété, Description, 143

M
Migration d'un pool de stockage ZFS, Description, 99
Miroir, Définition, 28
Mise à niveau

Pool de stockage ZFS
Description, 109

N
Nettoyage

Exemple, 280
Validation des données, 280

Nettoyage et réargenture, Description, 281
Niveaux de réplication incohérents

Détection

Exemple, 60

334

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Notification

périphérique reconnecté dans ZFS(zpool online)

Exemple, 289

O
origin, propriété, Description, 144

Périphériques de swap et de vidage (Suite)

Description, 122
Points à prendre en compte, 122

Point de montage
Héritage, 169
Par défaut pour un système de fichiers ZFS, 135
Valeur par défaut des pools de stockage ZFS, 61

P
Package de flux

Récursif, 211
Réplication, 211

Package de flux de réplication, 211
Package de flux récursif, 211
Panne, 277
Paramètre

Partage d'un pool de stockage mis en miroir

Quota ZFS

Exemple, 164

(zpool split)
Exemple, 70

Partage de systèmes de fichiers ZFS, sharesmb,

propriété, 158

Périphérique de cache

Considérations d'utilisation, 56
Création d'un pool (exemple), 56

Périphérique de journalisation mis en miroir, ajout,

Exemple, 66

Périphérique en cours d'utilisation

Détection

Exemple, 59
Périphérique virtuel

Composant de pools de stockage ZFS, 58
Définition, 29

Périphériques de cache , suppression, Exemple, 67
Périphériques de cache, ajout, Exemple, 67
Périphériques de journalisation distincts,
considérations pour l'utilisation, 54

Périphériques de journalisation mis en miroir, Création

d'un pool de stockage ZFS (exemple), 54

Périphériques de swap et de vidage

Ajustement de la taille, 123

Index

335

Points de montage

Automatique, 168
Gestion dans ZFS

Description, 168

Pool, Définition, 28
Pool de stockage mis en miroir (zpool create),

Exemple, 51

Pool de stockage ZFS

Affichage de l'état de maintenance

Exemple, 96

Exemple, 97

Affichage de l'état détaillé du fonctionnement

Affichage de l'état fonctionnel, 95
Ajout de périphérique (zpool add)

Exemple, 63
Composants, 43
Configuration en miroir, description, 48
Configuration RAID-Z, création (zpool create)

Configuration RAID-Z, description, 48
Connexion de périphériques (zpool attach)

Exemple, 53

Exemple, 68

Corruption de données identifiée (zpool status -v)

Exemple, 285

Création (zpool create)

Exemple, 51

Création d'une configuration mise en miroir (zpool

create)
Exemple, 51

Exemple, 62

-x)
Description, 283

Description, 290

Destruction (zpool destroy)

Détection de problèmes éventuels (zpool status

Détermination du type de panne de périphérique

Index

Pool de stockage ZFS (Suite)

Déterminer si un périphérique peut être remplacé

Description, 292
Données corrompues
Description, 278

Entrelacement dynamique, 50
Exportation

Exemple, 101

(zpool status -v)
Exemple, 300

Identification du type de corruption de données

Identification pour l'importation (zpool import -a)

Pool de stockage ZFS (Suite)

Périphérique manquant (faulted)

Description, 278

Périphérique virtuel, 58
Point de montage par défaut, 61
Pool

Définition, 28

Pool racine de remplacement, 274
Problème d'identification

Importation à partir de répertoires alternatifs (zpool

Récupération d'un pool détruit

Informations globales d'état des pools pour la

résolution de problèmes
Description, 283

Notification d'un périphérique reconnecté dans ZFS

Statistiques d'E/S à l'échelle du pool

336

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Exemple, 101

Importation

Exemple, 104

import -d)
Exemple, 103

Liste

Exemple, 88

Migration

Description, 99

Miroir

Définition, 28

Description, 73

Exemple, 74

Nettoyage de données

Exemple, 280

Nettoyage des données

Description, 280

Nettoyage et réargenture de données

Description, 281

(zpool online)
Exemple, 289

Panne, 277
Périphérique endommagé

Description, 278

Mise en ligne et hors ligne de périphérique

Mise hors ligne d'un périphérique (zpool offline)

Description, 282
Profils de droits, 31
RAID-Z

Définition, 28

Réargenture

Définition, 29

Exemple, 108

Exemple, 76

Exemple, 287

Exemple, 293

endommagé
Description, 301

Description, 304

Remplacement d'un périphérique (zpool replace)

Remplacement d'un périphérique manquant

Remplacement de périphérique (zpool replace)

Réparation d'un fichier ou d'un répertoire

Réparation d'un système qui ne peut être initialisé

Réparation d'une configuration ZFS

endommagée, 287

Réparation de dommages au niveau d'un pool

Description, 304

Réparation des données

Description, 279

Script de sortie de pool de stockage

Séparation des périphériques (zpool detach)

Exemple, 90

Exemple, 70

Exemple, 93

Statistiques d'E/S vdev

Exemple, 94

Suppression d'un périphérique

Exemple, 76

Pool de stockage ZFS (Suite)

Suppression des erreurs de périphérique (zpool

Partage d'un pool de stockage mis en miroir (zpool

primarycache, propriété, Description, 143
Profils de droits, Gestion de systèmes de fichiers et pools

Pool ZFS, propriété, version, 87
Pools de stockage ZFS

Affichage du processus de resynchronisation

clear)
Exemple, 291

Test (zpool create -n)

Exemple, 61

Utilisation de disques entiers, 45
Validation des données

Description, 280

Version

Description, 325

Pool de stockage ZFS (zpool online)

Mise en ligne d'un périphérique

Exemple, 75

Pool racine de remplacement

Création

Exemple, 275
Description, 274
Importation

Exemple, 275

Exemple, 298

Messages d'erreur système

Description, 286

Mise à niveau

Description, 109

split)
Exemple, 70

Utilisation de fichiers, 46

de stockage ZFS, 31

Propriété capacity, description, 85
Propriété de pool ZFS

allocated (propriété), 84
altroot, 85
autoreplace (propriété), 85
capacity, 85
delegation, 85
failmode (propriété), 86
guid, 86

Propriété de pool ZFS (Suite)

health, 86
size, 87

propriété delegation, désactivation, 252
Propriété delegation, description, 85
Propriété guid, description, 86
Propriété health (description), 86
Propriété size, description, 87
Propriété ZFS, exec, 142
Propriété ZFS pouvant être définie, exec, 142
Propriétés de pool ZFS

bootfs, 85
cachefile (propriété), 85
dedupditto, 85
dedupratio, 85
free (propriété), 86
listsnapshots, 86

Propriétés en lecture seule de ZFS,

usedbychildren, 149

Propriétés ZFS

aclinherit, 138
aclmode, 138
atime, 138
available, 139
canmount, 139

Description détaillée, 154

casesensitivity, 140
checksum, 140
compression, 141
compressratio, 141
copies, 141
creation, 141
dedup, 141
Définissables, 152
Description, 137
Description des propriétés héritées, 137
devices, 142
Gestion au sein d'une zone

Description, 271

Héritées, description, 137
logbias, 142
mlslabel, 143
mounted, 143
mountpoint, 143

Index

337

Index

Propriétés ZFS (Suite)

origin, 144
Propriétés définies par l'utilisateur

Description détaillée, 159

quota, 144
read-only, 145
recordsize, 145

Description détaillée, 157

referenced, 145
refquota, 145
refreservation, 146
reservation, 146
secondarycache, 143, 146
setuid, 147
shadow, 147
sharenfs, 147
sharesmb, 147
snapdir, 148
sync, 148
type, 148
used, 149

Description détaillée, 151

usedbychildren, 149
usedbydataset, 149
usedbyrefreservation, 149
usedbysnapshots, 149
version, 149
volblocksize, 150
volsize, 150

Description détaillée, 159

xattr, 150
zoned, 150
zoned, propriété

Description détaillée, 272

Propriétés ZFS définies par l'utilisateur

Description détaillée, 159
Exemple, 159

Propriétés ZFS définissables

aclmode, 138
atime, 138

canmount

Description détaillée, 154

casesensitivity, 140
compression, 141

Propriétés ZFS définissables (Suite)

copies, 141
dedup, 141
Description, 152
primarycache, 143
quota, 144
read-only, 145

recordsize

Description détaillée, 157

refquota, 145
refreservation, 146
secondarycache, 146
setuid, 147
shadow, 147
sharenfs, 147
sharesmb, 147
sync, 148

used

Description détaillée, 151

volblocksize, 150
volsize, 150

Description détaillée, 159

xattr, 150

Propriétés ZFS en lecture seule

available, 139
creation, 141
mounted, 143
used, 149
usedbydataset, 149
usedbyrefreservation, 149
usedbysnapshots, 149

Q
quota, propriété, Description, 144
Quotas et réservations, Description, 182

R
RAID-Z, Définition, 28
read-only (propriété), Description, 145
Réargenture, Définition, 29

338

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Exemple, 213

recordsize, propriété, Description, 145
recordsize (propriété), Description détaillée, 157
Récupération

Pool de stockage ZFS détruit

Exemple, 108

referenced, propriété, Description, 145
refquota, propriété, Description, 145
refreservation (propriété), Description, 146
Remplacement

Périphérique (zpool replace)

Exemple, 76, 293, 298
Périphérique manquant

Exemple, 287

Renommer

Instantané ZFS

Exemple, 202

Réparation

Configuration ZFS endommagée

Description, 287

Dommages au niveau d'un pool

Description, 304

Réparation d'un fichier ou d'un répertoire

endommagé
Description, 301

Système qui ne peut être initialisé

Description, 304

reservation, propriété, Description, 146
Résolution de problèmes

Corruption de données identifiée (zpool status -v)

Détermination du type de corruption de données

Exemple, 285

(zpool status -v)
Exemple, 300

Description, 290

Description, 283

(zpool online)
Exemple, 289

Détermination du type de panne de périphérique

Informations globales d'état des pools

Notification d'un périphérique reconnecté dans ZFS

Remplacement de périphérique (zpool replace)

Réparation d'un fichier ou d'un répertoire

Exemple, 287

Exemple, 293

endommagé
Description, 301

Suppression des erreurs de périphérique (zpool

ACL triviale sur un fichier ZFS (mode détaillé)

clear)
Exemple, 291

Restauration

Exemple, 235

Instantané ZFS

Exemple, 205

S
savecore, commande, Enregistrement de vidage

mémoire sur incident, 124

script

Sortie de pool de stockage ZFS

Exemple, 90

secondarycache, propriété, Description, 146
Sémantique transactionnelle, Description, 25
Séparation

Périphérique, d'un pool de stockage ZFS (zpool

detach)
Exemple, 70

setuid (propriété), description, 147
shadow, propriété, Description, 147
sharenfs, propriété, Description, 147
sharesmb, propriété, Description, 147
sharesmb, propriété, Description, détaillée, 158
snapdir, propriété, Description, 148
Somme de contrôle, Définition, 27
Somme de contrôle de données, Description, 26
Stockage requis, Identification, 33
Stockage sur pool, Description, 25
Suppression

Erreurs de périphérique (zpool clear)

Exemple, 291

Index

339

Réception

Résolution de problèmes (Suite)

Données de système de fichiers ZFS (zfs receive)

Remplacement d'un périphérique manquant

Rapport syslog de messages d'erreur ZFS, 286

Périphériques de cache (exemple), 67

Suppression d'autorisations, zfs unallow, 256
Suppression d'un périphérique

Système de fichiers ZFS (Suite)

Définition des ACL sur un fichier ZFS (mode

Index

Pool de stockage ZFS

Exemple, 76

sync, propriété, Description, 148
Système de fichiers, Définition, 28
Système de fichiers ZFS

ACL dans un fichier ZFS

Description détaillée, 230
ACL dans un répertoire ZFS
Description détaillée, 231

Administration simplifiée

Description, 27

globale
Exemple, 269

Clone, 208

Définition, 27
Description, 206
Remplacement d'un système de fichiers

(exemple), 208

Comptabilisation d'espace d'instantané, 204
Configuration d'ACL dans des fichiers ZFS

Description, 229

compact)
Description, 242
Exemple, 243

Délégation d'un jeu de données à une zone non

détaillé)
Description, 231

globale
Exemple, 270

Démontage

Exemple, 172
Description, 133
Destruction

Exemple, 135

Destruction avec les systèmes dépendants

Enregistrement d'un flux de données (zfs send)

Exemple, 212
Envoi et réception

Description, 209

Gestion de propriété au sein d'une zone

Description, 271

Gestion des points de montage

Description, 168

Gestion des points de montage hérités

Description, 169

Exemple, 236

Initialisation d'un environnement d'initialisation

ZFS avec boot -L et boot -Z
(exemple SPARC), 127

Initialisation d'un système de fichiers racine

Ajout d'un système de fichiers ZFS à une zone non

Exemple, 136

Configuration d'ACL dans un fichier ZFS (mode

Héritage d'ACL dans un fichier ZFS (mode détaillé)

Convention d'attribution de nom de composant, 29
Création

Exemple, 134

Création d'un volume ZFS

Exemple, 265

Création de clone, 207
Définition d'un point de montage (zfs set

mountpoint)
Exemple, 169

Définition d'un point de montage hérité

Exemple, 170

Définition d'une réservation

Exemple, 187

Définition de la propriété atime

Exemple, 163

Description, 125

Instantané

Définition, 29
Description, 199
Renommer, 202

Jeu de données

Définition, 28

Liste

Exemple, 160

Exemple, 165

Exemple, 167

340

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Liste de propriétés (zfs list)

Liste des propriétés pour l'exécution de scripts

Modification d'une ACL triviale dans un fichier ZFS

Système de fichiers ZFS (Suite)

Liste des types

Exemple, 162
Liste sans en-tête
Exemple, 162

(mode détaillé)
Exemple, 232

Modification du nom

Exemple, 136

Montage

Exemple, 171

Périphériques de swap et de vidage

Ajustement de la taille, 123
Description, 122
Points à prendre en compte, 122

Point de montage par défaut

Exemple, 135

Profils de droits, 31
Restauration d'une ACL triviale sur un fichier ZFS

(mode détaillé)
Exemple, 235

Sémantique transactionnelle

Somme de contrôle de données

Description, 25
Somme de contrôle
Définition, 27

Description, 26

Stockage sur pool

Description, 25
Système de fichiers
Définition, 28

Types de jeux de données

Description, 162

Description, 268
système de fichiers ZFS

Version

Description, 325
Système de fichiers ZFS

Volume

Définition, 29

Utilisation d'un système Solaris avec zones installées

Index

Système de stockage ZFS
Périphérique virtuel

Définition, 29

Systèmes de fichiers ZFS

Ajout d'un volume ZFS à une zone non globale

Exemple, 270
Description, 24
Gestion des points de montage automatiques, 168
Héritage d'une propriété de (zfs inherit)

Exemple, 164

Instantané

Accès, 203
Création, 200
Destruction, 201
Restauration, 205
Liste des descendants
(Exemple de), 161

Exemple, 167

Mise à niveau

Description, 198

Exemple, 164

Exemple, 213

Liste des propriétés par valeur de source

Paramétrage quota, propriété

Réception de flux de données (zfs receive)

Systèmes de fichiers ZFS (zfs set quota)

Définition d'un quota

Exemple, 183

T
Terminologie
Clone, 27
Instantané, 29
Jeu de données, 28
Miroir, 28
Périphérique virtuel, 29
Pool, 28
RAID-Z, 28
Réargenture, 29
Somme de contrôle, 27
Système de fichiers, 28
Volume, 29

341

Index

Test

Création de pool de stockage ZFS (zpool create -n)

Exemple, 61

type, propriété, Description, 148

U
used (propriété)

Description, 149
Description détaillée, 151

usedbychildren, propriété, Description, 149
usedbydataset Propriété, Description, 149
usedbyrefreservation Propriété, Description, 149
usedbysnapshotsPropriété, Description, 149

V
version (propriété), Description, 149
version propriété, description, 87
Version ZFS

Fonction ZFS et SE Solaris

Description, 325

Vidage mémoire sur incident, Enregistrement, 124
volblocksize (propriété), description, 150
volsize, propriété, Description, 150
volsize (propriété), Description détaillée, 159
Volume, Définition, 29
Volume ZFS, Description, 265

X
xattr, Description, 150

Z
ZFS, Propriétés, Lecture seule, 151

zfs allow

Affichage des autorisations déléguées, 260
Description, 255

zfs create

Description, 134

zfs create (Suite)

Exemple, 36, 134

zfs destroy, Exemple, 135
zfs destroy -r, Exemple, 136
zfs get, Exemple, 165
zfs get -H -o, commande, Exemple, 167
zfs get -s, Exemple, 167
zfs inherit, Exemple, 164

zfs list

Exemple, 37, 160

zfs list -H, Exemple, 162
zfs list -r, (Exemple de), 161
zfs list -t, Exemple, 162
zfs mount, Exemple, 171
zfs promote, Promotion d'un clone (exemple), 208
zfs receive, Exemple, 213
zfs rename, Exemple, 136
zfs send, Exemple, 212
zfs set atime, commande, Exemple, 163
zfs set compression, Exemple, 36

zfs set mountpoint

Exemple, 36, 169

zfs set mountpoint=legacy, Exemple, 170

zfs set quota

Exemple, 37, 183

zfs set quota, Exemple, 164
zfs set reservation, Exemple, 187
zfs set sharenfs, Exemple, 36
zfs unallow, Description, 256
zfs unmount, Exemple, 172
zfs upgrade, 198
Zone

Gestion de propriétés ZFS au sein d'une zone

Description, 271

Utilisation de systèmes de fichiers ZFS

Description, 268

zoned, propriété

Description, 150

Ajout d'un système de fichiers ZFS à une zone non

Délégation d'un jeu de données à une zone non

globale
Exemple, 269

globale
Exemple, 270

342

Administration d'Oracle Solaris : Systèmes de fichiers ZFS • Février 2012

Ajout d'un volume ZFS à une zone non globale

zoned, propriété (Suite)

Description détaillée, 272

Zones

Exemple, 270
zoned, propriété

Description détaillée, 272

zpool add, Exemple, 63
zpool attach, Exemple, 68

zpool clear

Description, 76
Exemple, 76

zpool create

Exemple, 33, 34
Pool de base

Exemple, 51

Pool de stockage mis en miroir

Exemple, 51

Pool de stockage RAID-Z

Exemple, 53

zpool create -n, Test ( exemple), 61
zpool destroy, Exemple, 62
zpool detach, Exemple, 70
zpool export, Exemple, 101
zpool import -a, Exemple, 101
zpool import -D, Exemple, 108
zpool import -d, Exemple, 103
zpool import nom, Exemple, 104
zpool iostat, pool complet, exemple, 93
zpool iostat -v, vdev, exemple, 94

zpool list

Description, 87
Exemple, 34, 88

zpool list -Ho name, Exemple, 90
zpool offline, Exemple, 74
zpool online, Exemple, 75
zpool replace, Exemple, 76
zpool split, Exemple, 70
zpool status -v, Exemple, 97
zpool status -x, commande, Exemple, 96
zpool upgrade, 109

Index

343

344

