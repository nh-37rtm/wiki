Guide d'administration Solaris
ZFS

Sun Microsystems, Inc.
4150 Network Circle
Santa Clara, CA 95054
U.S.A.

Référence : 820–2315–14
Octobre 2009

Copyright 2009 Sun Microsystems, Inc.

4150 Network Circle, Santa Clara, CA 95054 U.S.A.

Tous droits réservés.

Sun Microsystems, Inc. détient les droits de propriété intellectuelle de la technologie utilisée par le produit décrit dans le présent document. En particulier, et sans
limitation, ces droits de propriété intellectuelle peuvent inclure des brevets américains ou dépôts de brevets en cours d'homologation aux États-Unis et dans les autres
pays.
Droits du gouvernement américain – logiciel commercial. Les utilisateurs gouvernementaux sont soumis au contrat de licence standard de Sun Microsystems, Inc. et
aux dispositions du Federal Acquisition Regulation (FAR, règlements des marchés publics fédéraux) et de leurs suppléments.
Cette distribution peut contenir des éléments développés par des tiers.
Des parties du produit peuvent être dérivées de systèmes Berkeley-BSD, sous licence de l'Université de Californie. UNIX est une marque déposée aux États-Unis et
dans les autres pays, sous licence exclusive de X/Open Company, Ltd.
Sun, Sun Microsystems, le logo Sun, le logo Solaris, le logo Java (tasse de café), docs.sun.com, Java et Solaris sont des marques de fabrique ou des marques déposées de
Sun Microsystems, Inc. aux États-Unis et dans les autres pays. Toutes les marques SPARC sont utilisées sous licence et sont des marques de fabrique ou des marques
déposées de SPARC International, Inc. aux États-Unis et dans les autres pays. Les produits portant les marques SPARC sont constitués selon une architecture
développée par Sun Microsystems, Inc. Legato NetWorker est une marque commerciale ou une marque déposée de Legato Systems, Inc.
L'interface utilisateur graphique OPEN LOOK et SunTM a été développée par Sun Microsystems, Inc. pour ses utilisateurs et détenteurs de licence. Sun reconnaît le
travail précurseur de Xerox en matière de recherche et de développement du concept d'interfaces utilisateur visuelles ou graphiques pour le secteur de l'informatique.
Sun détient une licence Xerox non exclusive sur l'interface utilisateur graphique Xerox. Cette licence englobe également les détenteurs de licences Sun qui
implémentent l’interface utilisateur graphique OPEN LOOK et qui, en outre, se conforment aux accords de licence écrits de Sun.
Les produits cités dans la présente publication et les informations qu'elle contient sont soumis à la législation américaine relative au contrôle sur les exportations et, le
cas échéant, aux lois sur les importations ou exportations dans les autres pays. Il est strictement interdit d'employer ce produit conjointement à des missiles ou armes
biologiques, chimiques, nucléaires ou de marine nucléaire, directement ou indirectement. Il est strictement interdit d'effectuer des exportations et réexportations
vers des pays soumis à l'embargo américain ou vers des entités identifiées sur les listes noires des exportations américaines, notamment les individus non autorisés et
les listes nationales désignées.
LA DOCUMENTATION EST FOURNIE "EN L'ÉTAT" ET TOUTES AUTRES CONDITIONS, REPRÉSENTATIONS ET GARANTIES EXPRESSES OU TACITES,
Y COMPRIS TOUTE GARANTIE IMPLICITE RELATIVE À LA COMMERCIALISATION, L'ADÉQUATION À UN USAGE PARTICULIER OU LA
NON-VIOLATION DE DROIT, SONT FORMELLEMENT EXCLUES. CETTE EXCLUSION DE GARANTIE NE S'APPLIQUERAIT PAS DANS LA MESURE OÙ
ELLE SERAIT TENUE JURIDIQUEMENT NULLE ET NON AVENUE.

091006@22749

Table des matières

Préface ...................................................................................................................................................11

1

Système de fichiers ZFS (présentation) ........................................................................................... 15
Nouveautés de ZFS .............................................................................................................................. 15
Prise en charge de l'installation de ZFS et Flash ....................................................................... 16
Quotas d'utilisateurs et de groupes ZFS .................................................................................... 16
Héritage direct ACL ZFS pour l'autorisation d'exécution ....................................................... 17
Améliorations apportées aux propriétés ZFS ........................................................................... 18
Récupération de périphérique de journal ZFS ......................................................................... 20
L'utilisation des périphériques de cache dans votre pool de stockage ZFS ........................... 21
Migration de zone dans un environnement ZFS ...................................................................... 22
Prise en charge de l'installation et de l'initialisation de ZFS .................................................... 22
Rétablissement d'un jeu de données sans démontage ............................................................. 22
Améliorations apportées à la commande zfs send .................................................................22
Quotas et réservations ZFS pour les données de système de fichiers uniquement ............... 23
Propriétés de pool de stockage ZFS ............................................................................................ 24
Améliorations apportées à l'historique des commandes ZFS (zpool history) ...................25
Mise à niveau des systèmes de fichiers ZFS (zfs upgrade) .....................................................27
Administration déléguée de ZFS ................................................................................................ 27
Configuration de périphériques de journalisation ZFS distincts ........................................... 28
Création de jeux de données ZFS intermédiaires ..................................................................... 29
Améliorations apportées à la connexion à chaud à ZFS .......................................................... 30
Renommage récursif d'instantanés ZFS (zfs rename -r) .......................................................30
La compression GZIP est disponible pour ZFS ........................................................................ 31
Stockage de plusieurs copies de données utilisateur ZFS ........................................................ 32
Amélioration de la sortie de la commande zpool status .......................................................33
Améliorations de ZFS et Solaris iSCSI ....................................................................................... 33
Historique de commande ZFS (zpool history) ......................................................................33

3

Table des matières

Améliorations de propriétés ZFS ............................................................................................... 34
Affichage de la totalité des informations de systèmes de fichiers ZFS ................................... 35
Nouvelle option zfs receive -F ................................................................................................36
Instantanés ZFS récursifs ............................................................................................................ 36
Double Parité RAID-Z (raidz2) ................................................................................................ 36
Disques hot spare pour périphériques de pool de stockage ZFS ............................................ 37
Remplacement d'un système de fichiers ZFS par un clone ZFS (zfs promote) ....................37
Mise à niveau des pools de stockage ZFS (zpool upgrade) .....................................................37
Commandes de sauvegarde et de rétablissement ZFS renommées ........................................ 38
Récupération de pools de stockage détruits .............................................................................. 38
Intégration de ZFS au gestionnaire de pannes .......................................................................... 38
Nouvelle commande zpool clear .............................................................................................39
Format NFSv4 ACL compact ...................................................................................................... 39
Outil de contrôle de système de fichiers (fsstat) .................................................................... 39
Gestion Web ZFS ......................................................................................................................... 39
Description de ZFS .............................................................................................................................. 40
Stockage ZFS mis en pool ............................................................................................................ 40
Sémantique transactionnelle ...................................................................................................... 41
Sommes de contrôle et données d'autorétablissement ............................................................ 42
Évolutitivé inégalée ...................................................................................................................... 42
Instantanés ZFS ............................................................................................................................ 42
Administration simplifiée ........................................................................................................... 43
Terminologie ZFS ................................................................................................................................ 43
Exigences d'attribution de noms de composants ZFS ..................................................................... 46

2 Guide de démarrage de ZFS ............................................................................................................... 47
Exigences et recommandations en matière de matériel et de logiciel ZFS ................................... 47
Création d'un système de fichiers ZFS basique ................................................................................ 48
Création d'un pool de stockage ZFS .................................................................................................. 49
▼ Identification des exigences de stockage du pool de stockage ZFS ........................................ 49
▼ Création d'un pool de stockage ZFS ........................................................................................... 50
Création d'une hiérarchie de systèmes de fichiers ZFS ................................................................... 50
▼ Détermination de la hiérarchie du système de fichiers ZFS .................................................... 51
▼ Création de systèmes de fichiers ZFS ......................................................................................... 52

4

Guide d'administration Solaris ZFS • Octobre 2009

Table des matières

3 Différences entre ZFS et les systèmes de fichiers classiques ........................................................55
Granularité du système de fichiers ZFS ............................................................................................ 55
Comptabilisation de l'espace ZFS ...................................................................................................... 56
Comportement d'espace saturé .................................................................................................. 56
Montage de système de fichiers ZFS .................................................................................................. 57
Gestion de volumes classique ............................................................................................................. 57
Nouveau modèle ACL Solaris ............................................................................................................ 57

4 Gestion des pools de stockage ZFS .................................................................................................. 59
Composants d'un pool de stockage ZFS ........................................................................................... 59
Utilisation de disques dans un pool de stockage ZFS .............................................................. 59
Utilisation de tranches dans un pool de stockage ZFS ............................................................. 61
Utilisation de fichiers dans un pool de stockage ZFS ............................................................... 63
Fonctions de réplication d'un pool de stockage ZFS ....................................................................... 63
Configuration de pool de stockage mis en miroir .................................................................... 63
Configuration de pool de stockage RAID-Z ............................................................................. 64
Pool de stockage ZFS hybride ..................................................................................................... 65
Données d'autorétablissement dans une configuration redondante ..................................... 65
Entrelacement dynamique dans un pool de stockage .............................................................. 65
Création et destruction de pools de stockage ZFS ........................................................................... 66
Création d'un pool de stockage ZFS ........................................................................................... 66
Affichage des informations d'un périphérique virtuel de pool de stockage .......................... 71
Gestion d'erreurs de création de pools de stockage ZFS .......................................................... 72
Destruction de pools de stockage ZFS ....................................................................................... 76
Gestion de périphériques dans un pool de stockage ZFS ................................................................ 77
Ajout de périphériques à un pool de stockage .......................................................................... 77
Connexion et séparation de périphériques dans un pool de stockage ................................... 82
Mise en ligne et mise hors ligne de périphériques dans un pool de stockage ........................ 84
Suppression des périphériques de pool de stockage ................................................................ 86
Remplacement de périphériques dans un pool de stockage ................................................... 87
Désignation des disques hot spare dans le pool de stockage ................................................... 89
Gestion des propriétés de pool de stockage ZFS .............................................................................. 93
Requête d'état de pool de stockage ZFS ............................................................................................. 96
Affichage des informations de pools de stockage ZFS de base ................................................ 96
Visualisation de statistiques d'E/S de pools de stockage ZFS ................................................ 100

5

Table des matières

Détermination de l'état de maintenance des pools de stockage ZFS .................................... 102
Migration de pools de stockage ZFS ................................................................................................ 105
Préparatifs de migration de pool de stockage ZFS ................................................................. 105
Exportation d'un pool de stockage ZFS ................................................................................... 106
Définition des pools de stockage disponibles pour importation .......................................... 106
Recherche de pools de stockage ZFS dans d'autres répertoires ............................................ 108
Importation de pools de stockage ZFS .................................................................................... 109
Récupération de pools de stockage ZFS détruits .................................................................... 110
Mise à niveau de pools de stockage ZFS .................................................................................. 112

5

Installation et initialisation d'un système de fichiers racine ZFS ...............................................115
Installation et initialisation d'un système de fichiers racine ZFS (présentation) ....................... 116
Fonctions d'installation de ZFS ................................................................................................ 116
Configuration requise pour l'installation de Solaris et de Solaris Live Upgrade pour la prise
en charge de ZFS ........................................................................................................................ 117
Installation d'un système de fichiers racine ZFS (installation initiale) ........................................ 120
Installation d'un système de fichiers racine ZFS (installation d'archive Flash) .......................... 127
Installation d'un système de fichiers racine ZFS (installation JumpStart) .................................. 130
Exemples de profils JumpStart de ZFS ..................................................................................... 130
Mots clés JumpStart de ZFS ...................................................................................................... 131
Problèmes liés à l'installation JumpStart d'un ZFS ................................................................. 133

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris
Live Upgrade) .................................................................................................................................... 134
Problèmes de migration d'un ZFS avec Solaris Live Upgrade .............................................. 135
Utilisation de Solaris Live Upgrade pour migrer vers un système de fichiers racine ZFS
(sans zones) ................................................................................................................................. 136
Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones
(Solaris 10 10/08) ........................................................................................................................ 141
Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones
(Solaris 10 5/09 et Solaris 10 10/09) ......................................................................................... 147
Prise en charge ZFS des périphériques de swap et de vidage ........................................................ 158
Ajustement de la taille de vos périphériques de swap et de vidage ZFS ............................... 159
Initialisation à partir d'un système de fichiers racine ZFS ............................................................ 161
Initialisation à partir d'un disque alternatif d'un pool racine ZFS mis en miroir ............... 162
Initialisation à partir d'un système de fichiers racine ZFS sur un système SPARC ............ 163
Initialisation à partir d'un système de fichiers racine ZFS sur un système x86 ................... 165

6

Guide d'administration Solaris ZFS • Octobre 2009

Table des matières

Résolution des problèmes de point de montage ZFS responsables de l'échec de
l'initialisation .............................................................................................................................. 166
Restauration du pool racine ZFS ou des instantanés du pool racine ........................................... 167
▼ Remplacement d'un disque dans le pool racine ZFS .............................................................. 167
▼ Création d'instantanés de pool racine ..................................................................................... 169
▼ Recréation d'un pool racine ZFS et restauration d'instantanés de pool racine .................. 171
▼ Restauration des instantanés d'un pool racine à partir d'une initialisation de secours ..... 172

6 Gestion des systèmes de fichiers ZFS ............................................................................................. 175
Création et destruction de systèmes de fichiers ZFS ..................................................................... 176
Création d'un système de fichiers ZFS ..................................................................................... 176
Destruction d'un système de fichiers ZFS ............................................................................... 177
Modification du nom d'un système de fichiers ZFS ............................................................... 178
Présentation des propriétés ZFS ...................................................................................................... 179
Propriétés ZFS natives en lecture seule .................................................................................... 188
Propriétés ZFS natives définies ................................................................................................. 189
Propriétés ZFS définies par l'utilisateur ................................................................................... 192
Envoi de requêtes sur les informations des systèmes de fichiers ZFS .......................................... 193
Affichage des informations de base des systèmes ZFS ........................................................... 193
Création de requêtes ZFS complexes ....................................................................................... 194
Gestion des propriétés ZFS .............................................................................................................. 196
Définition des propriétés ZFS ................................................................................................... 196
Héritage des propriétés ZFS ...................................................................................................... 197
Envoi de requêtes sur les propriétés ZFS ................................................................................. 198
Montage et partage des systèmes de fichiers ZFS ........................................................................... 201
Gestion des points de montage ZFS ......................................................................................... 201
Montage de système de fichiers ZFS ........................................................................................ 204
Utilisation de propriétés de montage temporaires ................................................................ 205
Démontage des systèmes de fichiers ZFS ................................................................................ 205
Activation et annulation du partage des systèmes de fichiers ZFS ....................................... 206
Définition des quotas et réservations ZFS ...................................................................................... 208
Définitions de quotas sur les systèmes de fichiers ZFS .......................................................... 209
Définition de réservations sur les systèmes de fichiers ZFS .................................................. 213

7

Table des matières

7 Utilisation des instantanés et des clones ZFS ............................................................................... 215
Présentation des instantanés ZFS .................................................................................................... 215
Création et destruction d'instantanés ZFS .............................................................................. 216
Affichage et accès des instantanés ZFS .................................................................................... 218
Restauration d'un instantané ZFS ............................................................................................ 219
Présentation des clones ZFS ............................................................................................................. 220
Création d'un clone ZFS ............................................................................................................ 221
Destruction d'un clone ZFS ...................................................................................................... 221
Remplacement d'un système de fichiers ZFS par un clone ZFS ............................................ 222
Envoi et réception de données ZFS ................................................................................................. 223
Envoi d'un instantané ZFS ........................................................................................................ 224
Réception d'un instantané ZFS ................................................................................................. 225
Envoi et réception de flux d'instantanés ZFS complexes ....................................................... 226
Enregistrement de données ZFS à l'aide d'autres produits de sauvegarde .......................... 229

8 Utilisation des ACL pour la protection de fichiers ZFS .................................................................231
Nouveau modèle ACL Solaris .......................................................................................................... 231
Descriptions de syntaxe pour la configuration des ACL ....................................................... 233
Héritage d'ACL ........................................................................................................................... 236
Modes de propriétés d'ACL ...................................................................................................... 237
Configuration d'ACL dans des fichiers ZFS ................................................................................... 238
Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé ................................ 240
Configuration d'héritage d'ACL dans des fichiers ZFS en format détaillé .......................... 246
Configuration et affichage d'ACL dans des fichiers ZFS en format compact ............................. 254

9

Administration déléguée de ZFS ....................................................................................................261
Présentation de l'administration déléguée de ZFS ........................................................................ 261
Désactivation des droits délégués de ZFS ................................................................................ 262
Délégation de droits ZFS .................................................................................................................. 262
Description de la syntaxe de délégation des autorisations (zfs allow) ............................. 264
Suppression des droits délégués de ZFS (zfs unallow) ....................................................... 265
Utilisation de l'administration déléguée de ZFS ............................................................................ 266
Affichage des droits ZFS délégués (exemples) ........................................................................ 266
Délégation de droits ZFS (exemples) ....................................................................................... 268
Suppression de droits ZFS (exemples) .................................................................................... 272

8

Guide d'administration Solaris ZFS • Octobre 2009

Table des matières

10

11

Sections avancées de ZFS .................................................................................................................275
Volumes ZFS ...................................................................................................................................... 275
Utilisation d'un volume ZFS en tant que périphérique de swap ou de dump ..................... 276
Utilisation d'un volume ZFS en tant que cible iSCSI Solaris ................................................. 277
Utilisation de ZFS dans un système Solaris avec zones installées ................................................ 278
Ajout de systèmes de fichiers ZFS à une zone non globale .................................................... 279
Délégation de jeux de données à une zone non globale ......................................................... 280
Ajout de volumes ZFS à une zone non globale ....................................................................... 281
Utilisation de pools de stockage ZFS au sein d'une zone ....................................................... 281
Gestion de propriétés ZFS au sein d'une zone ........................................................................ 282
Explication de la propriété zoned ............................................................................................. 283
Utilisation de pools racine ZFS de remplacement ......................................................................... 284
Création de pools racine de remplacement ZFS ..................................................................... 284
Importation de pools racine de remplacement ...................................................................... 285
Profils de droits ZFS .......................................................................................................................... 285

Résolution de problèmes et récupération de données ZFS ....................................................... 287
Modes de panne ZFS ......................................................................................................................... 287
Périphériques manquants dans un pool de stockage ZFS ..................................................... 288
Périphériques endommagés dans un pool de stockage ZFS ................................................. 288
Données ZFS corrompue .......................................................................................................... 288
Vérification de l'intégrité des données ZFS .................................................................................... 289
Réparation de données .............................................................................................................. 289
Validation de données ............................................................................................................... 289
Contrôle du nettoyage de données ZFS ................................................................................... 290
Identification de problèmes dans ZFS ............................................................................................. 291
Recherche de problèmes éventuels dans un pool de stockage ZFS ...................................... 293
Consultation de la sortie de zpool status ............................................................................. 293
Rapport système de messages d'erreur ZFS ............................................................................ 296
Réparation d'un configuration ZFS endommagée ........................................................................ 297
Réparation d'un périphérique manquant ....................................................................................... 297
Reconnexion physique du périphérique ................................................................................. 298
Notification relative à la disponibilité de périphériques dans ZFS ....................................... 298
Remplacement ou réparation d'un périphérique endommagé .................................................... 299
Détermination du type de panne de périphérique ................................................................. 299

9

Table des matières

Suppression des erreurs transitoires ........................................................................................ 300
Remplacement d'un périphérique dans un pool de stockage ZFS ........................................ 301
Réparation de données endommagées ........................................................................................... 308
Identification du type de corruption de données ................................................................... 309
Réparation d'un fichier ou répertoire corrompu .................................................................... 310
Réparation de dommages présents dans l'ensemble du pool de stockage ZFS ................... 311
Réparation d'un système impossible à réinitialiser ....................................................................... 311

Index ................................................................................................................................................... 313

10

Guide d'administration Solaris ZFS • Octobre 2009

Préface

Le Guide d'administration Solaris ZFS fournit des informations sur la configuration et la gestion
des systèmes de fichiers ZFS de SolarisTM.

Ce guide contient des informations sur les systèmes SPARC® et x86.

Remarque – Cette version de Solaris prend en charge les systèmes utilisant les architectures de
processeur SPARC et x86 : UltraSPARC®, SPARC64, AMD64, Pentium et Xeon EM64T. Les
systèmes pris en charge sont répertoriés dans laliste de compatibilité matérielle de
Solarisdisponible à l'adresse http://www.sun.com/bigadmin/hcl Ce document présente les
différences d'implémentation en fonction des divers types de plates-formes.

Dans ce document, les termes x86 ci-dessous ont les significations suivantes :

■

■

■

“x86” désigne la famille des produits compatibles x86 64 bits et 32 bits.
“x64” désigne des informations 64 bits spécifiques relatives aux systèmes AMD64 ou
EM64T.
“x86 32 bits” désigne des informations 32 bits spécifiques relatives aux systèmes x86.

Pour connaître les systèmes pris en charge, reportez-vous à la liste de compatibilité matérielle de
Solaris 10.

Utilisateurs de ce manuel

Ce guide est destiné à toute personne souhaitant configurer et gérer des systèmes de fichiers ZFS
Solaris. Il est préférable d'être préalablement expérimenté en matière d'utilisation du système
d'exploitation Solaris ou autre version UNIX®.

11

Préface

Organisation de ce document

Le tableau suivant décrit les chapitres de ce document.

Chapitre

Description

Chapitre 1, “Système de fichiers
ZFS (présentation)”

Présente ZFS, ses fonctionnalités et ses avantages. Il aborde également des
concepts de base, ainsi que la terminologie.

Chapitre 2, “Guide de
démarrage de ZFS”

Décrit les instruction pas à pas d'une configuration ZFS simple, avec pools et
systèmes de fichiers simples. Ce chapitre indique également le matériel et
logiciels requis pour la création de systèmes de fichiers ZFS.

Chapitre 3, “Différences entre
ZFS et les systèmes de fichiers
classiques”

Identifie les fonctionnalités importantes qui différencie ZFS des systèmes de
fichiers classiques. La compréhension de ces différences clés permet d'éviter
les confusions lors de l'utilisation d'outils classiques en interaction avec ZFS.

Chapitre 4, “Gestion des pools
de stockage ZFS”

Décrit en détail les méthodes de création et d'administration de pools de
stockage.

Chapitre 5, “Installation et
initialisation d'un système de
fichiers racine ZFS”

Décrit la procédure d'installation et d'initialisation d'un système de fichiers
ZFS. La migration d'un système de fichiers racine UFS vers un système de
fichiers racine ZFS à l'aide de Solaris Live Upgrade est également abordée.

Chapitre 6, “Gestion des
systèmes de fichiers ZFS”

Décrit en détail les méthodes de gestion de systèmes de fichiers ZFS. Ce
chapitre décrit des concepts tels que la disposition hiérarchique de systèmes
de fichiers, l'héritage de propriétés, la gestion automatique de points de
montage et les interactions de partage.

Chapitre 7, “Utilisation des
instantanés et des clones ZFS”

Décrit les méthodes de création et d'administration d'instantanés ZFS et de
clones.

Chapitre 8, “Utilisation des
ACL pour la protection de
fichiers ZFS”

Décrit l'utilisation des listes de contrôle d'accès (ACL, Access Control List)
pour la protection des fichiers ZFS en fournissant davantage de droits
granulaires que les droits standard UNIX.

Chapitre 9, “Administration
déléguée de ZFS”

Décrit la méthode d'utilisation de l'administration déléguée de ZFS pour
permettre aux utilisateurs ne disposant pas de privilèges appropriés
d'effectuer des tâches d'administration ZFS.

Chapitre 10, “Sections avancées
de ZFS”

Décrit l'utilisation de volumes ZFS, l'utilisation de ZFS dans un système
Solaris comprenant des zones, ainsi que les pools racine de remplacement.

Chapitre 11, “Résolution de
problèmes et récupération de
données ZFS”

Décrit l'identification des modes de défaillance de ZFS et les solutions
existantes. Les étapes de prévention de ces défaillances sont également
abordées.

12

Guide d'administration Solaris ZFS • Octobre 2009

Préface

Documentation connexe

Vous trouverez des informations générales à propos de l'administration de systèmes Solaris
dans les documents suivants :

■

■

■

■

■

Solaris System Administration: Basic Administration
Solaris System Administration: Advanced Administration
Solaris System Administration: Devices and File Systems
Solaris System Administration: Security Services
Solaris Volume Manager Administration Guide

Documentation, support et formation

Le site Web Sun fournit des informations sur les ressources supplémentaires suivantes :
■ documentation (http://www.sun.com/documentation/)

■

■

support (http://www.sun.com/support/)
formation (http://www.sun.com/training/)

Sun attend vos commentaires.

Afin d'améliorer sa documentation, Sun vous encourage à faire des commentaires et à apporter
des suggestions. Pour nous faire part de vos commentaires, accédez au site
http://docs.sun.com, puis cliquez sur Feedback.

Conventions typographiques

Le tableau ci-dessous décrit les conventions typographiques utilisées dans ce manuel.

TABLEAU P–1 Conventions typographiques

Type de caractères

Signification

Exemple

AaBbCc123

Noms des commandes, fichiers et répertoires,
ainsi que messages système.

Modifiez votre fichier .login.
Utilisez ls -a pour afficher la liste
de tous les fichiers.

nom_machine% Vous avez reçu du

courrier.

13

Préface

TABLEAU P–1 Conventions typographiques
Type de caractères

Signification

(Suite)

AaBbCc123

aabbcc123

AaBbCc123

Ce que vous entrez, par opposition à ce qui
s'affiche à l'écran.

Paramètre fictif : à remplacer par un nom ou une
valeur réel(le).

Titres de manuel, nouveaux termes et termes
importants.

Exemple

nom_machine% su

Mot de passe :

La commande permettant de
supprimer un fichier est rm
nom_fichier.

Reportez-vous au chapitre 6 du
Guide de l'utilisateur.
Un cache est une copie des éléments
stockés localement.
N'enregistrez pas le fichier.
Remarque : en ligne, certains
éléments mis en valeur s'affichent en
gras.

Invites de shell dans les exemples de commandes

Le tableau suivant présente les invites système et les invites de superutilisateur UNIX par défaut
des C shell, Bourne shell et Korn shell.

TABLEAU P–2

Invites de shell

Shell

C shell

C shell pour superutilisateur

Bourne shell et Korn shell

Bourne shell et Korn shell pour superutilisateur

Invite

nom_machine%

nom_machine#

$

#

14

Guide d'administration Solaris ZFS • Octobre 2009

1C H A P I T R E

1

Système de fichiers ZFS (présentation)

Ce chapitre présente le système de fichiers ZFS, ses fonctions et ses avantages. Il aborde
également la terminologie de base utilisée dans le reste de ce document.

Il contient les sections suivantes :
■ “Nouveautés de ZFS” à la page 15
■ “Description de ZFS” à la page 40
■ “Terminologie ZFS” à la page 43
■ “Exigences d'attribution de noms de composants ZFS” à la page 46

Nouveautés de ZFS

Cette section décrit les nouvelles fonctions du système de fichier ZFS.
■ “Prise en charge de l'installation de ZFS et Flash” à la page 16
■ “Quotas d'utilisateurs et de groupes ZFS” à la page 16
■ “Migration de zone dans un environnement ZFS” à la page 22
■ “Héritage direct ACL ZFS pour l'autorisation d'exécution” à la page 17
■ “Améliorations apportées aux propriétés ZFS ” à la page 18
■ “Récupération de périphérique de journal ZFS” à la page 20
■ “L'utilisation des périphériques de cache dans votre pool de stockage ZFS” à la page 21
■ “Prise en charge de l'installation et de l'initialisation de ZFS” à la page 22
■ “Rétablissement d'un jeu de données sans démontage” à la page 22
■ “Améliorations apportées à la commande zfs send” à la page 22
■ “Quotas et réservations ZFS pour les données de système de fichiers uniquement”

à la page 23

■ “Propriétés de pool de stockage ZFS” à la page 24
■ “Améliorations apportées à l'historique des commandes ZFS (zpool history)” à la page 25
■ “Mise à niveau des systèmes de fichiers ZFS (zfs upgrade)” à la page 27
■ “Administration déléguée de ZFS” à la page 27
■ “Configuration de périphériques de journalisation ZFS distincts” à la page 28

15

Nouveautés de ZFS

■ “Création de jeux de données ZFS intermédiaires” à la page 29
■ “Améliorations apportées à la connexion à chaud à ZFS” à la page 30
■ “Renommage récursif d'instantanés ZFS (zfs rename -r)” à la page 30
■ “La compression GZIP est disponible pour ZFS” à la page 31
■ “Stockage de plusieurs copies de données utilisateur ZFS” à la page 32
■ “Amélioration de la sortie de la commande zpool status” à la page 33
■ “Améliorations de ZFS et Solaris iSCSI” à la page 33
■ “Historique de commande ZFS (zpool history)” à la page 33
■ “Améliorations de propriétés ZFS” à la page 34
■ “Affichage de la totalité des informations de systèmes de fichiers ZFS” à la page 35
■ “Nouvelle option zfs receive -F” à la page 36
■ “Instantanés ZFS récursifs” à la page 36
■ “Double Parité RAID-Z (raidz2)” à la page 36
■ “Disques hot spare pour périphériques de pool de stockage ZFS” à la page 37
■ “Remplacement d'un système de fichiers ZFS par un clone ZFS (zfs promote)” à la page 37
■ “Mise à niveau des pools de stockage ZFS (zpool upgrade)” à la page 37
■ “Commandes de sauvegarde et de rétablissement ZFS renommées” à la page 38
■ “Récupération de pools de stockage détruits” à la page 38
■ “Intégration de ZFS au gestionnaire de pannes” à la page 38
■ “Nouvelle commande zpool clear” à la page 39
■ “Format NFSv4 ACL compact” à la page 39
■ “Outil de contrôle de système de fichiers (fsstat)” à la page 39
■ “Gestion Web ZFS” à la page 39

Prise en charge de l'installation de ZFS et Flash
Version Solaris 10 10/09 : dans cette version de Solaris, vous pouvez définir un profil JumpStart
pour identifier une archive Flash d'un pool racine ZFS. Pour plus d'informations, reportez-vous
à la section “Installation d'un système de fichiers racine ZFS (installation d'archive Flash)”
à la page 127.

Quotas d'utilisateurs et de groupes ZFS
Version Solaris 10 10/09 : dans les versions précédentes de Solaris, vous pouviez appliquer des
quotas et des réservations aux systèmes de fichiers ZFS pour gérer et réserver de l'espace.

Dans cette version de Solaris, vous pouvez définir un quota sur la quantité d'espace utilisée par
les fichiers appartenant à un utilisateur ou à un groupe spécifique. Vous pouvez envisager la
définition de quotas d'utilisateurs et de groupes dans un environnement contenant un grand
nombre d'utilisateurs ou de groupes.

Vous pouvez définir des quotas d'utilisateurs ou de groupes à l'aide des propriétés zfs
userquota et zfs groupquota comme suit :

16

Guide d'administration Solaris ZFS • Octobre 2009

# zfs set userquota@user1=5G tank/data

# zfs set groupquota@staff=10G tank/staff/admins

Vous pouvez afficher le paramètre de quota actuel d'un utilisateur ou d'un groupe comme suit :

Nouveautés de ZFS

# zfs get userquota@user1 tank/data

NAME

PROPERTY

VALUE

tank/data userquota@user1 5G

SOURCE

local

# zfs get groupquota@staff tank/staff/admins

NAME

PROPERTY

VALUE

tank/staff/admins groupquota@staff 10G

SOURCE

local

Affichez des informations générales sur les quotas comme suit :

# zfs userspace tank/data

TYPE

NAME

USED QUOTA

POSIX User root

3K

none

POSIX User user1

0

5G

# zfs groupspace tank/staff/admins

TYPE

NAME

USED QUOTA

POSIX Group root

3K

none

POSIX Group staff

0

10G

Vous pouvez visualiser l'utilisation de l'espace par chaque utilisateur ou groupe en affichant les
propriétés userused@user et groupused@ group comme suit :

# zfs get userused@user1 tank/staff

NAME

PROPERTY

VALUE

SOURCE

tank/staff userused@user1 213M

local

# zfs get groupused@staff tank/staff

NAME

PROPERTY

VALUE

tank/staff groupused@staff 213M

SOURCE

local

Pour plus d'informations sur la définition des quotas d'utilisateurs, consultez la section
“Définition des quotas et réservations ZFS” à la page 208.

Héritage direct ACL ZFS pour l'autorisation
d'exécution
Version Solaris 10 10/09 : dans les versions précédentes de Solaris, vous pouviez appliquer
l'héritage ACL afin que tous les fichiers soient créés avec l'autorisation 0664 ou 0666. Si vous
souhaitez inclure le bit d'exécution de manière facultative à partir du mode de création de
fichier dans l'ACL héritée, vous pouvez utiliser l'héritage direct pour une autorisation
d'exécution dans cette version.

Chapitre 1 • Système de fichiers ZFS (présentation)

17

Nouveautés de ZFS

Si aclinherit=passthrough-x est activée sur un jeu de données ZFS, vous pouvez inclure une
autorisation d'exécution sur un fichier de sortie généré par l'outil cc ou gcc. Si l'ACL héritée ne
comprend pas les autorisations d'exécution, la sortie exécutable du compilateur ne sera pas
exécutable jusqu'à ce que vous utilisiez la commande chmod pour modifier les autorisations du
fichier.

Pour plus d'informations, reportez-vous à l'Exemple 8–12.

Améliorations apportées aux propriétés ZFS
Version Solaris 10/09 : les améliorations apportées aux systèmes de fichiers ZFS sont incluses
dans ces versions.
■ Définition des propriétés de système de fichiers ZFS lors de la création du pool - Vous
pouvez définir les propriétés de système de fichiers ZFS lors de la création du pool. Dans
l'exemple suivant, la compression est activée sur le système de fichiers ZFS qui est créé lors
de la création du pool.

# zpool create -O compression=on pool mirror c0t1d0 c0t2d0

■ Définition des propriétés de cache sur un système de fichiers ZFS – Deux nouvelles

propriétés de système de fichiers ZFS sont fournies afin de vous permettre de déterminer
quels sont les éléments mis en mémoire cache dans le cache principal (ARC) ou le cache
secondaire (L2ARC). Les propriétés du cache sont définies comme suit :

■

■

Primarycache : contrôle les éléments qui sont mis en cache dans le cache principal.
Secondarycache : contrôle les éléments qui sont mis en cache dans le cache secondaire.

■ Les valeurs possibles de ces deux propriétés sont all, none et metadata. Si elles sont

définies sur all, les données d'utilisateur et les métadonnées sont mises en cache. Si elle
est définie sur none, ni les données d'utilisateur ni les métadonnées ne sont mises en
cache. Si elles sont définies sur metadata, seules les métadonnées sont mises en cache. La
valeur par défaut est all.

Vous pouvez définir ces propriétés dans un système de fichiers existant ou lors de la création
du système de fichiers. Exemple :

# zfs set primarycache=metadata tank/datab

# zfs create -o primarycache=metadata tank/newdatab

Lorsque ces propriétés sont définies sur des systèmes de fichiers existants, seule la nouvelle
E/S est mise en cache en fonction de la valeur de ces propriétés.

Certains environnements de bases de données pourraient bénéficier de la non-mise en
cache des données d'utilisateur. Vous devrez déterminer si la définition des propriétés du
cache est appropriée pour votre environnement.

18

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

■ Propriétés de comptabilisation d'espace - De nouvelles propriétés de système de fichiers en

lecture seule vous aident à identifier l'utilisation de l'espace pour les clones, les systèmes de
fichiers et les volumes, mais pas les instantanés. Les propriétés sont les suivantes :

■

■

■

■

usedbychildren : indique la quantité d'espace utilisée par les enfants de ce jeu de
données, qui serait libérée si tous les enfants étaient supprimés. L'abréviation de la
propriété est usedchild.
Usedbydataset : indique la quantité d'espace utilisée par le jeu de données lui-même,
qui serait libérée si le jeu de données était supprimé, après la destruction préalable de
tous les instantanés et la suppression de toutes les valeurs refreservation.
L'abréviation de la propriété est usedds.
Usedbyrefreservation : indique la quantité d'espace utilisée par une valeur
refreservation définie sur ce jeu de données, qui serait libérée si la valeur
refreservation était supprimée. L'abréviation de la propriété est usedrefreserv.
Usedbysnapshots : identifie la quantité d'espace utilisée par les instantanés de ce jeu de
données. En particulier, elle correspond à la quantité d'espace qui serait libérée si
l'ensemble des instantanés de ce jeu de données étaient supprimés. Notez qu'il ne s'agit
pas simplement de la somme des propriétés used des instantanés, car l'espace peut être
partagé par plusieurs instantanés. L'abréviation de la propriété est usedsnap.

Ces nouvelles propriétés divisent la valeur de la propriété used dans les divers éléments qui
utilisent de l'espace. En particulier, la valeur de la propriété used est divisée comme suit :

used property = usedbychildren + usedbydataset + usedbyrefreservation + usedbysnapshots

Vous pouvez visualiser ces propriétés à l'aide de la commande zfs list -o space.
Exemple :

$ zfs list -o space

NAME

rpool

AVAIL

USED USEDSNAP USEDDS USEDREFRESERV USEDCHILD

25.4G 7.79G

rpool/ROOT

25.4G 6.29G

rpool/ROOT/snv_98 25.4G 6.29G

rpool/dump

25.4G 1.00G

rpool/export

25.4G

38K

rpool/export/home 25.4G

18K

rpool/swap

25.8G

512M

0

0

0

0

0

0

0

64K

18K

6.29G

1.00G

20K

18K

111M

0

0

0

0

0

0

401M

7.79G

6.29G

0

0

18K

0

0

La commande ci-dessus est équivalente à la commande zfs list
- o name,avail,used,usedsnap,usedds,usedrefreserv,usedchild -t filesystem,volume.

■ Liste d'instantanés– La propriété de pool listsnapshots vérifie si les informations sur les

instantanés sont affichées par la commande zfs list. La valeur par défaut est on, ce qui
signifie que les informations sur les instantanés sont affichées par défaut.
Si vous désactivez la propriété listsnapshots, vous pouvez utiliser la commande zfs list
-t snapshots pour afficher les informations sur l'instantané.

Chapitre 1 • Système de fichiers ZFS (présentation)

19

Nouveautés de ZFS

Récupération de périphérique de journal ZFS
Version Solaris 10 10/09 : dans la présente version, ZFS identifie les défaillances de journal
d'intention dans la commande zpool status. FMA signale également ces erreurs. ZFS et FMA
décrivent comment récupérer les données en cas de défaillance du journal d'intention.

Par exemple, si le système s'arrête soudainement avant que les opérations d'écriture synchrone
ne soient affectées à un pool disposant d'un périphérique de journal distinct, un message tel que
le suivant s'affiche :

# zpool status -x

pool: pool

state: FAULTED

status: One or more of the intent logs could not be read.

Waiting for adminstrator intervention to fix the faulted pool.

action: Either restore the affected device(s) and run ’zpool online’,

or ignore the intent log records by running ’zpool clear’.

scrub: none requested

config:

NAME

pool

STATE

READ WRITE CKSUM

FAULTED

mirror

ONLINE

c0t1d0 ONLINE

c0t4d0 ONLINE

logs

FAULTED

c0t5d0

UNAVAIL

0

0

0

0

0

0

0

0

0

0

0

0

0 bad intent log

0

0

0

0 bad intent log

0 cannot open

Vous devrez résoudre la défaillance de périphérique de journal de l'une des manières suivantes :
■ Remplacez ou récupérez le périphérique de journal. Dans cet exemple, le périphérique est

c0t5d0.

■ Mettez le périphérique de journal en ligne.

# zpool online pool c0t5d0

■ Réinitialisez la condition d'erreur de périphérique de journal défaillante.

# zpool clear pool

Si vous souhaitez effectuer une récupération suite à cette erreur sans remplacer la défaillance de
périphérique de journal, vous pouvez effacer l'erreur à l'aide de la commande zpool clear.
Dans ce scénario, le pool fonctionnera en mode dégradé et les enregistrements de journal seront
enregistrés dans le pool principal jusqu'à ce que le périphérique de journal distinct soit
remplacé.

Envisagez d'utiliser des périphériques de journal mis en miroir afin de réduire la probabilité
d'un scénario de défaillance de périphérique de journal.

20

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

L'utilisation des périphériques de cache dans votre
pool de stockage ZFS
Version Solaris 10 10/09 : dans cette version de Solaris, vous pouvez créer un pool et spécifier
les périphériques de cache, qui sont utilisés pour mettre en cache les données du pool de
stockage.

Les périphériques de stockage fournissent une couche de mise en cache supplémentaire entre la
mémoire principale et le disque. L'utilisation de périphériques de cache constitue la meilleure
amélioration de performances pour les charges de travail de lecture aléatoire constituées
principalement de contenu statique.

Vous pouvez spécifier un ou plusieurs périphériques de cache à la création du pool. Exemple :

# zpool create pool mirror c0t2d0 c0t4d0 cache c0t0d0

# zpool status pool

pool: pool

state: ONLINE

scrub: none requested

config:

NAME

pool

ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c0t4d0 ONLINE

cache

c0t0d0

ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Une fois les périphériques de cache ajoutés, ils se remplissent progressivement de contenu
provenant de la mémoire principale. En fonction de la taille du périphérique de cache, le
remplissage peut prendre plus d'une heure. La capacité et les lectures sont contrôlables à l'aide
de la commande zpool iostat comme indiqué ci-dessous :

# zpool iostat -v pool 5

Une fois le pool créé, vous pouvez y ajouter des périphériques de cache ou les en supprimer.

Pour plus d'informations, reportez-vous à la section “Création d'un pool de stockage ZFS avec
des périphériques de cache” à la page 70 et à l'Exemple 4–4.

Chapitre 1 • Système de fichiers ZFS (présentation)

21

Nouveautés de ZFS

Migration de zone dans un environnement ZFS
Solaris 10 5/09 Release : cette version étend la prise en charge des zones de migration dans un
environnement ZFS avec Live Upgrade. Pour plus d'informations, reportez-vous à la section
“Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones
(Solaris 10 5/09 et Solaris 10 10/09)” à la page 147.

Vous trouverez la liste des problèmes connus relatifs à cette version dans les notes de version de
Solaris 10 5/09.

Prise en charge de l'installation et de l'initialisation de
ZFS
Version Solaris 10 10/08 : cette version permet d'installer et d'initialiser un système de fichiers
racine ZFS. Vous pouvez utiliser l'option d'installation initiale ou la fonction JumpStart pour
installer un système de fichiers racine ZFS. Vous pouvez sinon utiliser la fonction Live Upgrade
pour migrer d'un système de fichiers racine UFS vers un système de fichiers racine ZFS. La prise
en charge ZFS des périphériques de swap et de vidage est également disponible. Pour plus
d'informations, reportez-vous au Chapitre 5, “Installation et initialisation d'un système de
fichiers racine ZFS”.

Vous trouverez la liste des problèmes connus relatifs à cette version dans les notes de version de
Solaris 10 10/08.

Rétablissement d'un jeu de données sans démontage
Version Solaris 10 10/08 : cette version permet de rétablir un jeu de données sans avoir à le
démonter au préalable. Cette fonction signifie que l'option zfs rollback -f n'est plus
nécessaire pour effectuer un démontage forcé. L'option -f n'est plus prise en charge et est
ignorée si elle est spécifiée.

Améliorations apportées à la commande zfs send
Version Solaris 10 10/08 D : les améliorations suivantes ont été apportées à la commande zfs
send dans cette version.
■ Envoi de tous les flux incrémentiels d'un instantané vers un instantané cumulatif. Exemple :

# zfs list

NAME

pool

USED AVAIL REFER MOUNTPOINT

428K 16.5G

20K /pool

pool/fs

71K 16.5G

21K /pool/fs

22

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

pool/fs@snapA

pool/fs@snapB

pool/fs@snapC

16K

17K

17K

- 18.5K -

-

20K -

- 20.5K -

pool/fs@snapD

0

-

21K -

# zfs send -I pool/fs@snapA pool/fs@snapD > /snaps/fs@combo

Tous les instantanés incrémentiels de fs@snapA à fs@snapD sont envoyés vers fs@combo.

■ Envoi d'un flux incrémentiel à partir de l'instantané d'origine pour créer un clone.
L'instantané d'origine doit déjà exister sur le côté récepteur afin d'accepter le flux
incrémentiel. Exemple :

# zfs send -I pool/fs@snap1 pool/clone@snapA > /snaps/fsclonesnap-I

.

.

# zfs receive -F pool/clone < /snaps/fsclonesnap-I

■ Envoi d'un flux de réplication de tous les systèmes de fichiers descendants, jusqu'aux
instantanés nommés. Une fois reçus, les propriétés, instantanés, systèmes de fichiers
descendants et clones sont conservés. Exemple :

zfs send -R pool/fs@snap > snaps/fs-R

Pour une illustration détaillée, reportez-vous à l'Exemple 7–1.

■ Envoi d'un flux de réplication incrémentiel.

zfs send -R -[iI] @snapA pool/fs@snapD

Pour une illustration détaillée, reportez-vous à l'Exemple 7–1.

Pour plus d'informations, reportez-vous à la section “Envoi et réception de flux d'instantanés
ZFS complexes” à la page 226.

Quotas et réservations ZFS pour les données de
système de fichiers uniquement
Version Solaris 10 10/08 : outre les fonctionnalités de quotas et de réservation ZFS, cette
version fournit des quotas et réservations de jeux de données excluant les données
descendantes, telles que les instantanés et les clones, lors de la comptabilisation de l'utilisation
de l'espace.
■ La propriété refquota limite la quantité d'espace consommable par un jeu de données.

Cette propriété définit une quantité d'espace utilisable maximale. Cette limite fixe n'inclut
pas l'espace utilisé par les descendants, tels que les instantanés et les clones.

■ La propriété refreservation définit la quantité minimale d'espace accordée à un jeu de

données (descendants exclus).

Chapitre 1 • Système de fichiers ZFS (présentation)

23

Nouveautés de ZFS

Par exemple, en définissant la propriété refquota de studentA sur 10 Go, vous spécifiez une
limite fixe de 10 Go d'espace référencé. Pour une plus grande flexibilité, vous pouvez définir un
quota de 20 Go qui vous permet de gérer les instantanés de studentA.

# zfs set refquota=10g tank/studentA

# zfs set quota=20g tank/studentA

Pour plus d'informations, reportez-vous à la section “Définition des quotas et réservations ZFS”
à la page 208.

Propriétés de pool de stockage ZFS
Version Solaris 10 10/08 : les propriétés de pool de stockage ZFS ont été présentées dans une
version antérieure. Cette version fournit des informations sur les propriétés supplémentaires.
Exemple :

# zpool get all mpool

NAME

PROPERTY

VALUE

SOURCE

mpool size

mpool used

33.8G

5.76G

mpool available

28.0G

mpool capacity

17%

-

-

-

-

mpool altroot

-

default

mpool health

ONLINE

-

mpool guid

2689713858991441653 -

mpool version

10

default

mpool bootfs

mpool/ROOT/zfsBE local

mpool delegation

on

mpool autoreplace off

mpool cachefile

-

default

default

default

mpool failmode

continue

local

Pour une description de ces propriétés, reportez-vous au Tableau 4–1.
■ Propriété cachefile – Version Solaris 10 10/08 : cette version fournit la propriété

cachefile qui contrôle l'emplacement de mise en cache des informations de configuration
de pool. Tous les pools du cache sont importés automatiquement au démarrage du système.
Toutefois, dans les environnements d'installation et de clustering, il peut s'avérer nécessaire
de placer ces informations en cache à un autre endroit afin d'éviter l'importation
automatique des pools.
Vous pouvez définir cette propriété afin de mettre la configuration de pool en cache à un
autre emplacement. Il reste alors possible d'importer ultérieurement ce dernier à l'aide de la
commande zpool import c. Cette propriété n'est pas utilisée dans la plupart des
configurations ZFS.

24

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

La propriété cachefile n'est pas persistante et n'est pas stockée sur le disque. Elle remplace
la propriété temporary qui, dans les versions précédentes de Solaris, indiquait que les
informations de pool ne devaient pas être mises en cache.

■ Propriété failmode – Version Solaris 10 10/08 : dans cette version, la propriété failmode
permet de déterminer l'action à effectuer en cas d'échec catastrophique de pool causé par la
perte de connectivité d'un périphérique ou la panne de tous les périphériques du pool. Vous
pouvez définir la propriété failmode sur l'une des valeurs suivantes : wait, continue ou
panic. La valeur par défaut est wait : vous devez reconnecter le périphérique ou remplacer
le périphérique défaillant, puis effacer l'erreur à l'aide de la commande zpool clear.
La propriété failmode est définie comme les autres propriétés ZFS définissables, avant ou
après la création du pool. Exemple :

# zpool set failmode=continue tank

# zpool get failmode tank

NAME PROPERTY VALUE

SOURCE

tank failmode continue local

# zpool create -o failmode=continue users mirror c0t1d0 c1t1d0

Pour une description des propriétés de pool ZFS, reportez-vous au Tableau 4–1.

Améliorations apportées à l'historique des
commandes ZFS (zpool history)
Version Solaris 10 10/08 : la commande zpool history a été améliorée afin de fournir les
nouvelles fonctions suivantes :
■ Les informations sur les événements des systèmes de fichiers ZFS s'affichent. Exemple :

# zpool history

History for ’rpool’:

2009-08-26.16:49:07 zpool create -f -o failmode=continue -R /a -m legacy -o cachefile=

/tmp/root/etc/zfs/zpool.cache rpool c1t1d0s0

2009-08-26.16:49:08 zfs set canmount=noauto rpool

2009-08-26.16:49:08 zfs set mountpoint=/rpool rpool

2009-08-26.16:49:09 zfs create -o mountpoint=legacy rpool/ROOT

2009-08-26.16:49:10 zfs create -b 8192 -V 2048m rpool/swap

2009-08-26.16:49:11 zfs create -b 131072 -V 1024m rpool/dump

2009-08-26.16:49:14 zfs create -o canmount=noauto rpool/ROOT/zfs1009BE

2009-08-26.16:49:15 zpool set bootfs=rpool/ROOT/zfs1009BE rpool

2009-08-26.16:49:15 zfs set mountpoint=/ rpool/ROOT/zfs1009BE

2009-08-26.16:49:16 zfs set canmount=on rpool

2009-08-26.16:49:17 zfs create -o mountpoint=/export rpool/export

2009-08-26.16:49:18 zfs create rpool/export/home

2009-08-28.08:17:59 zpool attach rpool c1t1d0s0 c1t0d0s0

Chapitre 1 • Système de fichiers ZFS (présentation)

25

Nouveautés de ZFS

■ L'option -l permet d'afficher un format complet comprenant le nom de l'utilisateur, le nom

de l'hôte et la zone dans laquelle l'opération a été effectuée. Exemple :

# zpool history -l rpool

History for ’rpool’:

2009-08-26.16:49:07 zpool create -f -o failmode=continue -R /a -m legacy -o cachefile=

/tmp/root/etc/zfs/zpool.cache rpool c1t1d0s0 [user root on neo:global]

2009-08-26.16:49:08 zfs set canmount=noauto rpool [user root on neo:global]

2009-08-26.16:49:08 zfs set mountpoint=/rpool rpool [user root on neo:global]

2009-08-26.16:49:09 zfs create -o mountpoint=legacy rpool/ROOT [user root on neo:global]

2009-08-26.16:49:10 zfs create -b 8192 -V 2048m rpool/swap [user root on neo:global]

2009-08-26.16:49:11 zfs create -b 131072 -V 1024m rpool/dump [user root on neo:global]

2009-08-26.16:49:14 zfs create -o canmount=noauto rpool/ROOT/zfs1009BE [user root on neo:global]

2009-08-26.16:49:15 zpool set bootfs=rpool/ROOT/zfs1009BE rpool [user root on neo:global]

2009-08-26.16:49:15 zfs set mountpoint=/ rpool/ROOT/zfs1009BE [user root on neo:global]

2009-08-26.16:49:16 zfs set canmount=on rpool [user root on neo:global]

2009-08-26.16:49:17 zfs create -o mountpoint=/export rpool/export [user root on neo:global]

2009-08-26.16:49:18 zfs create rpool/export/home [user root on neo:global]

2009-08-28.08:17:59 zpool attach rpool c1t1d0s0 c1t0d0s0 [user root on neo:global]

■ Option -i pour l'affichage des informations relatives aux événements internes utilisables

pour établir des diagnostics. Exemple :

# zpool history -i rpool

History for ’rpool’:

2009-08-26.16:49:07 zpool create -f -o failmode=continue -R /a -m legacy -o cachefile=

/tmp/root/etc/zfs/zpool.cache rpool c1t1d0s0

2009-08-26.16:49:07 [internal property set txg:6] mountpoint=/ dataset = 16

2009-08-26.16:49:07 [internal property set txg:7] mountpoint=legacy dataset = 16

2009-08-26.16:49:08 [internal property set txg:8] canmount=2 dataset = 16

2009-08-26.16:49:08 zfs set canmount=noauto rpool

2009-08-26.16:49:08 [internal property set txg:10] mountpoint=/rpool dataset = 16

2009-08-26.16:49:08 zfs set mountpoint=/rpool rpool

2009-08-26.16:49:09 [internal create txg:12] dataset = 31

2009-08-26.16:49:09 [internal property set txg:13] mountpoint=legacy dataset = 31

2009-08-26.16:49:09 zfs create -o mountpoint=legacy rpool/ROOT

2009-08-26.16:49:09 [internal create txg:15] dataset = 37

2009-08-26.16:49:10 [internal property set txg:16] refreservation=2147483648 dataset = 37

2009-08-26.16:49:10 [internal refreservation set txg:16] 2147483648 dataset = 37

2009-08-26.16:49:10 zfs create -b 8192 -V 2048m rpool/swap

2009-08-26.16:49:10 [internal create txg:18] dataset = 43

2009-08-26.16:49:10 [internal property set txg:19] refreservation=1073741824 dataset = 43

2009-08-26.16:49:10 [internal refreservation set txg:19] 1073741824 dataset = 43

.

.

.

26

Pour plus d'informations sur l'utilisation de la commande zpool history, reportez-vous à la
section “Identification de problèmes dans ZFS” à la page 291.

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

Mise à niveau des systèmes de fichiers ZFS (zfs
upgrade)
Version Solaris 10 10/08 : la commande zfs upgrade a été ajoutée à cette version pour pouvoir
apporter des améliorations futures aux systèmes de fichiers ZFS existants. Les pools de stockage
disposent d'une fonctionnalité de mise à niveau similaire permettant d'apporter des
améliorations aux pools de stockage existants.

Exemple :

# zfs upgrade

This system is currently running ZFS filesystem version 3.

All filesystems are formatted with the current version.

Remarque – Les systèmes de fichiers mis à niveau et tout flux créé à partir de ces systèmes de
fichiers mis à niveau à l'aide de la commande zfs send ne sont pas accessibles sur des systèmes
exécutant des versions antérieures du logiciel.

Administration déléguée de ZFS
Version Solaris 10 10/08 : cette version vous permet de déléguer des droits précis à des
utilisateurs sans privilèges pour qu'ils effectuent des tâches d'administration ZFS.

Les commandes zfs allow et zfs unallow permettent d'accorder ou de retirer les
autorisations.

Vous pouvez modifier la capacité d'utilisation de l'administration déléguée à l'aide de la
propriété delegation du pool. Exemple :

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation on

default

# zpool set delegation=off users

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation off

local

Par défaut, la propriété delegation est activée.

Pour plus d'informations, reportez-vous au Chapitre 9, “Administration déléguée de ZFS” et à
la page de manuel zfs(1M).

Chapitre 1 • Système de fichiers ZFS (présentation)

27

Nouveautés de ZFS

Configuration de périphériques de journalisation ZFS
distincts
Version Solaris 10 10/08 : le journal d'intention ZFS (ZIL) permet de répondre aux exigences
de la norme POSIX dans le cadre de transactions synchronisées. Par exemple, les transactions
de base de données doivent souvent se trouver sur des périphériques de stockage stables
lorsqu'elles sont obtenues à partir d'un appel système. NFS et d'autres applications peuvent
également assurer la stabilité des données à l'aide de fsync(). Par défaut, le ZIL est attribué à
partir de blocs dans le pool de stockage principal. Cependant, les performances peuvent parfois
être améliorées en vous servant de périphériques de journalisation d'intention distincts dans
votre pool de stockage ZFS, notamment d'une NVRAM ou d'un disque dédié.

Les périphériques de journalisation du ZIL ne sont pas liés aux fichiers journaux de base de
données.

Vous pouvez configurer un périphérique de journalisation ZFS à la création du pool de stockage
ou à un moment ultérieur. Pour obtenir des exemples de configuration de périphériques de
journalisation, reportez-vous à la section “Création d'un pool de stockage ZFS avec des
périphériques de journalisation” à la page 69 et à la section “Ajout de périphériques à un pool
de stockage” à la page 77.

connexion d'un périphérique de journal à un périphérique journal existant afin de créer un
périphérique mis en miroir. Cette opération est similaire à la connexion d'un périphérique à un
pool de stockage qui n'est pas mis en miroir.

Considérez les points suivants pour déterminer si la configuration d'un périphérique de
journalisation ZFS convient à votre environnement :
■ Toute amélioration des performances observée suite à l'implémentation d'un périphérique
de journalisation distinct dépend du type de périphérique, de la configuration matérielle du
pool et de la charge de travail de l'application. Pour des informations préliminaires sur les
performances, consultez le blog suivant :

http://blogs.sun.com/perrin/entry/slog_blog_or_blogging_on

■ Les périphériques de journalisation peuvent être mis en miroir et leur réplication peut être

annulée, mais RAID-Z n'est pas pris en charge pour les périphériques de journalisation.
Si un périphérique de journalisation distinct n'est pas mis en miroir et que le périphérique
contenant le journal échoue, le stockage des blocs de journal retourne sur le pool de
stockage.

■

■ Les périphériques de journalisation peuvent être ajoutés, remplacés, connectés,

déconnectés, importés et exportés en tant que partie du pool de stockage. Les périphériques
de journalisation ne peuvent pour le moment pas être supprimés.

28

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

■ La taille minimale d'un périphérique de journalisation correspond à la taille minimale de

chaque périphérique d'un pool, à savoir 64 Mo. La quantité de données en jeu pouvant être
stockée sur un périphérique de journalisation est relativement petite. Les blocs de journal
sont libérés lorsque la transaction du journal (appel système) est validée.

■ La taille maximale d'un périphérique de journalisation doit être approximativement égale à
la moitié de la taille de la mémoire physique car il s'agit de la quantité maximale de données
en jeu potentielles pouvant être stockée. Si un système dispose par exemple de 16 Go de
mémoire physique, considérez une taille maximale de périphérique de journalisation de
8 Go.

Création de jeux de données ZFS intermédiaires
Version Solaris 10 10/08 : vous pouvez appliquer l'option -p aux commandes zfs create, zfs
clone et zfs rename afin de créer rapidement un jeu de données intermédiaire s'il n'existe pas
encore.

Par exemple, créez les jeux de données ZFS users/area51 dans le pool de stockage datab.

# zfs list

NAME

datab

USED AVAIL REFER MOUNTPOINT

106K 16.5G

18K /datab

# zfs create -p -o compression=on datab/users/area51

Si le jeu de données intermédiaire existe pendant l'opération de création, celle-ci est réussie.

Les propriétés spécifiées s'appliquent au jeu de données cible, mais pas aux jeux de données
intermédiaires. Exemple :

# zfs get mountpoint,compression datab/users/area51

NAME

PROPERTY

VALUE

SOURCE

datab/users/area51 mountpoint

/datab/users/area51 default

datab/users/area51 compression on

local

Le jeu de données intermédiaire est créé avec le point de montage par défaut. Toute propriété
supplémentaire est désactivée pour ce jeu de données. Exemple :

# zfs get mountpoint,compression datab/users

NAME

PROPERTY

VALUE

SOURCE

datab/users mountpoint

/datab/users default

datab/users compression off

default

Pour de plus amples informations, reportez-vous à la page de manuel zfs(1M).

Chapitre 1 • Système de fichiers ZFS (présentation)

29

Nouveautés de ZFS

Améliorations apportées à la connexion à chaud à ZFS
Version Solaris 10 10/08 : dans cette version, ZFS répond de manière plus efficace aux
périphériques supprimés et fournit un mécanisme permettant d'identifier automatiquement
des périphériques insérés, avec les améliorations suivantes :
■ Vous pouvez remplacer un périphérique existant par un périphérique équivalent sans

utiliser la commande zpool replace.
La propriété autoreplace contrôle le remplacement automatique de périphériques. Si la
propriété est désactivée, l'administrateur doit initier le remplacement du périphérique à
l'aide de la commande zpool replace. Si la propriété est activée, tout nouveau périphérique
se trouvant au même emplacement physique qu'un périphérique qui appartenait au pool est
automatiquement formaté et remplacé. Le comportement par défaut est "off".

■ L'état de pool de stockage REMOVED est fourni lors du retrait du périphérique ou d'un disque

hot spare, en cas de retrait physique du périphérique alors que le système est en cours
d'exécution. Si un disque hot spare est disponible, il remplace le périphérique retiré.
Si un périphérique est retiré, puis inséré, il est mis en ligne. Si un disque hot spare est activé
lors de la réinsertion du périphérique, le disque hot spare est retiré une fois l'opération en
ligne terminée.

■

■ La détection automatique du retrait ou de l'insertion de périphériques dépend du matériel

utilisé. Il est possible qu'elle ne soit pas prise en charge sur certaines plates-formes. Par
exemple, les périphériques USB sont configurés automatiquement après insertion. Il peut
être toutefois nécessaire d'utiliser la commande cfgadm -c configure pour configurer un
lecteur SATA.

■ Les disques hot spare sont consultés régulièrement afin de vérifier qu'ils sont en ligne et

disponibles.

Pour de plus amples informations, reportez-vous à la page de manuel zpool(1M).

Renommage récursif d'instantanés ZFS (zfs rename
-r)
Version Solaris 10 10/08 : vous pouvez renommer tous les instantanés ZFS descendants de
manière récursive à l'aide de la commande zfs rename -r.

Prenez par exemple un instantané de plusieurs systèmes de fichiers ZFS.

# zfs snapshot -r users/home@today

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

216K 16.5G

20K /users

users/home

76K 16.5G

22K /users/home

30

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

users/home@today

0

-

22K -

users/home/markm

18K 16.5G

18K /users/home/markm

users/home/markm@today

0

-

18K -

users/home/marks

18K 16.5G

18K /users/home/marks

users/home/marks@today

0

-

18K -

users/home/neil

18K 16.5G

18K /users/home/neil

users/home/neil@today

0

-

18K -

Renommez ensuite les instantanés de la manière suivante.

# zfs rename -r users/home@today @yesterday

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

216K 16.5G

20K /users

users/home

76K 16.5G

22K /users/home

users/home@yesterday

0

-

22K -

users/home/markm

18K 16.5G

18K /users/home/markm

users/home/markm@yesterday

0

-

18K -

users/home/marks

18K 16.5G

18K /users/home/marks

users/home/marks@yesterday

0

-

18K -

users/home/neil

18K 16.5G

18K /users/home/neil

users/home/neil@yesterday

0

-

18K -

Les instantanés sont le seul jeu de données qu'il est possible de renommer de façon récursive.
Pour plus d'informations sur les instantanés, reportez-vous à la section “Présentation des
instantanés ZFS” à la page 215 et à l'entrée de blog décrivant la création d'instantanés restaurés :

http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy

La compression GZIP est disponible pour ZFS
Version Solaris 10 10/08 : dans cette version de Solaris, vous pouvez définir la compression
gzip sur des systèmes de fichiers ZFS en plus de la compression lzjb. Vous pouvez spécifier la
compression gzip (format par défaut) ou gzip-N où N correspond à un chiffre entre 1 et 9.
Exemple :

# zfs create -o compression=gzip users/home/snapshots

# zfs get compression users/home/snapshots

NAME

PROPERTY

VALUE

users/home/snapshots compression gzip

SOURCE

local

# zfs create -o compression=gzip-9 users/home/oldfiles

# zfs get compression users/home/oldfiles

NAME

PROPERTY

VALUE

SOURCE

users/home/oldfiles

compression gzip-9

local

Pour plus d'informations sur la configuration des propriétés ZFS, reportez-vous à la section
“Définition des propriétés ZFS” à la page 196.

Chapitre 1 • Système de fichiers ZFS (présentation)

31

Nouveautés de ZFS

Stockage de plusieurs copies de données utilisateur
ZFS
Version Solaris 10 10/08 : à des fins de fiabilité, les métadonnées d'un système de fichiers ZFS
sont automatiquement stockées plusieurs fois sur divers disques, si possible. Cette fonction est
connue sous le terme anglais de ditto blocks.

Cette version vous permet également de demander à stocker plusieurs copies des données
utilisateur par système de fichiers à l'aide de la commande zfs set copies. Exemple :

# zfs set copies=2 users/home

# zfs get copies users/home

NAME

PROPERTY VALUE

SOURCE

users/home copies

2

local

Les valeurs disponibles sont 1, 2 et 3. La valeur par défaut est 1. Ces copies constituent un ajout à
toute redondance de niveau pool, par exemple dans une configuration en miroir ou RAID-Z.

Stocker plusieurs copies des données utilisateur ZFS présente les avantages suivants :
■ Cela améliore la rétention des données en autorisant leur récupération à partir d'erreurs de

lecture de blocs irrécupérables, comme par exemple des défaillances de média pour
l'ensemble des configurations ZFS.

■ Cela garantit la sécurité des données même si un seul disque est disponible.
■ Cela permet de choisir les stratégies de protection des données par système de fichiers et de

dépasser les capacités du pool de stockage.

Selon l'allocation des blocs "ditto" dans le pool de stockage, plusieurs copies peuvent être
placées sur un seul disque. La saturation ultérieure d'un disque peut engendrer l'indisponibilité
de tous les blocs "ditto".

Vous pouvez envisager l'utilisation des blocs "ditto" lorsque vous créez accidentellement un
pool non redondant et lorsque vous avez besoin de définir des stratégies de conservation de
données.

Pour une description détaillée de l'impact de la configuration de copies sur un système
comprenant un pool d'un seul disque ou un pool de plusieurs disques, sur la protection globale
des données, consultez le blog suivant :

http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection

Pour plus d'informations sur la configuration des propriétés ZFS, reportez-vous à la section
“Définition des propriétés ZFS” à la page 196.

32

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

Amélioration de la sortie de la commande zpool

status
Version Solaris 10 8/07 : la commande zpool status -v permet d'afficher la liste des fichiers
comportant des erreurs persistantes. Auparavant, il fallait utiliser la commande find -inum
pour identifier les noms de fichiers à partir de la liste d'inodes affichée.

Pour obtenir des informations supplémentaires sur l'affichage d'une liste de fichiers comportant
des erreurs persistantes, consultez la section “Réparation d'un fichier ou répertoire corrompu”
à la page 310.

Améliorations de ZFS et Solaris iSCSI
Version Solaris 10 8/07 : dans cette version de Solaris, vous pouvez créer un volume ZFS en
tant que périphérique cible Solaris iSCSI en configurant la propriété shareiscsi sur le volume
ZFS. Cette méthode permet de configurer rapidement et facilement une cible Solaris iSCSI.
Exemple :

# zfs create -V 2g tank/volumes/v2

# zfs set shareiscsi=on tank/volumes/v2

# iscsitadm list target

Target: tank/volumes/v2

iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a

Connections: 0

Une fois la cible iSCSI créée, configurez l'initiateur iSCSI. Pour obtenir des informations sur la
configuration d'un initiateur Solaris iSCSI, consultez le Chapitre 14, “Configuring Solaris iSCSI
Targets and Initiators (Tasks)” du System Administration Guide: Devices and File Systems.

Pour obtenir des informations supplémentaires sur la gestion d'un volume ZFS en tant que cible
iSCSI, consultez la section “Utilisation d'un volume ZFS en tant que cible iSCSI Solaris”
à la page 277.

Historique de commande ZFS (zpool history)
Version Solaris 10 8/07 : dans cette version de Solaris, ZFS consigne automatiquement les
commandes zfs et zpool qui modifient les informations concernant l'état du pool. Exemple :

# zpool history

History for ’newpool’:

2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0

2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0

2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0

Chapitre 1 • Système de fichiers ZFS (présentation)

33

Nouveautés de ZFS

2007-04-25.11:38:09 zfs create newpool/user1

2007-04-25.11:38:15 zfs destroy newpool/user1

History for ’tank’:

2007-04-25.11:46:28 zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0

Cette fonction vous permet, ainsi qu'au personnel de support Sun, d'identifier le jeu exact de
commandes ZFS exécutées pour résoudre un scénario d'erreur.

Vous pouvez identifier un pool de stockage spécifique grâce à la commande zpool history.
Exemple :

# zpool history newpool

History for ’newpool’:

2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0

2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0

2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0

2007-04-25.11:38:09 zfs create newpool/user1

2007-04-25.11:38:15 zfs destroy newpool/user1

Dans cette version de Solaris, la commande zpool history n'enregistre pas l'ID utilisateur, le
nom d'hôte ni le nom de la zone. Pour plus d'informations, reportez-vous à la section
“Améliorations apportées à l'historique des commandes ZFS (zpool history)” à la page 25.

Pour obtenir des informations supplémentaires sur la résolution de problèmes relatifs à ZFS,
consultez la section“Identification de problèmes dans ZFS” à la page 291.

Améliorations de propriétés ZFS

Propriété ZFS xattr
Version Solaris 10 8/07 : vous pouvez utiliser la propriété xattr pour désactiver ou activer les
attributs avancés d'un système de fichiers ZFS spécifique. Par défaut, les attributs sont activés.
Pour obtenir une description des propriétés ZFS, reportez-vous à la section “Présentation des
propriétés ZFS” à la page 179.

Propriété ZFS canmount
Version Solaris 10 8/07 : la nouvelle propriété canmount permet de spécifier s'il est possible de
monter un jeu de données à l'aide de la commande zfs mount. Pour plus d'informations,
reportez-vous à la section “Propriété canmount” à la page 190.

34

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

Propriétés ZFS définies par l'utilisateur
Version Solaris 10 8/07 : outre les propriétés natives standard qui permettent d'exporter des
statistiques internes ou de contrôler le comportement du système de fichiers ZFS, ZFS assure la
prise en charge des propriétés définies par l'utilisateur. Les propriétés définies par l'utilisateur
n'ont aucune incidence sur le comportement du système ZFS. En revanche, elles permettent
d'annoter les jeux de données avec des informations adaptées à votre environnement.

Pour obtenir des informations supplémentaires, consultez la section “Propriétés ZFS définies
par l'utilisateur” à la page 192.

Définition des propriétés lors de la création de systèmes de fichiers ZFS
Version Solaris 10 8/07 : dans cette version de Solaris, vous pouvez non seulement définir les
propriétés d'un système de fichiers existant, mais vous avez également la possibilité de définir
les propriétés lors de la création du système de fichiers.

Les exemples suivants montrent la syntaxe équivalente :

# zfs create tank/home

# zfs set mountpoint=/export/zfs tank/home

# zfs set sharenfs=on tank/home

# zfs set compression=on tank/home

# zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home

Affichage de la totalité des informations de systèmes
de fichiers ZFS
Version Solaris 10 8/07 : dans cette version de Solaris, vous pouvez utiliser diverses formes de
la commande zfs get pour afficher des informations sur l'ensemble des jeux de données si vous
n'en spécifiez pas un en particulier ou si vous ne spécifiez pas all. Dans les versions
précédentes, la commande zfs get ne permettait pas de récupérer la totalité des informations
relatives au jeu de données.

Exemple :

# zfs get -s local all

tank/home

atime

tank/home/bonwick

atime

tank/home/marks

quota

off

off

50G

local

local

local

Chapitre 1 • Système de fichiers ZFS (présentation)

35

Nouveautés de ZFS

Nouvelle option zfs receive -F
Solaris 10 8/07 : dans cette version de Solaris, vous pouvez utiliser la nouvelle option -F avec la
commande zfs receive pour forcer une restauration du système de fichiers à l'instantané le
plus récent avant d'effectuer la réception. Cette option peut s'avérer nécessaire en cas de
modification du système entre le moment où une restauration se produit et celui où la réception
est initialisée.

Pour plus d'informations, reportez-vous à la section “Réception d'un instantané ZFS”
à la page 225.

Instantanés ZFS récursifs
Version Solaris 10 11/06 : lorsque vous créez un instantané de système de fichiers à l'aide de la
commande zfs snapshot, spécifiez l'option -r si vous souhaitez créer des instantanés de façon
récursive pour l'ensemble des systèmes de fichiers descendants. Cette option- permet en outre
de détruire récursivement tous les instantanés descendants lors de la destruction d'un
instantané.

Une seule opération, dite atomique, permet de créer rapidement des instantanés ZFS récursifs.
Ceux-ci sont tous créés simultanément ou ne sont pas créés du tout. Grâce à ce type de sélection
instantanée, les opérations atomiques assurent ainsi la cohérence des données, y compris pour
les systèmes de fichiers descendants.

Pour obtenir des informations supplémentaires, consultez la section “Création et destruction
d'instantanés ZFS” à la page 216.

Double Parité RAID-Z (raidz2)
Version Solaris 10 11/06 : une configuration RAID-Z redondante peut désormais présenter
une parité simple ou double. En d'autres termes, le système peut subir respectivement une ou
deux pannes de périphérique sans perte de données. Le mot-clé raidz2 permet de spécifier une
configuration RAID-Z à deux parités. Pour spécifier une configuration RAID-Z à une parité,
vous avez le choix entre les mots-clés raidz et raidz1.

Pour plus d'informations, consultez la section “Création de pools de stockage RAID-Z”
à la page 68 ou la page de manuel zpool(1M).

36

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

Disques hot spare pour périphériques de pool de
stockage ZFS
Version Solaris 10 11/06 : La fonction de disque hot spare permet d'identifier les disques
pouvant être utilisés pour remplacer un périphérique défaillant dans un ou plusieurs pools de
stockage. Le périphérique hot spare peut immédiatement remplacer tout périphérique actif du
pool qui tombe en panne. Vous pouvez également effectuer ce remplacement manuellement.

Pour plus d'informations, reportez-vous à la section “Désignation des disques hot spare dans le
pool de stockage” à la page 89 et à la page de manuel zpool(1M).

Remplacement d'un système de fichiers ZFS par un
clone ZFS (zfs promote)
Version Solaris 10 11/06 : la commande zfs promote permet de remplacer un système de
fichiers ZFS existant par un clone de celui-ci. Cette fonctionnalité permet de tester la nouvelle
version d'un système de fichiers, puis de la définir comme système de fichiers actif.

Pour plus d'informations, reportez-vous à la section “Remplacement d'un système de fichiers
ZFS par un clone ZFS” à la page 222 et à la page de manuel zfs(1M).

Mise à niveau des pools de stockage ZFS (zpool
upgrade)
Version Solaris 10 6/06 : vous pouvez mettre à niveau vos pools de stockage à l'aide de la
commande zpool upgrade et tirer ainsi profit des fonctions les plus récentes. De plus, la
commande zpool status indique dorénavant si la version actuelle des pools est plus ancienne.

Pour plus d'informations, reportez-vous à la section “Mise à niveau de pools de stockage ZFS”
à la page 112 et à la page de manuel zpool(1M).

Si vous souhaitez utiliser la console d'administration ZFS dans un système comprenant un pool
provenant d'une version précédente de Solaris, veillez préalablement à mettre les pools à niveau.
La commande zpool status permet de savoir si une mise à niveau des pools est requise. Pour
obtenir des informations sur la console d'administration ZFS, reportez-vous à la rubrique
“Gestion Web ZFS” à la page 39.

Chapitre 1 • Système de fichiers ZFS (présentation)

37

Nouveautés de ZFS

Commandes de sauvegarde et de rétablissement ZFS
renommées
Version Solaris 10 6/06 : dans cette version de Solaris les commandes zfs backup et zfs
restore ont été renommées zfs send et zfs receive afin de mieux décrire leur fonction. Ces
commandes ont pour fonction d'enregistrer et de restaurer les représentations de flux de
données ZFS.
Pour plus d'informations sur ces commandes, reportez-vous à la section “Envoi et réception de
données ZFS” à la page 223.

Récupération de pools de stockage détruits
Version Solaris 10 6/06 : cette version inclut la commande zpool import -D qui permet la
récupération de pools précédemment détruits à l'aide de la commande zpool destroy.
Pour obtenir des informations supplémentaires, reportez-vous à la section “Récupération de
pools de stockage ZFS détruits” à la page 110.

Intégration de ZFS au gestionnaire de pannes
Version Solaris 10 6/06 : cette version intègre un moteur de diagnostic ZFS capable de
diagnostiquer et de signaler les pannes des périphériques et des pools. Les erreurs liées aux
sommes de contrôle, aux E/S et aux périphériques font également l'objet d'un rapport
lorsqu'elles sont liées à la défaillance d'un pool ou d'un périphérique.
Le moteur de diagnostic n'effectue pas d'analyses prédictives d'erreurs liées aux sommes de
contrôle ou aux E/S et n'inclut aucune action proactive basée sur l'analyse de pannes.
En cas de panne ZFS, la commande fmd peut émettre un message tel que le suivant :

SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Fri Aug 28 09:10:27 PDT 2009

PLATFORM: SUNW,Sun-Fire-T200, CSN: -, HOSTNAME: neo

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: d6725ad6-4546-6c48-fa16-eace4d371981

DESC: A ZFS device failed. Refer to http://sun.com/msg/ZFS-8000-D3 for more information.

AUTO-RESPONSE: No automated response will occur.

IMPACT: Fault tolerance of the pool may be compromised.

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Il est recommandé de suivre les instructions spécifiques de la commande zpool status, afin
d'identifier et de résoudre rapidement la panne.
Pour obtenir un exemple de récupération suite à un problème de ZFS signalé, consultez la
section “Réparation d'un périphérique manquant” à la page 297.

38

Guide d'administration Solaris ZFS • Octobre 2009

Nouveautés de ZFS

Nouvelle commande zpool clear
Version Solaris 10 6/06 : cette version inclut la commande zpool clear permettant d'effacer
les décomptes d'erreurs associés à un périphérique ou au pool. Auparavant, les décomptes
d'erreurs étaient effacés lors de la mise en ligne du périphérique d'un pool avec la commande
zpool online. Pour plus d'informations, reportez-vous à la page de manuel zpool(1M) et à la
section “Suppression des périphériques de pool de stockage” à la page 86.

Format NFSv4 ACL compact
Version Solaris 10 6/06 : cette version offre trois formats ACL NFSv4 : détaillé, positionnel et
compact. Les nouveaux formats ACL compact et positionnel permettent de définir et d'afficher
les listes de contrôle. La commande chmod permet de définir les trois formats ACL. Vous pouvez
utiliser la commande ls -V pour l'affichage des formats ACL compact et positionnel et la
commande ls - v pour l'affichage du format ACL détaillé.

Pour plus d'informations, reportez-vous à la section “Configuration et affichage d'ACL dans des
fichiers ZFS en format compact” à la page 254, à la page de manuel chmod(1) et à la page de
manuel ls(1).

Outil de contrôle de système de fichiers (fsstat)
Version Solaris 10 6/06 : un nouvel outil de contrôle de système de fichiers, fsstat, est
disponible pour la génération de rapports relatifs aux opérations de système de fichiers. Les
activités peuvent être consignées par point de montage ou par type de système de fichiers.
L'exemple suivant illustre les activités générales de système de fichiers ZFS.

$ fsstat zfs

new name

name attr attr lookup rddir read read write write

file remov chng

get

set

ops

ops

ops bytes

ops bytes

7.82M 5.92M 2.76M 1.02G 3.32M 5.60G 87.0M 363M 1.86T 20.9M 251G zfs

Pour de plus amples informations, reportez-vous à la page de manuel fsstat(1M).

GestionWeb ZFS
Version Solaris 10 6/06 : un outil Web de gestion ZFS permet d'effectuer de nombreuses
opérations administratives. Cet outil permet de réaliser les tâches suivantes :

■

■

créer un nouveau pool de stockage ;
augmenter la capacité d'un pool existant ;

■ déplacer (exporter) un pool de stockage vers un autre système ;

Chapitre 1 • Système de fichiers ZFS (présentation)

39

Description de ZFS

■

■

■

■

importer un pool de stockage précédemment exporté afin qu'il soit disponible dans un autre
système ;
visualiser des informations sur les pools de stockage ;
créer un système de fichiers ;
créer un volume ;

■ prendre un instantané d'un système de fichiers ou d'un volume ;

■

restaurer un système de fichiers à l'aide d'un instantané précédent.

Vous pouvez accéder à la console d'administration ZFS via un navigateur Web sécurisé à l'URL
suivant :

https://system-name:6789/zfs

Si vous avez saisi l'URL correct mais ne parvenez pas à atteindre la console d'administration
ZFS, il est possible que le serveur ne soit pas démarré. Pour démarrer le serveur, exécutez la
commande suivante :

# /usr/sbin/smcwebserver start

Pour exécutez le serveur automatiquement à l'initialisation du système, tapez la commande
suivante :

# /usr/sbin/smcwebserver enable

Remarque – Les systèmes de fichiers et les pools de stockage ZFS ne peuvent pas être gérés à l'aide
de la console de gestion Solaris (smc, Solaris Management Console).

Description de ZFS

Le système de fichiers ZFS présente des fonctions et des avantages uniques au monde. Ce
système de fichiers révolutionnaire modifie radicalement les méthodes d'administration des
systèmes de fichiers. ZFS a été conçu afin d'être robuste, évolutif et facile à gérer.

Stockage ZFS mis en pool
ZFS utilise le concept de pools de stockage pour la gestion du stockage physique. Auparavant,
l'élaboration des systèmes de fichiers reposait sur un périphérique physique unique. Afin de
traiter plusieurs périphériques et d'assurer la redondance de données, le concept de gestionnaire
de volume a été introduit pour fournir l'image d'un périphérique. Ainsi, il n'est plus nécessaire
de modifier les systèmes de fichiers pour bénéficier de plusieurs périphériques. Cette
conception ajoutait un niveau de complexité supplémentaire et empêchait finalement les

40

Guide d'administration Solaris ZFS • Octobre 2009

Description de ZFS

avancées de certains systèmes de fichiers, car le système de fichiers ne pouvait pas contrôler le
placement physique des données dans les volumes virtualisés.

Grâce à ZFS, la gestion de volumes devient inutile. Plutôt que de vous obliger à créer des
volumes virtualisés, ZFS regroupe les périphériques dans un pool de stockage. Le pool de
stockage décrit les caractéristiques physiques du stockage (disposition de périphérique,
redondance de données, etc.) et agit en tant qu'espace de stockage de données arbitraires à
partir duquel il est possible de créer des systèmes de fichiers. Désormais, les systèmes de fichiers
ne sont plus limités à des périphériques individuels. Ainsi, ils peuvent partager l'espace avec
l'ensemble des systèmes de fichiers du pool. Il n'est plus nécessaire de prédéterminer la taille des
systèmes de fichiers, car celle-ci augmentent automatiquement au sein de l'espace alloué au pool
de stockage. En cas d'ajout d'espace de stockage, tous les systèmes de fichiers du pool peuvent
immédiatement utiliser l'espace supplémentaire, sans requérir des tâches supplémentaires.
Selon divers aspects, le pool de stockage agit comme un système de mémoire virtuel. Lors de
l'ajout d'un DIMM mémoire à un système, le système d'exploitation ne vous force pas à
configurer la mémoire et à l'assigner à des processus, à l'aide de commandes spécifiques. Tous
les processus du système utilisent automatiquement la mémoire supplémentaire.

Sémantique transactionnelle
ZFS étant un système de fichiers transactionnel, l'état du système de fichiers reste toujours
cohérent sur le disque. Les systèmes de fichiers classiques écrasent les données en place. Ainsi,
en cas de réduction de la puissance de la machine, par exemple, entre le moment où un bloc de
données est alloué et celui où il est lié à un répertoire, le système de fichiers reste incohérent.
Auparavant, la commande fsck permettait de résoudre ce problème. Elle parcourait et vérifiait
l'état du système de fichiers et tentait de réparer les incohérences détectées. Cette situation
représentait une grande source de problème pour les administrateurs et la résolution de tous les
problèmes potentiels n'était pas garantie. Plus récemment, les systèmes de fichiers ont introduit
le concept de journalisation. Le processus de journalisation enregistre les actions dans un
journal séparé, lequel peut ensuite être lu en toute sécurité en cas de panne du système. Ce
processus requiert un temps système inutile car les données sont écrites deux fois. En outre, il
entraîne souvent d'autres problèmes, par exemple l'impossibilité de relire correctement le
journal.

Avec un système de fichiers transactionnel, la gestion de données s'effectue avec une sémantique
de copie lors de l'écriture. Les données ne sont jamais écrasées et toute séquence d'opération est
entièrement validée ou entièrement ignorée. Grâce à ce mécanisme, la corruption du système
de fichier en raison d'une coupure de courant ou d'un arrêt du système est impossible. Ainsi,
aucune commande n'est requise pour remplacer fsck. Il est possible que les données
récemment écrites soient perdues, mais le système de fichiers reste toujours cohérent. De plus,
les données synchrones (écrites avec l'indicateur O_DSYNC) sont toujours écrites avant le renvoi.
Ainsi, toute perte est impossible.

Chapitre 1 • Système de fichiers ZFS (présentation)

41

Description de ZFS

Sommes de contrôle et données d'autorétablissement
Avec ZFS, une somme de contrôle des données et métadonnées est effectué selon l'algorithme
sélectionné par l'utilisateur. Les systèmes de fichiers classiques fournissant le contrôle de
sommes l'effectuaient " par bloc ", en raison de la couche de gestion de volumes et de la
conception classique de système de fichiers. Dans la conception classique, certains modes
d'échec, comme l'écriture d'un bloc complet dans un emplacement incorrect, peut générer des
données dont la somme de contrôle s'est effectuée correctement mais qui sont en réalité
incorrectes. Les somme de contrôle ZFS sont stockées de manière à ce que ces modes d'échec
soient détectés et qu'une solution y soit apportée. Toutes les opérations de contrôle de somme et
de récupération des données sont effectuées sur la couche système de fichiers et sont
transparentes aux applications.

De plus, ZFS fournit des données d'autorétablissement. ZFS prend en charge les pools de
stockage avec divers niveaux de redondance de données, y compris la mise en miroir et une
variante de RAID-5. Lorsqu'un bloc de données endommagé est détecté, ZFS récupère les
données correctes à partir d'une autre copie redondante et répare les données endommagées en
les remplaçant par celles de la copie.

Évolutitivé inégalée
ZFS a été conçu dès le départ avec l'objectif d'en faire le système de fichiers le plus évolutif ayant
jamais existé. La taille du système de fichiers lui-même est de 128 bits et vous pouvez utiliser
jusqu'à 256 quadrillion de zettaoctets de stockage. L'ensemble des métadonnées est alloué de
façon dynamique. Il est donc inutile de pré-allouer des inodes ou de limiter l'évolutivité du
système de fichiers lors de sa création. Tous les algorithmes ont été écrits selon cette exigence
d'évolutivité. Les répertoires peuvent contenir jusqu'à 248 (256 trillions) d'entrées et le nombre
de systèmes de fichiers ou de fichiers contenus dans un système de fichiers est illimité.

Instantanés ZFS
Un instantané est une copie en lecture seule d'un système de fichiers ou d'un volume. La
création d'instantanés est rapide et facile. Ils n'utilisent initialement aucun espace
supplémentaire dans le pool.

À mesure que le jeu de données actif est modifié, l'espace occupé par l'instantané augmente
tandis que l'instantané continue de référencer les anciennes données. Par conséquent,
l'instantané évite que les données soit libérées à nouveau dans le pool.

42

Guide d'administration Solaris ZFS • Octobre 2009

Terminologie ZFS

Administration simplifiée
Point le plus important, ZFS fournit un modèle administration qui a été énormément simplifié.
Grâce à une disposition hiérarchique des systèmes de fichiers, à l'héritage des propriétés et à la
gestion automatique des points de montage et de la sémantique de partage NFS, ZFS facilite la
création et la gestion de systèmes de fichiers sans requérir de nombreuses commandes, ni la
modification de fichiers de configuration. Vous pouvez définir des quotas ou des réservations,
activer ou désactiver la compression ou encore gérer les point de montage pour plusieurs
systèmes de fichiers avec une seule commande. Vous pouvez examiner ou réparer les
périphériques sans utiliser un jeu distinct de commandes de gestion de volumes. Vous pouvez
prendre un nombre illimité d'instantanés de systèmes de fichiers. Vous pouvez sauvegarder et
restaurer des systèmes de fichiers individuels.

ZFS assure la gestion des systèmes de fichiers par le biais d'une hiérarchie qui facilite la gestion
des propriétés telles que les quotas, les réservations, la compression et les points de montage.
Dans ce modèle, les systèmes de fichiers deviennent le point de contrôle central. Les systèmes de
fichiers eux-mêmes étant très peu coûteux (autant qu'un nouveau répertoire), il est
recommandé de créer un système de fichiers pour chaque utilisateur, projet, espace de travail
etc. Cette conception permet de définir des points de gestion détaillés.

Terminologie ZFS

Cette section décrit la terminologie de base utilisée dans ce document :
Environnement d'initialisation alternatif

Environnement d'initialisation créé à l'aide de la
commande lucreate et éventuellement mis à jour
à l'aide de la commande luupgrade mais qui n'est
pas actuellement l'environnement d'initialisation
principal ou actif. L'environnement
d'initialisation alternatif peut être remplacé par
l'environnement d'initialisation principal en
exécutant la commande luactivate.
Hachage de 256 bits des données dans un bloc de
système de données. La fonctionnalité de contrôle
de somme regroupe entre autres, le contrôle de
somme simple et rapide fletcher2 (paramètre par
défaut), ainsi que les puissantes fonctions de
hachage cryptographique telles que SHA256.
Système de fichiers dont le contenu initial est
identique à celui d'un instantané.

Somme de contrôle

Clone

Chapitre 1 • Système de fichiers ZFS (présentation)

43

Terminologie ZFS

Jeu de données

Pour plus d'informations sur les clones,
reportez-vous à la section “Présentation des
clones ZFS” à la page 220.
Nom générique pour les entités ZFS suivantes :
clones, systèmes de fichiers, instantanés ou
volumes.

Chaque jeu de données est identifié par un nom
unique dans l'espace de noms ZFS. Les jeux de
données sont identifiés à l'aide du format suivant :

pool/chemin[ @instantané]
pool

Identifie le nom d'un pool de
stockage contenant le jeu de
données.
Nom de chemin délimité par slash
pour l'objet du jeu de données.
Composant optionnel identifiant
l'instantané d'un jeu de données.

chemin

instantané

Système de fichiers

Miroir

Pool

Pour plus d'informations sur les jeux de données,
reportez-vous au Chapitre 6, “Gestion des
systèmes de fichiers ZFS”.
Jeu de données ZFS de type filesystem monté au
sein de l'espace de noms système standard et se
comportant comme les autres systèmes de
fichiers.

Pour plus d'informations sur les systèmes de
fichiers, reportez-vous au Chapitre 6, “Gestion des
systèmes de fichiers ZFS”.
Périphérique virtuel stockant des copies
identiques de données sur un ou plusieurs
disques. Lorsqu'un disque d'un miroir est
défaillant, tout autre disque du miroir est en
mesure de fournir les mêmes données.
Groupe logique de périphériques décrivant la
disposition et les caractéristiques physiques du
stockage disponible. L'espace pour les jeux de
données est alloué à partir d'un pool.

44

Guide d'administration Solaris ZFS • Octobre 2009

Environnement d'initialisation principal

RAID-Z

Réargenture

Instantané

Périphérique virtuel

Terminologie ZFS

Pour plus d'informations sur les pools de
stockage, reportez-vous au Chapitre 4, “Gestion
des pools de stockage ZFS”.
Environnement d'initialisation utilisé par la
commande lucreate pour créer un
environnement d'initialisation alternatif. Par
défaut, l'environnement d'initialisation principal
correspond à l'environnement d'initialisation
actuel. Ce paramètre par défaut peut être modifié
à l'aide de l'option lucreate - s.
Périphérique virtuel stockant les données et la
parité sur plusieurs disques, similaire à RAID-5.
Pour plus d'informations sur RAID-Z,
reportez-vous à la section “Configuration de pool
de stockage RAID-Z” à la page 64.
Processus de transfert de données d'un
périphérique à un autre. Par exemple, si un
composant de miroir est remplacé ou mis hors
ligne, les données du composant de miroir le plus
actuel est copié dans le composant de miroir
nouvellement restauré. Dans les produits de
gestion de volumes classiques, ce processus est
appelé resynchronisation de miroir.

Pour plus d'informations sur la réargenture ZFS,
reportez-vous à la section “Affichage de l'état de
réargenture” à la page 306.
Image en lecture seule d'un système de fichiers ou
d'un volume à un instant t.

Pour plus d'informations sur les instantanés,
reportez-vous à la section “Présentation des
instantanés ZFS” à la page 215.
Périphérique logique dans un pool. il peut s'agir
d'un périphérique physique, d'un fichier ou d'une
collection de périphériques.

Pour plus d'informations sur les périphériques
virtuels, reportez-vous à la section “Affichage des
informations d'un périphérique virtuel de pool de
stockage” à la page 71.

Chapitre 1 • Système de fichiers ZFS (présentation)

45

Exigences d'attribution de noms de composants ZFS

Volume

Jeu de données utilisé pour émuler un
périphérique physique. Vous pouvez par exemple
créer un volume ZFS en tant que périphérique de
swap.

Pour plus d'informations sur les volumes ZFS,
reportez-vous à la section “Volumes ZFS”
à la page 275.

Exigences d'attribution de noms de composants ZFS

L'attribution de noms de chaque composant ZFS doit respecter les règles suivantes :
■ Les composants vides sont interdits.
■ Chaque composant ne peut contenir que des caractères alphanumériques en plus des quatre

caractères spéciaux suivants :

■

Soulignement (_)
■ Trait d'union (-)
■ Deux points (:)
■ Point (.)

■ Les noms de pools doivent commencer par une lettre, à l'exception des restrictions

suivantes :
■ La séquence de début c[0-9] n'est pas autorisée
■ Le nom log est réservé
■ Les noms commençant par mirror, raidz ou spare ne sont pas autorisés, car ils sont

réservés.

En outre, les noms de pools ne doivent pas contenir le signe de pourcentage (%).

■ Les noms de jeux de données doivent commencer par un caractère alphanumérique. Les

noms de jeux de données ne doivent pas contenir le signe de pourcentage (%).

46

Guide d'administration Solaris ZFS • Octobre 2009

2C H A P I T R E

2

Guide de démarrage de ZFS

Ce chapitre décrit les étapes à suivre pour définir une configuration ZFS simple. Il offre une
vision globale du fonctionnement des commandes ZFS et explique les méthodes de création de
pools et de systèmes de fichiers simples. Cette section ne constitue pas une présentation
exhaustive. Pour des informations plus détaillées, reportez-vous aux autres chapitres, comme
indiqué.

Il contient les sections suivantes :
■ “Exigences et recommandations en matière de matériel et de logiciel ZFS ” à la page 47
■ “Création d'un système de fichiers ZFS basique” à la page 48
■ “Création d'un pool de stockage ZFS” à la page 49
■ “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 50

Exigences et recommandations en matière de matériel et de
logiciel ZFS

Avant d'utiliser le logiciel ZFS, passez en revue les exigences et recommandations matérielles et
logicielles suivantes :

■

Système SPARC® ou x86 exécutant la version 10 6/06 ou une version plus récente de Solaris.
■ La taille minimale du disque est de 128 Ko. L'espace disque minimum requis pour un pool

■

de stockage est de 64 Mo.
Il est actuellement recommandé de disposer d'au moins 768 Mo de mémoire pour installer
un système Solaris. Cependant, pour assurer des performances correctes de ZFS, il
préférables de disposer d'au moins un gigaoctet de mémoire.

■ En cas de création d'une configuration de disques mise en miroir, il est conseillé de disposer

de plusieurs contrôleurs.

47

Création d'un système de fichiers ZFS basique

Création d'un système de fichiers ZFS basique

L'administration de ZFS a été conçue dans un but de simplicité. La conception de ZFS a
notamment pour objectif de réduire le nombre de commandes nécessaires à la création d'un
système de fichiers utilisable. Lors de la création d'un pool, un système de fichiers ZFS est
automatiquement créé et monté.

L'exemple suivant illustre la création d'un pool de stockage à miroir simple appelé tank et d'un
système de fichiers ZFS appelé tank, en une seule commande. Supposons que l'intégralité des
disques /dev/dsk/c1t0d0 et /dev/dsk/c2t0d0 puissent être utilisés.

# zpool create tank mirror c1t0d0 c2t0d0

Pour plus d'informations sur les configurations redondantes de pools ZFS, reportez-vous à la
section “Fonctions de réplication d'un pool de stockage ZFS” à la page 63.

Le nouveau système de fichiers ZFS, tank, peut utiliser autant d'espace de disque que nécessaire
et est monté automatiquement sur /tank.

# mkfile 100m /tank/foo

# df -h /tank

Filesystem

size

used avail capacity Mounted on

tank

80G

100M

80G

1%

/tank

Au sein d'un pool, vous souhaiterez probablement créer des systèmes de fichiers
supplémentaires. Les systèmes de fichiers fournissent des points d'administration qui
permettent de gérer différents jeux de données au sein du même pool.

L'exemple illustre la création d'un système de fichiers nommé fs dans le pool de stockage tank.

# zfs create tank/fs

Le nouveau système de fichiers ZFS, tank/fs, peut utiliser autant d'espace de disque que
nécessaire et est monté automatiquement sur /tank/fs.

# mkfile 100m /tank/fs/foo

# df -h /tank/fs

Filesystem

size

used avail capacity Mounted on

tank/fs

80G

100M

80G

1%

/tank/fs

Il s'avère souvent nécessaire de créer et d'organiser une hiérarchie de systèmes de fichiers
correspondant à des besoins spécifiques en matière d'organisation. Pour de plus amples
informations sur la création d'une hiérarchie de systèmes de fichiers ZFS, reportez-vous à la
section “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 50.

48

Guide d'administration Solaris ZFS • Octobre 2009

Création d'un pool de stockage ZFS

Création d'un pool de stockage ZFS

L'exemple suivant illustre la simplicité de ZFS. Vous trouverez dans la suite de cette section un
exemple plus complet, similaire à ce qui pourrait exister dans votre environnement. Les
premières tâches consistent à identifier les besoins en matière de stockage et à créer un pool de
stockage. Le pool décrit les caractéristiques physiques du stockage et doit être créé
préalablement à tout système de fichiers.

▼ Identification des exigences de stockage du pool de

stockage ZFS

1

2

Déterminez les périphériques disponibles.
Avant de créer un pool de stockage, vous devez définir les périphériques à utiliser pour stocker
les données. Ces périphériques doivent être des disques de 128 Mo minimum et ne doivent pas
être en cours d'utilisation par d'autres parties du système d'exploitation. Il peut s'agir de
tranches individuelles d'un disque préformaté ou de disques entiers formatés par ZFS sous
forme d'une seule grande tranche.
Pour l'exemple de stockage utilisé dans la section “Création d'un pool de stockage ZFS”
à la page 50, partez du principe que les disques entiers /dev/dsk/c2t0d0 et /dev/dsk/c0t1d0
sont disponibles.
Pour de plus amples informations sur les disques, leur utilisation et leur étiquetage,
reportez-vous à la section “Utilisation de disques dans un pool de stockage ZFS” à la page 59.

Sélectionnez la réplication de données.
ZFS assure la prise en charge de plusieurs types de réplication de donnés qui déterminent les
types de pannes matérielles que pourra supporter le pool. ZFS assure la prise en charge des
configurations non redondantes (entrelacées), ainsi que la mise en miroir et RAID-Z (une
variante de RAID-5).
L'exemple de stockage de la section “Création d'un pool de stockage ZFS” à la page 50 utilise la
mise en miroir de base de deux disques disponibles.
Pour de plus amples informations sur les fonctions de réplication ZFS, reportez-vous à la
section “Fonctions de réplication d'un pool de stockage ZFS” à la page 63.

Chapitre 2 • Guide de démarrage de ZFS

49

Création d'une hiérarchie de systèmes de fichiers ZFS

▼ Création d'un pool de stockage ZFS

1

2

3

Connectez-vous en tant qu'utilisateur root ou endossez un rôle équivalent avec un profil de
droits ZFS adéquat.
Pour de plus amples informations sur les droits de profils ZFS, reportez-vous à la section
“Profils de droits ZFS” à la page 285.

Choisissez un nom de pool.
Le nom de pool sert à identifier le pool de stockage lorsque vous exécutez les commandes zpool
ou zfs. La plupart des systèmes ne requièrent qu'un pool. Vous pouvez donc utiliser le nom de
votre choix tant qu'il respecte les exigences d'attribution de nom décrites dans la section
“Exigences d'attribution de noms de composants ZFS” à la page 46.

Créez le pool.
Par exemple, créez un pool mis en miroir nommé tank.

# zpool create tank mirror c1t0d0 c2t0d0

Si des périphériques contiennent un autre système de fichiers ou sont en cours d'utilisation, la
commande ne peut pas créer le pool.
Pour de plus amples informations sur la création de pools de stockage, reportez-vous à la
section “Création d'un pool de stockage ZFS” à la page 66.
Pour plus d'informations sur la détection de l'utilisation de périphériques, reportez-vous à la
section “Détection des périphériques utilisés” à la page 73.

4

Affichez les résultats.
Vous pouvez déterminer si votre pool a été correctement créé à l'aide de la commande zpool
list.

# zpool list

NAME

tank

SIZE

USED

AVAIL

CAP HEALTH

ALTROOT

80G

137K

80G

0% ONLINE

-

Pour de plus amples informations sur la vérification de l'état de pool, reportez-vous à la section
“Requête d'état de pool de stockage ZFS” à la page 96.

Création d'une hiérarchie de systèmes de fichiers ZFS

Une fois le pool de stockage, vous pouvez créer la hiérarchie du système de fichiers. Les
hiérarchies sont des mécanismes d'organisation des informations à la fois simples et puissants.
Elles sont connues de toute personne ayant utilisé un système de fichiers.

ZFS permet d'organiser les systèmes de fichiers en hiérarchies arbitraires, dans lesquelles
chaque système de fichiers n'a qu'un seul parent. La racine de la hiérarchie correspond toujours

50

Guide d'administration Solaris ZFS • Octobre 2009

Création d'une hiérarchie de systèmes de fichiers ZFS

au nom du pool. ZFS exploite cette hiérarchie en assurant la prise en charge de l'héritage de
propriétés. Ainsi, vous pouvez définir les propriétés communes rapidement et facilement dans
des arborescences représentant l'intégralité des systèmes de fichiers.

▼ Détermination de la hiérarchie du système de fichiers

ZFS

1

2

3

Choisissez la granularité du système de fichiers.
Les systèmes de fichiers ZFS sont le point central d'administration. Ils sont légers et se créent
facilement. Un modèle correct se compose d'un système de fichiers par utilisateur ou par projet.
En effet, un tel modèle permet de contrôler les propriétés, les instantanés et les sauvegardes par
utilisateur ou par projet.
Deux systèmes de fichiers ZFS, bonwick et billm sont crées dans la section “Création de
systèmes de fichiers ZFS” à la page 52.
Pour plus d'informations sur la gestion des systèmes de fichiers, reportez-vous au Chapitre 6,
“Gestion des systèmes de fichiers ZFS”.

Regroupez les systèmes de fichiers similaires.
ZFS permet d'organiser les systèmes de fichiers en hiérarchie, pour regrouper les systèmes de
fichiers similaires. Ce modèle fournit un point d'administration central pour le contrôle des
propriétés et l'administration de systèmes de fichiers. Il est recommandé de créer les systèmes
de fichiers similaires sous un nom commun.
Dans l'exemple de la section “Création de systèmes de fichiers ZFS” à la page 52, les deux
systèmes de fichiers sont placés sous un système de fichiers appelé home.

Choisissez les propriétés du système de fichiers.
La plupart des caractéristiques de systèmes de fichiers se contrôlent à l'aide de propriétés
simples. Ces propriétés assurent le contrôle de divers comportements, y compris l'emplacement
de montage des systèmes de fichiers, leur méthode de partage, l'utilisation de la compression et
l'activation des quotas.
Dans l'exemple de la section “Création de systèmes de fichiers ZFS” à la page 52, tous les
répertoires de base sont montés dans /export/zfs/ utilisateur. Ils sont partagés à l'aide de NFS
et la compression est activée. De plus, un quota de 10 Go est appliqué dans bonwick.
Pour de plus amples informations sur les propriétés, reportez-vous à la section “Présentation
des propriétés ZFS” à la page 179.

Chapitre 2 • Guide de démarrage de ZFS

51

Création d'une hiérarchie de systèmes de fichiers ZFS

▼ Création de systèmes de fichiers ZFS

1

2

3

Connectez-vous en tant qu'utilisateur root ou endossez un rôle équivalent avec un profil de
droits ZFS adéquat.
Pour de plus amples informations sur les droits de profils ZFS, reportez-vous à la section
“Profils de droits ZFS” à la page 285.

Créez la hiérarchie souhaitée.
Dans cet exemple, un système de fichiers agissant en tant que conteneur de systèmes de fichiers
individuels est créé.

# zfs create tank/home

Ensuite, les systèmes de fichiers sont regroupés sous le système de fichiers home dans le pool
tank.

Définissez les propriétés héritées.
Une fois la hiérarchie du système de fichiers établie, définissez toute propriété destinée à être
partagée par l'ensemble des utilisateurs :

# zfs set mountpoint=/export/zfs tank/home

# zfs set sharenfs=on tank/home

# zfs set compression=on tank/home

# zfs get compression tank/home

NAME

PROPERTY

VALUE

tank/home

compression

on

SOURCE

local

Une nouvelle fonction permettant de définir les propriétés de système de fichiers lors de la
création du système de fichier est disponible. Exemple :

# zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home

Pour plus d'informations sur les propriétés et l'héritage des propriétés, reportez-vous à la
section “Présentation des propriétés ZFS” à la page 179.

4

Créez les systèmes de fichiers individuels.
Il est possible que les systèmes de fichiers aient été créés et que leurs propriétés aient ensuite été
modifiées au niveau home. Vous pouvez modifier les propriétés de manière dynamique lorsque
les systèmes de fichiers sont en cours d'utilisation.

# zfs create tank/home/bonwick

# zfs create tank/home/billm

Les paramètres de propriétés de ces systèmes de fichiers sont hérités de leur parent. Ils sont
donc montés sur /export/zfs/ utilisateur et partagés via NFS. Il est inutile de modifier le
fichier /etc/vfstab ou /etc/dfs/dfstab.

52

Guide d'administration Solaris ZFS • Octobre 2009

Création d'une hiérarchie de systèmes de fichiers ZFS

Pour de plus amples informations sur les systèmes de fichiers, reportez-vous à la section
“Création d'un système de fichiers ZFS” à la page 176.
Pour de plus amples informations sur le montage et le partage de systèmes de fichiers,
reportez-vous à la section “Montage et partage des systèmes de fichiers ZFS” à la page 201.

5

6

Définissez les propriétés spécifiques au système.
Dans cet exemple, un quota de 10 Go est attribué à l'utilisateur bonwick. Cette propriété place
une limite sur la quantité d'espace qu'il peut utiliser, indépendamment de l'espace disponible
dans le pool.

# zfs set quota=10G tank/home/bonwick

Affichez les résultats.
La commande zfs list permet de visualiser les informations disponibles sur le système de
fichiers :

# zfs list

NAME

tank

USED AVAIL REFER MOUNTPOINT

92.0K 67.0G

9.5K /tank

tank/home

24.0K 67.0G

8K /export/zfs

tank/home/billm

8K 67.0G

8K /export/zfs/billm

tank/home/bonwick

8K 10.0G

8K /export/zfs/bonwick

Notez que l'utilisateur bonwick ne dispose que de 10 Go d'espace alors que l'utilisateur billm
peut utiliser l'intégralité du pool (67 Go).
Pour de plus amples informations sur la visualisation de l'état du système de fichiers,
reportez-vous à la section “Envoi de requêtes sur les informations des systèmes de fichiers ZFS”
à la page 193.
Pour de plus amples informations sur l'utilisation et le calcul de l'espace, reportez-vous à la
section “Comptabilisation de l'espace ZFS” à la page 56.

Chapitre 2 • Guide de démarrage de ZFS

53

54

3C H A P I T R E

3

Différences entre ZFS et les systèmes de fichiers
classiques

Cette section aborde les différences significatives entre ZFS et les systèmes de fichiers classiques.
La compréhension de ces différences clé permet d'éviter les confusions lors de l'utilisation
d'outils classiques en interaction avec ZFS.
Il contient les sections suivantes :
■ “Granularité du système de fichiers ZFS” à la page 55
■ “Comptabilisation de l'espace ZFS” à la page 56
■ “Comportement d'espace saturé” à la page 56
■ “Montage de système de fichiers ZFS” à la page 57
■ “Gestion de volumes classique” à la page 57
■ “Nouveau modèle ACL Solaris” à la page 57

Granularité du système de fichiers ZFS

Traditionnellement, les systèmes de fichiers étaient restreints à un périphérique, ce qui limitait
les systèmes de fichiers eux-mêmes à la taille du périphérique. Les créations successives de
systèmes de fichiers classiques dues aux contraintes de taille demandent du temps et s'avèrent
parfois difficile. Les produits de gestion de volume traditionnels ont aidé à gérer ce processus.
Les systèmes de fichiers ZFS n'étant pas limités à des périphériques spécifiques, leur création est
facile et rapide, tout comme celle des répertoires. La taille des systèmes de fichiers ZFS
augmente automatiquement dans l'espace alloué au pool de stockage.
Au lieu de créer un système de fichier, comme /export/home, pour la gestion de plusieurs
sous-répertoires d'utilisateurs, vous pouvez créer un système de fichiers par utilisateur. De plus,
ZFS fournit une hiérarchie qui permet de paramétrer et de gérer facilement plusieurs systèmes
de fichiers en appliquant des propriétés dont peuvent hériter les systèmes de fichiers de la
hiérarchie.
Pour obtenir un exemple de création de hiérarchie de système de fichiers, reportez-vous à la
section “Création d'une hiérarchie de systèmes de fichiers ZFS” à la page 50.

55

Comptabilisation de l'espace ZFS

Comptabilisation de l'espace ZFS

ZFS repose sur un concept de stockage mis en pool. Contrairement aux systèmes de fichiers
classiques, qui sont mappés vers un stockage physique, tous les systèmes de fichiers ZFS d'un
pool partagent le stockage disponible dans le pool. Ainsi, l'espace disponible indiqué par des
utilitaires tels que df peut changer alors même que le système de fichiers est inactif, parce que
d'autres systèmes de fichiers du pool utilisent ou libèrent de l'espace. Notez que la taille
maximale du système de fichiers peut être limitée par l'utilisation des quotas. Pour obtenir des
informations sur les quotas, reportez-vous à la section “Définitions de quotas sur les systèmes
de fichiers ZFS” à la page 209. Les réservations permettent de garantir de l'espace disponible à un
système de fichiers spécifique. Pour obtenir des informations sur les réservations, reportez-vous
à la rubrique “Définition de réservations sur les systèmes de fichiers ZFS” à la page 213. Ce
modèle est très similaire au modèle NFS dans lequel plusieurs répertoires sont montés à partir
du même système de fichiers (par exemple : /home).
Toutes les métadonnées dans ZFS sont allouées dynamiquement. La plupart des autres
systèmes de fichiers pré-allouent une grande partie de leurs métadonnées. Par conséquent, un
coût d'espace immédiat est requis pour ces métadonnées lors de la création du système de
fichiers. En outre, en raison de ce comportement, le nombre total de fichiers pris en charge par
le système de fichiers est prédéterminé. Dans la mesure où ZFS alloue les métadonnées lorsqu'il
en a besoin, aucun coût d'espace initial n'est requis et le nombre de fichiers n'est limité que par
l'espace disponible. Dans le cas de ZFS, la sortie de la commande df -g ne s'interprète pas de la
même manière que pour les autres systèmes de fichiers. Le nombre de fichiers (total files)
indiqué n'est qu'une estimation basée sur la quantité de stockage disponible dans le pool.
ZFS est un système de fichiers transactionnel. La plupart des modifications apportées au
système de fichier sont rassemblées en groupes de transaction et validées sur le disque de façon
asynchrone. Tant que ces modifications ne sont pas validées sur le disque, elles sont considérées
comme des modifications en attente. La quantité d'espace utilisé disponible et référencé par un
fichier ou un système de fichier ne tient pas compte des modifications en attente. Ces
modifications sont généralement prises en compte au bout de quelques secondes. Même si vous
validez une modification apportée au disque avec la commande fsync(3c) ou O_SYNC, les
informations relatives à l'utilisation d'espace ne sont pas automatiquement mise à jour.
Pour plus de détails sur la consommation d'espace ZFS signalée par les commandes du et df,
reportez-vous aux liens suivants :

http://opensolaris.org/os/community/zfs/faq/#whydusize

Comportement d'espace saturé
La création d'instantanés de systèmes de fichiers est peu coûteuse et facile dans ZFS. En général,
les instantanés sont identiques dans la plupart des environnements ZFS. Pour plus
d'informations sur les instantanés ZFS, reportez-vous au Chapitre 7, “Utilisation des
instantanés et des clones ZFS”.

56

Guide d'administration Solaris ZFS • Octobre 2009

Nouveau modèle ACL Solaris

La présence d'instantanés peut entraîner des comportements inattendus lors des tentatives de
libération d'espace. En règle générale, si vous disposez des droits adéquats, vous pouvez
supprimer un fichier d'un système de fichiers plein, ce qui entraîne une augmentation de la
quantité d'espace disponible dans le système de fichiers. Cependant, si le fichier à supprimer
existe dans un instantané du système de fichiers, sa suppression ne libère pas d'espace. Les blocs
utilisés par le fichier continuent à être référencés à partir de l'instantané.

Par conséquent, la suppression du fichier peut occuper davantage d'espace disque, car une
nouvelle version du répertoire doit être créée afin de refléter le nouvel état de l'espace de noms.
En raison de ce comportement, l'erreur ENOSPC ou EDQUOT peut se produire lorsque vous tentez
de supprimer un fichier.

Montage de système de fichiers ZFS

ZFS a été conçu pour simplifier et faciliter l'administration. Par exemple, avec des systèmes de
fichiers existants, vous devez modifier le fichier /etc/vfstab à chaque fois que vous ajoutez un
système de fichiers. Avec ZFS, cela n'est plus nécessaire, grâce au montage et démontage
automatique en fonction des propriétés du jeu de données. Vous n'avez pas besoin de gérer les
entrées ZFS dans le fichier /etc/vfstab.

Pour de plus amples informations sur le montage et le partage de systèmes de fichiers ZFS,
reportez-vous à la section “Montage et partage des systèmes de fichiers ZFS” à la page 201.

Gestion de volumes classique

Comme décrit à la section “Stockage ZFS mis en pool” à la page 40, ZFS élimine la nécessité d'un
gestionnaire de volume séparé. ZFS opérant sur des périphériques bruts, il est possible de créer
un pool de stockage composé de volumes logiques logiciels ou matériels. Cette configuration est
déconseillée, car ZFS fonctionne mieux avec des périphériques bruts physiques. L'utilisation de
volumes logiques peut avoir un impact négatif sur les performances, la fiabilité, voire les deux,
et doit de ce fait être évitée.

Nouveau modèle ACL Solaris

Les versions précédentes du système d'exploitation Solaris assuraient la prise en charge d'une
implémentation ACL reposant principalement sur la spécification d'ACL POSIX-draft. Les
ACL POSIX-draft sont utilisées pour protéger des fichiers UFS. Un nouveau modèle ACL basé
sur la spécification NFSv4 est utilisé pour protéger les fichiers ZFS.

Les principales différences présentées par le nouveau modèle ACL Solaris sont les suivantes :
■ modèle basé sur la spécification NFSv4 et similaire aux ACL de type NT ;

Chapitre 3 • Différences entre ZFS et les systèmes de fichiers classiques

57

Nouveau modèle ACL Solaris

■

■

■

jeu de privilèges d'accès bien plus granulaire ;
configuration et affichage avec les commandes chmod et ls, et non les commandes setfacl
et getfacl ;
sémantique d'héritage bien plus riche pour déterminer comment les privilèges d'accès sont
appliqués d'un répertoire à un sous-répertoire, et ainsi de suite.

Pour plus d'informations sur l'utilisation de listes de contrôle d'accès (ACL) avec des fichiers
ZFS, reportez-vous au Chapitre 8, “Utilisation des ACL pour la protection de fichiers ZFS”.

58

Guide d'administration Solaris ZFS • Octobre 2009

4C H A P I T R E

4

Gestion des pools de stockage ZFS

Ce chapitre décrit la procédure de création et d'administration des pools de stockage ZFS.

Il contient les sections suivantes :
■ “Composants d'un pool de stockage ZFS” à la page 59
■ “Création et destruction de pools de stockage ZFS” à la page 66
■ “Gestion de périphériques dans un pool de stockage ZFS” à la page 77
■ “Gestion des propriétés de pool de stockage ZFS” à la page 93
■ “Requête d'état de pool de stockage ZFS” à la page 96
■ “Migration de pools de stockage ZFS” à la page 105
■ “Mise à niveau de pools de stockage ZFS” à la page 112

Composants d'un pool de stockage ZFS

Les sections ci-dessous contiennent des informations détaillées sur les composants de pools de
stockage suivants :
■ “Utilisation de disques dans un pool de stockage ZFS” à la page 59
■ “Utilisation de tranches dans un pool de stockage ZFS” à la page 61
■ “Utilisation de fichiers dans un pool de stockage ZFS” à la page 63

Utilisation de disques dans un pool de stockage ZFS
Le composant de base d'un pool de stockage est un élément de stockage physique. Le stockage
physique peut être constitué de tout périphérique en mode bloc d'une taille supérieure à
128 Mo. En général, ce périphérique est un disque dur que le système peut voir dans le
répertoire /dev/dsk .

Un disque entier (c1t0d0) ou une tranche individuelle (c0t0d0s7) peuvent constituer un
périphérique de stockage. Le mode opérationnel recommandé consiste à utiliser un disque

59

Composants d'un pool de stockage ZFS

entier. Dans ce cas, il est inutile de formater spécifiquement le disque. ZFS formate le disque à
l'aide d'une étiquette EFI de façon à ce qu'il contienne une grande tranche unique. Utilisé de
cette façon, le tableau de partition affiché par la commande format s'affiche comme suit :

Current partition table (original):

Total disk sectors available: 17672849 + 16384 (reserved sectors)

Part

Tag

Flag

First Sector

Size

Last Sector

0

usr

1 unassigned

2 unassigned

3 unassigned

4 unassigned

5 unassigned

6 unassigned

8

reserved

wm

wm

wm

wm

wm

wm

wm

wm

256

8.43GB

17672849

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

17672850

8.00MB

17689233

Pour utiliser des disques entiers, les disques doivent être nommés en utilisant la convention de
dénomination /dev/dsk/cXtXdX. Certains pilotes tiers suivent une convention de nom
différente ou placent les disques à un endroit autre que le répertoire /dev/dsk. Pour utiliser ces
disques, vous devez les étiqueter manuellement et fournir une tranche à ZFS.

ZFS applique une étiquette EFI lorsque vous créez un pool de stockage avec des disques entiers.
Pour plus d'informations sur les étiquettes EFI, reportez-vous à la section “EFI Disk Label” du
System Administration Guide: Devices and File Systems .

Un disque destiné à un pool racine ZFS doit être créé avec une étiquette SMI et non une
étiquette EFI. Vous pouvez réattribuer une étiquette SMI à un disque à l'aide de la commande
format - e.

Vous pouvez spécifier les disques soit en utilisant le chemin complet (/dev/dsk/c1t0d0, par
exemple) ou un nom abrégé composé du nom du périphérique dans le répertoire /dev/dsk
(c1t0d0, par exemple). Les exemples suivants constituent des noms de disques valides :

■

■

■

■

c1t0d0

/dev/dsk/c1t0d0

c0t0d6s2

/dev/foo/disk

L'utilisation de disques physiques constitue la méthode de création de pools de stockage ZFS la
plus simple. Les configurations ZFS deviennent de plus en plus complexes, en termes de
gestion, de fiabilité et de performance. Lorsque vous construisez des pools à partir de tranches
de disques, de LUN dans des baies RAID matérielles ou de volumes présentés par des
gestionnaires de volume basés sur des logiciels. Les considérations suivantes peuvent vous aider
à configurer ZFS avec d'autres solutions de stockage matérielles ou logicielles :

60

Guide d'administration Solaris ZFS • Octobre 2009

Composants d'un pool de stockage ZFS

■

Si vous élaborez des configurations ZFS sur des LUN à partir de baies RAID matérielles,
vous devez comprendre la relation entre les fonctionnalités de redondance ZFS et les
fonctionnalités de redondance proposées par la baie. Certaines configurations peuvent
fournir une redondance et des performances adéquates, mais d'autres non.

■ Vous pouvez construire des périphériques logiques pour ZFS à l'aide de volumes présentés

par des gestionnaires de volumes logiciels tels que SolarisTM Volume Manager (SVM) ou
Veritas Volume Manager (VxVM). Ces configurations sont cependant déconseillées. Bien
que ZFS fonctionne correctement sur de tels périphériques, les performances risquent d'être
médiocres.

Pour obtenir des informations supplémentaires sur les recommandations de pools de stockage,
consultez le site des pratiques ZFS recommandées :

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

Les disques sont identifiés par leur chemin et par l'ID de leur périphérique, s'il est disponible.
Cette méthode permet de reconfigurer les périphériques dans un système sans requérir la mise à
jour de tout état ZFS. Si un disque est déplacé du contrôleur 1 au contrôleur 2, ZFS utilise l'ID
du périphérique pour déterminer si le disque a été déplacé et si son accès s'effectue à partir du
contrôleur 2. L'ID du périphérique est unique pour le microprogramme du disque. Certaines
mises à jour de microprogrammes sont susceptibles de modifier l'ID du périphérique, mais il est
peu probable que cela se produise. Si cela se produit, ZFS peut tout de même accéder au
périphérique à l'aide de son chemin et mettre à jour automatiquement l'ID de périphérique
stocké. Si, par inadvertance, vous modifiez le chemin et l'ID du périphérique, exportez et
réimportez le pool pour pouvoir l'utiliser.

Utilisation de tranches dans un pool de stockage ZFS
Les disques peuvent être étiquetés avec une étiquette VTOC Solaris (SMI) classique lorsque
vous créez un pool de stockage avec une tranche de disque.

Pour un pool racine ZFS initialisable, les disques du pool doivent contenir des tranches et
doivent être étiquetés avec une étiquette SMI. La plus simple configuration consiste à placer
toute la capacité du disque dans la tranche 0 et à utiliser cette tranche pour le pool racine.

Sur un système SPARC, un disque de 72 Go dispose de 68 Go d'espace utilisable situé dans la
tranche 0, comme illustré dans la sortie format suivante.

# format

.

.

.

Specify disk (enter its number): 4

selecting c1t1d0

partition> p

Chapitre 4 • Gestion des pools de stockage ZFS

61

Composants d'un pool de stockage ZFS

Current partition table (original):

Total disk cylinders available: 14087 + 2 (reserved cylinders)

Part

Tag

Flag

Cylinders

Size

Blocks

0

root

1 unassigned

2

backup

3 unassigned

4 unassigned

5 unassigned

6 unassigned

7 unassigned

wm

wm

wm

wm

wm

wm

wm

wm

0 - 14086

68.35GB

(14087/0/0) 143349312

0

0

(0/0/0)

0

0 - 14086

68.35GB

(14087/0/0) 143349312

0

0

0

0

0

0

0

0

0

0

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

0

0

0

0

0

Sur un système x86, un disque de 72 Go dispose de 68 Go d'espace utilisable situé dans la
tranche 0, comme illustré dans la sortie format suivante. Une petite quantité d'informations
d'initialisation est contenue dans la tranche 8. La tranche 8 ne nécessite aucune administration
et ne peut pas être modifiée.

# format

.

.

.

selecting c1t0d0

partition> p

Current partition table (original):

Total disk cylinders available: 49779 + 2 (reserved cylinders)

Part

Tag

Flag

Cylinders

Size

Blocks

0

root

1 unassigned

2

backup

3 unassigned

4 unassigned

5 unassigned

6 unassigned

7 unassigned

8

boot

9 unassigned

wm

wu

wm

wu

wu

wu

wu

wu

wu

wu

1 - 49778

68.36GB

(49778/0/0) 143360640

0

0

(0/0/0)

0

0 - 49778

68.36GB

(49779/0/0) 143363520

0

0

0

0

0

0

0

0

0

0

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

(0/0/0)

0

0

0

0

0

0 -

0

1.41MB

(1/0/0)

2880

0

0

(0/0/0)

0

Si vous envisagez d'utiliser des tranches pour un pool de stockage ZFS qui n'est pas un pool
racine ZFS d'initialisation, examinez les conditions suivantes lorsque l'utilisation de tranches
risque d'être nécessaire :
■ Le nom du périphérique n'est pas standard.
■ ZFS et un autre système de fichier (UFS, par exemple) partagent un disque unique.

62

Guide d'administration Solaris ZFS • Octobre 2009

Fonctions de réplication d'un pool de stockage ZFS

Utilisation de fichiers dans un pool de stockage ZFS
ZFS permet également d'utiliser des fichiers UFS en tant que périphériques virtuels dans le pool
de stockage. Cette fonction est destinée principalement aux tests et à des essais simples, et non
pas à être utilisée dans un contexte de production. En effet, toute utilisation de fichier repose
sur le système de fichier sous-jacent pour la cohérence. Si vous créez un pool ZFS à partir de
fichiers stockés sur un système de fichiers UFS, la garantie d'une sémantique synchrone et juste
repose entièrement sur UFS.

Cependant, les fichiers peuvent s'avérer utiles lorsque vous employez ZFS pour la première fois
ou en cas de disposition complexe, parce que les périphériques physiques présents ne sont pas
suffisants. Tous les fichiers doivent être spécifiés avec leur chemin complet et leur taille doit être
de 64 Mo minimum. Si un fichier est déplacé ou renommé, le pool doit être exporté et réimporté
pour pouvoir être utilisé, car aucun ID de périphérique n'est associé aux fichiers pour permettre
de les localiser.

Fonctions de réplication d'un pool de stockage ZFS

ZFS offre une redondance des données, ainsi que des propriétés d'autorétablissement, dans une
configuration mise en miroir et une configuration RAID-Z.
■ “Configuration de pool de stockage mis en miroir” à la page 63
■ “Configuration de pool de stockage RAID-Z” à la page 64
■ “Données d'autorétablissement dans une configuration redondante” à la page 65
■ “Entrelacement dynamique dans un pool de stockage” à la page 65
■ “Pool de stockage ZFS hybride” à la page 65

Configuration de pool de stockage mis en miroir
Une configuration de pool de stockage en miroir requiert deux disques minimum, situés de
préférence dans des contrôleurs séparés. Vous pouvez utiliser un grand nombre de disques dans
une configuration en miroir. En outre, vous pouvez créer plusieurs miroirs dans chaque pool.
Conceptuellement, une configuration en miroir simple devrait ressembler à ce qui suit :

mirror c1t0d0 c2t0d0

Conceptuellement, une configuration en miroir plus complexe devrait ressembler à ce qui suit :

mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0

Pour obtenir des informations sur les pools de stockage mis en miroir, reportez-vous à la
section “Création d'un pool de stockage mis en miroir” à la page 67.

Chapitre 4 • Gestion des pools de stockage ZFS

63

Fonctions de réplication d'un pool de stockage ZFS

Configuration de pool de stockage RAID-Z
En plus d'une configuration en miroir de pool de stockage, ZFS fournit une configuration
RAID-Z disposant d'une tolérance de pannes à parité simple ou double. Une configuration
RAID-Z à parité simple est similaire à une configuration RAID-5. Une configuration RAID-Z à
double parité est similaire à une configuration RAID-6.

Tous les algorithmes similaires à RAID-5 traditionnels (RAID-4. RAID-6, RDP et EVEN-ODD,
par exemple) présentent un problème connu appelé "RAID-5 write hole". Si seule une partie
d'un entrelacement RAID-5 est écrite, et qu'une perte d'alimentation se produit avant que tous
les blocs aient été transmis au disque, la parité n'est pas synchronisée avec les données, et est par
conséquent inutile à tout jamais (à moins qu'elle ne soit remplacée par une écriture
d'entrelacement total). Dans RAID-Z, ZFS utilise des entrelacements RAID de largeur variable
pour que toutes les écritures correspondent à des entrelacements entiers. Cette conception n'est
possible que parce que ZFS intègre le système de fichiers et la gestion de périphérique de telle
façon que les métadonnées du système de fichiers disposent de suffisamment d'informations sur
le modèle de redondance de données pour gérer les entrelacements RAID de largeur variable.
RAID-Z est la première solution au monde pour le trou d'écriture de RAID-5.

Une configuration RAID-Z avec N disques de taille X et des disques de parité P présente une
contenance d'environ (N-P)*X octets et peut supporter la panne d'un ou de plusieurs
périphériques P avant que l'intégrité des données ne soit compromise. Vous devez disposer d'au
moins deux disques pour une configuration RAID-Z à parité simple et d'au moins trois disques
pour une configuration RAID-Z à double parité. Par exemple, si vous disposez de trois disques
pour une configuration RAID-Z à parité simple, les données de parité occupent un espace égal à
l'un des trois disques. Dans le cas contraire, aucun matériel spécifique n'est requis pour la
création d'une configuration RAID-Z.

Conceptuellement, une configuration RAID-Z à trois disques serait similaire à ce qui suit :

raidz c1t0d0 c2t0d0 c3t0d0

Une configuration RAID-Z conceptuelle plus complexe serait similaire à ce qui suit :

raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0 raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0

c12t0d0 c13t0d0 c14t0d0

Si vous créez une configuration RAID-Z avec un grand nombre de disques, comme dans cet
exemple, il est recommandé de scinder une configuration RAID-Z avec 14 disques en deux
groupements de 7 disques. Les configurations RAID-Z disposant de groupements de moins de
10 disques devraient présenter de meilleures performances.

Pour plus d'informations sur la création de pools de stockage RAID-Z, consultez la section
“Création de pools de stockage RAID-Z” à la page 68.

64

Guide d'administration Solaris ZFS • Octobre 2009

Fonctions de réplication d'un pool de stockage ZFS

Pour obtenir des informations supplémentaires afin de choisir une configuration en miroir ou
une configuration RAID-Z en fonction de considérations de performances et d'espace,
consultez le blog suivant :

http://blogs.sun.com/roller/page/roch?entry=when_to_and_not_to

Pour obtenir des informations supplémentaires sur les recommandations relatives aux pools de
stockage RAID-Z, consultez le site des pratiques ZFS recommandées :

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

Pool de stockage ZFS hybride
Le pool de stockage ZFS hybride est disponible dans la gamme de produits Sun Storage 7000. Il
s'agit d'un pool de stockage spécial combinant de la RAM dynamique, des disques électroniques
et des disques durs, qui permet d'améliorer les performances et d'augmenter la capacité, tout en
réduisant la consommation électrique. Vous pouvez sélectionner la configuration de la
redondance ZFS du pool de stockage et facilement gérer d'autres options de configuration avec
l'interface de gestion de ce produit.

Pour plus d'informations sur ce produit, reportez-vous au Sun Storage Unified Storage System
Administration Guide.

Données d'autorétablissement dans une
configuration redondante
ZFS fournit des données d'autorétablissement dans une configuration RAID-Z ou en miroir.

Lorsqu'un bloc de données endommagé est détecté, ZFS récupère les données correctes à partir
d'une copie redondante et de plus, répare les données incorrectes en les remplaçant par celles de
la copie.

Entrelacement dynamique dans un pool de stockage
Pour chaque périphérique virtuel ajouté au pool, ZFS entrelace les données de façon dynamique
sur l'ensemble des périphériques disponibles. Le choix de l'emplacement des données est
effectué lors de l'écriture ; ainsi, aucun entrelacement de largeur fixe n'est créé lors de
l'allocation.

Lorsque des périphériques virtuels sont ajoutés à un pool, ZFS attribue graduellement les
données au nouveau périphérique afin de maintenir les performances et les politiques
d'allocation d'espace. Chaque périphérique virtuel peut également être constitué d'un miroir ou

Chapitre 4 • Gestion des pools de stockage ZFS

65

Création et destruction de pools de stockage ZFS

d'un périphérique RAID-Z contenant d'autres périphériques de disques ou d'autres fichiers.
Cette configuration assure un contrôle flexible des caractéristiques par défaut du pool. Par
exemple, vous pouvez créer les configurations suivantes à partir de 4 disques :
■ Quatre disques utilisant l'entrelacement dynamique
■ Une configuration RAID-Z à quatre directions
■ Deux miroirs bidirectionnels utilisant l'entrelacement dynamique

ZFS assure la prise en charge de différents types de périphériques virtuels au sein du même pool,
mais cette pratique est déconseillée. Vous pouvez par exemple créer un pool avec un miroir
bidirectionnel et une configuration RAID-Z à trois directions. Cependant, le niveau de
tolérance de pannes est aussi bon que le pire périphérique virtuel (RAID-Z dans ce cas). Il est
conseillée d'utiliser des périphériques virtuels de niveau supérieur du type identique et avec le
même niveau de redondance dans tous les périphériques.

Création et destruction de pools de stockage ZFS

Les sections suivantes illustrent différents scénarios de création et de destruction de pools de
stockage ZFS.
■ “Création d'un pool de stockage ZFS” à la page 66
■ “Gestion d'erreurs de création de pools de stockage ZFS” à la page 72
■ “Destruction de pools de stockage ZFS” à la page 76
■ “Affichage des informations d'un périphérique virtuel de pool de stockage” à la page 71

De par leur conception, la création et la destruction de pools est rapide et facile. Cependant, il se
doit de réaliser ces opérations avec prudence. Des vérifications sont effectuées pour éviter une
utilisation de périphériques déjà utilisés dans un nouveau pool, mais ZFS n'est pas
systématiquement en mesure de savoir si un périphérique est déjà en cours d'utilisation. La
destruction d'un pool est encore plus facile. Utilisez la commande zpool destroy avec
précaution. Cette commande simple a des conséquences significatives.

Création d'un pool de stockage ZFS
Pour créer un pool de stockage, exécutez la commande zpool create. Cette commande prend
un nom de pool et un nombre illimité de périphériques virtuels en tant qu'arguments. Le nom
de pool doit se conformer aux conventions d'attribution de noms décrites à la section
“Exigences d'attribution de noms de composants ZFS” à la page 46.

Création d'un pool de stockage de base
La commande suivante crée un pool appelé tank et composé des disques c1t0d0 et c1t1d0:

# zpool create tank c1t0d0 c1t1d0

66

Guide d'administration Solaris ZFS • Octobre 2009

Création et destruction de pools de stockage ZFS

Ces disques entiers se trouvent dans le répertoire /dev/dsk et ont été étiquetés de façon
adéquate par ZFS afin de contenir une tranche unique de grande taille. Les données sont
entrelacées de façon dynamique sur les deux disques.

Création d'un pool de stockage mis en miroir
Pour créer un pool mis en miroir, utilisez le mot-clé mirror suivi du nombre de périphériques
de stockage que doit contenir le miroir. Pour spécifier plusieurs miroirs, répétez le mot-clé
mirror dans la ligne de commande. La commande suivante crée un pool avec deux miroirs
bidirectionnels :

# zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0

Le second mot-clé mirror indique qu'un nouveau périphérique virtuel de niveau supérieur est
spécifié. Les données sont dynamiquement entrelacées sur les deux miroirs, ce qui les rend
redondantes sur chaque disque.

Pour plus d'informations sur les configurations mises en miroir recommandées, consultez le
site suivant :

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

Actuellement, les opérations suivantes sont prises en charge dans une configuration ZFS en
miroir :
■ Ajout d'un autre jeu de disques comme stockage vdev de niveau supérieur supplémentaire à
une configuration en miroir existante. Pour plus d'informations, reportez-vous à la rubrique
“Ajout de périphériques à un pool de stockage” à la page 77.

■ Connexion de disques supplémentaires à une configuration en miroir existante ou

connexion de disques supplémentaires à une configuration non répliquée pour créer une
configuration en miroir. Pour plus d'informations, reportez-vous à la section “Connexion et
séparation de périphériques dans un pool de stockage ” à la page 82.

■ Remplacement d'un ou de plusieurs disques dans une configuration en miroir existante, à

condition que les disques de remplacement soient d'une taille supérieure ou égale au celle du
périphérique remplacé. Pour plus d'informations, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 87.
Séparation d'un ou de plusieurs disques dans une configuration existante, à condition que
les périphériques restants procurent la redondance qui convient à la configuration. Pour
plus d'informations, reportez-vous à la section “Connexion et séparation de périphériques
dans un pool de stockage ” à la page 82.

■

Actuellement, les opérations suivantes ne sont pas prises en charge dans une configuration en
miroir :
■ Vous ne pouvez pas supprimer totalement un périphérique d'un pool de stockage mis en

miroir. Cette fonction fait l'objet d'une demande d'amélioration.

Chapitre 4 • Gestion des pools de stockage ZFS

67

Création et destruction de pools de stockage ZFS

■ Vous ne pouvez pas diviser ou fractionner un miroir à des fins de sauvegarde. Cette fonction

fait l'objet d'une demande d'amélioration.

Création d'un pool racine ZFS
Dans les versions actuelles de Solaris, vous pouvez effectuer l'installation et l'initialisation à
partir d'un système de fichiers racine ZFS. Vérifiez les informations de configuration du pool
racine :
■ Les disques utilisés pour le pool racine doivent avoir une étiquette VTOC (SMI) et le pool

doit être créé avec des tranches de disque.

■ Un pool racine doit être créé sous la forme d'une configuration en miroir ou d'une

configuration à disque unique. Vous ne pouvez pas ajouter d'autres disques pour créer
plusieurs périphériques mis en miroir, mais vous pouvez étendre un périphérique mis en
miroir à l'aide de la commande zpool attach.

■ Les volumes RAID-Z et les configurations entrelacées ne sont pas pris en charge.
■ Un pool racine ne peut pas avoir de périphérique de journalisation distinct.

■

Si vous tentez d'utiliser une configuration non prise en charge pour un pool racine, un
message tel que le suivant s'affiche :

ERROR: ZFS pool <pool-name> does not support boot environments

# zpool add -f rpool log c0t6d0s0

cannot add to ’rpool’: root pool can not have multiple vdevs or separate logs

Pour plus d'informations sur l'installation et l'initialisation d'un système de fichiers racine ZFS,
reportez-vous Chapitre 5, “Installation et initialisation d'un système de fichiers racine ZFS”.

Création de pools de stockage RAID-Z
La création d'un pool RAID-Z à parité simple est identique à celle d'un pool mis en miroir, à la
seule différence que le mot-clé raidz ou raidz1 est utilisé à la place du mot-clé mirror. Les
exemples suivants illustrent la création d'un pool avec un périphérique RAID-Z unique
composé de cinq disques :

# zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0

Cet exemple montre que les disques peuvent être spécifiés à l'aide de leurs chemins complets. Le
périphérique /dev/dsk/c5t0d0 est identique au périphérique c5t0d0.

Il est possible de créer une configuration similaire avec des tranches de disque. Exemple :

# zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0

Cependant, vous devez préformater les disques afin de disposer d'une tranche zéro de taille
adéquate.

68

Guide d'administration Solaris ZFS • Octobre 2009

Pour créer une configuration RAID-Z à double parité, appliquez le mot-clé raidz2 lors de la
création du pool. Exemple :

Création et destruction de pools de stockage ZFS

# zpool create tank raidz2 c1t0d0 c2t0d0 c3t0d0

# zpool status -v tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

raidz2

ONLINE

c1t0d0

ONLINE

c2t0d0

ONLINE

c3t0d0

ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Actuellement, les opérations suivantes sont prises en charge dans une configuration ZFS
RAID-Z :
■ Ajout d'un autre jeu de disques comme stockage vdev de niveau supérieur supplémentaire à
une configuration RAID-Z existante. Pour plus d'informations, reportez-vous à la rubrique
“Ajout de périphériques à un pool de stockage” à la page 77.

■ Remplacement d'un ou de plusieurs disques dans une configuration RAID-Z existante, à

condition que les disques de remplacement soient d'une taille supérieure ou égale au celle du
périphérique remplacé. Pour plus d'informations, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 87.

Actuellement, les opérations suivantes ne sont pas prises en charge dans une configuration
RAID-Z :
■ Connexion d'un disque supplémentaire à une configuration RAID-Z existante.

■

Séparation d'un disque d'une configuration RAID-Z.

■ Vous ne pouvez pas supprimer totalement un périphérique d'une configuration RAID-Z.

Cette fonction fait l'objet d'une demande d'amélioration.

Pour obtenir des informations supplémentaire, reportez-vous à la section “Configuration de
pool de stockage RAID-Z” à la page 64.

Création d'un pool de stockage ZFS avec des périphériques de
journalisation
Par défaut, le ZIL est attribué à partir de blocs dans le pool principal. Il est cependant possible
d'obtenir de meilleures performances en utilisant des périphériques de journalisation

Chapitre 4 • Gestion des pools de stockage ZFS

69

Création et destruction de pools de stockage ZFS

d'intention distincts, notamment une NVRAM ou un disque dédié. Pour plus d'informations
sur les périphériques de journalisation ZFS, reportez-vous à la section “Configuration de
périphériques de journalisation ZFS distincts” à la page 28.

Vous pouvez configurer un périphérique de journalisation ZFS à la création du pool de stockage
ou à un moment ultérieur.

Créez par exemple un pool de stockage mis en miroir à l'aide de périphériques de journalisation
mis en miroir.

# zpool create datap mirror c1t1d0 c1t2d0 mirror c1t3d0 c1t4d0 log mirror c1t5d0 c1t8d0

# zpool status

pool: datap

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

datap

ONLINE

mirror

ONLINE

c1t1d0 ONLINE

c1t2d0 ONLINE

mirror

ONLINE

c1t3d0 ONLINE

c1t4d0 ONLINE

logs

ONLINE

mirror

ONLINE

c1t5d0 ONLINE

c1t8d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Pour plus d'informations sur la récupération suite à une défaillance de périphérique de journal,
reportez-vous à l'Exemple 11–2.

Création d'un pool de stockage ZFS avec des périphériques de cache
Vous pouvez créer un pool de stockage avec des périphériques de cache afin de mettre en cache
des données de pool de stockage. Par exemple :

# zpool create tank mirror c2t0d0 c2t1d0 c2t3d0 cache c2t5d0 c2t8d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

70

Guide d'administration Solaris ZFS • Octobre 2009

Création et destruction de pools de stockage ZFS

tank

ONLINE

mirror

ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

cache

c2t5d0

ONLINE

c2t8d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Vérifiez les points suivants lorsque vous envisagez de créer un pool de stockage ZFS avec des
périphériques de cache :
■ L'utilisation de périphériques de cache constitue la meilleure amélioration de performances

pour les charges de travail de lecture aléatoire constituées principalement de contenu
statique.

■ La capacité et les lectures sont contrôlables à l'aide de la commande zpool iostat.
■ Un ou plusieurs périphériques de cache peuvent être ajoutés lors de la création du pool ou

ajoutés et retirés après la création du pool. Pour plus d'informations, reportez-vous à
l'Exemple 4–4.

■

■ Les périphériques de cache ne peuvent pas être mis en miroir ou faire partie d'une

configuration RAID-Z.
Si une erreur de lecture est détectée sur un périphérique de cache, cette E/S de lecture est à
nouveau exécutée sur le périphérique de pool de stockage d'origine, qui peut faire partie
d'une configuration RAID-Z ou mise en miroir. Le contenu du périphérique de cache est
considéré comme étant volatile, comme c'est le cas avec les autres caches système.

Affichage des informations d'un périphérique virtuel
de pool de stockage
Chaque pool de stockage se compose d'un ou plusieurs périphériques virtuels. Un périphérique
virtuel est une représentation interne du pool de stockage qui décrit la disposition du stockage
physique et ses caractéristiques par défaut. Ainsi, un périphérique virtuel représente les
périphériques de disque ou les fichiers utilisés pour créer le pool de stockage. Un pool peut
contenir un nombre indéfini de périphériques virtuels à la racine de la configuration, appelés
périphériques virtuels racine.

Deux périphériques virtuels racine, ou de niveau supérieur, assurent la redondance des
données : les périphériques virtuels miroir et RAID-Z. Ces périphériques virtuels se composent
de disques, de tranches de disques ou de fichiers. Un périphérique de rechange est un
périphérique virtuel particulier qui maintient la liste des hot spare disponibles d'un pool.

L'exemple suivant illustre la création d'un pool composé de deux périphériques virtuels racine,
chacun étant un miroir de deux disques.

Chapitre 4 • Gestion des pools de stockage ZFS

71

Création et destruction de pools de stockage ZFS

# zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0

L'exemple suivant illustre la création d'un pool composé d'un périphérique virtuel racine de 4
disques.

# zpool create mypool raidz2 c1d0 c2d0 c3d0 c4d0

Vous pouvez ajouter un autre périphérique virtuel racine à ce pool à l'aide de la commande
zpool add. Exemple :

# zpool add mypool raidz2 c2d0 c3d0 c4d0 c5d0

Les disques, tranches de disque ou fichiers utilisés dans des pools non redondants fonctionnent
en tant que périphériques virtuels de niveau supérieur. Les pools de stockage contiennent en
règle générale plusieurs périphériques virtuels de niveau supérieur. ZFS entrelace
automatiquement les données entre l'ensemble des périphériques virtuels de niveau supérieur
dans un pool.

Les périphériques virtuels et les périphériques physiques contenus dans un pool de stockage
ZFS s'affichent avec la commande zpool status. Exemple :

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror

ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Gestion d'erreurs de création de pools de stockage ZFS
Les erreurs de création de pool peuvent se produire pour de nombreuses raisons. Certaines
raisons sont évidentes, par exemple lorsqu'un périphérique spécifié n'existe pas, mais d'autres le
sont moins.

72

Guide d'administration Solaris ZFS • Octobre 2009

Création et destruction de pools de stockage ZFS

Détection des périphériques utilisés
Avant de formater un périphérique, ZFS vérifie que le disque n'est pas utilisé par ZFS ou une
autre partie du système d'exploitation. Si le disque est en cours d'utilisation, les erreurs
suivantes peuvent se produire :

# zpool create tank c1t0d0 c1t1d0

invalid vdev specification

use ’-f’ to override the following errors:

/dev/dsk/c1t0d0s0 is currently mounted on /. Please see umount(1M).

/dev/dsk/c1t0d0s1 is currently mounted on swap. Please see swap(1M).

/dev/dsk/c1t1d0s0 is part of active ZFS pool zeepool. Please see zpool(1M).

Certaines d'entre elles peuvent être ignorées à l'aide de l'option -f, mais pas toutes. les
utilisations suivantes ne peuvent pas à être ignorées via l'option - f et doivent être corrigées
manuellement :
Système de fichiers monté

Système de fichiers dans /etc/vfstab

Périphérique de vidage dédié

Elément d'un pool ZFS

Le disque ou une de ses tranches contient un système
de fichiers actuellement monté. La commande umount
permet de corriger cette erreur.
Le disque contient un système de fichiers répertorié
dans le fichier /etc/vfstab, mais le système de
fichiers n'est pas monté. Pour corriger cette erreur,
supprimez ou commentez la ligne dans le fichier
/etc/vfstab.
Le disque est utilisé en tant que périphérique de vidage
dédié pour le système. La commande dumpadm permet
de corriger cette erreur.
Le disque ou fichier fait partie d'un pool de stockage
ZFS. Pour corriger cette erreur, utilisez la commande
zpool destroy afin de détruire l'autre pool s'il est
obsolète. Utilisez sinon la commande zpool detach
pour déconnecter le disque de l'autre pool. Vous
pouvez déconnecter un disque que s'il est connecté à
un pool de stockage mis en miroir.

Les vérifications en cours d'utilisation suivantes constituent des avertissements. Pour les
ignorer, appliquez l'option -f afin de créer le pool :
Contient un système de fichiers

Le disque contient un système de fichiers connu bien qu'il
ne soir pas monté et n'apparaisse pas comme étant en
cours d'utilisation.
Le disque fait partie d'un volume SVM.

Elément d'un volume

Chapitre 4 • Gestion des pools de stockage ZFS

73

Création et destruction de pools de stockage ZFS

Live upgrade

Elément d'un pool ZFS exporté

Le disque est en cours d'utilisation en tant
qu'environnement d'initialisation de remplacement pour
Solaris Live Upgrade.
Le disque fait partie d'un pool de stockage exporté ou
supprimé manuellement d'un système. Dans le deuxième
cas, le pool est signalé comme étant potentiellement
actif, dans la mesure où il peut s'agir d'un disque
connecté au réseau en cours d'utilisation par un autre
système. Faites attention lorsque vous ignorez un pool
potentiellement activé.

L'exemple suivant illustre l'utilisation de l'option -f :

# zpool create tank c1t0d0

invalid vdev specification

use ’-f’ to override the following errors:

/dev/dsk/c1t0d0s0 contains a ufs filesystem.

# zpool create -f tank c1t0d0

Si possible, corrigez les erreurs au lieu d'utiliser l'option -f.

Niveaux de réplication incohérents
Il est déconseillé de créer des pools avec des périphériques virtuels de niveau de réplication
différents. La commande zpool tente de vous empêcher de créer par inadvertance un pool
comprenant des niveaux de redondance différents. Si vous tentez de créer un pool avec un telle
configuration, les erreurs suivantes s'affichent :

# zpool create tank c1t0d0 mirror c2t0d0 c3t0d0

invalid vdev specification

use ’-f’ to override the following errors:

mismatched replication level: both disk and mirror vdevs are present

# zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0

invalid vdev specification

use ’-f’ to override the following errors:

mismatched replication level: 2-way mirror and 3-way mirror vdevs are present

L'option -f permet d'ignorer ces erreurs, mais cette pratique est déconseillée. La commande
affiche également un avertissement relatif à la création d'un pool RAID-Z ou mis en miroir à
l'aide de périphériques de tailles différentes. Cette configuration est autorisée mais, si les
niveaux de redondance sont incohérents, une partie de l'espace du périphérique de plus grande
taille reste inutilisée. L'option -f est alors requise pour ignorer l'avertissement.

74

Guide d'administration Solaris ZFS • Octobre 2009

Création et destruction de pools de stockage ZFS

Réalisation d'un test à la création d'un pool de stockage
La création d'un pool peut échouer de manière inopinée, de différentes façons. Comme le
formatage de disques constitue une action potentiellement néfaste, la commande zpool create
dispose d'une option supplémentaire, -n, qui simule la création d'un pool sans écriture réelle
sur le périphérique. Cette option vérifie le périphérique en cours d'utilisation et valide le niveau
de réplication, puis répertorie les erreurs survenues au cours du processus. Si aucune erreur
n'est détectée, la sortie est similaire à la suivante :

# zpool create -n tank mirror c1t0d0 c1t1d0

would create ’tank’ with the following layout:

tank

mirror

c1t0d0

c1t1d0

Certaines erreurs sont impossibles à détecter sans création effective du pool. L'exemple le plus
courant consiste à spécifier le même périphérique deux fois dans la même configuration. Cette
erreur ne peut pas être détectée sans écriture des données. La commande create -n peut
indiquer que l'opération s'effectue correctement, alors que la création du pool lors de son
exécution effective échoue.

Point de montage par défaut pour les pools de stockage
Lors de la création d'un pool, le point de montage par défaut du jeu de données racine est
/nom-pool. Le répertoire doit être inexistant ou vide. Le répertoire est créé automatiquement s'il
n'existe pas. Si le répertoire est vide, le jeu de données racine est monté sur le répertoire existant.
Pour créer un pool avec un point de montage par défaut différent, utilisez l'option - m de la
commande zpool create :

# zpool create home c1t0d0

default mountpoint ’/home’ exists and is not empty

use ’-m’ option to specify a different default

# zpool create -m /export/zfs home c1t0d0

# zpool create home c1t0d0

default mountpoint ’/home’ exists and is not empty

use ’-m’ option to provide a different default

# zpool create -m /export/zfs home c1t0d0

Cette commande crée un pool home et le jeu de données home avec le point de montage
/export/zfs.

Pour de plus amples informations sur les points de montage, reportez-vous à la section
“Gestion des points de montage ZFS” à la page 201.

Chapitre 4 • Gestion des pools de stockage ZFS

75

Création et destruction de pools de stockage ZFS

Destruction de pools de stockage ZFS
La commande zpool destroy permet de détruire les pools. Cette commande détruit le pool
même s'il contient des jeux de données montés.

# zpool destroy tank

Attention – Faites très attention lorsque vous détruisez un pool. Assurez-vous de détruire le pool
souhaité et de toujours disposer de copies de vos données. En cas de destruction accidentelle
d'un pool, vous pouvez tenter de le récupérer. Pour obtenir des informations supplémentaires,
reportez-vous à la section “Récupération de pools de stockage ZFS détruits” à la page 110.

Destruction d'un pool avec des périphériques défaillants
La destruction d'un pool requiert l'écriture des données sur le disque pour indiquer que le pool
n'est désormais plus valide. Ces informations d'état évitent que les périphériques ne s'affichent
en tant que pool potentiel lorsque vous effectuez une importation. La destruction du pool est
tout de même possible si un ou plusieurs périphériques ne sont pas disponibles. Cependant, les
informations d'état requises ne sont pas écrites sur ces périphériques endommagés.

Ces périphériques, lorsqu'ils sont correctement réparés, sont signalés comme étant
potentiellement actifs lors de la création d'un pool et s'affichent en tant que périphériques valides
lorsque vous recherchez des pools à importer. Si un pool a tant de périphérique défaillants que
le pool lui-même est défaillant (en d'autres termes, un périphérique virtuel de niveau supérieur
est défaillant), alors la commande émet un avertissement et ne peut pas s'exécuter sans l'option
-f. Cette option est requise car l'ouverture du pool est impossible et il est impossible de savoir
si des données y sont stockées ou non. Exemple :

# zpool destroy tank

cannot destroy ’tank’: pool is faulted

use ’-f’ to force destruction anyway

# zpool destroy -f tank

Pour de plus amples informations sur les pools et la maintenance des périphériques,
reportez-vous à la section “Détermination de l'état de maintenance des pools de stockage ZFS”
à la page 102.

Pour de plus amples informations sur l'importation de pools, reportez-vous à la section
“Importation de pools de stockage ZFS” à la page 109.

76

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

Gestion de périphériques dans un pool de stockage ZFS

Vous trouverez la plupart des informations de base concernant les périphériques dans la section
“Composants d'un pool de stockage ZFS” à la page 59. Une fois la création d'un pool terminée,
vous pouvez effectuer plusieurs tâches de gestion des périphériques physiques au sein du pool.
■ “Ajout de périphériques à un pool de stockage” à la page 77
■ “Connexion et séparation de périphériques dans un pool de stockage ” à la page 82
■ “Mise en ligne et mise hors ligne de périphériques dans un pool de stockage” à la page 84
■ “Suppression des périphériques de pool de stockage” à la page 86
■ “Remplacement de périphériques dans un pool de stockage” à la page 87
■ “Désignation des disques hot spare dans le pool de stockage” à la page 89

Ajout de périphériques à un pool de stockage
Vous pouvez ajouter de l'espace à un pool de façon dynamique, en ajoutant un périphérique
virtuel de niveau supérieur. Cet espace est disponible immédiatement pour l'ensemble des jeux
de données au sein du pool. Pour ajouter un périphérique virtuel à un pool, utilisez la
commande zpool add. Exemple :

# zpool add zeepool mirror c2t1d0 c2t2d0

Le format de spécification des périphériques virtuels est le même que pour la commande zpool
create et obéit aux mêmes règles. Une vérification des périphériques est effectuée afin de
déterminer s'ils sont en cours d'utilisation et la commande ne peut pas modifier le niveau de
redondance sans l'option -f. La commande prend également en charge l'option -n, ce qui
permet d'effectuer un test. Exemple :

# zpool add -n zeepool mirror c3t1d0 c3t2d0

would update ’zeepool’ to the following configuration:

zeepool

mirror

c1t0d0

c1t1d0

mirror

c2t1d0

c2t2d0

mirror

c3t1d0

c3t2d0

Cette syntaxe de commande ajouterait les périphériques mis en miroir c3t1d0 et c3t2d0 à la
configuration existante du fichier zeepool.

Chapitre 4 • Gestion des pools de stockage ZFS

77

Gestion de périphériques dans un pool de stockage ZFS

Pour plus d'informations sur la validation des périphériques virtuels, reportez-vous à la section
“Détection des périphériques utilisés” à la page 73.

EXEMPLE 4–1 Ajout de disques à une configuration ZFS mise en miroir
Dans l'exemple suivant, un autre miroir est ajouté à la configuration ZFS mise en miroir
existante sur un système Sun Fire x4500.

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

STATE

READ WRITE CKSUM

NAME

tank

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add tank mirror c0t3d0 c1t3d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror

ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

errors: No known data errors

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

78

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–2 Ajout de disques à une configuration RAID-Z

De la même façon, vous pouvez ajouter des disques supplémentaires à une configuration
RAID-Z. L'exemple suivant illustre la conversion d'un pool de stockage avec un périphérique
RAID–Z composé de 3 disques en pool de stockage avec deux périphériques RAID-Z composé
de 3 disques.

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

raidz1

ONLINE

c1t2d0

ONLINE

c1t3d0

ONLINE

c1t4d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add rpool raidz c2t2d0 c2t3d0 c2t4d0

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

raidz1

ONLINE

c1t2d0

ONLINE

c1t3d0

ONLINE

c1t4d0

ONLINE

raidz1

ONLINE

c2t2d0

ONLINE

c2t3d0

ONLINE

c2t4d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

EXEMPLE 4–3 Ajout d'un périphérique de journalisation mis en miroir à un pool de stockage ZFS

L'exemple suivant illustre l'ajout d'un périphérique de journalisation mis en miroir à un pool de
stockage mis en miroir.Pour plus d'informations sur l'utilisation de périphériques de
journalisation dans votre pool de stockage, reportez-vous à la section “Configuration de
périphériques de journalisation ZFS distincts” à la page 28.

Chapitre 4 • Gestion des pools de stockage ZFS

79

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–3 Ajout d'un périphérique de journalisation mis en miroir à un pool de stockage ZFS
(Suite)

# zpool status newpool

pool: newpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

newpool

ONLINE

mirror

ONLINE

c1t9d0

ONLINE

c1t10d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool add newpool log mirror c1t11d0 c1t12d0

# zpool status newpool

pool: newpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

newpool

ONLINE

mirror

ONLINE

c1t9d0

ONLINE

c1t10d0 ONLINE

logs

ONLINE

mirror

ONLINE

c1t11d0 ONLINE

c1t12d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

connexion d'un périphérique de journal à un périphérique journal existant afin de créer un
périphérique mis en miroir. Cette opération est similaire à la connexion d'un périphérique à un
pool de stockage qui n'est pas mis en miroir.

EXEMPLE 4–4 Ajout et suppression de périphériques de cache sur votre pool de stockage ZFS

Vous pouvez ajouter et supprimer des périphériques de cache sur votre pool de stockage ZFS.

Utilisez la commande zpool add pour ajouter des périphériques de cache. Par exemple :

# zpool add tank cache c2t5d0 c2t8d0

# zpool status tank

80

Guide d'administration Solaris ZFS • Octobre 2009

EXEMPLE 4–4 Ajout et suppression de périphériques de cache sur votre pool de stockage ZFS

(Suite)

Gestion de périphériques dans un pool de stockage ZFS

pool: tank

state: ONLINE

scrub: none requested

config:

STATE

READ WRITE CKSUM

NAME

tank

ONLINE

mirror

ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

cache

c2t5d0

ONLINE

c2t8d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Les périphériques de cache ne peuvent pas être mis en miroir ou faire partie d'une configuration
RAID-Z.

Utilisez la commande zpool remove pour supprimer des périphériques de cache. Exemple :

# zpool remove tank c2t5d0 c2t8d0

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

STATE

READ WRITE CKSUM

mirror

ONLINE

c2t0d0 ONLINE

c2t1d0 ONLINE

c2t3d0 ONLINE

errors: No known data errors

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Actuellement, la commande zpool remove prend uniquement en charge la suppression des
disques hot spare et des périphériques de cache. Les périphériques faisant partie de la
configuration de pool mis en miroir principale peuvent être supprimés à l'aide de la commande
zpool detach. Les périphériques non redondants et RAID-Z ne peuvent pas être supprimés
d'un pool.

Chapitre 4 • Gestion des pools de stockage ZFS

81

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–4 Ajout et suppression de périphériques de cache sur votre pool de stockage ZFS

(Suite)

Pour plus d'informations sur l'utilisation des périphériques de cache dans un pool de stockage
ZFS, reportez-vous à la section “Création d'un pool de stockage ZFS avec des périphériques de
cache” à la page 70.

Connexion et séparation de périphériques dans un
pool de stockage
Outre la commande zpool add, vous pouvez utiliser la commande zpool attach pour ajouter
un périphérique à un périphérique existant, mis en miroir ou non.

Si vous ajoutez et déconnectez un disque dans un pool racine ZFS pour remplacer un disque,
reportez-vous à la section “Remplacement d'un disque dans le pool racine ZFS” à la page 167.

EXEMPLE 4–5 Conversion d'un pool de stockage bidirectionnel mis en miroir en un pool de stockage
tridirectionnel mis en miroir
Dans cet exemple, zeepool est un miroir bidirectionnel. Il est transformé en un miroir
tridirectionnel via la connexion de c2t1d0, le nouveau périphérique, au périphérique existant,
c1t1d0.

# zpool status

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zpool attach zeepool c1t1d0 c2t1d0

# zpool status

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Fri Aug 28 14:11:33 2009

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

82

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

EXEMPLE 4–5 Conversion d'un pool de stockage bidirectionnel mis en miroir en un pool de stockage
tridirectionnel mis en miroir

(Suite)

c2t1d0 ONLINE

0

0

0 73.5K resilvered

Si le périphérique existant fait partie d'un miroir bidirectionnel, la connexion d'un nouveau
périphérique crée un miroir tridirectionnel, et ainsi de suite. Dans tous les cas, la réargenture du
nouveau périphérique commence immédiatement.

EXEMPLE 4–6 Conversion d'un pool de stockage ZFS non redondant en un pool de stockage ZFS mis en
miroir

En outre, vous pouvez convertir un pool de stockage non redondant en un pool de stockage
redondant à l'aide de la commande zpool attach. Exemple :

# zpool create tank c0t1d0

# zpool status

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

c0t1d0

ONLINE

0

0

0

0

0

0

errors: No known data errors

# zpool attach tank c0t1d0 c1t1d0

# zpool status

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Fri Aug 28 14:13:12 2009

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0 73.5K resilvered

Vous pouvez utiliser la commande zpool detach pour séparer un périphérique d'un pool de
stockage mis en miroir. Exemple :

# zpool detach zeepool c2t1d0

Cependant, cette opération est refusée en l'absence de réplique valide des données. Exemple :

Chapitre 4 • Gestion des pools de stockage ZFS

83

Gestion de périphériques dans un pool de stockage ZFS

# zpool detach newpool c1t2d0

cannot detach c1t2d0: only applicable to mirror and replacing vdevs

Mise en ligne et mise hors ligne de périphériques dans
un pool de stockage
ZFS permet la mise en ligne ou hors ligne de périphériques. Lorsque le matériel n'est pas fiable
ou fonctionne mal, ZFS continue de lire ou d'écrire les données dans le périphérique en partant
du principe que le problème est temporaire. Dans le cas contraire, il est possible d'indiquer à
ZFS d'ignorer le périphérique en le mettant hors ligne. ZFS n'envoie aucune requête aux
périphériques hors ligne.

Remarque – Il est inutile de mettre les périphériques hors ligne pour les remplacer.

Vous pouvez utiliser la commande offline pour déconnecter temporairement le stockage. Par
exemple, si vous devez déconnecter physiquement une baie d'un jeu de commutateurs Fibre
Channel et la connecter à un autre jeu, vous pouvez mettre les LUN hors ligne dans la baie
utilisée dans les pools de stockage ZFS. Une fois la baie reconnectée et opérationnelle sur le
nouveau jeu de commutateurs, vous pouvez mettre les mêmes LUN en ligne. La réargenture des
données ajoutées aux pools de stockage alors que les LUN étaient hors ligne s'effectue sur les
LUN, une fois ceux-ci en ligne.

Ce scénario est possible si les systèmes en question ont accès au stockage une fois qu'il est
connecté aux nouveaux commutateurs, éventuellement par le biais de contrôleurs différents, et
si les pools sont définis en tant que configurations RAID-Z ou en miroir.

Mise hors ligne d'un périphérique
La commande zpool offline permet de mettre un périphérique hors ligne. Vous pouvez
spécifier le périphérique via son chemin ou via son nom abrégé s'il s'agit d'un disque. Exemple :

# zpool offline tank c1t0d0

bringing device c1t0d0 offline

Gardez les points suivants à l'esprit lors de la mise hors ligne de périphériques :
■ Vous ne pouvez pas mettre un périphérique hors ligne au point où il devient défaillant. Vous

ne pouvez par exemple pas mettre hors ligne deux périphériques d'une configuration
RAID-Z ni ne pouvez mettre hors ligne un périphérique virtuel de niveau supérieur.

# zpool offline tank c1t0d0

cannot offline c1t0d0: no valid replicas

84

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

■ Par défaut, l'état hors ligne est persistant. Le périphérique reste hors ligne lors du

redémarrage du système.
Pour mettre un périphérique hors ligne temporairement, utilisez l'option -t de la
commande zpool offline. Exemple :

# zpool offline -t tank c1t0d0

bringing device ’c1t0d0’ offline

En cas de réinitialisation du système, ce périphérique revient automatiquement à l'état
ONLINE.

■ Lorsqu'un périphérique est mis hors ligne, il n'est pas séparé du pool de stockage. En cas de

tentative d'utilisation du périphérique hors ligne dans un autre pool, même en cas de
destruction du pool d'origine, un message similaire au suivant s'affiche :

device is part of exported or potentially active ZFS pool. Please see zpool(1M)

Si vous souhaitez utiliser le périphérique hors ligne dans un autre pool de stockage après
destruction du pool de stockage d'origine, remettez le périphérique en ligne puis détruisez le
pool de stockage d'origine.

Une autre mode d'utilisation d'un périphérique provenant d'un autre pool de stockage si
vous souhaitez conserver le pool de stockage d'origine consiste à remplacer le périphérique
existant dans le pool de stockage d'origine par un autre périphérique similaire. Pour obtenir
des informations sur le remplacement de périphériques, reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 87.

Les périphériques mis hors ligne s'affichent dans l'état OFFLINE en cas de requête de l'état de
pool. Pour obtenir des informations sur les requêtes d'état de pool, reportez-vous à la section
“Requête d'état de pool de stockage ZFS” à la page 96.

Pour de plus amples informations sur la maintenance des périphériques, reportez-vous à la
section “Détermination de l'état de maintenance des pools de stockage ZFS” à la page 102.

Mise en ligne d'un périphérique
Lorsqu'un périphérique est mis hors ligne, il peut être restauré grâce à la commande zpool
online :

# zpool online tank c1t0d0

bringing device c1t0d0 online

Lorsqu'un périphérique est mis en ligne, toute donnée écrite dans le pool est resynchronisée sur
le périphérique nouvellement disponible. Notez que vous ne pouvez pas utiliser la mise en ligne

Chapitre 4 • Gestion des pools de stockage ZFS

85

Gestion de périphériques dans un pool de stockage ZFS

d'un périphérique pour remplacer un disque. Si vous mettez un périphérique hors ligne,
remplacez le lecteur puis tentez de le mettre en ligne, son état continue à indiquer qu'il est
défaillant.

Si vous tentez de mettre un périphérique défaillant en ligne, un message similaire au suivant
s'affiche :

# zpool online tank c1t0d0

warning: device ’c1t0d0’ onlined, but remains in faulted state

use ’zpool replace’ to replace devices that are no longer present

Le message de disque défaillant de fmd peut également s'afficher.

SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Fri Aug 28 14:08:39 MDT 2009

PLATFORM: SUNW,Sun-Fire-T200, CSN: -, HOSTNAME: neo2

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: 9da778a7-a828-c88a-d679-c9a7873f4808

DESC: A ZFS device failed. Refer to http://sun.com/msg/ZFS-8000-D3 for more information.

AUTO-RESPONSE: No automated response will occur.

IMPACT: Fault tolerance of the pool may be compromised.

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Pour obtenir des informations sur le remplacement d'un périphérique défaillant, reportez-vous
à la section “Réparation d'un périphérique manquant” à la page 297.

Suppression des périphériques de pool de stockage
Si un périphérique est mis hors ligne en raison d'une défaillance qui entraîne l'affichage
d'erreurs dans la sortie zpool status, la commande zpool clear permet d'effacer les nombres
d'erreurs.

Si elle est spécifiée sans argument, cette commande efface toutes les erreurs de périphérique
dans le pool. Exemple :

# zpool clear tank

Si un ou plusieurs périphériques sont spécifiés, cette commande n'efface que les erreurs
associées aux périphériques spécifiés. Exemple :

# zpool clear tank c1t0d0

Pour de plus amples informations sur l'effacement d'erreurs de zpool reportez-vous à la section
“Suppression des erreurs transitoires” à la page 300.

86

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

Remplacement de périphériques dans un pool de
stockage
Vous pouvez remplacer un périphérique dans un pool de stockage à l'aide de la commande
zpool replace.

Pour remplacer physiquement un périphérique par un autre, en conservant le même
emplacement dans le pool redondant, il vous suffit alors d'identifier le périphérique remplacé.
ZFS reconnaît qu'il s'agit d'un disque différent situé au même emplacement. Par exemple, pour
remplacer un disque défaillant (c1t1d0), supprimez-le, puis ajoutez le disque de rechange au
même emplacement à l'aide d'une syntaxe similiaire à la suivante :

# zpool replace tank c1t1d0

Si vous remplacez le périphérique unique d'un pool de stockage non redondant, vous devez
indiquer les deux périphériques. Exemple :

# zpool replace tank c1t1d0 c1t2d0

Si vous remplacez un disque dans le pool racine ZFS, reportez-vous à la section “Remplacement
d'un disque dans le pool racine ZFS” à la page 167.

Les étapes de base de remplacement d'un disque sont les suivantes D :
■ Le cas échéant, mettez le disque hors ligne à l'aide de la commande zpool offline.
■ Enlevez le disque à remplacer.

■

Insérez le disque de remplacement.

■ Exécutez la commande zpool replace. Exemple :

# zpool replace tank c1t1d0

■ Remettez le disque en ligne à l'aide de la commande zpool online.

Sur certains systèmes, notamment sur Sun Fire x4500, vous devez annuler la configuration d'un
disque avant de le mettre hors ligne. Si vous remplacez simplement un disque par un autre dans
le même logement de ce système, vous pouvez exécuter la commande zpool replace comme
indiqué précédemment.

L'Exemple 11–1 illustre le remplacement d'un disque sur ce système.

Chapitre 4 • Gestion des pools de stockage ZFS

87

Gestion de périphériques dans un pool de stockage ZFS

Gardez à l'esprit les considérations suivantes lorsque vous remplacez des périphériques dans un
pool de stockage ZFS :

■

Si vous activez la propriété de pool autoreplace (valeur "on"), tout nouveau périphérique
détecté au même emplacement physique qu'un périphérique appartenant précédemment au
pool est automatiquement formaté et remplacé sans recourir à la commande zpool
replace. Cette fonction n'est pas disponible sur tous les types de matériel.

■ Le périphérique de remplacement doit être d'une taille supérieure ou égale à la taille

minimale de tous les périphériques dans une configuration en miroir ou RAID-Z.
Si la taille du périphérique de remplacement est supérieure, la capacité du pool s'accroît une
fois le remplacement terminé. Actuellement, vous devez exporter et importer le pool pour
connaître la capacité étendue. Exemple :

■

# zpool list tank

NAME

SIZE

USED AVAIL

CAP HEALTH ALTROOT

tank 16.8G

94K 16.7G

0% ONLINE -

# zpool replace tank c0t0d0 c0t4d0

# zpool list tank

NAME

SIZE

USED AVAIL

CAP HEALTH ALTROOT

tank 16.8G

112K 16.7G

0% ONLINE -

# zpool export tank

# zpool import tank

# zpool list tank

NAME

SIZE

USED AVAIL

CAP HEALTH ALTROOT

tank 33.9G

114K 33.9G

0% ONLINE -

Pour de plus amples informations sur l'importation et l'exportation de pools, reportez-vous
à la section “Migration de pools de stockage ZFS” à la page 105.

■ À l'heure actuelle, pour accroître la taille d'un volume LUN existant appartenant à un pool
de stockage, vous devez également effectuer les étapes d'exportation et d'importation afin
d'afficher la capacité de disque étendue.

■ Le remplacement des nombreux disques dans un pool volumineux prend du temps, en

raison de la réargenture des données sur les nouveaux disques. En outre, il peut s'avérer utile
d'exécuter la commande zpool scrub entre chaque remplacement afin de garantir le
fonctionnement des périphériques de remplacement et l'exactitude des données écrites.
Si un disque défectueux a été remplacé automatiquement par un disque hot spare, il se peut
que vous deviez déconnecter le disque hot spare une fois le disque défectueux remplacé.
Pour plus d'informations sur la déconnexion d'un disque hot spare, reportez-vous à la
section “Activation et désactivation de disque hot spare dans le pool de stockage”
à la page 91.

■

Pour plus d'informations sur le remplacement de périphériques, reportez-vous aux sections
“Réparation d'un périphérique manquant” à la page 297 et “Remplacement ou réparation d'un
périphérique endommagé ” à la page 299.

88

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

Désignation des disques hot spare dans le pool de
stockage
La fonction de disque hot spare permet d'identifier les disques utilisables pour remplacer un
périphérique défaillant dans un ou plusieurs pools de stockage. Un périphérique désigné en tant
que disque hot spare n'est pas actif dans un pool mais en cas d'échec d'un périphérique actif du
pool, le disque hot spare le remplace automatiquement

Pour désigner des périphériques en tant que disques hot spare, vous avez le choix entre les
méthodes suivantes :

■

■

■

lors de la création du pool à l'aide de la commande zpool create ;
après la création du pool à l'aide de la commande zpool create ;
les périphériques hot spare peuvent être partagés entre plusieurs pools.

Désignez les périphériques en tant que disques hot spare une fois le pool créé. Exemple :

# zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

c1t1d0

ONLINE

c2t1d0

ONLINE

spares

c1t2d0

AVAIL

c2t2d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

Désignez les disques hot spare en les ajoutant à un pool après la création de ce dernier.
Exemple :

# zpool add zeepool spare c1t3d0 c2t3d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

0

0

0

0

0

0

Chapitre 4 • Gestion des pools de stockage ZFS

89

Gestion de périphériques dans un pool de stockage ZFS

c1t1d0

ONLINE

c2t1d0

ONLINE

0

0

0

0

0

0

spares

c1t3d0

AVAIL

c2t3d0

AVAIL

Plusieurs pools peuvent partager des périphériques désignés en tant que disque hot spare.
Exemple :

# zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0

# zpool create tank raidz c3t1d0 c4t1d0 spare c1t2d0 c2t2d0

Vous pouvez supprimer les disques hot spare d'un pool de stockage à l'aide de la commande
zpool remove. Exemple :

# zpool remove zeepool c1t2d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

c1t1d0

ONLINE

c2t1d0

ONLINE

spares

c1t3d0

AVAIL

0

0

0

0

0

0

0

0

0

0

0

0

Un disque hot spare ne peut pas être supprimé s'il est en cours d'utilisation par le pool de
stockage.

Lorsque vous utilisez des disques hot spare ZFS, gardez à l'esprit les points suivants :
■ Actuellement, la commande zpool remove ne peut être utilisée que pour la suppression de

disques hot spare et de périphériques de cache.

■ Ajoutez un disque de rechange d'une taille égale ou supérieure à celle du disque le plus
volumineux au sein du pool. L'ajout d'un disque de rechange plus petit dans le pool est
autorisé. Toutefois, lorsque le plus petit disque de rechange est activé, automatiquement ou
via la commande zpool replace, l'opération échoue et une erreur du type suivant s'affiche :

cannot replace disk3 with disk4: device is too small

■ Un disque hot spare peut être partagé entre plusieurs pools. Toutefois, vous ne pouvez pas

exporter un pool contenant un disque spare partagé utilisé à moins de vous servir de l'option
zpool export -f (force). Ce comportement empêche l'éventuelle corruption de données

90

Guide d'administration Solaris ZFS • Octobre 2009

Gestion de périphériques dans un pool de stockage ZFS

lors de l'exportation d'un pool contenant un disque spare partagé utilisé alors qu'un autre
pool tente d'utiliser le disque spare partagé du pool exporté. Lorsque vous exportez, à l'aide
de l'option -f, un pool contenant un disque spare partagé utilisé, sachez que l'opération
risque d'entraîner une corruption des données si un autre pool tente d'activer le disque spare
partagé utilisé.

Activation et désactivation de disque hot spare dans le pool de
stockage
Les disques hot spare s'activent des façons suivantes :
■ Remplacement manuel – Remplacez un périphérique défaillant dans un pool de stockage

par un disque hot spare avec la commande zpool replace.

■ Remplacement automatique – En cas de réception d'une défaillance, un agent FMA examine
le pool pour déterminer s'il y a des disques hot spare. Dans ce cas, le périphérique défaillant
est remplacé par un disque hot spare disponible.
En cas de défaillance d'un disque hot spare en cours d'utilisation, l'agent sépare le disque hot
spare et annule ainsi le remplacement. L'agent tente ensuite de remplacer le périphérique
par un autre disque hot spare s'il y en a un de disponible. Cette fonction est actuellement
limitée par le fait que le moteur de diagnostics ZFS ne génère des défaillances qu'en cas de
disparition d'un périphérique du système.
Si vous remplacez physiquement un périphérique défaillant par un disque spare actif, vous
pouvez réactiver l'original mais remplacez le périphérique à l'aide de la commande zpool
detach pour déconnecter le disque spare. Si vous activez la propriété autoreplace (valeur
"on"), le disque spare est automatiquement déconnecté du pool de disques spare lorsque le
nouveau périphérique est inséré et que l'opération en ligne s'achève.

La commande zpool replace permet de remplacer un périphérique manuellement par un
disque hot spare. Exemple :

# zpool replace zeepool c2t1d0 c2t3d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Fri Aug 28 14:16:04 2009

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

mirror

ONLINE

c1t2d0

ONLINE

spare

ONLINE

c2t1d0

ONLINE

c2t3d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0 76.5K resilvered

Chapitre 4 • Gestion des pools de stockage ZFS

91

Gestion de périphériques dans un pool de stockage ZFS

spares

c2t3d0

INUSE

currently in use

errors: No known data errors

Tout périphérique défaillant est remplacé automatiquement si un disque hot spare est
disponible. Exemple :

# zpool status -x

pool: zeepool

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-D3

scrub: resilver completed after 0h12m with 0 errors on Fri Aug 28 14:18:16 2009

config:

NAME

STATE

READ WRITE CKSUM

zeepool

DEGRADED

mirror

DEGRADED

c1t2d0

ONLINE

spare

DEGRADED

c2t1d0

UNAVAIL

c2t3d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

spares

0

0

0

0

0 cannot open

0 58.5K resilvered

c2t3d0

INUSE

currently in use

errors: No known data errors

ll existe actuellement trois façons de désactiver les disques hot spare disponibles :

■

■

■

annuler le disque hot spare en le supprimant du pool de stockage ;
remplacer le périphérique d'origine par un disque hot spare ;
échanger le disque hot spare de façon permanente.

Une fois le périphérique défaillant remplacé, utilisez la commande zpool detach pour
retourner le disque hot spare au jeu restant. Exemple :

# zpool detach zeepool c2t3d0

# zpool status zeepool

pool: zeepool

state: ONLINE

scrub: resilver completed with 0 errors on Fri Aug 28 14:21:02 2009

config:

NAME

STATE

READ WRITE CKSUM

zeepool

ONLINE

0

0

0

92

Guide d'administration Solaris ZFS • Octobre 2009

Gestion des propriétés de pool de stockage ZFS

mirror

ONLINE

c1t2d0

ONLINE

c2t1d0

ONLINE

0

0

0

0

0

0

0

0

0

spares

c2t3d0

AVAIL

errors: No known data errors

Gestion des propriétés de pool de stockage ZFS

Vous pouvez vous servir de la commande zpool get pour afficher des informations sur les
propriétés du pool. Exemple :

# zpool get all mpool

NAME

PROPERTY

VALUE

SOURCE

mpool size

mpool used

33.8G

5.91G

mpool available

27.8G

mpool capacity

17%

-

-

-

-

mpool altroot

-

default

mpool health

ONLINE

-

mpool guid

17361998391267837263 -

mpool version

10

default

mpool bootfs

mpool/ROOT/zfs2BE

local

mpool delegation

mpool autoreplace

mpool cachefile

on

on

-

mpool failmode

continue

mpool listsnapshots on

default

local

default

local

default

Les propriétés d'un pool de stockage peuvent être définies à l'aide de la commande zpool set.
Exemple :

# zpool set autoreplace=on mpool

# zpool get autoreplace mpool

NAME PROPERTY

VALUE

SOURCE

mpool autoreplace on

default

Chapitre 4 • Gestion des pools de stockage ZFS

93

Gestion des propriétés de pool de stockage ZFS

TABLEAU 4–1 Description des propriétés d'un pool ZFS

Nom de la propriété

Type

Valeur par
défaut

Description

altroot

Chaîne

off

available

Valeur
numérique

SO

autoreplace

Booléen

off

bootfs

Booléen

SO

capacity

Valeur
numérique

SO

delegation

Booléen

on

Identifie un répertoire racine alternatif. S'il est défini, ce
répertoire est ajouté au début de tout point de montage
figurant dans le pool. Cette propriété peut être utilisée lors de
l'examen d'un pool inconnu si vous ne pouvez pas faire
confiance aux points de montage ou dans un environnement
d'initialisation alternatif dans lequel les chemins types sont
incorrects.

Valeur en lecture seule identifiant la quantité de stockage
disponible au sein du pool.
Cette propriété peut également s'afficher sous la forme du nom
de colonne contracté, avail.

Contrôle le remplacement automatique d'un périphérique. Si
la propriété est désactivée, l'administrateur doit initier le
remplacement du périphérique à l'aide de la commande zpool
replace. Si la propriété est activée, tout nouveau périphérique
se trouvant au même emplacement physique qu'un
périphérique qui appartenait au pool est automatiquement
formaté et remplacé. Le comportement par défaut est "off".
Cette propriété peut également s'afficher sous la forme du nom
de colonne contracté, replace.

Identifie le jeu de données d'initialisation par défaut du pool
racine. Cette propriété est censée être définie par les
programmes d'installation et de mise à niveau.

Valeur en lecture seule identifiant le pourcentage d'espace
utilisé du pool.
Cette propriété peut également s'afficher sous la forme du nom
de colonne contracté, cap.

Contrôle l'octroi des droits d'accès définis pour le jeu de
données à un utilisateur sans privilège. Pour plus
d'informations, reportez-vous au Chapitre 9, “Administration
déléguée de ZFS”.

94

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 4–1 Description des propriétés d'un pool ZFS

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

Gestion des propriétés de pool de stockage ZFS

failmode

Chaîne

wait

guid

health

Chaîne

Chaîne

SO

SO

listsnapshots Chaîne

off

size

used

Valeur
numérique

Valeur
numérique

SO

SO

Contrôle le comportement du système en cas de panne
catastrophique du pool. Cette condition résulte
habituellement d'une perte de connectivité aux périphériques
de stockage sous-jacents ou d'une panne de tous les
périphériques au sein du pool. Le comportement d'un tel
événement est déterminé par une des valeurs suivantes :

■

■

■

wait : bloque tout accès d'E/S jusqu'au rétablissement de la
connectivité des périphériques et jusqu'à l'effacement des
erreurs à l'aide de la commande zpool clear. Il s'agit du
comportement par défaut.
continue : renvoie une erreur EIO à toute nouvelle
requête d'E/S d'écriture mais autorise les lectures de tout
autre périphérique fonctionnel. Toute requête d'écriture
devant encore être validée sur disque est bloquée. Une fois
le périphérique reconnecté ou remplacé, les erreurs
doivent être effacées à l'aide de la commande zpool
clear.
panic : imprime un message sur la console et génère un
vidage mémoire sur incident du système.

Propriété en lecture seule identifiant l'identificateur unique du
pool.

Propriété en lecture seule indiquant la fonctionnalité actuelle
du pool ; les valeurs possibles sont : ONLINE, DEGRADED,
FAULTED, OFFLINE, REMOVED ou UNAVAIL..

Détermine si les informations sur les instantanés associées à ce
groupe s'affichent avec la commande zfs list. Si cette
propriété est désactivée, les informations sur les instantanés
peuvent être affichées à l'aide de la commande zfs list
-t snapshot. La valeur par défaut est on.

Propriété en lecture seule identifiant la taille totale du pool de
stockage.

Propriété en lecture seule identifiant la quantité de stockage
disponible au sein du pool.

Chapitre 4 • Gestion des pools de stockage ZFS

95

Requête d'état de pool de stockage ZFS

TABLEAU 4–1 Description des propriétés d'un pool ZFS

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

version

Valeur
numérique

SO

Identifie la version actuelle sur disque du pool. La valeur de
cette propriété peut être augmentée mais ne peut jamais être
diminuée. La méthode recommandée de mise à jour des pools
consiste à utiliser la commande zpool upgrade, bien que cette
propriété puisse être utilisée lorsqu'une version spécifique est
requise pour des raisons de compatibilité ascendante. Cette
propriété peut être définie sur tout numéro compris entre 1 et
la version actuelle signalée par la commande zpool upgrade
-v. La valeur current est un alias de la toute dernière version
prise en charge.

Requête d'état de pool de stockage ZFS

La commande zpool list offre plusieurs moyens d'effectuer des requêtes sur l'état du pool. Les
informations disponibles se répartissent généralement en trois catégories : informations
d'utilisation de base, statistiques d'E/S et état de maintenance. Les trois types d'information sur
un pool de stockage sont traités dans cette section.
■ “Affichage des informations de pools de stockage ZFS de base” à la page 96
■ “Visualisation de statistiques d'E/S de pools de stockage ZFS” à la page 100
■ “Détermination de l'état de maintenance des pools de stockage ZFS” à la page 102

Affichage des informations de pools de stockage ZFS
de base
La commande zpool list permet d'afficher les informations de base relatives aux pools.

Liste des informations relatives aux pools de stockage
Sans arguments, la commande affiche tous les champs pour tous les pools dans le système.
Exemple :

# zpool list

NAME

tank

dozer

SIZE

USED

AVAIL

CAP HEALTH

ALTROOT

80.0G

22.3G

47.7G

28% ONLINE

1.2T

384G

816G

32% ONLINE

-

-

La sortie affiche les informations suivantes :

NAME

SIZE

Nom du pool.
Taille totale du pool, égale à la somme de la taille de tous les périphériques
virtuels de niveau supérieur.

96

Guide d'administration Solaris ZFS • Octobre 2009

Requête d'état de pool de stockage ZFS

USED

Quantité d'espace utilisée, c'est-à-dire allouée par tous les jeux de données
et métadonnées internes. Notez que cette quantité d'espace est différente de
celle qui est rapportée au niveau des systèmes de fichiers.

AVAILABLE

CAPACITY (CAP)

HEALTH

ALTROOT

Pour de plus amples informations sur la détermination de l'espace de
systèmes de fichiers disponible, reportez-vous à la section
“Comptabilisation de l'espace ZFS” à la page 56.
Quantité d'espace disponible, c'est-à-dire non allouée dans le pool.
Capacité, ou quantité d'espace utilisée, exprimée en tant que pourcentage
d'espace total.
État de maintenance actuel du pool.

Pour de plus amples informations sur la maintenance des pools,
reportez-vous à la section “Détermination de l'état de maintenance des
pools de stockage ZFS” à la page 102.
Racine de remplacement, le cas échéant.

Pour de plus amples informations sur les pools racine de remplacement,
reportez-vous à la section “Utilisation de pools racine ZFS de
remplacement” à la page 284.

Vous pouvez également rassembler des statistiques pour un pool donné en spécifiant le nom du
pool. Exemple :

# zpool list tank

NAME

tank

SIZE

USED

AVAIL

CAP HEALTH

ALTROOT

80.0G

22.3G

47.7G

28% ONLINE

-

Liste de statistiques spécifiques à un pool de stockage
L'option -o permet d'effectuer une requête concernant des statistiques spécifiques. Cette option
permet de générer des rapports personnalisés ou de générer rapidement une liste
d'informations pertinentes. Par exemple, pour ne répertorier que le nom et la taille de chaque
pool, utilisez la syntaxe suivante :

# zpool list -o name,size

NAME

tank

dozer

SIZE

80.0G

1.2T

Les noms de colonnes correspondent aux propriétés répertoriées à la section “Liste des
informations relatives aux pools de stockage” à la page 96.

Chapitre 4 • Gestion des pools de stockage ZFS

97

Requête d'état de pool de stockage ZFS

Script de sortie du pool de stockage ZFS
La sortie par défaut de la commande zpool list a été conçue pour améliorer la lisibilité. Elle
n'est pas facile à utiliser en tant que partie d'un script shell. Pour faciliter l'utilisation de la
commande dans le cadre de la programmation, l'option -H permet de supprimer les en-têtes de
colonnes et de séparer les champs par des onglets plutôt que par des espaces. La requête
suivante permet d'obtenir la liste des noms de pool dans le système :

# zpool list -Ho name

tank

dozer

Voici un autre exemple :

# zpool list -H -o name,size

tank

80.0G

dozer 1.2T

Affichage de l'historique des commandes du pool de stockage ZFS
ZFS consigne automatiquement les commandes zfs et zpool ayant pour effet de modifier les
informations d'état du pool. Cette information peut être affichée à l'aide de la commande zpool
history.

Par exemple, la syntaxe suivante affiche la sortie de la commande pour le pool racine.

# zpool history

History for ’rpool’:

2009-05-07.13:51:00 zpool create -f -o failmode=continue -R /a -m legacy -o cachefile=

/tmp/root/etc/zfs/zpool.cache rpool c1t0d0s0

2009-05-07.13:51:01 zfs set canmount=noauto rpool

2009-05-07.13:51:02 zfs set mountpoint=/rpool rpool

2009-05-07.13:51:02 zfs create -o mountpoint=legacy rpool/ROOT

2009-05-07.13:51:03 zfs create -b 8192 -V 2048m rpool/swap

2009-05-07.13:51:04 zfs create -b 131072 -V 1024m rpool/dump

2009-05-07.13:51:09 zfs create -o canmount=noauto rpool/ROOT/snv_114

2009-05-07.13:51:10 zpool set bootfs=rpool/ROOT/snv_114 rpool

2009-05-07.13:51:10 zfs set mountpoint=/ rpool/ROOT/snv_114

2009-05-07.13:51:11 zfs set canmount=on rpool

2009-05-07.13:51:12 zfs create -o mountpoint=/export rpool/export

2009-05-07.13:51:12 zfs create rpool/export/home

Vous pouvez utiliser une sortie similaire sur votre système pour identifier l'ensemble exact de
commandes ZFS exécutées pour résoudre les conditions d'erreur.

Les caractéristiques de l'historique sont les suivantes :

98

Guide d'administration Solaris ZFS • Octobre 2009

Requête d'état de pool de stockage ZFS

■ Le journal ne peut pas être désactivé.
■ Le journal est enregistré sur le disque de façon persistante, ce qui signifie qu'il est enregistré

au travers des réinitialisations du système.

■ Le journal est implémenté en tant que tampon d'anneau. La taille minimale est de 128 Ko. La

taille maximale est de 32 Mo.

■ Pour des pools de taille inférieure, la taille maximum est plafonnées à 1 % de la taille du pool,

la taille étant déterminée lors de la création du pool.

■ Ne nécessite aucune administration, ce qui signifie qu'il n'est pas nécessaire d'ajuster la taille

du journal ou de modifier son emplacement.

Pour identifier l'historique des commandes d'un pool de stockage spécifique, utilisez une
syntaxe similaire à la suivante :

# zpool history mypool

History for ’mypool’:

2009-06-02.10:56:54 zpool create mypool mirror c0t4d0 c0t5d0

2009-06-02.10:57:31 zpool add mypool spare c0t6d0

2009-06-02.10:57:54 zpool offline mypool c0t5d0

2009-06-02.10:58:02 zpool online mypool c0t5d0

L'option -l permet d'afficher un format complet comprenant le nom de l'utilisateur, le nom de
l'hôte et la zone dans laquelle l'opération a été effectuée. Exemple :

# zpool history -l mypool

History for ’mypool’:

2009-06-02.10:56:54 zpool create mypool mirror c0t4d0 c0t5d0 [user root on neo:global]

2009-06-02.10:57:31 zpool add mypool spare c0t6d0 [user root on neo:global]

2009-06-02.10:57:54 zpool offline mypool c0t5d0 [user root on neo:global]

2009-06-02.10:58:02 zpool online mypool c0t5d0 [user root on neo:global]

L'option -i permet d'afficher des informations relatives aux événements internes utilisables
pour établir des diagnostics. Exemple :

# zpool history -i mypool

History for ’mypool’:

2009-06-02.10:56:54 zpool create mypool mirror c0t4d0 c0t5d0

2009-06-02.10:57:31 zpool add mypool spare c0t6d0

2009-06-02.10:57:54 zpool offline mypool c0t5d0

2009-06-02.10:58:02 zpool online mypool c0t5d0

2009-06-02.11:02:20 [internal create txg:23] dataset = 24

2009-06-02.11:02:20 [internal property set txg:24] mountpoint=/data dataset = 24

2009-06-02.11:02:20 zfs create -o mountpoint=/data mypool/data

2009-06-02.11:02:34 [internal create txg:26] dataset = 30

2009-06-02.11:02:34 zfs create mypool/data/datab

Chapitre 4 • Gestion des pools de stockage ZFS

99

Requête d'état de pool de stockage ZFS

Visualisation de statistiques d'E/S de pools de
stockage ZFS
La commande zpool iostat permet d'effectuer une requête de statistiques d'E/S pour un pool
ou des périphériques virtuels spécifiques. Cette commande est similaire à iostat. Elle permet
d'afficher un instantané statique des activités d'E/S effectuées, ainsi que les statistiques mises à
jour pour chaque intervalle spécifié. Les statistiques suivantes sont rapportées :

USED CAPACITY

AVAILABLE CAPACITY

READ OPERATIONS

WRITE OPERATIONS

READ BANDWIDTH

WRITE BANDWIDTH

Capacité utilisée, c'est-à-dire quantité de données actuellement
stockées dans le pool ou le périphérique. Ce chiffre diffère quelque
peu de la quantité d'espace disponible pour les systèmes de fichiers
effectifs en raison de détails d'implémentation interne.

Pour de plus amples informations sur la différence entre l'espace de
pool et l'espace de jeux de données, reportez-vous à la section
“Comptabilisation de l'espace ZFS” à la page 56.
Capacité disponible, c'est-à-dire quantité d'espace disponible dans le
pool ou le périphérique. Comme pour la capacité utilisée, cette
quantité diffère légèrement de la quantité d'espace disponible pour
les jeux de données.
Nombre d'opérations de lecture d'E/S envoyées au pool ou au
périphérique, y compris les requêtes de métadonnées.
Nombre d'opérations d'écriture d'E/S envoyées au pool ou au
périphérique.
Bande passante de toutes les opérations de lecture (métadonnées
incluses), exprimée en unités par seconde.
Bande passante de toutes les opérations d'écriture, exprimée en
unités par seconde.

Liste de statistiques relatives à l'ensemble du pool
Sans options, la commande zpool iostat affiche les statistiques accumulées depuis
l'initialisation pour tous les pools du système. Exemple :

# zpool iostat

capacity

operations

bandwidth

pool

used avail

read write

read write

---------- ----- ----- ----- ----- ----- -----

tank

100G 20.0G

1.2M

102K

1.2M 3.45K

dozer

12.3G 67.7G

132K 15.2K 32.1K 1.20K

100

Guide d'administration Solaris ZFS • Octobre 2009

Requête d'état de pool de stockage ZFS

Comme ces statistiques sont cumulatives depuis le démarrage, la bande passante peut sembler
basse si l'activité du pool est relativement faible. Vous pouvez effectuer une requête pour une
vue plus précise de l'utilisation actuelle de la bande passante en spécifiant un intervalle.
Exemple :

# zpool iostat tank 2

capacity

operations

bandwidth

pool

used avail

read write

read write

---------- ----- ----- ----- ----- ----- -----

tank

tank

tank

100G 20.0G

1.2M

102K

1.2M 3.45K

100G 20.0G

134

0 1.34K

0

100G 20.0G

94

342 1.06K

4.1M

Dans cet exemple, la commande affiche les statistiques d'utilisation pour le pool tank
uniquement, toutes les deux secondes, jusqu'à ce que vous saisissiez Ctrl-C. Vous pouvez
également spécifier un paramètre count supplémentaires pour entraîner l'interruption de la
commande une fois le nombre spécifié d'itérations effectuées. Par exemple, zpool iostat 2 3
imprimerait un résumé toutes les deux secondes pour trois itérations, pendant six secondes. S'il
y a un pool unique, les statistiques s'affichent sur des lignes consécutives. S'il existe plusieurs
pools, une ligne pointillée supplémentaire délimite chaque itération pour fournir une
séparation visuelle.

Liste de statistiques de périphériques virtuels
Outre les statistiques d'E/S à l'échelle du pool, la commande zpool iostat permet d'afficher des
statistiques pour des périphériques virtuels spécifiques. Ainsi, vous pouvez identifier les
périphériques anormalement lents ou, tout simplement, consulter la répartition d'E/S générés
par ZFS. Pour effectuer une requête relative à la disposition complète des périphériques virtuels,
ainsi que l'ensemble des statistiques d'E/S, utilisez la commande zpool iostat -v. Exemple :

# zpool iostat -v

capacity

operations

bandwidth

tank

used avail

read write

read write

---------- ----- ----- ----- ----- ----- -----

mirror

20.4G 59.6G

c1t0d0

c1t1d0

-

-

-

-

0

1

1

22

0 6.00K

295 11.2K

148K

299 11.2K

148K

---------- ----- ----- ----- ----- ----- -----

total

24.5K

149M

0

22

0 6.00K

Notez deux points importants lors de l'affichage de statistiques d'E/S par périphérique virtuel :
■ Tout d'abord, l'utilisation d'espace n'est disponible que pour les périphériques virtuels de

niveau supérieur. L'allocation d'espace entre les périphériques virtuels RAID-Z et les miroirs
est spécifique à l'implémentation et ne s'exprime pas facilement en tant que chiffre unique.

Chapitre 4 • Gestion des pools de stockage ZFS

101

Requête d'état de pool de stockage ZFS

■ De plus, il est possible que les chiffres s'additionnent de façon inattendue. En particulier, les

opérations au sein des périphériques RAID-Z et mis en miroir ne sont pas parfaitement
identiques. Cette différence se remarque particulièrement après la création d'un pool, car
une quantité significative d'E/S est réalisée directement sur les disques en tant que partie de
création de pool qui n'est pas comptabilisée au niveau du miroir. Avec le temps, ces chiffres
devraient s'égaliser graduellement, mais les périphériques défaillants, ne répondant pas ou
mis hors ligne peuvent également affecter cette symétrie.

Vous pouvez utiliser les mêmes options (interval et count) lorsque vous étudiez les statistiques
de périphériques virtuels.

Détermination de l'état de maintenance des pools de
stockage ZFS
ZFS offre une méthode intégrée pour examiner la maintenance des pools et des périphériques.
La maintenance d'un pool se détermine par l'état de l'ensemble de ses périphériques. La
commande zpool status permet d'afficher ces informations d'état. En outre, les défaillances
potentielles des pools et des périphériques sont rapportées par la commande fmd et s'affichent
dans la console système et dans le fichier /var/adm/messages. Cette section décrit les
méthodes permettant de déterminer la maintenance des pools et des périphériques. Ce chapitre
n'aborde cependant pas les méthodes de réparation ou de récupération de pools en mauvais état
de maintenance. Pour plus d'informations sur le dépannage et la récupération des données,
reportez-vous au Chapitre 11, “Résolution de problèmes et récupération de données ZFS”.

Chaque périphérique peut se trouver dans l'un des états suivants :

ONLINE

DEGRADED

FAULTED

OFFLINE

Le périphérique est en état de fonctionnement normal. Bien que certaines
erreurs transitoires puissent se produire, le périphérique est en état de bon
fonctionnement.
Le périphérique virtuel a subi une défaillance mais est toujours capable de
fonctionner. Cet état est le plus commun lorsqu'un miroir ou un périphérique
RAID-Z a perdu un ou plusieurs périphériques le constituant. La tolérance de
pannes du pool peut être compromise dans la mesure où une défaillance
ultérieure d'un autre périphérique peut être impossible à résoudre.
Le périphérique virtuel est totalement inaccessible. Cet état indique en règle
général une défaillance totale du périphérique, de telle façon que ZFS est
incapable d'y envoyer des données ou d'en recevoir de lui. Si un périphérique
virtuel de niveau supérieur se trouve dans cet état, le pool est totalement
inaccessible.
Le périphérique virtuel a été mis hors ligne explicitement par
l'administrateur.

102

Guide d'administration Solaris ZFS • Octobre 2009

Requête d'état de pool de stockage ZFS

UNAVAILABLE

REMOVED

L'ouverture du périphérique ou du périphérique virtuel est impossible. Dans
certains cas, les pools avec des périphériques en état UNAVAILABLE s'affichent
en mode DEGRADED. Si un périphérique de niveau supérieur est indisponible,
aucun élément du pool n'est accessible.
Le périphérique a été retiré alors que le système était en cours d'exécution. La
détection du retrait d'un périphérique dépend du matériel et n'est pas pris en
charge sur toutes les plate-formes.

La maintenance d'un pool est déterminée à partir de celle de l'ensemble de ses périphériques
virtuels. Si l'état de tous les périphériques virtuels est ONLINE, l'état du pool est également
ONLINE. Si l'état d'un des périphériques virtuels est DEGRADED ou UNAVAILABLE, l'état du pool est
également DEGRADED. Si l'état d'un des périphériques virtuels est FAULTED ou OFFLINE, l'état du
pool est également FAULTED. Un pool en état défaillant est totalement inaccessible. Aucune
donnée ne peut être récupérée tant que les périphériques nécessaires n'ont pas été connectés ou
réparés. Un pool en état dégradé continue de s'exécuter, mais vous risquez de ne pas atteindre le
même niveau de redondance de données ou de traitement de données que si le pool était en
ligne.

État de maintenance de base de pool de stockage
La commande zpool status constitue la manière la plus simple d'effectuer une requête relative
à une vue d'ensemble de l'état de maintenance d'un pool :

# zpool status -x

all pools are healthy

Il est possible d'examiner des pools spécifiques en spécifiant un nom de pool dans la commande.
Tout pool n'étant pas en état ONLINE doit être passé en revue pour vérifier tout problème
potentiel, comme décrit dans la section suivante.

État de maintenance détaillé
L'option -v permet d'effectuer une requête pour obtenir un résumé détaillé de l'état de
maintenance. Exemple :

# zpool status -v tank

pool: tank

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist

for the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: none requested

config:

Chapitre 4 • Gestion des pools de stockage ZFS

103

Requête d'état de pool de stockage ZFS

NAME

tank

mirror

DEGRADED

c1t0d0

FAULTED

c1t1d0

ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

DEGRADED

0

0

0

0

0

0

0

0

0

0

0 cannot open

0

Cette sortie affiche une description complète des raisons de l'état actuel du pool, y compris une
description lisible du problème et un lien vers un article de connaissances contenant de plus
amples informations. Les articles de connaissances donnent les informations les plus récentes
vous permettant de résoudre le problème. Les informations détaillées de configuration doivent
vous permettre de déterminer les périphériques endommagés et la manière de réparer le pool.

Dans l'exemple ci-dessus, le périphérique défaillant devrait être remplacé. Une fois le
périphérique remplacé, exécutez la commande zpool online pour le remettre en ligne.
Exemple :

# zpool online tank c1t0d0

Bringing device c1t0d0 online

# zpool status -x

all pools are healthy

Si un périphérique d'un pool est hors ligne, la sortie de commande identifie le pool qui pose
problème. Exemple :

# zpool status -x

pool: tank

state: DEGRADED

status: One or more devices has been taken offline by the adminstrator.

Sufficient replicas exist for the pool to continue functioning in a

degraded state.

action: Online the device using ’zpool online’ or replace the device with

’zpool replace’.

scrub: none requested

config:

NAME

tank

DEGRADED

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

mirror

DEGRADED

c1t0d0 ONLINE

c1t1d0 OFFLINE

errors: No known data errors

Les colonnes READ et WRITE indiquent le nombre d'erreurs d'E/S détectées dans le périphérique,
tandis que la colonne CKSUM indique le nombre d'erreurs de somme de contrôle qui se sont
produites sur le périphérique. Il est probable que ces erreurs correspondent à une défaillance

104

Guide d'administration Solaris ZFS • Octobre 2009

Migration de pools de stockage ZFS

potentielle de périphérique et que des actions correctives soient requises. Si le nombre d'erreurs
est non nul pour un périphérique virtuel de niveau supérieur, il est possible que des parties de
vos données soient inaccessibles. Le nombre d'erreurs identifie toute erreur de données connue.

Dans l'exemple de sortie ci-dessus, le périphérique mis en ligne ne cause aucune erreur de
données.

Pour plus d'informations sur le diagnostic et la réparation de pools et de données défaillants,
reportez-vous au Chapitre 11, “Résolution de problèmes et récupération de données ZFS”.

Migration de pools de stockage ZFS

Parfois, il est possible que vous deviez déplacer un pool de stockage d'une machine à l'autre.
Pour ce faire, les périphériques de stockage doivent être déconnectés de la machine d'origine et
reconnectés à la machine de destination. Pour accomplir cette tâche, vous pouvez raccorder
physiquement les périphériques ou utiliser des périphériques multiport, par exemple les
périphériques d'un SAN. ZFS permet d'exporter le pool à partir d'une machine et de l'importer
vers la machine de destination, même si les endianness des deux machines sont différents. Pour
plus d'informations sur la réplication ou la migration de systèmes de fichiers d'un pool de
stockage à un autre résidant éventuellement sur des machines différentes, reportez-vous à la
section “Envoi et réception de données ZFS” à la page 223.
■ “Préparatifs de migration de pool de stockage ZFS” à la page 105
■ “Exportation d'un pool de stockage ZFS” à la page 106
■ “Définition des pools de stockage disponibles pour importation” à la page 106
■ “Recherche de pools de stockage ZFS dans d'autres répertoires” à la page 108
■ “Importation de pools de stockage ZFS” à la page 109
■ “Récupération de pools de stockage ZFS détruits” à la page 110
■ “Mise à niveau de pools de stockage ZFS” à la page 112

Préparatifs de migration de pool de stockage ZFS
Il est conseillé d'exporter les pools de stockage explicitement afin d'indiquer qu'ils sont prêts à la
migration. Cette opération vide toute donnée non écrite sur le disque, écrit les données sur le
disque en indiquant que l'exportation a été effectuée et supprime toute connaissance du pool
dans le système.

Si vous retirez les disques manuellement, au lieu d'exporter le pool explicitement, vous pouvez
toujours importer le pool résultant dans un autre système. Cependant, vous pourriez perdre les
dernières secondes de transactions de données et le pool s'affichera alors comme étant défaillant
sur la machine d'origine dans la mesure où les périphériques ne sont plus présents. Par défaut, la
machine de destination refuse d'importer un pool qui n'a pas été exporté implicitement. Cette

Chapitre 4 • Gestion des pools de stockage ZFS

105

Migration de pools de stockage ZFS

condition est nécessaire car elle évite les importations accidentelles d'un pool composé de
stockage connecté au réseau toujours en cours d'utilisation dans un autre système.

Exportation d'un pool de stockage ZFS
La commande zpool export permet d'exporter un pool. Exemple :

# zpool export tank

Une fois la commande exécutée, le pool tank n'est plus visible sur le système. La commande
tente de démonter tout système de fichiers démonté au sein du pool avant de continuer. Si le
démontage d'un des système de fichiers est impossible, vous pouvez le forcer à l'aide de l'option
-f. Exemple :

# zpool export tank

cannot unmount ’/export/home/eschrock’: Device busy

# zpool export -f tank

Si les périphériques ne sont pas disponibles lors de l'export, les disques ne peuvent pas être
spécifiés comme étant exportés sans défaut. Si un de ces périphériques est connecté
ultérieurement à un système sans aucun des périphériques en mode de fonctionnement, il
s'affiche comme étant "potentiellement actif". Si des volumes ZFS sont utilisés dans le pool, ce
dernier ne peut pas être exporté, même avec l'option -f. Pour exporter un pool contenant un
volume ZFS, vérifiez au préalable que tous les utilisateurs du volume ne sont plus actifs.

Pour de plus amples informations sur les volumes ZFS, reportez-vous à la section “Volumes
ZFS” à la page 275.

Définition des pools de stockage disponibles pour
importation
Une fois le pool supprimé du système (soit par le biais de l'exportation, soit par le biais d'une
suppression forcée des périphériques), connectez les périphériques au système cible. Bien que
ZFS soit capable de gérer des situations dans lesquelles seule une partie des périphériques est
disponible, tous les périphériques dans le pool doivent être déplacés d'un système à l'autre. Il
n'est pas nécessaire que les périphériques soient connectés sous le même nom de périphérique.
ZFS détecte tout périphérique déplacé ou renommé et ajuste la configuration de façon adéquate.
Pour connaître les pools disponibles, exécutez la commande zpool import sans option.
Exemple :

# zpool import

pool: tank

106

Guide d'administration Solaris ZFS • Octobre 2009

Migration de pools de stockage ZFS

id: 3778921145927357706

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

tank

ONLINE

mirror

ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

Dans cet exemple, le pool tank est disponible pour être importé dans le système cible. Chaque
pool est identifié par un nom et un identifiant numérique unique. Si plusieurs pools à importer
portent le même nom, vous pouvez utiliser leur identificateur numérique afin de les distinguer.

Tout comme la commande zpool status, la commande zpool import se rapporte à un article
de connaissances disponible sur le Web avec les informations les plus récentes sur les
procédures de réparation pour les problèmes qui empêchent l'importation d'un pool. Dans ce
cas, l'utilisateur peut forcer l'importation du pool. Cependant, l'importation d'un pool en cours
d'utilisation par un autre système au sein d'un réseau de stockage peut entraîner une corruption
des données et des erreurs graves si les deux systèmes tentent d'écrire dans le même stockage. Si
certains périphériques dans le pool ne sont pas disponibles, mais que la redondance est
suffisante pour obtenir un pool utilisable, le pool s'affiche dans l'état DEGRADED. Exemple :

# zpool import

pool: tank

id: 3778921145927357706

state: DEGRADED

status: One or more devices are missing from the system.

action: The pool can be imported despite missing or damaged devices. The

fault tolerance of the pool may be compromised if imported.

see: http://www.sun.com/msg/ZFS-8000-2Q

config:

tank

DEGRADED

mirror

DEGRADED

c1t0d0

UNAVAIL

cannot open

c1t1d0

ONLINE

Dans cet exemple, le premier disque est endommagé ou manquant, mais il est toujours possible
d'importer le pool car les données mises en miroir restent accessibles. Si le nombre de
périphériques défaillants ou manquant est trop importants, l'importation du pool est
impossible. Exemple :

# zpool import

pool: dozer

id: 12090808386336829175

Chapitre 4 • Gestion des pools de stockage ZFS

107

Migration de pools de stockage ZFS

state: FAULTED

action: The pool cannot be imported. Attach the missing

devices and try again.

see: http://www.sun.com/msg/ZFS-8000-6X

config:

raidz

FAULTED

c1t0d0

ONLINE

c1t1d0

FAULTED

c1t2d0

ONLINE

c1t3d0

FAULTED

Dans cet exemple, deux disques manquent dans un périphérique virtuel RAID-Z, ce qui signifie
que les données redondantes disponibles ne sont pas suffisantes pour reconstruire le pool. Dans
certains cas, les périphériques présents ne sont pas suffisants pour déterminer la configuration
complète. Dans ce cas, ZFS ne peut pas déterminer les autres périphériques faisaient partie du
pool, mais fournit autant d'informations que possible sur la situation. Exemple :

# zpool import

pool: dozer

id: 12090808386336829175

state: FAULTED

status: One or more devices are missing from the system.

action: The pool cannot be imported. Attach the missing

devices and try again.

see: http://www.sun.com/msg/ZFS-8000-6X

config:

dozer

FAULTED

missing device

raidz

ONLINE

c1t0d0

ONLINE

c1t1d0

ONLINE

c1t2d0

ONLINE

c1t3d0

ONLINE

Additional devices are known to be part of this pool, though their

exact configuration cannot be determined.

Recherche de pools de stockage ZFS dans d'autres
répertoires
Par défaut, la commande zpool import ne recherche les périphériques que dans le répertoire
/dev/dsk. Si les périphériques existent dans un autre répertoire, ou si vous utilisez des pools
sauvegardés dans des fichiers, utilisez l'option -d pour effectuer des recherches dans différents
répertoires. Exemple :

108

Guide d'administration Solaris ZFS • Octobre 2009

Migration de pools de stockage ZFS

# zpool create dozer mirror /file/a /file/b

# zpool export dozer

# zpool import -d /file

pool: dozer

id: 10952414725867935582

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

dozer

ONLINE

mirror

ONLINE

/file/a ONLINE

/file/b ONLINE

# zpool import -d /file dozer

Si les périphériques se trouvent dans plusieurs répertoires, vous pouvez utiliser plusieurs
options - d.

Importation de pools de stockage ZFS
Une fois le pool identifié pour importation, vous pouvez l'importer en spécifiant son nom ou
son identifiant numérique en tant qu'argument pour la commande zpool import. Exemple :

# zpool import tank

Si plusieurs pools disponibles possèdent le même nom, vous pouvez spécifier le pool à importer
à l'aide de l'identifiant numérique. Exemple :

# zpool import

pool: dozer

id: 2704475622193776801

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

dozer

ONLINE

c1t9d0

ONLINE

pool: dozer

id: 6223921996155991199

state: ONLINE

action: The pool can be imported using its name or numeric identifier.

config:

dozer

ONLINE

c1t8d0

ONLINE

Chapitre 4 • Gestion des pools de stockage ZFS

109

Migration de pools de stockage ZFS

# zpool import dozer

cannot import ’dozer’: more than one matching pool

import by numeric ID instead

# zpool import 6223921996155991199

Si le nom du pool est en conflit avec un nom de pool existant, vous pouvez importer le pool sous
un nom différent. Exemple :

# zpool import dozer zeepool

Cette commande importe le pool dozer exporté sous le nouveau nom zeepool. Si l'exportation
du pool ne s'effectue pas correctement, l'indicateur -f est requis par ZFS pour empêcher les
utilisateurs d'importer par erreur un pool en cours d'utilisation dans un autre système.
Exemple :

# zpool import dozer

cannot import ’dozer’: pool may be in use on another system

use ’-f’ to import anyway

# zpool import -f dozer

Les pools peuvent également être importés sous une racine de remplacement à l'aide de l'option
-R. Pour plus d'informations sur les pools racine de remplacement, reportez-vous à la section
“Utilisation de pools racine ZFS de remplacement” à la page 284.

Récupération de pools de stockage ZFS détruits
La commande zpool import -D permet de récupérer un pool de stockage détruit. Exemple :

# zpool destroy tank

# zpool import -D

pool: tank

id: 3778921145927357706

state: ONLINE (DESTROYED)

action: The pool can be imported using its name or numeric identifier. The

pool was destroyed, but can be imported using the ’-Df’ flags.

config:

tank

ONLINE

mirror

ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

Dans la sortie de zpool import, vous pouvez identifier ce pool comme étant le pool détruit en
raison des informations d'état suivantes :

110

Guide d'administration Solaris ZFS • Octobre 2009

Migration de pools de stockage ZFS

state: ONLINE (DESTROYED)

Pour récupérer le pool détruit, exécutez la commande zpool import -D à nouveau avec le pool à
récupérer. Exemple :

# zpool import -D tank

# zpool status tank

pool: tank

state: ONLINE

scrub: none requested

config:

NAME

tank

ONLINE

mirror

ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

Même si l'un des périphériques du pool détruit est défaillant ou indisponible, vous devriez être
en mesure de récupérer le pool détruit en incluant l'option -f. Dans ce cas, importez le pool
défaillant et tentez ensuite de réparer la défaillance du périphérique. Exemple :

# zpool destroy dozer

# zpool import -D

pool: dozer

id:

state: DEGRADED (DESTROYED)

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: resilver completed after 0h0m with 0 errors on Fri Aug 28 09:33:56 2009

config:

NAME

STATE

READ WRITE CKSUM

dozer

DEGRADED

raidz2

DEGRADED

c2t8d0

ONLINE

c2t9d0

ONLINE

c2t10d0 ONLINE

c2t11d0 UNAVAIL

c2t12d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

35

0

0

0

0

0

0

1 cannot open

0

errors: No known data errors

# zpool import -Df dozer

Chapitre 4 • Gestion des pools de stockage ZFS

111

Migration de pools de stockage ZFS

# zpool status -x

pool: dozer

state: DEGRADED

status: One or more devices could not be opened. Sufficient replicas exist for

the pool to continue functioning in a degraded state.

action: Attach the missing device and online it using ’zpool online’.

see: http://www.sun.com/msg/ZFS-8000-2Q

scrub: resilver completed after 0h0m with 0 errors on Fri Aug 28 09:33:56 2009

config:

NAME

STATE

READ WRITE CKSUM

dozer

DEGRADED

raidz2

DEGRADED

c2t8d0

ONLINE

c2t9d0

ONLINE

c2t10d0 ONLINE

c2t11d0 UNAVAIL

c2t12d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

37

0

0

0

0

0

0

0 cannot open

0

errors: No known data errors

# zpool online dozer c2t11d0

Bringing device c2t11d0 online

# zpool status -x

all pools are healthy

Mise à niveau de pools de stockage ZFS
Si certains pools de stockage ZFS proviennent d'une version Solaris précédente (la version
Solaris 10 6/06, par exemple), vous pouvez mettre les pools à niveau à l'aide de la commande
zpool upgrade pour bénéficier des fonctions des pools de la version Solaris 10 11/06. De plus, la
commande zpool status indique dorénavant si la version actuelle des pools est plus ancienne.
Exemple :

# zpool status

pool: test

state: ONLINE

status: The pool is formatted using an older on-disk format. The pool can

still be used, but some features are unavailable.

action: Upgrade the pool using ’zpool upgrade’. Once this is done, the

pool will no longer be accessible on older software versions.

scrub: none requested

config:

NAME

test

STATE

READ WRITE CKSUM

ONLINE

0

0

0

112

Guide d'administration Solaris ZFS • Octobre 2009

Migration de pools de stockage ZFS

c1t27d0

ONLINE

0

0

0

errors: No known data errors

Vous pouvez utiliser la syntaxe suivante afin d'identifier des informations supplémentaires sur
une version donnée et sur les versions prises en charge.

# zpool upgrade -v

This system is currently running ZFS version 3.

The following versions are supported:

VER DESCRIPTION

--- --------------------------------------------------------

1

2

3

Initial ZFS version

Ditto blocks (replicated metadata)

Hot spares and double parity RAID-Z

For more information on a particular version, including supported releases, see:

http://www.opensolaris.org/os/community/zfs/version/N

Where ’N’ is the version number.

Vous pouvez ensuite mettre tous vos pools à niveau en exécutant la commande zpool upgrade.
Exemple :

# zpool upgrade -a

Remarque – Si vous mettez à niveau votre pool vers une version ZFS ultérieure, le pool ne sera
pas accessible sur un système qui exécute une version ZFS plus ancienne.

Chapitre 4 • Gestion des pools de stockage ZFS

113

114

5C H A P I T R E

5

Installation et initialisation d'un système de
fichiers racine ZFS

Ce chapitre décrit la procédure d'installation et d'initialisation d'un système de fichiers ZFS. La
migration d'un système de fichiers racine UFS vers un système de fichiers ZFS à l'aide de Solaris
Live Upgrade est également abordée.

Il contient les sections suivantes :
■ “Installation et initialisation d'un système de fichiers racine ZFS (présentation)” à la page 116
■ “Configuration requise pour l'installation de Solaris et de Solaris Live Upgrade pour la prise

en charge de ZFS” à la page 117

■ “Installation d'un système de fichiers racine ZFS (installation initiale)” à la page 120
■ “Installation d'un système de fichiers racine ZFS (installation d'archive Flash)” à la page 127
■ “Installation d'un système de fichiers racine ZFS (installation JumpStart)” à la page 130
■ “Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS

(Solaris Live Upgrade)” à la page 134

■ “Prise en charge ZFS des périphériques de swap et de vidage” à la page 158
■ “Initialisation à partir d'un système de fichiers racine ZFS” à la page 161
■ “Restauration du pool racine ZFS ou des instantanés du pool racine” à la page 167

Vous trouverez la liste des problèmes connus relatifs à cette version dans les notes de version de
Solaris 10 10/09.

Vous trouverez des informations de dépannage mises à jour sur le site suivant :

http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide

115

Installation et initialisation d'un système de fichiers racine ZFS (présentation)

Installation et initialisation d'un système de fichiers racine
ZFS (présentation)

À partir de la version Solaris 10 10/08, vous pouvez installer et initialiser un système de fichiers
racine ZFS des manières suivantes :

■

Il est possible d'effectuer une installation initiale lorsque ZFS est sélectionné comme système
de fichiers racine.

■ Vous pouvez utiliser la fonction Solaris Live Upgrade pour migrer d'un système de fichiers

racine UFS vers un système de fichiers racine ZFS. Vous pouvez en outre utiliser Solaris Live
Upgrade pour effectuer les tâches suivantes :
■ Créer un nouvel environnement d'initialisation dans un pool racine ZFS existant.
■ Créer un nouvel environnement d'initialisation dans un nouveau pool racine ZFS.

■ Vous pouvez utiliser un profil Solaris JumpStart pour installer automatiquement un système

avec un système de fichiers racine ZFS.

■ Dans la version Solaris 10 10/09, vous pouvez utiliser un profil JumpStart pour installer

automatiquement un système avec une archive ZFS Flash.

Une fois qu'un système SPARC ou x86 est installé avec un système de fichiers racine ZFS ou
migré vers un système de fichiers racine ZFS, le système s'initialise automatiquement à partir du
système de fichiers racine ZFS. Pour plus d'informations sur les modifications apportées à
l'initialisation, reportez-vous à la section “Initialisation à partir d'un système de fichiers racine
ZFS” à la page 161.

Fonctions d'installation de ZFS
Les fonctions d'installation de ZFS suivantes sont disponibles dans cette version de Solaris :
■ Le programme interactif d'installation en mode texte de Solaris vous permet d'installer un
système de fichiers racine UFS ou ZFS. Le système de fichiers par défaut est toujours UFS
pour cette version de Solaris. Vous pouvez accéder au programme interactif d'installation en
mode texte d'une des manières suivantes :

■

■

■

Sur un système SPARC, utilisez la syntaxe suivante à partir du DVD d'installation de
Solaris :

ok boot cdrom - text

Sur un système SPARC, utilisez la syntaxe suivante lors d'une initialisation à partir du
réseau :

ok boot net - text

Sur un système x86, sélectionnez l'option d'installation en mode texte lorsque vous y êtes
invité.

116

Guide d'administration Solaris ZFS • Octobre 2009

Installation et initialisation d'un système de fichiers racine ZFS (présentation)

■ L'installation JumpStartTM personnalisée fournit les fonctions suivantes :

■ Vous pouvez définir un profil pour la création d'un pool de stockage ZFS et désigner un

système de fichiers ZFS d'initialisation.

■ Vous pouvez définir un profil pour identifier une archive Flash d'un pool racine ZFS.

■ Vous pouvez utiliser la fonction Solaris Live Upgrade pour migrer d'un système de fichiers
racine UFS vers un système de fichiers racine ZFS. Les commandes lucreate et luactivate
ont été améliorées afin de prendre en charge les pools et systèmes de fichiers ZFS. Les
commandes lustatus et ludelete fonctionnent comme dans les versions précédentes de
Solaris.

■ Vous pouvez configurer un pool racine ZFS mis en miroir en sélectionnant deux disques au

cours de l'installation. Vous pouvez également créer un pool racine ZFS mis en miroir en
connectant d'autres disques une fois l'installation terminée.

■ Les périphériques de swap et de vidage sont automatiquement créés sur les volumes ZFS

dans le pool racine ZFS.

Les fonctions d'installation suivantes ne sont pas disponibles dans la présente version :
■ La fonction d'installation de l'interface graphique permettant d'installer un système de

fichiers racine ZFS n'est actuellement pas disponible.

■ La fonction d'installation Flash SolarisTM pour l'installation d'un système de fichiers racine

ZFS n'est pas disponible en sélectionnant l'option d'installation Flash à partir l'option
d'installation initiale. Cependant, vous pouvez créer un profil JumpStart pour identifier une
archive Flash d'un pool racine ZFS. Pour plus d'informations, reportez-vous à la section
“Installation d'un système de fichiers racine ZFS (installation d'archive Flash)” à la page 127.
■ Le programme de mise à niveau standard ne peut pas être utilisé pour mettre à niveau votre
système de fichiers racine UFS avec un système de fichiers racine ZFS. Si vous disposez d'au
moins une tranche UFS d'initialisation, l'option de mise à niveau standard devrait être
disponible. Si vous disposez de pools ZFS d'initialisation mais d'aucune tranche UFS
d'initialisation, la seule manière d'effectuer une mise à niveau consiste à utiliser Live
Upgrade à la place du programme de mise à niveau standard. Si vous disposez à la fois d'un
pool ZFS d'initialisation et d'une tranche UFS d'initialisation, l'option de mise à niveau
standard devrait être disponible bien que seule la tranche UFS puisse être mise à niveau.

Configuration requise pour l'installation de Solaris et
de Solaris Live Upgrade pour la prise en charge de ZFS
Vérifiez que vous disposez de la configuration requise suivante avant d'installer un système avec
un système de fichiers racine ZFS ou avant de migrer un système de fichiers racine UFS vers un
système de fichiers racine ZFS.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

117

Installation et initialisation d'un système de fichiers racine ZFS (présentation)

Version de Solaris requise
Vous pouvez installer et initialiser un système de fichiers racine ZFS ou migrer vers un système
de fichiers racine ZFS de différentes manières :

■

Installation d'un système de fichiers racine ZFS - disponible à partir de la version 10 10/08 de
Solaris.

■ Migration à partir d'un système de fichiers racine UFS vers un système de fichiers racine ZFS

avec Solaris Live Upgrade : vous devez avoir installé Solaris 10 10/08 ou une version
ultérieure, ou vous devez avoir effectué la mise à niveau vers cette version ou une version
ultérieure.

Configuration requise générale relative au pool de stockage ZFS
Consultez les sections ci-après décrivant l'espace de pool racine ZFS et la configuration requise.

Espace de pool de stockage ZFS requis
La quantité minimale d'espace disponible requis sur le pool d'un système de fichiers racine ZFS
est supérieure à celle d'un système de fichiers racine UFS car les périphériques de swap et de
vidage doivent être distincts dans un environnement racine ZFS. Par défaut, le périphériques de
swap et le périphériques de vidage ne sont qu'un même périphérique sur un système de fichiers
racine UFS.

Lorsqu'un système est installé ou mis à niveau avec un système de fichiers racine ZFS, la taille de
la zone de swap et du périphérique de vidage dépend de la quantité de mémoire physique.
L'espace minimum du pool d'un système de fichiers racine ZFS d'initialisation dépend de la
quantité de mémoire physique, de l'espace disque disponible et du nombre d'environnements
d'initialisation à créer.

Consultez les exigences en termes d'espace de pool de stockage ZFS suivantes :
■ L'installation d'un système de fichiers racine ZFS requiert 768 Mo de mémoire minimum.

■

■

1 Go de mémoire est recommandé pour optimiser les performances globales du ZFS.
16 Go d'espace disque sont recommandés. L'espace est utilisé comme suit :
■ Zone de swap et périphérique de vidage : les capacités par défaut des volumes de swap

et de vidage créés par le programme d'installation de Solaris sont les suivantes :

■

Installation initiale Solaris : la taille du volume de swap par défaut est calculée
comme la moitié de la taille de la mémoire physique, généralement dans une plage
allant de 512 Mo à 2 Go, dans le nouvel environnement d'initialisation ZFS. Vous
pouvez régler la taille du volume de swap au cours de l'installation initiale.

■ La taille par défaut du volume de vidage est calculée par le noyau en fonction des

informations dumpadm et de la taille de la mémoire physique. Vous pouvez régler la
taille du volume de vidage au cours de l'installation initiale.

118

Guide d'administration Solaris ZFS • Octobre 2009

Installation et initialisation d'un système de fichiers racine ZFS (présentation)

■

Solaris Live Upgrade : lors de la migration d'un système de fichiers racine UFS vers
un système de fichiers racine ZFS, la taille par défaut du volume de swap de
l'environnement d'initialisation ZFS est calculée comme la taille du périphérique de
swap de l'environnement d'initialisation UFS. La taille par défaut du volume de swap
est calculée en additionnant la taille de tous les périphériques de swap de
l'environnement d'initialisation UFS. Un volume ZFS de cette taille est ensuite créé
dans l'environnement d'initialisation ZFS. Si aucun périphérique de swap n'est défini
dans l'environnement d'initialisation UFS, la taille par défaut du volume de swap est
définie sur 512 Mo.

■ La valeur du volume de vidage par défaut est définie sur la moitié de la taille de la

mémoire physique, entre 512 Mo et 2 Go, dans l'environnement d'initialisation ZFS.

Vous pouvez définir la taille de votre choix pour les volumes de swap et de vidage, dès
lors qu'elle supporte les opérations du système. Pour plus d'informations, reportez-vous
à la section “Ajustement de la taille de vos périphériques de swap et de vidage ZFS”
à la page 159.

■ Environnement d'initialisation : Outre l'espace requis pour une nouvelle zone de swap
et un nouveau périphérique de vidage ou, les tailles ajustées d'une zone de swap et d'un
périphérique de vidage, un environnement d'initialisation ZFS migré à partir d'un
environnement d'initialisation UFS requiert environ 6 Go. Chaque environnement
d'initialisation ZFS cloné à partir d'un autre environnement d'initialisation ZFS ne
requiert pas d'espace disque supplémentaire ; toutefois, prenez en compte le fait que la
taille de l'environnement d'initialisation est susceptible d'augmenter lors de l'application
de patchs. Tous les environnements d'initialisation ZFS d'un même pool racine utilisent
les mêmes périphériques de swap et de vidage.

Par exemple, un système disposant de 12 Go d'espace disque risque d'être trop petit pour un
environnement d'initialisation ZFS car chaque périphérique de swap et de vidage requiert
2 Go et l'environnement d'initialisation ZFS qui est migré à partir de l'environnement
d'initialisation UFS requiert environ 6 Go.

Configuration requise relative au pool de stockage ZFS
Vérifiez la configuration requise suivante pour le pool de stockage ZFS :
■ Le pool utilisé comme pool racine doit contenir une étiquette SMI. Cette condition doit être

respectée si le pool est créé avec des tranches de disque.

■ Le pool doit exister sur une tranche de disque ou sur des tranches de disque qui sont mises
en miroir. Si vous tentez d'utiliser une configuration de pool non prise en charge lors d'une
migration effectuée par Live Upgrade, un message du type suivant s'affiche :

ERROR: ZFS pool name does not support boot environments

Pour obtenir une description détaillée des configurations de pool racine ZFS prises en
charge, reportez-vous à la section “Création d'un pool racine ZFS” à la page 68.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

119

Installation d'un système de fichiers racine ZFS (installation initiale)

■

Sur un système x86, le disque doit contenir une partition fdisk Solaris. Une partitionfdisk
Solaris est créée automatiquement lors de l'installation du système x86. Pour plus
d'informations sur les partitions fdisk Solaris, reportez-vous à la section “Guidelines for
Creating an fdisk Partition” du System Administration Guide: Devices and File Systems.

■ Les disques désignés comme disques d'initialisation dans un pool racine ZFS doivent être

limités à 1 To sur les systèmes SPARC tout comme sur les systèmes x86.

■ La compression peut être activée sur le pool racine uniquement après l'installation de ce
dernier. Aucun moyen ne permet d'activer la compression sur un pool racine au cours de
l'installation. L'algorithme de compression gzip n'est pas pris en charge sur les pools racine.

Installation d'un système de fichiers racine ZFS (installation
initiale)

Cette version de Solaris vous permet d'effectuer une installation initiale à l'aide du programme
interactif d'installation en mode texte de Solaris pour créer un pool de stockage ZFS contenant
un système de fichiers racine ZFS d'initialisation. Si vous disposez d'un pool de stockage ZFS
que vous souhaitez utiliser pour votre système de fichiers racine ZFS, servez-vous de Solaris
Live Upgrade pour migrer votre système de fichiers racine UFS existant vers un système de
fichiers racine ZFS dans un pool de stockage ZFS existant. Pour plus d'informations,
reportez-vous à la section “Migration d'un système de fichiers racine UFS vers un système de
fichiers racine ZFS (Solaris Live Upgrade)” à la page 134.

Si vous décidez de configurer des zones après l'installation initiale d'un système de fichiers
racine ZFS et si vous prévoyez l'application d'un patch au système ou la mise à niveau de ce
dernier, reportez-vous aux sections “Utilisation de Solaris Live Upgrade pour migrer ou mettre
à jour un système avec zones (Solaris 10 10/08)” à la page 141 ou “Utilisation de Solaris Live
Upgrade pour migrer ou mettre à jour un système avec zones (Solaris 10 5/09 et Solaris 10
10/09)” à la page 147.

Si vous disposez déjà de pools de stockage ZFS sur votre système, ces derniers sont reconnus par
le message suivant, mais restent inchangés, sauf si vous sélectionnez les disques des pools
existants pour créer le nouveau pool de stockage.

There are existing ZFS pools available on this system. However, they can only be upgraded

using the Live Upgrade tools. The following screens will only allow you to install a ZFS root system,

not upgrade one.

Attention – Tous les pools existants dont l'un des disques aura été sélectionné pour le nouveau
pool seront détruits.

120

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation initiale)

Avant de lancer l'installation initiale pour créer un pool de stockage ZFS, reportez-vous à la
section “Configuration requise pour l'installation de Solaris et de Solaris Live Upgrade pour la
prise en charge de ZFS” à la page 117.

Installation initiale d'un système de fichiers racine ZFS d'initialisation

EXEMPLE 5–1
Le processus interactif d'installation en mode texte de Solaris est le même que dans les
précédentes versions de Solaris, exception faite d'un message vous invitant à créer un système
de fichiers racine UFS ou ZFS. UFS demeure dans cette version le système de fichiers par défaut.
Si vous sélectionnez un système de fichiers racine ZFS, un message vous invite à créer un pool
de stockage ZFS. L'installation d'un système de fichiers racine ZFS implique les étapes suivantes
:
1. Sélectionnez la méthode d'installation interactive de Solaris puisqu'une installation Flash de

Solaris n'est pas disponible pour créer un système de fichiers racine ZFS d'initialisation.
Cependant, vous pouvez créer une archive Flash ZFS à utiliser pendant une installation
JumpStart. Pour plus d'informations, reportez-vous à la section “Installation d'un système
de fichiers racine ZFS (installation d'archive Flash)” à la page 127.
À compter de la version Solaris 10 10/08, vous pouvez faire migrer un système de fichier
racine UFS vers un système de fichiers racine ZFS à condition que Solaris 10 10/08 ou une
version ultérieure soit déjà installé. Pour plus d'informations sur la migration vers un
système de fichiers racine ZFS, reportez-vous à la section “Migration d'un système de
fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)”
à la page 134.

2. Pour créer un système de fichiers racine ZFS, sélectionnez l'option ZFS. Exemple :

Choose Filesystem Type

Select the filesystem to use for your Solaris installation

[ ] UFS

[X] ZFS

3. Une fois que le logiciel à installer est sélectionné, un message vous invite à sélectionner les

disques pour créer le pool de stockage ZFS. Cet écran est similaire à celui des versions
précédentes de Solaris :

Select Disks

On this screen you must select the disks for installing Solaris software.

Start by looking at the Suggested Minimum field; this value is the

approximate space needed to install the software you’ve selected. For ZFS,

multiple disks will be configured as mirrors, so the disk you choose, or the

slice within the disk must exceed the Suggested Minimum value.

NOTE: ** denotes current boot disk

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

121

Installation d'un système de fichiers racine ZFS (installation initiale)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

Disk Device

Available Space

=============================================================================

[X] ** c1t1d0

[ ]

c1t2d0

69994 MB

69994 MB (F4 to edit)

Maximum Root Size: 69994 MB

Suggested Minimum:

7466 MB

Vous pouvez sélectionner le ou les disques à utiliser pour le pool racine ZFS. Si vous
sélectionnez deux disques, une configuration de double disque mis en miroir est définie
pour le pool racine. Un pool mis en miroir double ou triple disque est optimal. Si vous
disposez de huit disques et les sélectionner tous, ces huit disques sont utilisés pour le pool
racine comme miroir géant. Cette configuration n'est pas optimale. Une autre possibilité
consiste à créer un pool racine mis en miroir une fois l'installation initiale terminée. La
configuration de pool RAID-Z n'est pas prise en charge pour le pool racine. Pour plus
d'informations sur la configuration des pools de stockage ZFS, reportez-vous à la section
“Fonctions de réplication d'un pool de stockage ZFS” à la page 63.

4. Si vous souhaitez sélectionner deux disques pour créer un pool racine mis en miroir, utilisez

les touches de contrôle du curseur pour sélectionner le deuxième disque. Par exemple, les
deux disques c1t1d0 et c1t2d0 sont sélectionnés pour les disques pool racine. Les deux
disques doivent posséder une étiquette SMI et une tranche 0. Si les disques ne sont pas
identifiés par une étiquette SMI ou contiennent des tranches, vous devez quitter le
programme d'installation, utiliser l'utilitaire de formatage pour réattribuer une étiquette et
repartitionner les disques, puis relancer le programme d'installation.

Select Disks

On this screen you must select the disks for installing Solaris software.

Start by looking at the Suggested Minimum field; this value is the

approximate space needed to install the software you’ve selected. For ZFS,

multiple disks will be configured as mirrors, so the disk you choose, or the

slice within the disk must exceed the Suggested Minimum value.

NOTE: ** denotes current boot disk

Disk Device

Available Space

=============================================================================

[X] ** c1t1d0

[X]

c1t2d0

69994 MB

69994 MB (F4 to edit)

Maximum Root Size: 69994 MB

Suggested Minimum:

7466 MB

Si la colonne d'espace disponible indique 0 Mo, cela signifie généralement que le disque
possède une étiquette EFI.

122

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation initiale)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

5. Une fois que vous avez sélectionné un ou plusieurs disques pour le pool de stockage ZFS, un

écran similaire au suivant s'affiche :

Configure ZFS Settings

Specify the name of the pool to be created from the disk(s) you have chosen.

Also specify the name of the dataset to be created within the pool that is

to be used as the root directory for the filesystem.

ZFS Pool Name: rpool

ZFS Root Dataset Name: s10s_u8wos_08a

ZFS Pool Size (in MB): 69994

Size of Swap Area (in MB): 2048

Size of Dump Area (in MB): 1024

(Pool size must be between 6442 MB and 69995 MB)

[X] Keep / and /var combined

[ ] Put /var on a separate dataset

Vous pouvez, à partir de cet écran, modifier le nom du pool ZFS, le nom du jeu de données,
la taille du pool, ainsi que la taille du périphérique de swap et du périphérique de vidage en
déplaçant les touches de contrôle du curseur sur les entrées et en remplaçant la valeur de
texte par défaut par le nouveau texte. Vous pouvez aussi accepter les valeurs par défaut. De
plus, vous pouvez modifier la façon dont le système de fichiers /var est créé et monté.

Dans cet exemple, le nom de jeu de base de données racine a été modifié en zfs1009BE.

ZFS Pool Name: rpool

ZFS Root Dataset Name: zfs1009BE

ZFS Pool Size (in MB): 69994

Size of Swap Area (in MB): 2048

Size of Dump Area (in MB): 1024

(Pool size must be between 6442 MB and 69995 MB)

[X] Keep / and /var combined

[ ] Put /var on a separate dataset

6. Vous pouvez modifier le profil d'installation dans ce dernier écran de l'installation.

Exemple :

Profile

The information shown below is your profile for installing Solaris software.

It reflects the choices you’ve made on previous screens.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

123

Installation d'un système de fichiers racine ZFS (installation initiale)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

============================================================================

Installation Option: Initial

Boot Device: c1t1d0

Root File System Type: ZFS

Client Services: None

Regions: North America

System Locale: C ( C )

Software: Solaris 10, Entire Distribution

Pool Name: rpool

Boot Environment Name: zfs1009BE

Pool Size: 69994 MB

Devices in Pool: c1t1d0

c1t2d0

Une fois l'installation terminée, examinez les informations concernant le pool de stockage et le
système de fichiers ZFS. Exemple :

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

mirror

ONLINE

c1t1d0s0 ONLINE

c1t2d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

6.66G 60.3G

97K /rpool

rpool/ROOT

4.66G 60.3G

21K legacy

rpool/ROOT/zfs1009BE 4.66G 60.3G 4.66G /

rpool/dump

1.00G 60.3G 1.00G -

rpool/export

44K 60.3G

23K /export

rpool/export/home

21K 60.3G

21K /export/home

rpool/swap

1G 61.3G

16K -

L'exemple de sortie de la commande zfs list identifie les composants du pool racine,
notamment le répertoire rpool/ROOT, qui n'est pas accessible par défaut.

124

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation initiale)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

Si vous avez au départ créé un pool de stockage ZFS avec un disque, vous pouvez le convertir en
une configuration ZFS mise en miroir une fois l'installation terminée à l'aide de la commande
zpool attach pour y connecter un disque disponible. Exemple :

# zpool attach rpool c1t1d0s0 c1t2d0s0

# zpool status

pool: rpool

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scrub: resilver in progress for 0h0m, 5.03% done, 0h13m to go

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

mirror

ONLINE

c1t1d0s0 ONLINE

c1t2d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

La réargenture des données sur le nouveau disque prend un certain temps mais le pool reste
disponible.

En attendant une solution au problème CR 6668666, vous devez installer les informations
d'initialisation sur les disques supplémentaires que vous connectez, à l'aide de la commande
installboot ou de la commande installgrub si vous souhaitez activer l'initialisation sur les
autres disques du miroir. Si vous créez un pool racine ZFS mis en miroir par la méthode
d'installation initiale, cette étape n'est pas nécessaire. Pour en savoir plus sur l'installation des
informations d'initialisation, reportez-vous à la section “Initialisation à partir d'un disque
alternatif d'un pool racine ZFS mis en miroir” à la page 162.

Pour plus d'informations sur l'ajout ou la connexion de disques, reportez-vous à la section
“Gestion de périphériques dans un pool de stockage ZFS” à la page 77.

Pour créer un autre environnement d'initialisation ZFS dans le même pool de stockage, vous
pouvez utiliser la commande lucreate. Dans l'exemple suivant, un nouvel environnement
d'initialisation nommé zfs10092BE est créé. L'environnement d'initialisation actuel nommé
zfs509BE (affiché dans la sortie de la commande zfs list) n'est pas reconnu dans la sortie de la
commande lustatus tant que le nouvel environnement d'initialisation n'est pas créé.

# lustatus

ERROR: No boot environments are configured on this system

ERROR: cannot determine list of all boot environment names

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

125

Installation d'un système de fichiers racine ZFS (installation initiale)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

Pour créer un environnement d'initialisation ZFS dans le même pool, utilisez une syntaxe du
type suivant :

# lucreate -n zfs10092BE

Analyzing system configuration.

No name for current boot environment.

INFORMATION: The current boot environment is not named - assigning name <zfs1009BE>.

Current boot environment is named <zfs1009BE>.

Creating initial configuration for primary boot environment <zfs1009BE>.

The device </dev/dsk/c1t1d0s0> is not a root device for any boot environment; cannot get BE ID.

PBE configuration successful: PBE name <zfs1009BE> PBE Boot Device </dev/dsk/c1t1d0s0>.

Comparing source boot environment <zfs1009BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

Creating configuration for boot environment <zfs10092BE>.

Source boot environment is <zfs1009BE>.

Creating boot environment <zfs10092BE>.

Cloning file systems from boot environment <zfs1009BE> to create boot environment <zfs10092BE>.

Creating snapshot for <rpool/ROOT/zfs1009BE> on <rpool/ROOT/zfs1009BE@zfs10092BE>.

Creating clone for <rpool/ROOT/zfs1009BE@zfs10092BE> on <rpool/ROOT/zfs10092BE>.

Setting canmount=noauto for </> in zone <global> on <rpool/ROOT/zfs10092BE>.

Population of boot environment <zfs10092BE> successful.

Creation of boot environment <zfs10092BE> successful.

La création d'un environnement d'initialisation ZFS dans le même pool fait appel aux mêmes
fonctions de clonage et d'instantané ZFS afin de créer l'environnement d'initialisation
instantanément. Pour plus d'informations sur l'utilisation de Solaris Live Upgrade pour une
migration racine ZFS, reportez-vous à la section “Migration d'un système de fichiers racine UFS
vers un système de fichiers racine ZFS (Solaris Live Upgrade)” à la page 134.

Vérifiez ensuite les nouveaux environnements d'initialisation. Exemple :

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs1009BE

zfs10092BE

# zfs list

NAME

rpool

yes

yes

yes

yes

no

no

no

yes

-

-

USED AVAIL REFER MOUNTPOINT

6.66G 60.3G

97K /rpool

rpool/ROOT

4.66G 60.3G

21K legacy

rpool/ROOT/zfs10092BE

93K 60.3G 4.66G /

rpool/ROOT/zfs1009BE

4.66G 60.3G 4.66G /

126

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation d'archive Flash)

EXEMPLE 5–1

Installation initiale d'un système de fichiers racine ZFS d'initialisation

(Suite)

rpool/ROOT/zfs1009BE@zfs10092BE 81.5K

- 4.66G -

rpool/dump

rpool/export

1.00G 60.3G 1.00G -

44K 60.3G

23K /export

rpool/export/home

21K 60.3G

21K /export/home

rpool/swap

1G 61.3G

16K -

Pour effectuer l'initialisation à partir d'un environnement d'initialisation alternatif, utilisez la
commande luactivate. Après avoir activé l'environnement d'initialisation sur un système
SPARC, vous pouvez utiliser la commande boot -L pour identifier les environnements
d'initialisation disponibles lorsque le périphérique d'initialisation contient un pool de stockage
ZFS. Lors de l'initialisation à partir d'un système x86, identifiez l'environnement d'initialisation
à partir duquel effectuer l'initialisation dans le menu GRUB.
Par exemple, sur un système SPARC, utilisez la commande boot - L pour afficher une liste
d'environnements d'initialisation disponibles. Pour effectuer l'initialisation à partir du nouvel
environnement d'initialisation zfs5092BE, sélectionnez l'option 2. Saisissez ensuite la
commande boot -Z affichée.

ok boot -L

Executing last command: boot -L

Boot device: /pci@1f,0/pci@1/scsi@8/disk@0,0:a File and args: -L

1 zfs1009BE

2 zfs10092BE

Select environment to boot: [ 1 - 2 ]: 2

To boot the selected entry, invoke:

boot [<root-device>] -Z rpool/ROOT/zfs10092BE

ok boot -Z rpool/ROOT/zfs10092BE

Pour plus d'informations sur l'initialisation d'un système de fichiers ZFS, reportez-vous à la
section “Initialisation à partir d'un système de fichiers racine ZFS” à la page 161.

Installation d'un système de fichiers racine ZFS (installation
d'archive Flash)

Dans la version Solaris 10 10/09, une archive Flash peut être créée sur un système fonctionnant
avec un système de fichiers racine UFS ou un système de fichiers racine ZFS. Une archive Flash
d'un pool racine ZFS contient l'intégralité de la hiérarchie du pool, à l'exception des volumes de
swap et de vidage, ainsi que des jeux de données exclus. Les volumes de swap et de vidage sont
créés quand l'archive Flash est installée. Vous pouvez utiliser la méthode d'installation d'archive
Flash pour :

■

générer une archive Flash qui peut être utilisée pour installer et initialiser un système avec
un système de fichiers racine ZFS ;

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

127

Installation d'un système de fichiers racine ZFS (installation d'archive Flash)

■

effectuer une installation JumpStart d'un système en utilisant une archive Flash ZFS. La
création d'une archive Flash ZFS clone l'intégralité du pool racine, pas les environnements
d'initialisation individuels. Les jeux de données individuels au sein du pool peuvent être
exclus à l'aide de l'option D des commandes flarcreate et -flar.

Vérifiez les limitations suivantes avant d'envisager d'installer un système avec une archive Flash
ZFS :

■

Seule l'installation JumpStart d'une archive Flash ZFS est prise en charge. Vous ne pouvez
pas utiliser l'option d'installation interactive d'une archive Flash pour installer un système
avec un système de fichiers racine ZFS. Vous ne pouvez pas non plus utiliser une archive
Flash pour installer un environnement d'initialisation ZFS avec Solaris Live Upgrade.
■ Vous pouvez uniquement installer un système doté de la même architecture avec une

archive Flash ZFS. Par exemple, une archive créée sur un système sun4u ne peut pas être
installée sur un système sun4v.
Seule une nouvelle installation complète d'une archive Flash ZFS est prise en charge. Vous
ne pouvez pas installer d'archive Flash différentielle d'un système de fichiers racine ZFS ni
installer une archive UFS/ZFS hybride.

■

■ Les archives Flash UFS existantes peuvent toujours être utilisées uniquement pour installer
un système de fichiers racine UFS. L'archive Flash ZFS peut uniquement être utilisée pour
installer un système de fichiers racine ZFS.

■ Bien que la totalité du pool racine, mis à part les jeux de données exclus explicitement, soit

archivée et installée, seul l'environnement d'initialisation ZFS qui est initialisé lors de la
création de l'archive est utilisable après l'installation de l'archive Flash. Cependant, les pools
qui sont archivés avec l'option - R rootdir de la commande flar ou flarcreate peuvent
être utilisés pour archiver un pool racine autre que celui qui est en cours d'initialisation.

■ Un nom de pool racine ZFS qui est créé avec une archive Flash doit correspondre au nom du

pool racine principal. Le nom du pool racine utilisé pour créer l'archive Flash est le nom
attribué au pool nouvellement créé. La modification du nom du pool n'est pas prise en
charge.

■ Les options des commandes flarcreate et flar d'inclusion et d'exclusion des fichiers

individuels ne sont pas prises en charge dans une archive Flash ZFS. Vous pouvez seulement
exclure des jeux de données entiers à partir d'une archive Flash ZFS.

■ La commande flar info n'est pas prise en charge par une archive Flash ZFS. Exemple :

# flar info -l zfs10u8flar

ERROR: archive content listing not supported for zfs archives.

Une fois un système principal installé avec la version Solaris 10 10/09 ou mis à niveau vers cette
version, vous pouvez créer une archive Flash ZFS à utiliser pour installer un système cible. Le
processus de base est comme décrit ci-après :

■

Installez ou mettez à niveau la version Solaris 10 10/09 sur le système principal. Ajoutez
toutes les personnalisations que vous souhaitez.

128

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation d'archive Flash)

■ Créez l'archive Flash ZFS avec la commande flarcreate sur le système principal. Tous les
jeux de données du pool racine, à l'exception des volumes de swap et de vidage, sont inclus
dans l'archive Flash ZFS.

■ Créez un profil JumpStart pour inclure les informations d'archive Flash sur le serveur

d'installation.
Installez l'archive Flash ZFS sur le système cible.

■

Les options d'archive suivantes sont prises en charge lors de l'installation d'un pool racine ZFS
avec une archive Flash :
■ Utilisez la commande flarcreate ou flar pour créer une archive Flash à partir du pool

racine ZFS spécifié. Si aucune information n'est spécifiée, une archive Flash du pool racine
par défaut est créée.

■ Utilisez flarcreate -D dataset pour exclure les jeux de données spécifiés de l'archive Flash.

Cette option peut être utilisée plusieurs fois pour exclure plusieurs jeux de données.

Une fois une archive Flash ZFS installée, le système est configuré comme suit :
■ L'ensemble de la hiérarchie du jeu de données qui existait sur le système sur lequel l'archive
Flash a été créée est recréé sur le système cible, mis à part tous les jeux de données qui ont été
spécifiquement exclus au moment de la création de l'archive. Les volumes de swap et de
vidage ne sont pas inclus dans l'archive Flash.

■ Le pool racine possède le même nom que le pool qui a été utilisé pour créer l'archive.
■ L'environnement d'initialisation qui était actif au moment où l'archive Flash a été créée est

l'environnement d'initialisation actif et par défaut sur les systèmes déployés.

Installation d'un système avec une archive Flash ZFS

EXEMPLE 5–2
Une fois le système principal installé ou mis à niveau vers la version Solaris 10 10/09, créez une
archive Flash du pool racine ZFS. Exemple :

# flarcreate -n zfs10u8BE zfs10u8flar

Full Flash

Checking integrity...

Integrity OK.

Running precreation scripts...

Precreation scripts done.

Determining the size of the archive...

The archive will be approximately 4.94GB.

Creating the archive...

Archive creation complete.

Running postcreation scripts...

Postcreation scripts done.

Running pre-exit scripts...

Pre-exit scripts done.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

129

Installation d'un système de fichiers racine ZFS (installation JumpStart)

Sur le système qui va être utilisé en tant que serveur d'installation, créez un profil JumpStart
comme vous le feriez pour installer un système quelconque. Par exemple, le profil suivant est
utilisé pour installer l'archive zfs10u8flar.

install_type flash_install

archive_location nfs system:/export/jump/zfs10u8flar

partitioning explicit

pool rpool auto auto auto mirror c0t1d0s0 c0t0d0s0

Installation d'un système de fichiers racine ZFS (installation
JumpStart)

Vous pouvez créer un profil JumpStart pour installer un système de fichiers racine ZFS ou un
système de fichiers racine UFS. Si le profil est configuré pour installer un système de fichiers
racine UFS, tous les mots clés de profil existants fonctionnent comme dans les versions
précédentes de Solaris.

Un profil spécifique à ZFS doit contenir le nouveau mot clé pool. Le mot clé "pool" installe un
nouveau pool racine et un nouvel environnement d'initialisation est par défaut créé. Vous
pouvez fournir le nom de l'environnement d'initialisation et créer un jeu de données /var
distinct à l'aide des mots clés bootenv installbe et, des options bename et dataset.

Pour plus d'informations sur l'utilisation des fonctions JumpStart, reportez-vous au Guide
d’installation Solaris 10 : Installation JumpStart personnalisée et installation avancée.

Si vous décidez de configurer des zones après l'installation JumpStart d'un système de fichiers
racine ZFS et si vous prévoyez l'application d'un patch au système ou la mise à niveau de ce
dernier, reportez-vous à la section “Utilisation de Solaris Live Upgrade pour migrer ou mettre à
jour un système avec zones (Solaris 10 10/08)” à la page 141.

Exemples de profils JumpStart de ZFS
Cette section fournit des exemples de profils JumpStart spécifiques à ZFS.

Le profil suivant effectue une installation initiale spécifiée avec install_type initial-install
dans un nouveau pool, identifié par pool newpool, dont la taille est automatiquement définie
sur la taille des disques spécifiés par le mot clé auto. La zone de swap et le périphérique de
vidage sont automatiquement dimensionnés par le mot clé auto dans une configuration de
disques mis en miroir (mot clé mirror et disques c0t0d0s0 et c0t1d0s0 ). Les caractéristiques de
l'environnement d'initialisation sont définies avec le mot clé bootenv afin d'installer un nouvel
environnement d'initialisation avec le mot clé installbe ; un environnement (bename) nommé
s10up-xx est créé.

130

Guide d'administration Solaris ZFS • Octobre 2009

Installation d'un système de fichiers racine ZFS (installation JumpStart)

install_type initial_install

pool newpool auto auto auto mirror c0t0d0s0 c0t1d0s0

bootenv installbe bename s10up-xx

Le profil suivant effectue une installation initiale avec le mot clé install_type initial-install du
métacluster SUNWCall dans un nouveau pool de 80 Go appelé newpool. Ce pool est créé avec un
volume de swap et un volume de vidage de 2 Go chacun, dans une configuration mise en miroir
comprenant deux périphériques disponibles dont la taille permet de créer un pool de 80 Go. Si
deux périphériques de ce type ne sont pas disponibles, l'installation échoue. Les caractéristiques
de l'environnement d'initialisation sont définies avec le mot clé bootenv afin d'installer un
nouvel environnement d'initialisation avec le mot clé installbe ; un environnement (bename)
nommé s10up-xx est créé.

install_type initial_install

cluster SUNWCall

pool newpool 80g 2g 2g mirror any any

bootenv installbe bename s10up-xx

La syntaxe d'installation JumpStart permet de conserver ou de créer un système de fichiers UFS
sur un disque qui contient également un pool racine ZFS. Cette configuration n'est pas
recommandée pour les systèmes de production, mais peut être utilisée pour les besoins en
transition ou migration sur un petit système, tel qu'un ordinateur portable.

Mots clés JumpStart de ZFS
Les mots clés suivants sont autorisés dans un profil spécifique à ZFS :

auto

bootenv

Spécifie la taille des tranches du pool, du volume de swap ou du volume de vidage
automatiquement. La taille du disque est contrôlée pour s'assurer que la taille
minimale peut être satisfaite. Si la taille minimale peut être satisfaite, la taille de
pool optimale est attribuée en fonction des contraintes, notamment de la taille des
disques, des tranches conservées, etc.

Par exemple, lorsque vous spécifiez c0t0d0s0, une tranche de taille optimale est
créée si vous spécifiez les mots clés all ou auto. Vous pouvez également indiquer
une taille spécifique de tranche, de volume de swap ou de volume de vidage.

Le mot clé auto fonctionne de manière similaire au mot clé all lorsqu'il est utilisé
avec un pool racine ZFS car les pools ne disposent pas du concept d'espace
inutilisé.
Ce mot clé identifie les caractéristiques de l'environnement d'initialisation.

Le mot clé bootenv existe déjà mais de nouvelles options sont définies. Utilisez la
syntaxe de mot clé bootenv suivante pour créer un environnement racine ZFS
d'initialisation :

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

131

Installation d'un système de fichiers racine ZFS (installation JumpStart)

bootenv installbe bename nom-d'environnement-d'initialisation [ dataset
point-de-montage]

installbe

bename nom-d'environnement-d'initialisation

dataset point-de-montage

Crée un nouvel environnement
d'initialisation identifié par
l'option bename et l'entrée
nom-d'environnement-d'initialisation,
puis l'installe.
Identifie le
nom-d'environnement-d'initialisation
à installer.

Si l'option bename n'est pas
utilisée avec le mot clé pool, un
environnement d'initialisation
par défaut est créé.
Utilisez le mot clé facultatif
dataset pour identifier un jeu
de données /var distinct du jeu
de données racine. La valeur
point-de-montage est
actuellement limitée à /var. Par
exemple, une ligne de syntaxe
bootenv d'un jeu de données
/var distinct est du type :

bootenv installbe bename zfsroot dataset /var

pool

Définit le pool racine à créer. La syntaxe de mot clé suivante doit être respectée :

poolname poolsize swapsize dumpsize vdevlist
poolname

poolsize

Identifie le nom du pool à créer. Le pool est créé d'après la taille de
pool spécifiée et avec les périphériques physiques spécifiés(vdevs).
L'option poolname ne doit pas identifier un nom de pool existant car
celui-ci serait écrasé.
Spécifie la taille du pool à créer. La valeur peut être auto ou
existing. La valeur auto signifie que la taille de pool optimale est
attribuée en fonction des contraintes, notamment de la taille des
disques, des tranches conservées, etc. La valeur existing signifie
que les limites des tranches existantes de ce nom sont conservées et
écrasées. L'unité de taille présumée est Mo à moins d'indiquer g
(Go).

132

Guide d'administration Solaris ZFS • Octobre 2009

swapsize

dumpsize

vdevlist

Installation d'un système de fichiers racine ZFS (installation JumpStart)

Spécifie la taille du volume de swap à créer. La valeur peut être auto
qui signifie que la taille de swap par défaut est utilisée ou, une taille
que vous spécifiez. L'unité de taille présumée est Mo à moins
d'indiquer g (Go).
Spécifie la taille du volume de vidage à créer. La valeur peut être
auto qui signifie que la taille de swap par défaut est utilisée ou, une
taille que vous spécifiez. L'unité de taille présumée est Mo à moins
d'indiquer g (Go).
Spécifie un ou plusieurs périphériques à utiliser pour créer le pool.
Le format de vdevlist est identique à celui de la commande zpool
create. À l'heure actuelle, seules les configurations mises en miroir
sont prises en charge lorsque plusieurs périphériques sont spécifiés.
Les périphériques figurant dans vdevlist doivent être des tranches du
pool racine. La chaîne any signifie que le logiciel d'installation
sélectionne le périphérique approprié.

Vous pouvez mettre en miroir autant de disques que vous le
souhaitez mais la taille du pool créé est déterminée par le plus petit
des disques spécifiés. Pour plus d'informations sur la création de
pools de stockage mis en miroir, reportez-vous à la section
“Configuration de pool de stockage mis en miroir” à la page 63.

Problèmes liés à l'installation JumpStart d'un ZFS
Prenez en compte les problèmes suivants avant de lancer une installation JumpStart d'un
système de fichiers racine ZFS d'initialisation.
■ Un pool de stockage ZFS existant ne peut pas être utilisé lors d'une installation JumpStart
pour créer un système de fichiers racine ZFS d'initialisation. Vous devez créer un pool de
stockage ZFS conformément au type de syntaxe suivant :

pool rpool 20G 4G 4G c0t0d0s0

La ligne complète du mot clé pool est nécessaire car vous ne pouvez pas utiliser un pool
existant. Exemple :

install_type initial_install

cluster SUNWCall

pool rpool 20G 4g 4g any

bootenv installbe bename newBE

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

133

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

■ Vous devez créer le pool avec des tranches de disque plutôt qu'avec des disques entiers,

comme décrit à la section “Configuration requise pour l'installation de Solaris et de Solaris
Live Upgrade pour la prise en charge de ZFS” à la page 117. Par exemple, la syntaxe en gras
n'est pas acceptable :

install_type initial_install

cluster SUNWCall

pool rpool all auto auto mirror c0t0d0 c0t1d0

bootenv installbe bename newBE

La syntaxe en gras suivante est acceptable :

install_type initial_install

cluster SUNWCall

pool rpool all auto auto mirror c0t0d0s0 c0t1d0s0

bootenv installbe bename newBE

Migration d'un système de fichiers racine UFS vers un système
de fichiers racine ZFS (Solaris Live Upgrade)

Les fonctions précédentes de Solaris Live Upgrade sont disponibles et, si liées aux composants
UFS, fonctionnent comme dans les versions précédentes de Solaris.

Voici l'ensemble des fonctions disponibles :
■ Lorsque vous migrez un système de fichiers racine UFS vers un système de fichiers racine

ZFS, vous devez désigner un pool de stockage ZFS existant à l'aide de l'option -p.
Si les composants du système de fichiers racine UFS sont répartis sur diverses tranches, ils
sont migrés vers le pool racine ZFS.

■

■ Vous pouvez faire migrer un système comportant des zones, mais les configurations prises

en charge sont limitées dans la version 10 10/08 de Solaris. La version 10 5/09 de Solaris
prend en charge davantage de configurations de zone. Pour plus d'informations, consultez
les sections suivantes :
■ “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones

(Solaris 10 10/08)” à la page 141

■ “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones

(Solaris 10 5/09 et Solaris 10 10/09)” à la page 147

Si vous migrez vers un système sans zones, reportez-vous à la section “Utilisation de Solaris
Live Upgrade pour migrer vers un système de fichiers racine ZFS (sans zones)” à la page 136.

134

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

■

Solaris Live Upgrade peut faire appel aux fonctions d'instantané et de clonage ZFS lorsque
vous créez un environnement d'initialisation ZFS dans le même pool. La création d'un
environnement d'initialisation est de ce fait plus rapide que dans les versions précédentes de
Solaris.

Pour plus d'informations sur l'installation de Solaris et les fonctions de Solaris Live Upgrade,
reportez-vous au Guide d’installation de Solaris 10 : Solaris Live Upgrade et planification de la
mise à niveau.

Le processus de base de migration d'un système de fichiers racine UFS vers un système de
fichiers racine ZFS (Solaris Live Upgrade) est le suivant :

■

Installez Solaris 10 10/08, 10 5/09 ou 10 10/09, ou utilisez le programme de mise à niveau
standard pour mettre à niveau une version précédente de Solaris 10 sur tout système SPARC
ou x 86 pris en charge.

■ Lorsque vous exécutez Solaris 10 10/08, 10 5/09 ou 10 10/09, créez si nécessaire un pool de

stockage ZFS pour votre système de fichiers racine ZFS.

■ Utilisez Solaris Live Upgrade pour migrer votre système de fichiers racine UFS vers un

système de fichiers racine ZFS.

■ Activez votre environnement d'initialisation ZFS à l'aide de la commande luactivate.

Pour plus d'informations sur les exigences de ZFS et de Solaris Live Upgrade, reportez-vous à la
section “Configuration requise pour l'installation de Solaris et de Solaris Live Upgrade pour la
prise en charge de ZFS” à la page 117.

Problèmes de migration d'un ZFS avec Solaris Live
Upgrade
Consultez la liste de problèmes suivante avant d'utiliser Solaris Live Upgrade pour migrer votre
système de fichiers racine UFS vers un système de fichiers racine ZFS :
■ L'option de mise à niveau standard de l'IG d'installation de Solaris n'est pas disponible pour

la migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS.
Pour migrer un système de fichiers UFS, vous devez utiliser Solaris Live Upgrade.

■ Vous devez créer le pool de stockage ZFS à utiliser pour l'initialisation avant de lancer

Solaris Live Upgrade. En outre, en raison des limitations actuelles de l'initialisation, le pool
racine ZFS doit être créé avec des tranches plutôt qu'avec des disques entiers. Exemple :

# zpool create rpool mirror c1t0d0s0 c1t1d0s0

Avant de créer le nouveau pool, assurez-vous que les disques à utiliser dans le pool portent
une étiquette SMI (VTOC) au lieu d'une étiquette EFI. Si le disque a été réétiqueté avec une
étiquette SMI, vérifiez que le processus d'étiquetage n'a pas modifié le schéma du

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

135

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

partitionnement. Dans la plupart des cas, la majeure partie de la capacité du disque doit se
trouver dans les tranches destinées au pool racine.

■ Vous ne pouvez pas utiliser Solaris Live Upgrade pour créer un environnement

d'initialisation UFS à partir d'un environnement d'initialisation ZFS. Si vous migrez votre
environnement d'initialisation UFS vers un environnement d'initialisation ZFS tout en
conservant votre environnement d'initialisation UFS, vous pouvez initialiser l'un ou l'autre
des environnements (UFS ou ZFS).

■ Ne renommez pas vos environnements d'initialisation ZFS à l'aide de la commande zfs
rename car la fonction Solaris Live Upgrade ne tient pas compte du changement de nom.
Toute commande utilisée ultérieurement, notamment ludelete, échoue. Ne renommez en
fait pas vos pools ZFS ni vos systèmes de fichiers ZFS si vous disposez d'environnements
d'initialisation que vous souhaitez continuer à utiliser.
Solaris Live Upgrade crée les jeux de données pour l'environnement d'initialisation, ainsi
que les volumes ZFS de la zone de swap et du périphérique de vidage mais ne prend pas en
compte les modifications de propriétés de jeux de données existants. Pour cette raison, pour
activer une propriété de jeu de données dans le nouvel environnement d'initialisation, vous
devez définir cette propriété avant l'exécution de la commande lucreate. Exemple :

■

# zfs set compression=on rpool/ROOT

■ Lorsque vous créez un environnement d'initialisation alternatif cloné sur l'environnement

d'initialisation principal, vous ne pouvez pas utiliser les options -f, -x, -y, - Y et -z pour
inclure ou exclure des fichiers de l'environnement d'initialisation principal. Vous pouvez
toutefois vous servir des options d'inclusion et d'exclusion dans les cas suivants :

UFS -> UFS

UFS -> ZFS

ZFS -> ZFS (different pool)

■ Bien que vous puissiez utiliser Solaris Live Upgrade pour mettre à niveau votre système de

fichiers racine UFS vers un système de fichiers racine ZFS, vous ne pouvez pas vous servir de
Solaris Live Upgrade pour mettre à niveau des systèmes de fichiers partagés ni des systèmes
de fichiers qui ne sont pas des systèmes de fichiers racine.

■ Vous ne pouvez pas utiliser une commande lu pour créer ou migrer un système de fichiers

racine ZFS.

Utilisation de Solaris Live Upgrade pour migrer vers
un système de fichiers racine ZFS (sans zones)
Les exemples suivants illustrent la procédure de migration d'un système de fichiers racine UFS
vers un système de fichiers racine ZFS.

136

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

Si vous migrez un système comportant des zones ou effectuez sa mise à jour, reportez-vous aux
sections suivantes :
■ “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones

(Solaris 10 10/08)” à la page 141

■ “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones

(Solaris 10 5/09 et Solaris 10 10/09)” à la page 147

EXEMPLE 5–3 Utilisation de Solaris Live Upgrade pour migrer un système de fichiers racine UFS vers un
système de fichiers racine ZFS
L'exemple suivant illustre la procédure de création d'un environnement d'initialisation d'un
système de fichiers racine ZFS à partir d'un système de fichiers racine UFS. L'environnement
d'initialisation actuel, ufs509BE, qui contient un système de fichiers racine UFS, est identifié par
l'option -c. Si vous n'incluez pas l'option -c, le nom actuel de l'environnement d'initialisation
sera par défaut le nom du périphérique. Le nouvel environnement d'initialisation, zfs1009BE,
est identifié par l'option -n. L'utilisation de la commande lucreate requiert l'existence
préalable d'un pool de stockage ZFS .

Afin de pouvoir être mis à niveau et initialisé, le pool de stockage ZFS doit être créé avec des
tranches plutôt qu'avec des disques entiers. Avant de créer le nouveau pool, assurez-vous que les
disques à utiliser dans le pool portent une étiquette SMI (VTOC) au lieu d'une étiquette EFI. Si
le disque a été réétiqueté avec une étiquette SMI, vérifiez que le processus d'étiquetage n'a pas
modifié le schéma du partitionnement. Dans la plupart des cas, la majeure partie de la capacité
du disque doit se trouver dans les tranches destinées au pool racine.

# zpool create mpool mirror c1t2d0s0 c2t1d0s0

# lucreate -c ufs1009BE -n zfs1009BE -p mpool

Analyzing system configuration.

No name for current boot environment.

Current boot environment is named <ufs1009BE>.

Creating initial configuration for primary boot environment <ufs1009BE>.

The device </dev/dsk/c1t0d0s0> is not a root device for any boot environment; cannot get BE ID.

PBE configuration successful: PBE name <ufs1009BE> PBE Boot Device </dev/dsk/c1t0d0s0>.

Comparing source boot environment <ufs1009BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

The device </dev/dsk/c1t2d0s0> is not a root device for any boot environment; cannot get BE ID.

Creating configuration for boot environment <zfs1009BE>.

Source boot environment is <ufs1009BE>.

Creating boot environment <zfs1009BE>.

Creating file systems on boot environment <zfs1009BE>.

Creating <zfs> file system for </> in zone <global> on <mpool/ROOT/zfs1009BE>.

Populating file systems on boot environment <zfs1009BE>.

Checking selection integrity.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

137

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

EXEMPLE 5–3 Utilisation de Solaris Live Upgrade pour migrer un système de fichiers racine UFS vers un
système de fichiers racine ZFS

(Suite)

Integrity check OK.

Populating contents of mount point </>.

Copying.

Creating shared file system mount points.

Creating compare databases for boot environment <zfs1009BE>.

Creating compare database for file system </mpool/ROOT>.

Creating compare database for file system </>.

Updating compare databases on boot environment <zfs1009BE>.

Making boot environment <zfs1009BE> bootable.

Creating boot_archive for /.alt.tmp.b-qD.mnt

updating /.alt.tmp.b-qD.mnt/platform/sun4u/boot_archive

Population of boot environment <zfs1009BE> successful.

Creation of boot environment <zfs1009BE> successful.

Une fois l'exécution de la commande lucreate terminée, utilisez la commande lustatus pour
afficher l'état de l'environnement d'initialisation. Exemple :

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

ufs1009BE

zfs1009BE

yes

yes

yes

yes

no

no

no

yes

-

-

Examinez ensuite la liste des composants ZFS. Exemple :

# zfs list

NAME

mpool

USED AVAIL REFER MOUNTPOINT

7.17G 59.8G 95.5K /mpool

mpool/ROOT

4.66G 59.8G

21K /mpool/ROOT

mpool/ROOT/zfs1009BE 4.66G 59.8G 4.66G /

mpool/dump

2G 61.8G

16K -

mpool/swap

517M 60.3G

16K -

Utilisez ensuite la commande luactivate pour activer le nouvel environnement d'initialisation
ZFS. Par exemple :

# luactivate zfs1009BE

A Live Upgrade Sync operation will be performed on startup of boot environment <zfs1009BE>.

**********************************************************************

The target boot environment has been activated. It will be used when you

reboot. NOTE: You MUST NOT USE the reboot, halt, or uadmin commands. You

138

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

EXEMPLE 5–3 Utilisation de Solaris Live Upgrade pour migrer un système de fichiers racine UFS vers un
système de fichiers racine ZFS

(Suite)

MUST USE either the init or the shutdown command when you reboot. If you

do not use either init or shutdown, the system will not boot using the

target BE.

**********************************************************************

.

.

.

Modifying boot archive service

Activation of boot environment <zfs1009BE> successful.

Réinitialisez ensuite le système afin d'utiliser l'environnement d'initialisation ZFS.

# init 6

Confirmez que l'environnement d'initialisation ZFS est actif.

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

ufs1009BE

zfs1009BE

yes

yes

no

no

yes

yes

yes

no

-

-

Si vous repassez à l'environnement d'initialisation UFS, vous devez réimporter tous les pools de
stockage ZFS créés au cours de l'initialisation de l'environnement d'initialisation ZFS car ils ne
sont pas disponibles automatiquement dans l'environnement d'initialisation UFS.

Lorsque l'environnement d'initialisation UFS est obsolète, vous pouvez le supprimer à l'aide de
la commande ludelete.

EXEMPLE 5–4 Utilisation de Solaris Live Upgrade pour créer un environnement d'initialisation ZFS à partir
d'un environnement d'initialisation ZFS

La création d'un environnement d'initialisation ZFS à partir d'un environnement
d'initialisation ZFS du même pool est très rapide car l'opération fait appel aux fonctions
d'instantané et de clonage ZFS. Si l'environnement d'initialisation réside sur le même pool ZFS,
mpool, par exemple, l'option -p est omise.

Si vous disposez de plusieurs environnements d'initialisation ZFS sur un système SPARC, vous
pouvez utiliser la commande boot -L pour identifier les environnements d'initialisation
disponibles et sélectionner un environnement d'initialisation à partir duquel effectuer

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

139

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

EXEMPLE 5–4 Utilisation de Solaris Live Upgrade pour créer un environnement d'initialisation ZFS à partir
d'un environnement d'initialisation ZFS

(Suite)

l'initialisation à l'aide de la commande boot -Z. Sur un système x86, vous pouvez sélectionner
un environnement d'initialisation à partir du menu GRUB. Pour plus d'informations,
reportez-vous à l'Exemple 5–9.

# lucreate -n zfs10092BE

Analyzing system configuration.

Comparing source boot environment <zfs1009BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

Creating configuration for boot environment <zfs10092BE>.

Source boot environment is <zfs1009BE>.

Creating boot environment <zfs10092BE>.

Cloning file systems from boot environment <zfs1009BE> to create boot environment <zfs10092BE>.

Creating snapshot for <mpool/ROOT/zfs1009BE> on <mpool/ROOT/zfs1009BE@zfs10092BE>.

Creating clone for <mpool/ROOT/zfs1009BE@zfs10092BE> on <mpool/ROOT/zfs10092BE>.

Setting canmount=noauto for </> in zone <global> on <mpool/ROOT/zfs10092BE>.

Population of boot environment <zfs10092BE> successful.

Creation of boot environment <zfs10092BE> successful.

EXEMPLE 5–5 Mise à niveau de votre environnement d'initialisation ZFS (luupgrade)

Vous pouvez mettre à niveau votre environnement d'initialisation ZFS à l'aide de packages ou
de patchs supplémentaires.

Le processus de base est le suivant :
■ Créez un environnement d'initialisation alternatif à l'aide de la commande lucreate.
■ Activez et initialisez le système à partir de l'environnement d'initialisation alternatif.
■ Mettrez votre environnement d'initialisation ZFS principal à niveau à l'aide de la commande

luupgrade pour ajouter des packages ou des patchs.

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs1009BE

zfs10092BE

yes

yes

no

no

yes

yes

yes

no

-

-

# luupgrade -p -n zfs1009BE -s /net/system/export/s10u8/Solaris_10/Product SUNWchxge

Validating the contents of the media </net/system/export/s10u8/Solaris_10/Product>.

Mounting the BE <zfs1009BE>.

140

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

EXEMPLE 5–5 Mise à niveau de votre environnement d'initialisation ZFS (luupgrade)

(Suite)

Adding packages to the BE <zfs1009BE>.

Processing package instance <SUNWchxge> from </net/install/export/s10u8/Solaris_10/Product>

Chelsio N110 10GE NIC Driver(sparc) 11.10.0,REV=2006.02.15.20.41

Copyright 2009 Sun Microsystems, Inc. All rights reserved.

Use is subject to license terms.

This appears to be an attempt to install the same architecture and

version of a package which is already installed. This installation

will attempt to overwrite this package.

Using </a> as the package base directory.

## Processing package information.

## Processing system information.

4 package pathnames are already properly installed.

## Verifying package dependencies.

## Verifying disk space requirements.

## Checking for conflicts with packages already installed.

## Checking for setuid/setgid programs.

This package contains scripts which will be executed with super-user

permission during the process of installing this package.

Do you want to continue with the installation of <SUNWchxge> [y,n,?] y

Installing Chelsio N110 10GE NIC Driver as <SUNWchxge>

## Installing part 1 of 1.

## Executing postinstall script.

Installation of <SUNWchxge> was successful.

Unmounting the BE <zfs1009BE>.

The package add to the BE <zfs1009BE> completed.

Utilisation de Solaris Live Upgrade pour migrer ou
mettre à jour un système avec zones (Solaris 10 10/08)
Vous pouvez utiliser Solaris Live Upgrade pour migrer un système avec zones. Les
configurations prises en charge sont toutefois limitées dans Solaris 10 10/08. Si vous installez
Solaris 10 5/09 ou effectuez une mise à niveau vers cette version, un plus grand nombre de
configurations de zone sont prises en charge. Pour plus d'informations, reportez-vous à la

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

141

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

section “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones
(Solaris 10 5/09 et Solaris 10 10/09)” à la page 147.

Cette section décrit comment configurer et installer un système avec zones de manière à ce qu'il
soit mis à jour et corrigé avec Solaris Live Upgrade. Si vous migrez vers un système de fichiers
racine ZFS sans zones, reportez-vous à “Utilisation de Solaris Live Upgrade pour migrer vers un
système de fichiers racine ZFS (sans zones)” à la page 136.

Si vous migrez un système avec zones ou configurez un système avec zones dans Solaris 10
10/08, consultez les procédures suivantes :
■ “Migration d'un système de fichiers racine UFS avec racines de zone sur UFS vers un système

de fichiers racine ZFS (Solaris 10 10/08)” à la page 142

■ “Configuration d'un système de fichiers racine ZFS avec racines de zone sur ZFS (Solaris 10

10/08)” à la page 144

■ “Mise à niveau ou application de patch à un système de fichiers racine ZFS avec racines de

zone sur ZFS (Solaris 10 10/08)” à la page 145

■ “Résolution des problèmes de point de montage ZFS responsables de l'échec de

l'initialisation” à la page 166

Suivez les procédures recommandées pour configurer des zones sur un système avec système de
fichiers racine ZFS pour vérifier que vous pouvez utiliser Live Upgrade sur ce système.

▼ Migration d'un système de fichiers racine UFS avec racines de zone sur

UFS vers un système de fichiers racine ZFS (Solaris 10 10/08)
Suivez les étapes ci-dessous pour migrer un système de fichiers racine UFS avec zones installées
vers un système de fichiers racine ZFS et une configuration de racine de zone ZFS pouvant être
mis à niveau ou corrigés.

Dans les étapes suivantes, le nom du pool est rpool et le nom de l'environnement d'initialisation
actuellement actif est S10BE*.

1

2

Mettez le système à niveau à la version Solaris 10 10/08 si la version Solaris 10 exécutée est
antérieure.
Pour plus d'informations sur la mise à niveau d'un système exécutant Solaris 10, reportez-vous
au Guide d’installation de Solaris 10 : Solaris Live Upgrade et planification de la mise à niveau.

Créez le pool racine.
Pour plus d'informations sur les exigences du pool racine, reportez-vous à la “Configuration
requise pour l'installation de Solaris et de Solaris Live Upgrade pour la prise en charge de ZFS”
à la page 117.

3

Confirmez que les zones de l'environnement UFS sont initialisées.

142

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

4

Créez le nouvel environnement d'initialisation.

# lucreate -n S10BE2 -p rpool

Cette commande crée des jeux de données dans le pool racine pour le nouvel environnement
d'initialisation et copie l'environnement d'initialisation actuel (zones incluses) vers ces jeux de
données.

5

Activez le nouvel environnement d'initialisation.

# luactivate s10BE2

Le système exécute maintenant un système de fichiers racine ZFS mais les racines de zone sur
UFS se trouvent toujours sur le système de fichiers racine UFS. Les étapes suivantes sont
nécessaires pour finaliser la migration des zones UFS vers une configuration ZFS prise en
charge.

6

7

8

Redémarrez le système.

# init 6

Migrez les zones vers un environnement d'initialisation ZFS.

a.

Initialisez les zones.

b. Créez un autre environnement d'initialisation dans le pool.

# lucreate S10BE3

c. Activez le nouvel environnement d'initialisation.

# luactivate S10BE3

d. Redémarrez le système.

# init 6

Cette étape vérifie que l'environnement d'initialisation ZFS et les zones ont été initialisés.

Résolvez tout problème potentiel de point de montage de cette version Solaris.
Il est possible qu'un bogue dans la fonctionnalité Live Upgrade provoque l'échec de
l'initialisation de l'environnement d'initialisation non actif. Ce problème est lié à la présence
d'un point de montage non valide dans un jeu de données ZFS ou dans un jeu de données ZFS
de zone de l'environnement d'initialisation.

a. Contrôlez la sortie zfs list.

Vérifiez qu'elle ne contient aucun point de montage temporaire erroné. Exemple :

# zfs list -r -o name,mountpoint rpool/ROOT/s10u6

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

143

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

NAME

MOUNTPOINT

rpool/ROOT/s10u6

/.alt.tmp.b-VP.mnt/

rpool/ROOT/s10u6/zones

/.alt.tmp.b-VP.mnt//zones

rpool/ROOT/s10u6/zones/zonerootA

/.alt.tmp.b-VP.mnt/zones/zonerootA

Le point de montage pour l'environnement d'initialisation ZFS racine (rpool/ROOT/s10u6)
doit être /.

b. Réinitialisez les points de montage pour l'environnement d'initialisation ZFS et ses jeux de

données.
Exemple :

# zfs inherit -r mountpoint rpool/ROOT/s10u6

# zfs set mountpoint=/ rpool/ROOT/s10u6

c. Redémarrez le système.

Lorsque vous avez la possibilité d'initialiser un environnement d'initialisation spécifique,
soit par le biais du menu GRUB, soit à l'invite OpenBoot Prom, sélectionnez
l'environnement d'initialisation dont les points de montage viennent d'être corrigés.

▼ Configuration d'un système de fichiers racine ZFS avec racines de zone

sur ZFS (Solaris 10 10/08)
Suivez les étapes ci-dessous pour installer un système de fichiers racine ZFS et une
configuration de racine de zone ZFS pouvant être mis à niveau ou corrigés. Dans cette
configuration, les racines de zone ZFS sont créées sous forme de jeux de données ZFS.

Dans les étapes suivantes, le nom du pool est rpool et le nom de l'environnement d'initialisation
actuellement actif est S10be.

1

2

3

Installez le système avec une racine ZFS en utilisant soit la méthode interactive d'installation
initiale, soit la méthode d'installation Solaris JumpStart.
Pour plus d'informations sur l'installation d'un système de fichiers racine ZFS en utilisant la
méthode d'installation initiale ou la méthode Solaris JumpStart, reportez-vous à “Installation
d'un système de fichiers racine ZFS (installation initiale)” à la page 120 ou à “Installation d'un
système de fichiers racine ZFS (installation JumpStart)” à la page 130.

Initialisez le système à partir du pool racine nouvellement créé.

Créez un jeu de données pour le regroupement des racines de zone.
Exemple :

# zfs create -o canmount=noauto rpool/ROOT/S10be/zones

Le nom du jeu de données de zones peut être tout nom de jeu de données légal. Dans les étapes
suivantes, le nom du jeu de données est zones.

144

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

La définition de la valeur noauto pour la propriété canmount permet d'éviter que le jeu de
données ne soit monté d'une manière autre que par l'action explicite de Solaris Live Upgrade et
le code de démarrage du système.

4

Montez le jeu de données de conteneur de zones nouvellement créé.

# zfs mount rpool/ROOT/S10be/zones

Le jeu de données est monté sous /zones.

5

Créez et montez un jeu de données pour chaque racine de zone.

# zfs create -o canmount=noauto rpool/ROOT/S10be/zones/zonerootA

# zfs mount rpool/ROOT/S10be/zones/zonerootA

6

7

8

9

Définissez les droits appropriés dans le répertoire de racine de zone.

# chmod 700 /zones/zonerootA

Configurez la zone en indiquant le chemin de zone comme suit :

# zonecfg -z zoneA

zoneA: No such zone configured

Use ’create’ to begin configuring a new zone.

zonecfg:zoneA> create

zonecfg:zoneA> set zonepath=/zones/zonerootA

Vous pouvez activer l'initialisation automatique des zones à l'initialisation du système en
utilisant la syntaxe suivante :

zonecfg:zoneA> set autoboot=true

Installez la zone.

# zoneadm -z zoneA install

Initialisez la zone.

# zoneadm -z zoneA boot

▼ Mise à niveau ou application de patch à un système de fichiers racine

ZFS avec racines de zone sur ZFS (Solaris 10 10/08)
Procédez aux étapes suivantes pour mettre à niveau ou corriger le système de fichiers racine ZFS
avec racines de zone sur ZFS. Ces mises à jour peuvent être une mise à niveau du système ou
l'application de correctifs.

Dans les étapes suivantes, newBE est le nom de l'environnement d'initialisation mis à niveau ou
corrigé.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

145

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

1

Créez l'environnement d'initialisation à mettre à jour ou à corriger.

2

3

4

5

# lucreate -n newBE

L'environnement d'initialisation existant, y compris toutes les zones, est cloné. De nouveaux
jeux de données sont créés pour chaque jeu de données de l'environnement d'initialisation
d'origine. Ils sont créés dans le même pool que le pool racine actuel.

Sélectionnez l'une des options suivantes pour mettre à niveau le système ou appliquer les
correctifs au nouvel environnement d'initialisation.
■ Mettez à niveau le système.

# luupgrade -u -n newBE -s /net/install/export/s10u7/latest

L'option -s représente l'emplacement d'un mode d'installation Solaris.

■ Appliquez les correctifs au nouvel environnement d'initialisation.

# luupgrade -t -n newBE -t -s /patchdir 139147-02 157347-14

Activez le nouvel environnement d'initialisation une fois que ses mises à jour ont été effectuées.

# luactivate newBE

Initialisez à partir de l'environnement d'initialisation nouvellement activé.

# init 6

Résolvez tout problème potentiel de point de montage dans la version 10/08 de Solaris.
Il est possible qu'un bogue dans la fonctionnalité Live Upgrade provoque l'échec de
l'initialisation de l'environnement d'initialisation non actif. Ce problème est lié à la présence
d'un point de montage non valide dans un jeu de données ZFS ou dans un jeu de données ZFS
de zone de l'environnement d'initialisation.

a. Contrôlez la sortie zfs list.

Vérifiez qu'elle ne contient aucun point de montage temporaire erroné. Exemple :

# zfs list -r -o name,mountpoint rpool/ROOT/newBE

NAME

MOUNTPOINT

rpool/ROOT/newBE

/.alt.tmp.b-VP.mnt/

rpool/ROOT/newBE/zones

/.alt.tmp.b-VP.mnt//zones

rpool/ROOT/newBE/zones/zonerootA

/.alt.tmp.b-VP.mnt/zones/zonerootA

Le point de montage pour l'environnement d'initialisation racine ZFS (rpool/ROOT/newBE)
doit être /.

146

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

b. Réinitialisez les points de montage pour l'environnement d'initialisation ZFS et ses jeux de

données.
Exemple :

# zfs inherit -r mountpoint rpool/ROOT/newBE

# zfs set mountpoint=/ rpool/ROOT/newBE

c. Redémarrez le système.

Lorsque vous avez la possibilité d'initialiser un environnement d'initialisation spécifique,
soit par le biais du menu GRUB, soit à l'invite OpenBoot Prom, sélectionnez
l'environnement d'initialisation dont les points de montage viennent d'être corrigés.

Utilisation de Solaris Live Upgrade pour migrer ou
mettre à jour un système avec zones (Solaris 10 5/09 et
Solaris 10 10/09)
À partir de la version 10 10/08 de Solaris, vous pouvez utiliser la fonctionnalité Live Upgrade
pour migrer ou mettre à niveau un système comportant des zones. Dans la version Solaris 10
5/09, des configurations de zone complète ou fragmentée sont prises en charge par Live
Upgrade.

Cette section décrit comment configurer et installer un système avec zones de manière à ce qu'il
soit mis à niveau et corrigé avec Solaris Live Upgrade dans la version Solaris 10 5/09. Si vous
migrez vers un système de fichiers racine ZFS sans zones, reportez-vous à la section “Utilisation
de Solaris Live Upgrade pour migrer vers un système de fichiers racine ZFS (sans zones)”
à la page 136.

Tenez compte des points suivants lorsque vous utilisez Live Upgrade avec ZFS et les zones dans
la version Solaris 10 5/09.

■

Si vous souhaitez utiliser Live Upgrade avec des configurations de zone prises en charge
dans Solaris 10 5/09, vous devez d'abord mettre à niveau votre système vers la version 10
5/09 ou 10 10/09 de Solaris à l'aide du programme de mise à niveau standard.

■ Puis, avec Live Upgrade, vous pouvez soit migrer votre système de fichiers racine UFS avec
racines de zone vers un système de fichiers racine ZFS, soit mettre à niveau votre système de
fichiers racine ZFS et vos racines de zone ou leur appliquer un patch.

■ Vous ne pouvez pas migrer directement des configurations de zone non prises en charge à

partir d'une version antérieure de Solaris 10 vers la version Solaris 10 5/09 ou Solaris 10
10/09.

À partir de Solaris 10 5/09, si vous migrez un système comportant des zones ou si vous
configurez un système comportant des zones, vérifiez les informations suivantes :

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

147

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

■ “ZFS pris en charge avec informations de configuration de la racine de zone (Solaris 10 5/09

ou Solaris 10 10/09)” à la page 148

■ “Création d'un environnement d'initialisation ZFS avec un système de fichiers racine ZFS et

une racine de zone (Solaris 10 5/09 ou Solaris 10 10/09)” à la page 149

■ “Mise à niveau ou correction d'un système de fichiers racine ZFS avec racines de zone (Solaris

10 5/09 ou SolarisSolaris 10 10/09)” à la page 151

■ “Migration d'un système de fichiers racine UFS avec racine de zone vers un système de

fichiers racine ZFS (Solaris 10 5/09 ou Solaris 10 10/09)” à la page 155

ZFS pris en charge avec informations de configuration de la racine de
zone (Solaris 10 5/09 ou Solaris 10 10/09)
Vérifiez les configurations de zone prises en charge avant d'utiliser la fonction Live Upgrade
pour migrer ou mettre à jour un système comportant des zones.
■ Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS. Les

configurations de racine de zone suivantes sont prises en charge :
■ Dans un répertoire du système de fichiers racine UFS
■ Dans le sous-répertoire d'un point de montage, dans le système de fichiers racine UFS
Système de fichiers racine UFS associé à une racine de zone (comme décrit ci-dessus) et
un pool non racine ZFS associé à une racine de zone

■

La configuration UFS/zone suivante n'est pas prise en charge :

■

Système de fichiers racine UFS ayant une racine de zone comme point de montage

■ Migration ou mise à niveau d'un système de fichiers racine ZFS. Les configurations de

racine de zone suivantes sont prises en charge :
■ Dans un jeu de données dans le pool racine ZFS. Dans certains cas, si un jeu de données
n'est pas fourni pour la racine de zone avant l'opération Live Upgrade, un jeu de données
pour la racine de zone (zoneds) est créé par Live Upgrade.
■ Dans un sous-répertoire du système de fichiers racine ZFS
■ Dans un jeu de données ne faisant pas partie du système de fichiers racine ZFS
■ Dans le sous-répertoire d'un jeu de données ne faisant pas partie du système de fichiers

racine ZFS

■ Dans un jeu de données d'un pool non-racine. Par exemple, zonepool/zones est un jeu

de données qui contient les racines de zone et rpool contient l'environnement
d'initialisation ZFS.

zonepool

zonepool/zones

zonepool/zones/myzone

rpool

148

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

rpool/ROOT

rpool/ROOT/myBE

L'opération Live Upgrade prend un instantané et clone les zones contenues dans
zonepool et dans l'environnement d'initialisation rpool, si vous utilisez la syntaxe
suivante :

# lucreate -n newBE

L'environnement d'initialisation newBE dans rpool/ROOT/newBE est créé. Une fois activé,
il permet d'accéder aux composants zonepool.

Dans l'exemple ci-dessus, si /zonepool/zones était un sous-répertoire et non un autre
jeu de données, Live Upgrade le migrerait comme composants du pool racine, rpool.

■ Migration de zones ou informations de mise à niveau avec zones pour UFS et ZFS :
vérifiez les éléments ci-dessous. Ils sont susceptibles d'affecter la migration ou la mise à
niveau d'un environnement ZFS ou UFS.

■

Si vous avez configuré les zones comme décrit à la section “Utilisation de Solaris Live
Upgrade pour migrer ou mettre à jour un système avec zones (Solaris 10 10/08)”
à la page 141 dans la version Solaris 10 10/08 et si vous avez effectué une mise à niveau
vers la version Solaris 10 5/09 ou Solaris 10 10/09, vous devriez pouvoir effectuer une
migration vers un système de fichiers racine ZFS ou bien une mise à niveau vers Solaris
10 5/09 ou Solaris 10 10/09 à l'aide de Live Upgrade.

■ Ne créez pas de racines de zone dans des répertoires imbriqués, par exemple

zones/zone1 et zones/zone1/zone2, sinon le montage risque d'échouer lors de
l'initialisation.

▼ Création d'un environnement d'initialisation ZFS avec un système de
fichiers racine ZFS et une racine de zone (Solaris 10 5/09 ou Solaris 10
10/09)
Effectuez cette procédure après avoir procédé à une nouvelle installation de la version Solaris 10
5/09 ou Solaris 10 10/09 pour créer un système de fichiers racine ZFS ou après avoir utilisé la
fonction luupgrade pour effectuer la mise à niveau d'un système de fichiers racine ZFS vers la
version Solaris 10 5/09 ou Solaris 10 10/09. Un environnement d'initialisation ZFS créé à l'aide
de cette procédure peut être mis à niveau ou corrigé à l'aide d'un patch.

Dans la procédure ci-après, l'exemple de système Solaris 10 10/09 comporte un système de
fichiers racine ZFS et un jeu de données de racine de zone dans /rpool/zones. Un
environnement d'initialisation ZFS nommé zfs10092BE est créé et peut être mis à niveau ou
corrigé à l'aide d'un patch.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

149

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

1

Vérifiez les systèmes de fichiers ZFS existants. Exemple :

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

7.26G 59.7G

98K /rpool

rpool/ROOT

4.64G 59.7G

21K legacy

rpool/ROOT/zfs1009BE 4.64G 59.7G 4.64G /

rpool/dump

1.00G 59.7G 1.00G -

rpool/export

44K 59.7G

23K /export

rpool/export/home

21K 59.7G

21K /export/home

rpool/swap

1G 60.7G

16K -

rpool/zones

633M 59.7G

633M /rpool/zones

2

Assurez-vous que les zones sont installées et initialisées. Exemple :

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

2 zfszone

running

/rpool/zones

native

shared

3

Créez l'environnement d'initialisation ZFS. Exemple :

# lucreate -n zfs10092BE

Analyzing system configuration.

Comparing source boot environment <zfs1009BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

Creating configuration for boot environment <zfs10092BE>.

Source boot environment is <zfs1009BE>.

Creating boot environment <zfs10092BE>.

Cloning file systems from boot environment <zfs1009BE> to create boot environment <zfs10092BE>.

Creating snapshot for <rpool/ROOT/zfs1009BE> on <rpool/ROOT/zfs1009BE@zfs10092BE>.

Creating clone for <rpool/ROOT/zfs1009BE@zfs10092BE> on <rpool/ROOT/zfs10092BE>.

Setting canmount=noauto for </> in zone <global> on <rpool/ROOT/zfs10092BE>.

Creating snapshot for <rpool/zones> on <rpool/zones@zfs10092BE>.

Creating clone for <rpool/zones@zfs10092BE> on <rpool/zones-zfs10092BE>.

Population of boot environment <zfs10092BE> successful.

Creation of boot environment <zfs10092BE> successful.

4

Activez l'environnement d'initialisation ZFS.

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs1009BE

zfs10092BE

yes

yes

yes

yes

no

no

no

yes

-

-

# luactivate zfs10092BE

150

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

A Live Upgrade Sync operation will be performed on startup of boot environment <zfs10092BE>.

.

.

.

# init 6

5

Confirmez que les systèmes de fichiers ZFS et les zones sont créés dans le nouvel environnement
d'initialisation. Exemple :

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

7.38G 59.6G

98K /rpool

rpool/ROOT

4.72G 59.6G

21K legacy

rpool/ROOT/zfs10092BE

4.72G 59.6G 4.64G /

rpool/ROOT/zfs10092BE@zfs10092BE 74.0M

- 4.64G -

rpool/ROOT/zfs1009BE

5.45M 59.6G 4.64G /.alt.zfs1009BE

rpool/dump

rpool/export

1.00G 59.6G 1.00G -

44K 59.6G

23K /export

rpool/export/home

21K 59.6G

21K /export/home

rpool/swap

rpool/zones

1G 60.6G

16K -

17.2M 59.6G

633M /rpool/zones

rpool/zones-zfs1009BE

653M 59.6G

633M /rpool/zones-zfs1009BE

rpool/zones-zfs1009BE@zfs10092BE 19.9M

-

633M -

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

- zfszone

installed /rpool/zones

native

shared

▼ Mise à niveau ou correction d'un système de fichiers racine ZFS avec

racines de zone (Solaris 10 5/09 ou SolarisSolaris 10 10/09)
Procédez aux étapes suivantes pour mettre à niveau ou corriger le système de fichiers racine ZFS
avec racines de zone sur Solaris 10 5/09 ou Solaris 10 10/09. Ces mises à jour peuvent être une
mise à niveau du système ou l'application de correctifs.

Dans les étapes suivantes, zfs10093BE est le nom de l'environnement d'initialisation mis à
niveau ou corrigé.

1

Vérifiez les systèmes de fichiers ZFS existants. Exemple :

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

7.38G 59.6G

100K /rpool

rpool/ROOT

4.72G 59.6G

21K legacy

rpool/ROOT/zfs10092BE

4.72G 59.6G 4.64G /

rpool/ROOT/zfs10092BE@zfs10092BE 75.0M

- 4.64G -

rpool/ROOT/zfs1009BE

5.46M 59.6G 4.64G /

rpool/dump

1.00G 59.6G 1.00G -

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

151

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

rpool/export

44K 59.6G

23K /export

rpool/export/home

21K 59.6G

21K /export/home

rpool/swap

rpool/zones

1G 60.6G

16K -

22.9M 59.6G

637M /rpool/zones

rpool/zones-zfs1009BE

653M 59.6G

633M /rpool/zones-zfs1009BE

rpool/zones-zfs1009BE@zfs10092BE 20.0M

-

633M -

2

Assurez-vous que les zones sont installées et initialisées. Exemple :

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

5 zfszone

running

/rpool/zones

native

shared

3

Créez l'environnement d'initialisation ZFS à mettre à jour ou à corriger. Exemple :

# lucreate -n zfs10093BE

Analyzing system configuration.

Comparing source boot environment <zfs10092BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

Creating configuration for boot environment <zfs10093BE>.

Source boot environment is <zfs10092BE>.

Creating boot environment <zfs10093BE>.

Cloning file systems from boot environment <zfs10092BE> to create boot environment <zfs10093BE>.

Creating snapshot for <rpool/ROOT/zfs10092BE> on <rpool/ROOT/zfs10092BE@zfs10093BE>.

Creating clone for <rpool/ROOT/zfs10092BE@zfs10093BE> on <rpool/ROOT/zfs10093BE>.

Setting canmount=noauto for </> in zone <global> on <rpool/ROOT/zfs10093BE>.

Creating snapshot for <rpool/zones> on <rpool/zones@zfs10093BE>.

Creating clone for <rpool/zones@zfs10093BE> on <rpool/zones-zfs10093BE>.

Population of boot environment <zfs10093BE> successful.

Creation of boot environment <zfs10093BE> successful.

4

Sélectionnez l'une des options suivantes pour mettre à niveau le système ou appliquer les
correctifs au nouvel environnement d'initialisation.
■ Mettez à niveau le système. Exemple :

# luupgrade -u -n zfs10093BE -s /net/install/export/s10uX/combined.s10s_uXwos/latest

L'option -s représente l'emplacement d'un mode d'installation Solaris.

Ce processus peut être très long.

Pour un exemple complet du processus luupgrade, reportez-vous à l'Exemple 5–6.

■ Appliquez les correctifs au nouvel environnement d'initialisation. Exemple :

152

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

# luupgrade -t -n zfs10093BE -t -s /patchdir patch-id-02 patch-id-04

5

Activez le nouvel environnement d'initialisation une fois que ses mises à jour ont été effectuées.

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs1009BE

zfs10092BE

zfs10093BE

yes

yes

yes

no

no

yes

yes

no

no

yes

no

yes

-

-

-

# luactivate zfs10093BE

A Live Upgrade Sync operation will be performed on startup of boot environment <zfs10093BE>.

.

.

.

6

Initialisez à partir de l'environnement d'initialisation nouvellement activé.

# init 6

Exemple 5–6

Mise à niveau d'un système de fichiers racine ZFS avec racine de zone vers un
système de fichiers racine ZFS Solaris 10 10/09
Dans cet exemple, un environnement d'initialisation ZFS (zfs509BE), créé sur un système
Solaris 10 5/09 avec un système de fichiers racine ZFS et une racine de zone dans un pool
non-racine, est mis à niveau vers Solaris 10 10/09. Ce processus peut être long.
L'environnement d'initialisation mis à niveau (zfs10092BE) est ensuite activé. Assurez-vous
que les zones sont installées et initialisées avant de tenter la migration.

Dans cet exemple, le pool zonepool, le jeu de données /zonepool/zones et zfszone sont créés
comme suit :

# zpool create zonepool mirror c2t1d0 c2t5d0

# zfs create zonepool/zones

# chmod 700 zonepool/zones

# zonecfg -z zfszone

zfszone: No such zone configured

Use ’create’ to begin configuring a new zone.

zonecfg:zfszone> create

zonecfg:zfszone> set zonepath=/zonepool/zones

zonecfg:zfszone> verify

zonecfg:zfszone> exit

# zoneadm -z zfszone install

cannot create ZFS dataset zonepool/zones: dataset already exists

Preparing to install zone <zfszone>.

Creating list of files to copy from the global zone.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

153

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

Copying <8960> files to the zone.

.

.

.

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

2 zfszone

running

/zonepool/zones

native

shared

# lucreate -n zfs1009BE

.

.

.

# luactivate zfs1009BE

.

.

.

# init 6

# luupgrade -u -n zfs1009BE -s /net/install/export/s10u8/combined.s10s_u8wos/latest

40410 blocks

miniroot filesystem is <lofs>

Mounting miniroot at </net/system/export/s10u8/latest/Solaris_10/Tools/Boot>

Validating the contents of the media </net/system/export/s10u8//latest>.

The media is a standard Solaris media.

The media contains an operating system upgrade image.

The media contains <Solaris> version <10>.

Constructing upgrade profile to use.

Locating the operating system upgrade program.

Checking for existence of previously scheduled Live Upgrade requests.

Creating upgrade profile for BE <zfs1009BE>.

Determining packages to install or upgrade for BE <zfs1009BE>.

Performing the operating system upgrade of the BE <zfs1009BE>.

CAUTION: Interrupting this process may leave the boot environment unstable

or unbootable.

Upgrading Solaris: 100% completed

Installation of the packages from this media is complete.

Updating package information on boot environment <zfs1009BE>.

Package information successfully updated on boot environment <zfs1009BE>.

Adding operating system patches to the BE <zfs1009BE>.

The operating system patch installation is complete.

INFORMATION: The file </var/sadm/system/logs/upgrade_log> on boot

environment <zfs1009BE> contains a log of the upgrade operation.

INFORMATION: The file </var/sadm/system/data/upgrade_cleanup> on boot

environment <zfs1009BE> contains a log of cleanup operations required.

INFORMATION: Review the files listed above. Remember that all of the files

154

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

are located on boot environment <zfs1009BE>. Before you activate boot

environment <zfs1009BE>, determine if any additional system maintenance is

required or if additional media of the software distribution must be

installed.

The Solaris upgrade of the boot environment <zfs1009BE> is complete.

Installing failsafe

Failsafe install is complete.

# luactivate zfs1009BE

# init 6

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs509BE

zfs1009BE

# zoneadm list -cv

yes

yes

no

no

yes

yes

yes

no

-

-

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

- zfszone

installed /zonepool/zones

native

shared

▼ Migration d'un système de fichiers racine UFS avec racine de zone vers
un système de fichiers racine ZFS (Solaris 10 5/09 ou Solaris 10 10/09)
Utilisez cette procédure pour mettre à niveau un système incluant un système de fichiers racine
UFS et une racine de zone vers la version Solaris 10 5/09 ou 10 10/09. Utilisez ensuite Live
Upgrade pour créer un environnement d'initialisation ZFS.

Dans la procédure ci-après, l'exemple de nom de l'environnement d'initialisation UFS est
c0t1d0s0, la racine de zone UFS est zonepool/zfszone et l'environnement d'initialisation
racine ZFS est zfs1009.

1

2

Mettez le système à niveau vers la version Solaris 10 5/09 ou 10 10/09 si la version Solaris 10
exécutée est antérieure.
Pour plus d'informations sur la mise à niveau d'un système exécutant Solaris 10, reportez-vous
au Guide d’installation de Solaris 10 : Solaris Live Upgrade et planification de la mise à niveau.

Créez le pool racine.
Pour plus d'informations sur les exigences du pool racine, reportez-vous à la “Configuration
requise pour l'installation de Solaris et de Solaris Live Upgrade pour la prise en charge de ZFS”
à la page 117.

3

Confirmez que les zones de l'environnement UFS sont initialisées. Exemple :

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

155

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

2 zfszone

running

/zonepool/zones

native

shared

4

Créez le nouvel environnement d'initialisation ZFS. Exemple :

# lucreate -c c1t1d0s0 -n zfs1009 -p rpool

Cette commande crée des jeux de données dans le pool racine pour le nouvel environnement
d'initialisation et copie l'environnement d'initialisation actuel (zones incluses) vers ces jeux de
données.

5

Activez le nouvel environnement d'initialisation ZFS. Exemple :

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

c1t1d0s0

zfs1009BE

yes

yes

yes

yes

no

no

no

yes

-

-

# luactivate zfs1009BE

A Live Upgrade Sync operation will be performed on startup of boot environment <zfs1009BE>.

.

.

.

6

7

Redémarrez le système.

# init 6

Confirmez que les systèmes de fichiers ZFS et les zones sont créés dans le nouvel environnement
d'initialisation. Exemple :

# zfs list

NAME

rpool

USED AVAIL REFER MOUNTPOINT

6.17G 60.8G

98K /rpool

rpool/ROOT

4.67G 60.8G

21K /rpool/ROOT

rpool/ROOT/zfs1009BE

4.67G 60.8G 4.67G /

rpool/dump

rpool/swap

zonepool

1.00G 60.8G 1.00G -

517M 61.3G

16K -

634M 7.62G

24K /zonepool

zonepool/zones

270K 7.62G

633M /zonepool/zones

zonepool/zones-c1t1d0s0

634M 7.62G

633M /zonepool/zones-c1t1d0s0

zonepool/zones-c1t1d0s0@zfs1009BE

262K

-

633M -

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

- zfszone

installed /zonepool/zones

native

shared

156

Guide d'administration Solaris ZFS • Octobre 2009

Migration d'un système de fichiers racine UFS vers un système de fichiers racine ZFS (Solaris Live Upgrade)

Exemple 5–7

Migration d'un système de fichiers racine UFS avec racine de zone vers un système
de fichiers racine ZFS
Dans cet exemple, un système Solaris 10 10/09 comprenant une racine UFS, une racine de zone
(/uzone/ufszone), un pool non-racine ZFS (pool ) et une racine de zone (/pool/zzone) est
migré vers un système de fichiers racine ZFS. Assurez-vous que le pool racine ZFS est créé et
que les zones sont installées et initialisées avant de tenter la migration.

# zoneadm list -cv

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

2 ufszone

running

/uzone/ufszone

native

shared

3 zfszone

running

/pool/zones/zfszone

native

shared

# lucreate -c ufs1009BE -n zfs1009BE -p rpool

Analyzing system configuration.

No name for current boot environment.

Current boot environment is named <ufs509BE>.

Creating initial configuration for primary boot environment <ufs509BE>.

The device </dev/dsk/c1t0d0s0> is not a root device for any boot environment; cannot get BE ID.

PBE configuration successful: PBE name <ufs1009BE> PBE Boot Device </dev/dsk/c1t0d0s0>.

Comparing source boot environment <ufs1009BE> file systems with the file

system(s) you specified for the new boot environment. Determining which

file systems should be in the new boot environment.

Updating boot environment description database on all BEs.

Updating system configuration files.

The device </dev/dsk/c1t1d0s0> is not a root device for any boot environment; cannot get BE ID.

Creating configuration for boot environment <zfs1009BE>.

Source boot environment is <ufs1009BE>.

Creating boot environment <zfs1009BE>.

Creating file systems on boot environment <zfs1009BE>.

Creating <zfs> file system for </> in zone <global> on <rpool/ROOT/zfs1009BE>.

Populating file systems on boot environment <zfs1009BE>.

Checking selection integrity.

Integrity check OK.

Populating contents of mount point </>.

Copying.

Creating shared file system mount points.

Copying root of zone <ufszone> to </.alt.tmp.b-EYd.mnt/uzone/ufszone>.

Creating snapshot for <pool/zones/zfszone> on <pool/zones/zfszone@zfs1009BE>.

Creating clone for <pool/zones/zfszone@zfs1009BE> on <pool/zones/zfszone-zfs1009BE>.

Creating compare databases for boot environment <zfs1009BE>.

Creating compare database for file system </rpool/ROOT>.

Creating compare database for file system </>.

Updating compare databases on boot environment <zfs1009BE>.

Making boot environment <zfs1009BE> bootable.

Creating boot_archive for /.alt.tmp.b-DLd.mnt

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

157

Prise en charge ZFS des périphériques de swap et de vidage

updating /.alt.tmp.b-DLd.mnt/platform/sun4u/boot_archive

Population of boot environment <zfs1009BE> successful.

Creation of boot environment <zfs1009BE> successful.

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

ufs1009BE

zfs1009BE

yes

yes

yes

yes

no

no

no

yes

-

-

# luactivate zfs1009BE

.

.

.

# init 6

.

.

.

# zfs list

NAME

pool

USED AVAIL REFER MOUNTPOINT

628M 66.3G

19K /pool

pool/zones

628M 66.3G

20K /pool/zones

pool/zones/zfszone

75.5K 66.3G

627M /pool/zones/zfszone

pool/zones/zfszone-ufs1009BE

628M 66.3G

627M /pool/zones/zfszone-ufs1009BE

pool/zones/zfszone-ufs1009BE@zfs1009BE

98K

-

627M -

rpool

rpool/ROOT

7.76G 59.2G

95K /rpool

5.25G 59.2G

18K /rpool/ROOT

rpool/ROOT/zfs1009BE

5.25G 59.2G 5.25G /

rpool/dump

rpool/swap

# zoneadm list -cv

2.00G 59.2G 2.00G -

517M 59.7G

16K -

ID NAME

STATUS

PATH

BRAND

IP

0 global

running

/

native

shared

- ufszone

installed /uzone/ufszone

native

shared

- zfszone

installed /pool/zones/zfszone

native

shared

Prise en charge ZFS des périphériques de swap et de vidage
Lors d'une installation initiale ou de l'utilisation de Solaris Live Upgrade à partir d'un système
de fichiers UFS, une zone de swap est créée sur un volume ZFS du pool racine ZFS. Par
exemple :

# swap -l

swapfile

dev

swaplo

blocks

free

/dev/zvol/dsk/mpool/swap 253,3

16 8257520 8257520

158

Guide d'administration Solaris ZFS • Octobre 2009

Prise en charge ZFS des périphériques de swap et de vidage

Lors d'une installation initiale ou de l'utilisation de Solaris Live Upgrade à partir d'un système
de fichiers UFS, un périphérique de vidage est créé sur un volume ZFS du pool racine ZFS. Le
périphérique de vidage ne nécessite aucune administration une fois configuré. Exemple :

# dumpadm

Dump content: kernel pages

Dump device: /dev/zvol/dsk/mpool/dump (dedicated)

Savecore directory: /var/crash/t2000

Savecore enabled: yes

Pour plus d'informations sur les tailles de volume de swap et de vidage qui sont créés par les
programmes d'installation, reportez-vous à la section “Configuration requise pour l'installation
de Solaris et de Solaris Live Upgrade pour la prise en charge de ZFS” à la page 117.

La taille des volume de swap et de vidage peut être ajustée pendant et après l'installation. Pour
plus d'informations, reportez-vous à la section “Ajustement de la taille de vos périphériques de
swap et de vidage ZFS” à la page 159.

Prenez en compte les points suivants lorsque vous travaillez avec des périphériques de swap et
de vidage ZFS :
■ Vous devez utiliser des volumes ZFS distincts pour les périphériques de swap et de vidage.
■ L'utilisation d'un fichier swap sur un système de fichiers ZFS n'est actuellement pas prise en

charge.

■ En raison de CR 6724860, vous devez exécuter la commande savecore manuellement pour
enregistrer un vidage mémoire sur incident lorsque vous utilisez un volume de vidage ZFS.

■ Pour modifier la zone de swap ou le périphérique de vidage une fois le système installé ou

mis à niveau, utilisez les commandes swap et dumpadm de la même façon que dans les
versions Solaris précédentes. Pour plus d'informations, reportez-vous au Chapitre 20,
“Configuring Additional Swap Space (Tasks)” du System Administration Guide: Devices and
File Systems et au Chapitre 17, “Managing System Crash Information (Tasks)” du System
Administration Guide: Advanced Administration (tous deux en anglais).

Ajustement de la taille de vos périphériques de swap
et de vidage ZFS
La façon dont une installation racine ZFS attribue une taille diffère selon qu'il s'agit d'un
périphérique de swap ou d'un périphérique de vidage ; il s'avère pour cela parfois nécessaire
d'ajuster la taille des périphériques de swap et de vidage avant, pendant ou après l'installation.
■ Les performances du volume de vidage ZFS sont bien supérieures lorsque le volume est créé

avec une taille de bloc de 128 Ko.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

159

Prise en charge ZFS des périphériques de swap et de vidage

■ Vous pouvez ajuster la taille de vos volumes de swap et de vidage au cours d'une installation

initiale. Pour plus d'informations, reportez-vous à l'Exemple 5–1.

■ Vous pouvez créer des volumes de swap et de vidage, ainsi que leur attribuer une taille, avant

de procéder à une opération Solaris Live Upgrade. Exemple :
1. Créer le pool de stockage.

# zpool create rpool mirror c0t0d0s0 c0t1d0s0

2. Créez le périphérique de vidage.

# zfs create -V 2G rpool/dump

3. Sélectionnez l'une des options suivantes pour créer la zone de swap :

■

■

Sur un système SPARC, créez la zone de swap. Configurez la taille du bloc sur 8 Ko.

# zfs create -V 2G -b 8k rpool/swap

Sur un système x86, créez la zone de swap. Configurez la taille du bloc sur 4 Ko.

# zfs create -V 2G -b 4k rpool/swap

4. Vous devez activer la zone de swap lorsqu'un nouveau périphérique de swap est ajouté

ou modifié.

Solaris Live Upgrade ne redimensionne pas des volumes de swap et de vidage existants.
■ Vous pouvez rétablir la propriété volsize du périphérique de vidage après l'installation

d'un système. Exemple :

# zfs set volsize=2G rpool/dump

# zfs get volsize rpool/dump

NAME

PROPERTY VALUE

SOURCE

rpool/dump volsize

2G

-

■ Vous pouvez redimensionner le volume de swap, mais tant que le problème CR 6765386

existe, il est préférable de supprimer préalablement le périphérique de swap. Recréez-le par
la suite. Exemple :

# swap -d /dev/zvol/dsk/rpool/swap

# zfs volsize=2G rpool/swap

# swap -a /dev/zvol/dsk/rpool/swap

Pour plus d'informations sur la suppression d'un périphérique de swap sur un système actif,
reportez-vous au site suivant :

http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide

■ Vous pouvez ajuster la taille des volumes de swap et de vidage d'un profil JumpStart à l'aide

d'une syntaxe de profil du type suivant :

160

Guide d'administration Solaris ZFS • Octobre 2009

Initialisation à partir d'un système de fichiers racine ZFS

install_type initial_install

cluster SUNWCXall

pool rpool 16g 2g 2g c0t0d0s0

Dans ce profil, les entrées 2g et 2g attribuent 2 Go comme taille de zone de swap et comme
taille de périphérique de vidage.
Si vous avez besoin de plus d'espace de swap sur un système déjà installé, il suffit d'ajouter un
autre volume de swap. Exemple :

■

# zfs create -V 2G rpool/swap2

Ensuite, activez le nouveau volume de swap. Exemple :

# swap -a /dev/zvol/dsk/rpool/swap2

# swap -l

swapfile

dev swaplo

blocks

free

/dev/zvol/dsk/rpool/swap 256,1

16 1058800 1058800

/dev/zvol/dsk/rpool/swap2 256,3

16 4194288 4194288

Initialisation à partir d'un système de fichiers racine ZFS

Les systèmes SPARC et les systèmes x86 utilisent le nouveau type d'initialisation à l'aide d'une
archive d'initialisation, qui est une image de système de fichiers contenant les fichiers requis
pour l'initialisation. Lorsque vous initialisez un système à partir d'un système de fichiers racine
ZFS, les noms de chemin du fichier archive et du fichier noyau sont résolus dans le système de
fichiers racine sélectionné pour l'initialisation.

Lorsque le système est initialisé pour l'installation, un disque RAM est utilisé pour le système de
fichiers racine pendant toute la procédure d'installation afin de ne pas avoir à effectuer
l'initialisation à partir d'un média amovible.

Si vous procédez à une installation initiale de Solaris 10 10/08 ou Solaris 10 5/09 ou utilisez
Solaris Live Upgrade pour migrer vers un système de fichiers racine ZFS dans cette version,
vous pouvez effectuer l'initialisation à partir d'un système de fichiers racine ZFS sur un système
SPARC ou x86.

L'initialisation à partir d'un système de fichiers ZFS diffère de celle d'un système de fichiers UFS
car avec ZFS, un spécificateur de périphérique identifie un pool de stockage par opposition à un
seul système de fichiers racine. Un pool de stockage peut contenir plusieurs jeux de données
d'initialisation ou systèmes de fichiers racine ZFS. Lorsque vous initialisez un système à partir
de ZFS, vous devez spécifier un périphérique d'initialisation et un système de fichiers racine
contenu dans le pool qui a été identifié par le périphérique d'initialisation.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

161

Initialisation à partir d'un système de fichiers racine ZFS

Par défaut, le jeu de données sélectionné pour l'initialisation est celui qui est identifié par la
propriété bootfs du pool. Cette sélection par défaut peut être remplacée en spécifiant un jeu de
données d'initialisation alternatif qui est inclus dans la commande boot -Z.

Initialisation à partir d'un disque alternatif d'un pool
racine ZFS mis en miroir
Vous pouvez créer un pool racine ZFS mis en miroir lors de l'installation du système ou pouvez
connecter un disque pour créer un pool racine ZFS mis en miroir après l'installation. Consultez
les problèmes connus suivants relatifs aux pools racine ZFS mis en miroir :
■ CR 6668666 – Vous devez installer les informations d'initialisation sur les disques

supplémentaires que vous connectez, à l'aide de la commande installboot ou de la
commande installgrub si vous souhaitez activer l'initialisation sur les autres disques du
miroir. Si vous créez un pool racine ZFS mis en miroir par la méthode d'installation initiale,
cette étape n'est pas nécessaire. Par exemple, si c0t1d0s0 est le deuxième disque ajouté au
miroir, la commande installboot ou la commande installgrub est exécutée comme
suit :

sparc# installboot -F zfs /usr/platform/‘uname -i‘/lib/fs/zfs/bootblk /dev/rdsk/c0t1d0s0

x86# installgrub /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c0t1d0s0

■ Vous pouvez effectuer l'initialisation à partir de divers périphériques d'un pool racine ZFS
mis en miroir. Selon la configuration matérielle, la mise à jour de la PROM ou du BIOS peut
s'avérer nécessaire pour spécifier un périphérique d'initialisation différent.
Vous pouvez par exemple effectuer l'initialisation à partir de l'un des deux disques
(c1t0d0s0 ou c1t1d0s0) de ce pool.

# zpool status

pool: rpool

state: ONLINE

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

mirror

ONLINE

c1t0d0s0 ONLINE

c1t1d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

Sur un système SPARC, saisissez le disque alternatif à l'invite ok.

ok boot /pci@7c0/pci@0/pci@1/pci@0,2/LSILogic,sas@2/disk@1

162

Guide d'administration Solaris ZFS • Octobre 2009

Initialisation à partir d'un système de fichiers racine ZFS

Une fois le système réinitialisé, confirmez le périphérique d'initialisation actif. Exemple :

SPARC# prtconf -vp | grep bootpath

bootpath: ’/pci@7c0/pci@0/pci@1/pci@0,2/LSILogic,sas@2/disk@1,0:a’

Sur un système x86, utilisez une syntaxe semblable à ce qui suit :

x86# prtconf -v|sed -n ’/bootpath/,/value/p’

name=’bootpath’ type=string items=1

value=’/pci@0,0/pci8086,25f8@4/pci108e,286@0/disk@0,0:a’

■

Sur un système x86, sélectionnez un disque alternatif dans le pool racine ZFS mis en miroir
dans le menu approprié du BIOS.

Initialisation à partir d'un système de fichiers racine
ZFS sur un système SPARC
Sur un système SPARC avec environnements d'initialisation multiples ZFS, vous pouvez
initialiser à partir de tout environnement d'initialisation en utilisant la commande luactivate.
Après avoir activé l'environnement d'initialisation, vous pouvez utiliser la commande boot -L
pour afficher la liste des environnements d'initialisation lorsque le périphérique d'initialisation
contient un pool de stockage ZFS.

Au cours de l'installation et de la procédure de Solaris Live Upgrade, le système de fichiers
racine ZFS est automatiquement désigné avec la propriété bootfs.

Un pool peut contenir plusieurs jeux de données d'initialisation. Par défaut, l'entrée du jeu de
données d'initialisation figurant dans le fichier /nom-du-pool/boot/menu.lst est identifiée par
la propriété bootfs du pool. Cependant, une entrée menu.lst peut contenir une commande
bootfs qui spécifie un jeu de données alternatif du pool. Le fichier menu.lst peut ainsi
contenir les entrées de plusieurs systèmes de fichiers racine du pool.

Lorsqu'un système est installé avec un système de fichiers racine ZFS ou est migré vers un
système de fichiers racine ZFS, une entrée du type suivant est ajoutée au fichier menu.lst :

title zfs1009BE

bootfs rpool/ROOT/zfs1009BE

title zfs509BE

bootfs rpool/ROOT/zfs509BE

Lorsqu'un nouvel environnement d'initialisation est créé, le fichier menu.lst est mis à jour
automatiquement.

Sur un système SPARC, deux nouvelles options d'initialisation sont disponibles :

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

163

Initialisation à partir d'un système de fichiers racine ZFS

■ Vous pouvez utiliser la commande d'initialisation -L pour afficher une liste de jeux de

données d'initialisation contenus dans un pool ZFS. Vous pouvez ensuite sélectionner un
des jeux de données d'initialisation de la liste. Des instructions détaillées permettant
d'initialiser ce jeu de données s'affichent. Vous pouvez initialiser le jeu de données
sélectionné en suivant ces instructions. Cette option n'est disponible que lorsque le
périphérique d'initialisation contient un pool de stockage ZFS.

■ Utilisez la commande -Z dataset pour initialiser un jeu de données ZFS spécifique.

Initialisation à partir d'un environnement d'initialisation ZFS spécifique

EXEMPLE 5–8
Si vous disposez de plusieurs environnements d'initialisation ZFS dans un pool de stockage ZFS
situé sur le périphérique d'initialisation de votre système, vous pouvez utiliser la commande
luactivate pour spécifier un environnement d'initialisation par défaut.

Par exemple, les environnements d'initialisation ZFS suivants sont disponibles comme décrit
par la sortie de lustatus :

# lustatus

Boot Environment

Is

Active Active

Can

Copy

Name

Complete Now

On Reboot Delete Status

-------------------------- -------- ------ --------- ------ ----------

zfs1009BE

zfs509BE

yes

yes

yes

yes

no

no

no

yes

-

-

Si vous disposez de plusieurs environnements d'initialisation ZFS sur votre système SPARC,
vous pouvez utiliser la commande boot -L. Exemple :

ok boot -L

Rebooting with command: boot -L

Boot device: /pci@8,600000/SUNW,qlc@2/fp@0,0/disk@w500000e01082bbd1,0:a File and args: -L

1 zfs1009BE

2 zfs509BE

Select environment to boot: [ 1 - 2 ]: 2

ok boot -Z rpool/ROOT/zfs509BE

EXEMPLE 5–9 SPARC : Initialisation d'un système de fichiers ZFS en mode de secours

Vous pouvez initialiser un système SPARC à partir de l'archive de secours située dans
/platform/‘uname -i‘/failsafe, comme suit . Exemple :

ok boot -F failsafe

Pour initialiser une archive de secours à partir d'un jeu de données d'initialisation ZFS donné,
employez une syntaxe du type suivant :

164

Guide d'administration Solaris ZFS • Octobre 2009

Initialisation à partir d'un système de fichiers racine ZFS

EXEMPLE 5–9 SPARC : Initialisation d'un système de fichiers ZFS en mode de secours

(Suite)

ok boot -Z rpool/ROOT/zfs1009BE -F failsafe

Initialisation à partir d'un système de fichiers racine
ZFS sur un système x86
Les entrées suivantes sont ajoutées au fichier /nom-de-pool /boot/grub/menu.lst au cours du
processus d'installation ou du lancement de Solaris Live Upgrade pour initialiser ZFS
automatiquement :

title Solaris 10 10/09 s10x_u8wos_07b X86

findroot (pool_rpool,0,a)

kernel$ /platform/i86pc/multiboot -B $ZFS-BOOTFS

module /platform/i86pc/boot_archive

title Solaris failsafe

findroot (pool_rpool,0,a)

kernel /boot/multiboot kernel/unix -s -B console=ttya

module /boot/x86.miniroot-safe

Si le périphérique identifié par GRUB comme périphérique d'initialisation contient un pool de
stockage ZFS, le fichier menu.lst est utilisé pour créer le menu GRUB.

Sur un système x86 contenant plusieurs environnements d'initialisation ZFS, vous pouvez
sélectionner un environnement d'initialisation à partir du menu GRUB. Si le système de fichiers
racine correspondant à cette entrée de menu est un jeu de données ZFS, l'option suivante est
ajoutée.

-B $ZFS-BOOTFS

EXEMPLE 5–10 x86 : Initialisation d'un système de fichiers ZFS
Lorsque vous effectuez l'initialisation à partir d'un système de fichiers ZFS, le périphérique
racine est spécifié par le paramètre d'initialisation -B $ZFS-BOOTFS sur la ligne kernel ou
module de l'entrée du menu GRUB. Cette valeur, tout comme tous les paramètres spécifiés par
l'option -B, est transmise par GRUB au noyau. Exemple :

title Solaris 10 10/09 s10x_u8wos_07b X86

findroot (pool_rpool,0,a)

kernel$ /platform/i86pc/multiboot -B $ZFS-BOOTFS

module /platform/i86pc/boot_archive

title Solaris failsafe

findroot (pool_rpool,0,a)

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

165

Initialisation à partir d'un système de fichiers racine ZFS

kernel /boot/multiboot kernel/unix -s -B console=ttya

module /boot/x86.miniroot-safe

EXEMPLE 5–11 x86 : Initialisation d'un système de fichiers ZFS en mode de secours
L'archive x86 de secours est /boot/x86.miniroot-safe et peut être initialisée en sélectionnant
l'entrée "Solaris failsafe" dans le menu GRUB. Exemple :

title Solaris failsafe

findroot (pool_rpool,0,a)

kernel /boot/multiboot kernel/unix -s -B console=ttya

module /boot/x86.miniroot-safe

Résolution des problèmes de point de montage ZFS
responsables de l'échec de l'initialisation
Le meilleur moyen de changer d'environnement d'initialisation actif est d'utiliser la commande
luactivate. En cas d'échec de l'initialisation de l'environnement actif, qu'il soit dû à un
correctif défectueux ou à une erreur de configuration, le seul moyen d'initialiser un
environnement différent consiste à le sélectionner lors de l'initialisation. Vous pouvez
sélectionner un environnement d'initialisation différent dans le menu GRUB sur un système
x86 ou l'initialiser à partir de PROM sur un système SPARC.

Il est possible qu'un bogue dans la fonctionnalité Live Upgrade de Solaris 10 10/08 provoque
l'échec de l'initialisation de l'environnement d'initialisation non actif. Ce problème est lié à la
présence d'un point de montage non valide dans les jeux de données ZFS ou dans le jeu de
données ZFS de zone de l'environnement d'initialisation. Le même bogue empêche également le
montage de l'environnement d'initialisation s'il dispose d'un jeu de données /var distinct.

Si le jeu de données d'une zone a un point de montage non valide, le point de montage peut être
corrigé en procédant comme suit.

▼ Comment résoudre les problèmes de point de montage ZFS

1

2

Initialisez le système à partir d'une archive de secours.

Importez le pool.
Exemple :

# zpool import rpool

166

Guide d'administration Solaris ZFS • Octobre 2009

Restauration du pool racine ZFS ou des instantanés du pool racine

3

Une fois le pool importé, contrôlez la sortie zfs list.
Vérifiez qu'elle ne contient aucun point de montage temporaire erroné. Exemple :

# zfs list -r -o name,mountpoint rpool/ROOT/s10u6

NAME

MOUNTPOINT

rpool/ROOT/s10u6

/.alt.tmp.b-VP.mnt/

rpool/ROOT/s10u6/zones

/.alt.tmp.b-VP.mnt//zones

rpool/ROOT/s10u6/zones/zonerootA

/.alt.tmp.b-VP.mnt/zones/zonerootA

Le point de montage pour l'environnement d'initialisation racine (rpool/ROOT/s10u6) doit être
/.
Si l'initialisation échoue à cause de problèmes de montage /var, recherchez un point de
montage temporaire erroné similaire pour le jeu de données /var.

Réinitialisez les points de montage pour l'environnement d'initialisation ZFS et ses jeux de
données.
Exemple :

# zfs inherit -r mountpoint rpool/ROOT/s10u6

# zfs set mountpoint=/ rpool/ROOT/s10u6

Redémarrez le système.
Lorsque vous avez la possibilité d'initialiser un environnement d'initialisation spécifique, soit
par le biais du menu GRUB, soit à l'invite OpenBoot Prom, sélectionnez l'environnement
d'initialisation dont les points de montage viennent d'être corrigés.

4

5

Restauration du pool racine ZFS ou des instantanés du pool
racine

Les sections suivantes décrivent comment effectuer les tâches ci-dessous :
■ “Remplacement d'un disque dans le pool racine ZFS” à la page 167
■ “ Création d'instantanés de pool racine” à la page 169
■ “ Recréation d'un pool racine ZFS et restauration d'instantanés de pool racine” à la page 171
■ “ Restauration des instantanés d'un pool racine à partir d'une initialisation de secours ”

à la page 172

▼ Remplacement d'un disque dans le pool racine ZFS

Vous pouvez être amené à remplacer un disque dans le pool racine pour les raisons suivantes :
■ Le pool racine est trop petit et vous souhaitez le remplacer par un disque plus grand.

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

167

Restauration du pool racine ZFS ou des instantanés du pool racine

■ Le disque du pool racine est défectueux. Si le disque est défectueux et empêche

l'initialisation du système, vous devrez initialiser votre système à partir d'un autre support,
par exemple un CD ou le réseau, avant de remplacer le disque du pool racine.

Identifiez les chemins d'accès du périphérique d'initialisation du nouveau disque et du disque
actuel afin de tester l'initialisation à partir du disque de remplacement et afin de pouvoir
initialiser manuellement le système à partir du disque existant, en cas de dysfonctionnement du
disque de remplacement. Dans l'exemple suivant, le disque du pool racine actuel (c1t10d0s0)
est :

/pci@8,700000/pci@3/scsi@5/sd@a,0

Dans l'exemple suivant, le disque d'initialisation de remplacement (c4t0d0s0) est :

/pci@8,700000/pci@3/scsi@5/sd@9,0

Connectez physiquement le disque de remplacement.

Confirmez que le (nouveau) disque de remplacement possède une étiquette SMI et une tranche
0.
Pour plus d'informations sur le nouvel étiquetage d'un disque destiné au pool racine, visitez le
site suivant :

http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide

Associez le nouveau disque au pool racine.
Exemple :

# zpool attach rpool c1t10d0s0 c4t0d0s0

Confirmez le statut du pool racine.
Exemple :

# zpool status rpool

pool: rpool

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scrub: resilver in progress, 25.47% done, 0h4m to go

config:

1

2

3

4

NAME

STATE

READ WRITE CKSUM

rpool

ONLINE

mirror

ONLINE

c1t0d0s0 ONLINE

c1t5d0s0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

168

Guide d'administration Solaris ZFS • Octobre 2009

Restauration du pool racine ZFS ou des instantanés du pool racine

5

6

7

8

errors: No known data errors

Une fois la réargenture terminée, appliquez les blocs d'initialisation au nouveau disque.
Exemple :
Sur un système SPARC :

# installboot -F zfs /usr/platform/‘uname -i‘/lib/fs/zfs/bootblk /dev/rdsk/c1t5d0s0

Sur un système x86 :

# installgrub /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c1t5d0s0

Vérifiez que vous pouvez initialiser le système à partir du nouveau disque.
Par exemple, sur un système SPARC :

ok boot /pci@8,700000/pci@3/scsi@5/sd@9,0

Si le système s'initialise à partir du nouveau disque, déconnectez l'ancien disque.
Exemple :

# zpool detach rpool c1t0d0s0

Configurez le système pour qu'il puisse s'initialiser automatiquement à partir du nouveau
disque, à l'aide de la commande eeprom ou de la commande setenv de la PROM d'initialisation
SPARC, ou reconfigurez le BIOS du PC.

▼ Création d'instantanés de pool racine

Créez des instantanées de pool racine à des fins de récupération. La meilleure façon de créer des
instantanés de pool racine consiste à effectuer un instantané récursif du pool racine.

La procédure ci-dessous crée un instantané de pool racine récursif et le stocke en tant que
fichier dans un pool sur un système distant. Dans le cas d'une défaillance du pool racine, le jeu
de données distant peut être monté à l'aide de NFS et du fichier d'instantané reçu dans le pool
recréé. Vous pouvez également stocker les instantanés de pool racine en tant qu'instantanés
réels dans un pool d'un système distant. L'envoi et la réception des instantanés à partir d'un
système distant est un peu plus complexe, car vous devez configurer ssh ou utiliser rsh pendant
que le système à réparer est initialisé à partir de la mini-racine du système d'exploitation Solaris.

Pour plus d'informations sur le stockage et la récupération d'instantanés de pool racine à
distance et afin d'obtenir les informations les plus récentes sur la récupération de pool racine,
visitez le site :

http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

169

Restauration du pool racine ZFS ou des instantanés du pool racine

1

2

3

La validation à distance d'instantanés stockés sous forme de fichiers ou d'instantanés est une
étape importante dans la récupération de pool racine et, quelle que soit la méthode utilisée, les
instantanés doivent être recréés régulièrement, notamment lorsque la configuration du pool est
modifiée ou lorsque le système d'exploitation Solaris est mis à niveau.

Dans l'exemple suivant, le système est initialisé à partir de l'environnement d'initialisation
zfs1009BE.

Créez un espace sur un système distant pour stocker tous les instantanés.
Exemple :
remote# zfs create rpool/snaps

Partagez l'espace sur le système local.
Exemple :
remote# zfs set sharenfs=’rw=local-system,root=local-system’ rpool/snaps

# share

-@rpool/snaps

/rpool/snaps

sec=sys,rw=local-system,root=local-system

""

Créez un instantané récursif du pool racine.
local# zfs snapshot -r rpool@0804
local# zfs list

NAME

rpool

rpool@0804

rpool/ROOT

USED AVAIL REFER MOUNTPOINT

6.17G 60.8G

98K /rpool

0

-

98K -

4.67G 60.8G

21K /rpool/ROOT

rpool/ROOT@0804

0

-

21K -

rpool/ROOT/zfs1009BE

4.67G 60.8G 4.67G /

rpool/ROOT/zfs1009BE@0804

386K

- 4.67G -

rpool/dump

1.00G 60.8G 1.00G -

rpool/dump@0804

0

- 1.00G -

rpool/swap

517M 61.3G

16K -

rpool/swap@0804

0

-

16K -

4

Envoyez les instantanés du pool racine au système distant.
Exemple :
local# zfs send -Rv rpool@0804 > /net/remote-system/rpool/snaps/rpool.0804

sending from @ to rpool@0804

sending from @ to rpool/swap@0804

sending from @ to rpool/ROOT@0804

sending from @ to rpool/ROOT/zfs1009BE@0804

sending from @ to rpool/dump@0804

170

Guide d'administration Solaris ZFS • Octobre 2009

Restauration du pool racine ZFS ou des instantanés du pool racine

▼ Recréation d'un pool racine ZFS et restauration

d'instantanés de pool racine
Dans ce scénario, on suppose les conditions suivantes :
■ Le pool racine ZFS ne peut pas être récupéré.

■

Les instantanés du pool racine ZFS sont stockés sur un système distant et sont partagés sur
NFS.

Toutes les étapes ci-dessous sont effectuées sur le système local.

1

Initialisez le système à partir du CD/DVD ou du réseau.
Sur un système SPARC, sélectionnez l'une des méthodes d'initialisation suivantes :

ok boot net -s

ok boot cdrom -s

Si vous n'utilisez pas l'option -s, vous devrez quitter le programme d'installation.
Sur un système x86, sélectionnez l'option d'initialisation à partir du DVD ou du réseau. Quittez
ensuite le programme d'installation.

Montez le jeu de données de l'instantané distant.
Exemple :
# mount -F nfs remote-system:/rpool/snaps /mnt
Si vos services réseau ne sont pas configurés, il peut être nécessaire de spécifier l'adresse IP du
système distant.

Si le disque du pool racine est remplacé et ne contient aucune étiquette de disque pouvant être
utilisée par ZFS, vous devez renommer le disque.
Pour en savoir plus sur l'affectation d'un nouveau nom au disque, reportez-vous au site suivant :

http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide

Recréez le pool racine.
Exemple :

# zpool create -f -o failmode=continue -R /a -m legacy -o cachefile=

/etc/zfs/zpool.cache rpool c1t1d0s0

Restaurez les instantanés du pool racine.
Cette étape peut prendre un certain temps. Exemple :

# cat /mnt/rpool.0804 | zfs receive -Fdu rpool

2

3

4

5

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

171

Restauration du pool racine ZFS ou des instantanés du pool racine

6

Vérifiez que les jeux de données du pool racine sont restaurés.
Exemple :

# zfs list

NAME

rpool

rpool@0804

rpool/ROOT

USED AVAIL REFER MOUNTPOINT

6.17G 60.8G

98K /a/rpool

0

-

98K -

4.67G 60.8G

21K /legacy

rpool/ROOT@0804

0

-

21K -

rpool/ROOT/zfs1009BE

4.67G 60.8G 4.67G /a

rpool/ROOT/zfs1009BE@0804

398K

- 4.67G -

rpool/dump

1.00G 60.8G 1.00G -

rpool/dump@0804

0

- 1.00G -

rpool/swap

517M 61.3G

16K -

rpool/swap@0804

0

-

16K -

7

8

Définissez la propriété bootfs sur l'environnement d'initialisation du pool racine.
Exemple :

# zpool set bootfs=rpool/ROOT/zfs1009BE rpool

Installez les blocs d'initialisation sur le nouveau disque.
Sur un système SPARC :

# installboot -F zfs /usr/platform/‘uname -i‘/lib/fs/zfs/bootblk /dev/rdsk/c1t5d0s0

Sur un système x86 :

# installgrub /boot/grub/stage1 /boot/grub/stage2 /dev/rdsk/c1t5d0s0

9

Redémarrez le système.

# init 6

▼ Restauration des instantanés d'un pool racine à partir

d'une initialisation de secours
Cette procédure part du principe que les instantanés du pool racine existant sont disponibles.
Dans cet exemple, les instantanés du pool racine sont disponibles sur le système local. Exemple :

# zfs snapshot -r rpool@0804

# zfs list

NAME

rpool

rpool@0804

rpool/ROOT

USED AVAIL REFER MOUNTPOINT

6.17G 60.8G

98K /rpool

0

-

98K -

4.67G 60.8G

21K /rpool/ROOT

172

Guide d'administration Solaris ZFS • Octobre 2009

Restauration du pool racine ZFS ou des instantanés du pool racine

rpool/ROOT@0804

0

-

21K -

rpool/ROOT/zfs1009BE

4.67G 60.8G 4.67G /

rpool/ROOT/zfs1009BE@0804

398K

- 4.67G -

rpool/dump

1.00G 60.8G 1.00G -

rpool/dump@0804

0

- 1.00G -

rpool/swap

517M 61.3G

16K -

rpool/swap@0804

0

-

16K -

1

Arrêtez le système et initialisez-le en mode secours.

ok boot -F failsafe

Multiple OS instances were found. To check and mount one of them

read-write under /a, select it from the following list. To not mount

any, select ’q’.

1 /dev/dsk/c1t1d0s0

Solaris 10 10/09 s10s_u8wos_04 SPARC

2 rpool:11306141908645873833

ROOT/zfs10092BE

Please select a device to be mounted (q for none) [?,??,q]: 2

mounting rpool on /a

Starting shell.

2

Restaurez les instantanés individuels du pool racine.

# zfs rollback -rf rpool@0804

# zfs rollback -rf rpool/ROOT@0804

# zfs rollback -rf rpool/ROOT/zfs1009BE@0804

3

Réinitialisez le système en mode multi-utilisateur.

# init 6

Chapitre 5 • Installation et initialisation d'un système de fichiers racine ZFS

173

174

6C H A P I T R E

6

Gestion des systèmes de fichiers ZFS

Ce chapitre contient des informations détaillées sur la gestion des systèmes de fichiers ZFS. Il
aborde notamment les concepts d'organisation hiérarchique des systèmes de fichiers, d'héritage
des propriétés, de gestion automatique des points de montage et d'interaction sur les partages.

La création d'un système de fichiers ZFS s'effectue sur un pool de stockage. La création et la
destruction des systèmes de fichiers peuvent s'effectuer de manière dynamique, sans allocation
ni formatage manuels de l'espace disque sous-jacent. En raison de leur légèreté et de leur rôle
central dans l'administration du système ZFS, la création de ces systèmes de fichiers constitue
généralement une opération extrêmement courante.

La gestion des systèmes de fichiers ZFS s'effectue à l'aide de la commande zfs. La commande
zfs offre un ensemble de sous-commandes permettant d'effectuer des opérations spécifiques
sur les systèmes de fichiers. Chacune de ces sous-commandes est décrite en détail dans ce
chapitre. Cette commande permet également de gérer les instantanés, les volumes et les clones.
Toutefois, ces fonctionnalités sont uniquement traitées de manière succincte dans ce chapitre.
Pour plus d'informations sur les instantanés et les clones, reportez-vous au Chapitre 7,
“Utilisation des instantanés et des clones ZFS”. Pour de plus amples informations sur les
volumes ZFS, reportez-vous à la section “Volumes ZFS” à la page 275.

Remarque – Dans ce chapitre, le terme jeu de données désigne de manière générique un système
de fichiers, un instantané, un clone ou un volume.

Il contient les sections suivantes :
■ “Création et destruction de systèmes de fichiers ZFS” à la page 176
■ “Présentation des propriétés ZFS” à la page 179
■ “Envoi de requêtes sur les informations des systèmes de fichiers ZFS” à la page 193
■ “Gestion des propriétés ZFS” à la page 196
■ “Montage et partage des systèmes de fichiers ZFS” à la page 201
■ “Définition des quotas et réservations ZFS” à la page 208

175

Création et destruction de systèmes de fichiers ZFS

Création et destruction de systèmes de fichiers ZFS

La création et la destruction des systèmes de fichiers ZFS s'effectuent respectivement à l'aide des
commandes zfs create et zfs destroy.
■ “Création d'un système de fichiers ZFS” à la page 176
■ “Destruction d'un système de fichiers ZFS” à la page 177
■ “Modification du nom d'un système de fichiers ZFS” à la page 178

Création d'un système de fichiers ZFS
La création des systèmes de fichiers ZFS s'effectue à l'aide de la commande zfs create. La
sous-commande create ne peut contenir qu'un argument : le nom du système de fichiers à
créer. Le nom de ce système de fichiers permet également de définir le nom du chemin par
rapport au nom du pool :

nom-pool/[nom-système-fichiers/]nom-système-fichiers

Le nom du pool et les noms des systèmes de fichiers existants mentionnés dans le chemin
déterminent l'emplacement du nouveau système de fichiers dans la structure hiérarchique.
Tous les noms des systèmes de fichiers intermédiaires doivent déjà être définis dans le pool. Le
dernier nom mentionné dans le chemin correspond au nom du système de fichiers à créer. Ce
nom doit respecter les conventions d'attribution de nom définies à la section “Exigences
d'attribution de noms de composants ZFS” à la page 46.

Dans l'exemple suivant, un système de fichiers nommé bonwick est créé dans le système de
fichiers tank/home.

# zfs create tank/home/bonwick

Si le processus de création se déroule correctement, le système de fichiers ZFS est
automatiquement monté. Par défaut, les systèmes de fichiers sont montés sous /jeu-données, à
l'aide du chemin défini pour le nom du système dans la commande create. Dans cet exemple,
le fichier créé bonwick est monté sous /tank/home/bonwick. Pour plus d'informations sur les
points de montage gérés automatiquement, reportez-vous à la section “Gestion des points de
montage ZFS” à la page 201.

Pour plus d'informations sur la commande zfs create, reportez-vous à la page de manuel
zfs(1M).

Il est possible de définir les propriétés du système de fichiers lors de la création de ce dernier.

Dans l'exemple ci-dessous, le point de montage /export/zfs est défini et créé pour le système
de fichiers tank/home.

# zfs create -o mountpoint=/export/zfs tank/home

176

Guide d'administration Solaris ZFS • Octobre 2009

Pour plus d'informations sur les propriétés des systèmes de fichiers, reportez-vous à la section
“Présentation des propriétés ZFS” à la page 179.

Création et destruction de systèmes de fichiers ZFS

Destruction d'un système de fichiers ZFS
La destruction d'un système de fichiers ZFS s'effectue à l'aide de la commande zfs destroy.
Les systèmes de fichiers détruits sont automatiquement démontés et ne sont plus partagés. Pour
plus d'informations sur les montages ou partages gérés automatiquement, reportez-vous à la
section “Points de montage automatiques” à la page 202.

L'exemple suivant illustre la destruction du système de fichiers tabriz.

# zfs destroy tank/home/tabriz

Attention – Aucune invite de confirmation ne s'affiche lors de l'exécution de la sous-commande
destroy. Son utilisation requiert une attention particulière.

Si le système de fichiers à détruire est occupé et ne peut pas être démonté, la commande zfs
destroy échoue. Pour détruire un système de fichiers actif, indiquez l'option -f. L'utilisation
de cette option requiert une attention particulière. En effet, elle permet de démonter, d'annuler
le partage et de détruire des systèmes de fichiers actifs, ce qui risque d'affecter le comportement
de certaines applications.

# zfs destroy tank/home/ahrens

cannot unmount ’tank/home/ahrens’: Device busy

# zfs destroy -f tank/home/ahrens

La commande zfs destroy échoue également si le système de fichiers possède des systèmes
enfant. Pour détruire un système de fichiers et l'ensemble des descendants de ce système de
fichiers, indiquez l'option -r. Ce type d'opération de destruction récursive entraîne également
la destruction des instantanés ; l'utilisation de cette option requiert donc une attention
particulière.

# zfs destroy tank/ws

cannot destroy ’tank/ws’: filesystem has children

use ’-r’ to destroy the following datasets:

tank/ws/billm

tank/ws/bonwick

tank/ws/maybee

# zfs destroy -r tank/ws

Chapitre 6 • Gestion des systèmes de fichiers ZFS

177

Création et destruction de systèmes de fichiers ZFS

Si le système de fichiers à détruire possède des systèmes indirectement dépendants, la
commande de destruction récursive mentionnée ci-dessus échoue. Pour forcer la destruction de
tous systèmes dépendants, y compris des systèmes de fichiers clonés situés en dehors de la
structure hiérarchique cible, vous devez indiquer l'option -R. Utilisez cette option avec
précaution.

# zfs destroy -r tank/home/schrock

cannot destroy ’tank/home/schrock’: filesystem has dependent clones

use ’-R’ to destroy the following datasets:

tank/clones/schrock-clone

# zfs destroy -R tank/home/schrock

Attention – Aucune invite de confirmation ne s'affiche lors de l'utilisation des options -f, - r ou
-R. L'utilisation de ces options requiert donc une attention particulière.

Pour plus d'informations sur les instantanés et les clones, reportez-vous au Chapitre 7,
“Utilisation des instantanés et des clones ZFS”.

Modification du nom d'un système de fichiers ZFS
La modification du nom d'un système de fichiers ZFS s'effectue à l'aide de la commande zfs
rename. Cette commande permet d'effectuer les opérations suivantes : Modification du nom
■ modifier le nom d'un système de fichiers ;
■ modifier l'emplacement d'un système de fichiers au sein de la hiérarchie ZFS ;
■ modifier le nom d'un système de fichiers et son emplacement au sein de la hiérarchie ZFS.

L'exemple ci-dessous illustre la modification du nom d'un système de fichiers à l'aide de la
sous-commande rename :

# zfs rename tank/home/kustarz tank/home/kustarz_old

Cette commande entraîne le remplacement du nom du système de fichiers kustarz par
kustarz_old.

L'exemple ci-dessous illustre la modification de l'emplacement d'un système de fichiers à l'aide
de la sous-commande zfs rename.

# zfs rename tank/home/maybee tank/ws/maybee

Dans cet exemple, le système de fichiers maybee est déplacé de tank/home vers tank/ws.
Lorsque vous modifiez l'emplacement d'un système de fichiers à l'aide de la commande rename,
le nouvel emplacement doit se trouver au sein du même pool et l'espace disponible doit être

178

Guide d'administration Solaris ZFS • Octobre 2009

Présentation des propriétés ZFS

suffisant pour contenir le nouveau système de fichiers. Si l'espace disponible du nouvel
emplacement est insuffisant (par exemple, si le quota d'espace disque est atteint), l'opération
échoue.
Pour plus d'informations sur les quotas, reportez-vous à la section “Définition des quotas et
réservations ZFS” à la page 208.
L'opération de modification du nom tente de démonter, puis de remonter le système de fichiers
ainsi que ses éventuels systèmes de fichiers descendants. Si la commande ne parvient pas à
démonter un système de fichiers actif, l'opération échoue. Dans ce cas, vous devez forcer le
démontage du système de fichiers.
Pour plus d'informations sur la modification du nom des instantanés, reportez-vous à la section
“Renommage d'instantanés ZFS” à la page 217.

Présentation des propriétés ZFS

Les propriétés constituent le mécanisme principal de contrôle du comportement des systèmes
de fichiers, des volumes, des instantanés et des clones. Sauf mention contraire, les propriétés
définies dans la section s'appliquent à tous les types de jeu de données.
■ “Propriétés ZFS natives en lecture seule” à la page 188
■ “Propriétés ZFS natives définies” à la page 189
■ “Propriétés ZFS définies par l'utilisateur” à la page 192
Les propriétés se divisent en deux catégories : les propriétés natives et les propriétés définies par
l'utilisateur. Les propriétés natives permettent d'exporter des statistiques internes ou de
contrôler le comportement des systèmes de fichiers ZFS. Certaines de ces propriétés peuvent
être définies tandis que d'autres sont en lecture seule. Les propriétés définies par l'utilisateur
n'ont aucune incidence sur le comportement des systèmes de fichiers ZFS. En revanche, elles
permettent d'annoter les jeux de données avec des informations adaptées à votre
environnement. Pour plus d'informations sur les propriétés définies par l'utilisateur,
reportez-vous à la section “Propriétés ZFS définies par l'utilisateur” à la page 192.
La plupart des propriétés pouvant être définies peuvent également être héritées. Les propriétés
pouvant être héritées sont des propriétés qui, une fois définies sur un système parent, peuvent
être appliquées à l'ensemble des descendants de ce parent.
Toutes ces propriétés sont associées à une source. Cette source indique la manière dont la
propriété a été obtenue. Les sources de propriétés peuvent être définies sur les valeurs
suivantes :

local

Une source définie sur la valeur local indique que la
propriété a été définie de manière explicite sur le jeu de
données à l'aide de la commande zfs set, selon la
procédure décrite à la section “Définition des propriétés
ZFS” à la page 196.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

179

Présentation des propriétés ZFS

inherited from nom-jeu-données

default

La valeur inherited from nom-jeu-données signifie que
la propriété a été héritée du système ascendant indiqué.
La valeur default signifie que le paramètre de la
propriété n'a été ni hérité, ni défini de manière locale.
Cette source est définie lorsque la propriété n'est
associée à la source local sur aucun système ascendant.

Le tableau suivant répertorie les propriétés de système de fichiers ZFS natives en lecture seule et
pouvant être définies. Les propriétés natives en lecture seule sont signalées comme tel. Les
autres propriétés natives répertoriées dans le tableau peuvent être définies. Pour plus
d'informations sur les propriétés définies par l'utilisateur, reportez-vous à la section “Propriétés
ZFS définies par l'utilisateur” à la page 192.

TABLEAU 6–1 Description des propriétés ZFS natives

Nom de la propriété

Type

aclinherit

Chaîne

Valeur par
défaut

secure

Description

Contrôle le processus d'héritage des entrées ACL lors de
la création de fichiers et de répertoires. Les valeurs
possibles sont discard, noallow, secure et
passthrough. Pour une description de ces valeurs,
reportez-vous à la section “Modes de propriétés d'ACL”
à la page 237.

aclmode

Chaîne

groupmask Contrôle le processus de modification des entrées ACL
lors des opérations chmod. Les valeurs possibles sont
discard, groupmask et passthrough. Pour une
description de ces valeurs, reportez-vous à la section
“Modes de propriétés d'ACL” à la page 237.

atime

Booléen

on

Détermine si l'heure d'accès aux fichiers est mise à jour
lorsqu'ils sont consultés. La désactivation de cette
propriété évite de produire du trafic d'écriture lors de la
lecture de fichiers et permet parfois d'améliorer
considérablement les performances ; elle risque
cependant de perturber les logiciels de messagerie et
autres utilitaires du même type.

180

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

Présentation des propriétés ZFS

available

Valeur
numérique

SO

canmount

Booléen

on

Somme de contrôle

Chaîne

on

Propriété en lecture seule indiquant la quantité d'espace
disponible pour le jeu de données et l'ensemble des
systèmes enfant, sans tenir compte des autres activités du
pool. L'espace étant partagé au sein d'un pool, l'espace
disponible peut être limité par divers facteurs, y compris
la taille du pool physique, les quotas, les réservations ou
les autres jeux de données présents au sein du pool.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté avail.
Pour plus d'informations sur la détermination de l'espace
disque, reportez-vous à la section “Comptabilisation de
l'espace ZFS” à la page 56.

Détermine si le système de fichiers peut être monté à
l'aide de la commande zfs mount. Cette propriété peut
être définie sur tous les systèmes de fichiers et ne peut pas
être héritée. En revanche, lorsque cette propriété est
définie sur off, un point de montage peut être hérité par
des systèmes de fichiers descendants. Le système de
fichiers à proprement parler n'est toutefois pas monté.
Lorsque l'option noauto est définie, un jeu de données ne
peut être monté et démonté que de manière explicite. Le
jeu de données n'est pas monté automatiquement
lorsqu'il est créé ou importé, et n'est pas monté par la
commande zfs mount- a ni démonté par la commande
zfs unmount-a.
Pour plus d'informations, reportez-vous à la section
“Propriété canmount” à la page 190.

Détermine la somme de contrôle permettant de vérifier
l'intégrité des données. La valeur par défaut est définie
sur on. Cette valeur permet de sélectionner
automatiquement l'algorithme approprié, actuellement
fletcher2. Les valeurs possibles sont on, off,
fletcher2, fletcher4 et sha256. La valeur off entraîne
la désactivation du contrôle d'intégrité des données
utilisateur. La valeur off n'est pas recommandée.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

181

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

compression

Chaîne

off

compressratio

Valeur
numérique

SO

copies

Valeur
numérique

1

creation

Chaîne

SO

devices

exec

Booléen

Booléen

on

on

Active ou désactive la compression de ce jeu de données.
Les valeurs sont on , off et lzjb, gzip ou gzip-N.
Donner à cette propriété la valeur lzjb, gzip ou la valeur
gzip- N a actuellement le même effet que la valeur on. La
valeur par défaut est off. L'activation de la compression
sur un système de fichiers contenant des données
existantes entraîne uniquement la compression des
nouvelles données. Les données actuelles restent non
compressées.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté compress.

Propriété en lecture seule indiquant le ratio de
compression obtenu pour le jeu de données, exprimé
sous la forme d'un multiple. La compression peut être
activée en exécutant zfs set compression=on jeu de
données.
Ce ratio est calculé sur la base de la taille logique de
l'ensemble des fichiers et de la quantité de données
physiques indiquée. La propriété induit un gain explicite
basé sur l'utilisation de la propriété compression.

Définit le nombre de copies des données utilisateur par
système de fichiers. Les valeurs disponibles sont 1, 2 et 3.
Ces copies viennent s'ajouter à toute redondance au
niveau du pool. L'espace utilisé par plusieurs copies de
données utilisateur est chargé dans le fichier et le jeu de
données correspondants et pénalise les quotas et les
réservations. En outre, la propriété used est mise à jour
lorsque plusieurs copies sont activées. Considérez la
définition de cette propriété à la création du système de
fichiers car lorsque vous la modifiez sur un système de
fichiers existant, les modifications ne s'appliquent qu'aux
nouvelles données.

Propriété en lecture seule identifiant la date et l'heure de
création de ce jeu de données.

Contrôle la capacité à ouvrir les fichiers des
périphériques dans le système de fichiers.

Contrôle l'autorisation d'exécuter les programmes
contenus dans le système de fichiers. Par ailleurs,
lorsqu'elle est définie sur off, les appels de la commande
mmap(2) avec PROT_EXEC ne sont pas autorisés.

182

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

Présentation des propriétés ZFS

mounted

Booléen

SO

mountpoint

Chaîne

SO

primarycache

Chaîne

off

origin

Chaîne

SO

Propriété en lecture seule indiquant si le système de
fichiers, le clone ou l'instantané est actuellement monté.
Cette propriété ne s'applique pas aux volumes. Les
valeurs possibles sont yes ou no.

Détermine le point de montage utilisé pour le système de
fichiers. Lorsque la propriété mountpoint d'un système
de fichiers est modifiée, ce système de fichiers ainsi que
les éventuels systèmes enfant héritant du point de
montage sont démontés. Si la nouvelle valeur est définie
sur legacy, ces systèmes restent démontés. Dans le cas
contraire, ils sont automatiquement remontés au nouvel
emplacement si la propriété était précédemment définie
sur legacy ou sur none ou s'ils étaient montés avant la
modification de la propriété. D'autre part, le partage de
tout système de fichiers est annulé puis rétabli au nouvel
emplacement.
Pour plus d'informations sur l'utilisation de cette
propriété, reportez-vous à la section “Gestion des points
de montage ZFS” à la page 201.

Contrôle les éléments qui sont mis en cache dans le cache
principal (ARC). Les valeurs possibles sont all, none et
metadata. Si elle est définie sur all, les données
d'utilisateur et les métadonnées sont mises en cache. Si
elle est définie sur none, ni les données d'utilisateur ni les
métadonnées ne sont mises en cache. Si elle est définie
sur metadata, seules les métadonnées sont mises en
cache. La valeur par défaut est all.

Propriété en lecture seule appliquée aux systèmes de
fichiers ou aux volumes clonés et indiquant l'instantané à
partir duquel le clone a été créé. Le système d'origine ne
peut pas être détruit (même à l'aide des options -r ou -f)
tant que le clone existe.
Les systèmes de fichiers non clonés n'indique aucune
origine.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

183

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Nom de la propriété

Type

quota

Valeur
numérique
(ou none)

Valeur par
défaut

none

readonly

Booléen

off

recordsize

Valeur
numérique

128K

referenced

Valeur
numérique

SO

Description

Limite la quantité d'espace disponible pour le jeu de
données et ses descendants. Cette propriété permet
d'appliquer une limite fixe à la quantité d'espace utilisée,
y compris l'espace utilisé par les descendants, qu'il
s'agisse de systèmes de fichiers ou d'instantanés. La
définition d'un quota sur un descendant d'un jeu de
données déjà associé à un quota n'entraîne pas le
remplacement du quota du système ascendant. Cette
opération entraîne au contraire l'application d'une limite
supplémentaire. Les quotas ne peuvent pas être définis
pour les volumes car la propriété volsize sert de quota
implicite.
Pour plus d'informations concernant la définition de
quotas, reportez-vous à la section “Définitions de quotas
sur les systèmes de fichiers ZFS” à la page 209.

Contrôle l'autorisation de modifier le jeu de données.
Lorsqu'elle est définie sur on, aucune modification ne
peut être apportée au jeu de données.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté rdonly.

Spécifie une taille de bloc suggérée pour les fichiers du
système de fichiers.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté recsize. Pour obtenir une
description détaillée de cette propriété, reportez-vous à la
section “Propriété recordsize” à la page 191.

Propriété en lecture seule identifiant la quantité de
données à laquelle ce jeu de données a accès, lesquelles
peuvent être ou non partagées avec d'autres jeux de
données du pool.
Lorsqu'un instantané ou un clone est créé, il indique dans
un premier temps la même quantité d'espace que le
système de fichiers ou l'instantané à partir duquel il a été
créé. En effet, son contenu est identique.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté refer.

184

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Présentation des propriétés ZFS

Nom de la propriété

Type

refquota

Valeur
numérique
(ou none)

Valeur par
défaut

none

refreservation

Valeur
numérique
(ou none)

none

reservation

Valeur
numérique
(ou none)

none

secondarycache

Chaîne

off

Description

Définit la quantité d'espace pouvant être utilisé par un jeu
de données. Cette propriété définit une quantité d'espace
maximale. Cette limite maximale n'inclut pas l'espace
utilisé par les descendants, notamment les instantanés et
les clones.

Définit la quantité d'espace minimale garantie pour un
jeu de données, à l'exclusion des descendants,
notamment les instantanés et les clones. Lorsque la
quantité d'espace utilisée est inférieure à cette valeur, le
système considère que le jeu de donnée utilise la quantité
d'espace spécifiée par refreservation La réservation
refreservation est prise en compte dans l'espace utilisé
des jeux de donnés parent et vient en déduction de leurs
quotas et réservations.
Lorsque la propriété refreservation est définie, un
instantané n'est autorisé que si suffisamment d'espace est
disponible dans le pool au-delà de cette réservation afin
de pouvoir contenir le nombre actuel d'octets référencés
dans le jeu de données.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté refreserv.

Quantité minimale d'espace réservée pour un jeu de
données et ses descendants. Lorsque la quantité d'espace
utilisée est inférieure à la valeur de cette propriété, le
système considère que le jeu de donnée utilise la quantité
d'espace réservée. Les réservations sont prises en compte
dans l'espace utilisé des jeux de données parent et
viennent en déduction de leurs quotas et réservations.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté reserv.
Pour plus d'informations, reportez-vous à la section
“Définition de réservations sur les systèmes de fichiers
ZFS” à la page 213.

Contrôle les éléments qui sont mis en cache dans le cache
secondaire (L2ARC). Les valeurs possibles sont all, none
et metadata. Si elle est définie sur all, les données
d'utilisateur et les métadonnées sont mises en cache. Si
elle est définie sur none, ni les données d'utilisateur ni les
métadonnées ne sont mises en cache. Si elle est définie
sur metadata, seules les métadonnées sont mises en
cache. La valeur par défaut est all.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

185

Présentation des propriétés ZFS

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Nom de la propriété

Type

Valeur par
défaut

Description

setuid

sharenfs

Booléen

on

Chaîne

off

snapdir

Chaîne

hidden

type

Chaîne

SO

used

Valeur
numérique

SO

usedbychildren

usedbydataset

Valeur
numérique

off

Valeur
numérique

off

Contrôle l'application du bit setuid dans le système de
fichiers.

Détermine si le système de fichiers est disponible via
NFS, ainsi que les options utilisées. Si cette propriété est
définie sur on, la commande zfs share est exécutée sans
option. Dans le cas contraire, la commande zfs share
est exécutée avec les options équivalentes au contenu de
cette propriété. Si elle est définie sur off, le système de
fichiers est géré à l'aide des commandes héritées share et
unshare et du fichier dfstab file.
Pour plus d'informations sur le partage des systèmes de
fichiers ZF, reportez-vous à la section “Activation et
annulation du partage des systèmes de fichiers ZFS”
à la page 206.

Détermine si le répertoire .zfs doit être affiché ou
masqué au niveau de la racine du système de fichiers.
Pour plus d'informations sur l'utilisation des instantanés,
reportez-vous à la section “Présentation des instantanés
ZFS” à la page 215.

Propriété en lecture seule identifiant le type de jeu de
données comme étant un système de fichiers,
(filesystem ; système de fichiers à proprement parler ou
clone), un volume (volume) ou un instantané
(snapshot).

Propriété en lecture seule identifiant la quantité d'espace
utilisée par le jeu de données et tous ses descendants.
Pour obtenir une description détaillée de cette propriété,
reportez-vous à la section “Propriété used” à la page 188.

Propriété en lecture seule indiquant la quantité d'espace
utilisée par les enfants de ce jeu de données, qui serait
libérée si tous ses enfants étaient détruits. L'abréviation
de la propriété est usedchild.

Propriété en lecture seule indiquant la quantité d'espace
utilisée par le jeu de données lui-même, qui serait libérée
si ce dernier était détruit, après la destruction préalable
de tous les instantanés et la suppression de toutes les
valeurs refreservation. L'abréviation de la propriété est
usedds.

186

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 6–1 Description des propriétés ZFS natives

(Suite)

Présentation des propriétés ZFS

Nom de la propriété

Type

usedbyrefreservation Valeur

numérique

Valeur par
défaut

off

usedbysnapshots

Valeur
numérique

off

volsize

Valeur
numérique

SO

volblocksize

Valeur
numérique

8 Kbytes

zoned

Booléen

SO

xattr

Booléen

on

Description

Propriété en lecture seule indiquant la quantité d'espace
utilisée par un jeu refreservation sur ce jeu de données,
qui serait libérée si le jeu refreservation était supprimé.
L'abréviation de la propriété est usedrefreserv.

Propriété en lecture seule indiquant la quantité d'espace
utilisée par les instantanés de ce jeu de données. En
particulier, elle correspond à la quantité d'espace qui
serait libérée si l'ensemble des instantanés de ce jeu de
données était supprimé. Notez qu'il ne s'agit pas
simplement de la somme des propriétés used des
instantanés, car l'espace peut être partagé par plusieurs
instantanés. L'abréviation de la propriété est usedsnap.

Spécifie la taille logique des volumes.
Pour obtenir une description détaillée de cette propriété,
reportez-vous à la section “Propriété volsize”
à la page 191.

(Volumes) Spécifie la taille de bloc du volume. Une fois
que des données ont été écrites sur un volume, la taille de
bloc ne peut plus être modifiée. Vous devez donc définir
cette valeur lors de la création du volume. La taille de bloc
par défaut des volumes est de 8 Ko. Toute puissance de
deux comprise entre 512 octets et 128 Kilo-octets est
correcte.
Cette propriété peut également s'afficher sous la forme du
nom de colonne contracté volblock.

Indique si le jeu de données a été ajouté à une zone non
globale. Si cette propriété est activée, le point de montage
ne figure pas dans la zone globale et le système ZFS ne
peut pas monter le système de fichiers en réponse aux
requêtes. Lors de la première installation d'une zone,
cette propriété est définie pour tout système de fichiers
ajouté.
Pour plus d'informations sur l'utilisation du système ZFS
avec des zones installées, reportez-vous à la section
“Utilisation de ZFS dans un système Solaris avec zones
installées” à la page 278.

Indique si les attributs étendus sont activés ou désactivés
pour le système de fichiers. La valeur par défaut est on.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

187

Présentation des propriétés ZFS

Propriétés ZFS natives en lecture seule
Les propriétés natives en lecture seule peuvent être récupérées, mais il est impossible de les
modifier. Elles ne peuvent pas non plus être héritées. Certaines propriétés natives sont
spécifiques à un type de jeu de données. Dans ce cas, le type de jeu de données correspondant est
mentionné dans la description figurant dans le Tableau 6–1.

Les propriétés natives en lecture seule sont répertoriées dans cette section et décrites dans le
Tableau 6–1.

■

■

■

■

■

■

■

■

■

■

■

■

available

creation

mounted

origin

compressratio

referenced

type

used

Pour plus d'informations sur cette propriété, reportez-vous à la section “Propriété used”
à la page 188.

usedbychildren

usedbydataset

usedbyrefreservation

usedbysnapshots

Pour plus d'informations sur la détermination de l'espace disque, notamment sur les propriétés
used, referenced et available, reportez-vous à la section “Comptabilisation de l'espace ZFS”
à la page 56.

Propriété used
Quantité d'espace utilisée par le jeu de données et l'ensemble de ses descendants. Cette valeur
est comparée au quota et à la réservation définis pour le jeu de données. L'espace utilisé n'inclut
pas la réservation du jeu de données. En revanche, elle prend en compte les réservations définies
pour les éventuels jeux de données descendants. La quantité d'espace utilisée sur le parent par
un jeu de données, ainsi que la quantité d'espace libérée si le jeu de données est détruit de façon
récursive, constituent la plus grande partie de son espace utilisé et sa réservation.

Lors de la création d'un instantané, l'espace correspondant est dans un premier temps partagé
entre cet instantané et le système de fichiers ainsi que les instantanés existants (le cas échéant).
Lorsque le système de fichiers est modifié, l'espace précédemment partagé devient dédié à
l'instantané. Il est alors comptabilisé dans l'espace utilisé par cet instantané. L'espace utilisé par

188

Guide d'administration Solaris ZFS • Octobre 2009

Présentation des propriétés ZFS

un instantané représente ses données uniques. La suppression d'instantanés peut également
augmenter l'espace dédié et utilisé par les autres instantanés. Pour plus d'informations sur les
instantanés et les questions d'espace, reportez-vous à la section “Comportement d'espace
saturé” à la page 56.

La quantité d'espace utilisée, disponible ou indiquée ne prend pas en compte les modifications
en cours d'exécution. Ces modifications sont généralement prises en compte au bout de
quelques secondes. La modification d'un disque utilisant fsync(3c) ou O_SYNC ne garantit pas
la mise à jour immédiate des informations concernant l'utilisation de l'espace.

Les informations de propriété usedbychildren, usedbydataset , usedbyrefreservation et
usedbysnapshots peuvent être affichées à l'aide de la commande zfs list -o space. Ces
propriétés divisent la propriété used en espace utilisé par les descendants. Pour plus
d'informations, reportez-vous au Tableau 6–1.

Propriétés ZFS natives définies
Les propriétés natives définies sont les propriétés dont les valeurs peuvent être récupérées et
modifiées. La définition des propriétés natives s'effectue à l'aide de la commande zfs set, selon
la procédure décrite à la section “Définition des propriétés ZFS” à la page 196 ou à l'aide de la
commande zfs create, selon la procédure décrite à la section “Création d'un système de
fichiers ZFS” à la page 176. À l'exception des quotas et des réservations, les propriétés natives
définies sont héritées. Pour plus d'informations sur les quotas et les réservations, reportez-vous
à la section “Définition des quotas et réservations ZFS” à la page 208.

Certaines propriétés natives définies sont spécifiques à un type de jeu de données. Dans ce cas,
le jeu de données correspondant est mentionné dans la description figurant dans le
Tableau 6–1. Sauf indication contraire, les propriétés s'appliquent à tous les types de jeu de
données : aux systèmes de fichiers, aux volumes, aux clones et aux instantanés.

Les propriétés pouvant être définies sont répertoriées dans cette section et décrites dans le
Tableau 6–1.

■

■

■

■

■

■

aclinherit

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section “Modes
de propriétés d'ACL” à la page 237.

aclmode

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section “Modes
de propriétés d'ACL” à la page 237.

atime

canmount

Somme de contrôle

compression

Chapitre 6 • Gestion des systèmes de fichiers ZFS

189

Présentation des propriétés ZFS

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

copies

devices

exec

mountpoint

primarycache

quota

readonly

recordsize

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section
“Propriété recordsize” à la page 191.

refquota

refreservation

reservation

secondarycache

sharenfs

setuid

snapdir

volsize

Pour obtenir une description détaillée de cette propriété, reportez-vous à la section
“Propriété volsize” à la page 191.

volblocksize

zoned

Propriété canmount
Si cette propriété est désactivée (valeur "off"), le système de fichiers ne peut pas être monté à
l'aide de la commande zfs mount ni de la commande zfs mount -a. Cette propriété est
comparable à la définition de la propriété mountpoint sur la valeur none. Toutefois, le jeu de
données possède toujours une propriété mountpoint standard susceptible d'être héritée. Vous
pouvez par exemple définir cette propriété sur la valeur "off" et définir des propriétés héritées
pour les systèmes de fichiers descendants. Toutefois, le système de fichiers à proprement parler
n'est jamais monté, ni accessible par les utilisateurs. Dans ce cas, le système de fichiers parent
sur lequel cette propriété est désactivée sert de conteneur afin de pouvoir définir des attributs
sur le conteneur ; toutefois, le conteneur à proprement parler n'est jamais accessible.

L'exemple suivant illustre la création du système de fichiers userpool avec la propriété
canmount désactivée. Les points de montage des systèmes de fichiers utilisateur descendants

190

Guide d'administration Solaris ZFS • Octobre 2009

sont définis sur un emplacement commun, /export/home. Les systèmes de fichiers descendants
héritent des propriétés définies sur le système de fichiers parent, mais celui-ci n'est jamais
monté.

Présentation des propriétés ZFS

# zpool create userpool mirror c0t5d0 c1t6d0

# zfs set canmount=off userpool

# zfs set mountpoint=/export/home userpool

# zfs set compression=on userpool

# zfs create userpool/user1

# zfs create userpool/user2

# zfs list -r userpool

NAME

USED AVAIL REFER MOUNTPOINT

userpool

140K 8.24G 24.5K /export/home

userpool/user1 24.5K 8.24G 24.5K /export/home/user1

userpool/user2 24.5K 8.24G 24.5K /export/home/user2

Définir la propriété canmount sur noauto indique que le jeu de données ne peut être monté que
de manière explicite, et non pas de manière automatique. Ce paramètre est utilisé par le logiciel
de mise à niveau de Solaris afin que seuls les jeux de données appartenant à l'environnement
d'initialisation actif soient montés lors de l'initialisation.

Propriété recordsize
Spécifie une taille de bloc suggérée pour les fichiers du système de fichiers.

Cette propriété s'utilise uniquement pour les charges de travail de base de données accédant à
des fichiers résidant dans des enregistrements à taille fixe. Le système ZFS ajuste
automatiquement les tailles en fonction d'algorithmes internes optimisés pour les schémas
d'accès classiques. Pour les bases de données générant des fichiers volumineux mais accédant
uniquement à certains fragments de manière aléatoire, ces algorithmes peuvent se révéler
inadaptés. La définition de la propriété recordsize sur une valeur supérieure ou égale à la taille
d'enregistrement de la base de données peut améliorer les performances du système de manière
significative. Il est vivement déconseillé d'utiliser cette propriété pour les systèmes de fichiers à
usage générique. En outre, elle peut affecter les performances du système. La taille spécifiée doit
être une puissance de deux supérieure ou égale à 512 octets et inférieure ou égale à 128 Ko. La
modification de la valeur de recordsize affecte uniquement les fichiers créés en aval. Cette
modification n'affecte pas les fichiers existants.

Cette propriété peut également s'afficher sous la forme du nom de colonne contracté recsize.

Propriété volsize
Taille logique du volume. Par défaut, la création d'un volume définit une réservation de taille
identique. Toute modification apportée à la valeur de la propriété volsize se répercute dans des
proportions identiques au niveau de la réservation. Ce fonctionnement permet d'éviter les
comportements inattendus lors de l'utilisation des volumes. L'utilisation de volumes contenant

Chapitre 6 • Gestion des systèmes de fichiers ZFS

191

Présentation des propriétés ZFS

moins d'espace disponible que la valeur indiquée risque, suivant le cas, d'entraîner des
comportements non valides et des corruptions de données. Ces symptômes peuvent également
survenir lors de la modification et notamment de la réduction de la taille d'un volume en cours
d'utilisation. Faites preuve de prudence lorsque vous ajustez la taille d'un volume.

Même s'il s'agit d'une opération déconseillée, vous avez la possibilité de créer des volumes
fragmentés. Pour ce faire, spécifiez l'attribut -s dans la commande zfs create -V ou modifiez
la réservation, une fois le volume créé. Le terme volume fragmenté désigne un volume dont la
réservation est différente de la taille de volume. Les modifications apportées à la propriété
volsize des volumes fragmentés ne sont pas répercutées au niveau de la réservation.

Pour plus d'informations sur l'utilisation des volumes, reportez-vous à la section “Volumes
ZFS” à la page 275.

Propriétés ZFS définies par l'utilisateur
Outre les propriétés natives standard, le système ZFS prend en charge des propriétés définies
par l'utilisateur. Les propriétés définies par l'utilisateur n'ont aucune incidence sur le
comportement du système ZFS. En revanche, elles permettent d'annoter les jeux de données
avec des informations adaptées à votre environnement.

Les noms de propriétés définies par l'utilisateur doivent respecter les conventions suivantes :

■

■

contenir le caractère ": " (deux points) afin de les distinguer des propriétés natives ;
contenir des lettres en minuscules, des chiffres, ainsi que les signes de ponctuation suivants :
':', '+','.', '_' ;

■ posséder un nom dont la longueur maximale est de 256 caractères.

La syntaxe attendue des noms de propriétés consiste à regrouper les deux composants suivants
(cet espace de noms n'est toutefois pas appliqué par les systèmes ZFS) :

module:property

Si vous utilisez des propriétés définies par l'utilisateur dans un contexte de programmation,
spécifiez un nom de domaine DNS inversé pour le composant module des noms de propriétés,
afin de réduire la probabilité que deux packages développés séparément n'utilisent un nom de
propriété identique à des fins différentes. Les noms de propriété commençant par "com.sun."
sont réservés à l'usage de Sun Microsystems.

Les valeurs des propriétés définies par l'utilisateur possèdent les caractéristiques suivantes :

■

Il s'agit de chaînes arbitraires systématiquement héritées et jamais validées.

■ Leur valeur se constitue de 1 024 caractères maximum.

Exemple :

192

Guide d'administration Solaris ZFS • Octobre 2009

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

# zfs set dept:users=finance userpool/user1

# zfs set dept:users=general userpool/user2

# zfs set dept:users=itops userpool/user3

Toutes les commandes fonctionnant avec des propriétés (par exemple, les commandes zfs
list, zfs get, zfs set, etc.) permettent d'utiliser des propriétés natives et des propriétés
définies par l'utilisateur.
Exemple :

zfs get -r dept:users userpool

NAME

PROPERTY

VALUE

SOURCE

userpool

dept:users all

userpool/user1 dept:users finance

userpool/user2 dept:users general

userpool/user3 dept:users itops

local

local

local

local

Pour supprimer une propriété définie par l'utilisateur, utilisez la commande zfs inherit.
Exemple :

# zfs inherit -r dept:users userpool

Si cette propriété n'est définie dans aucun jeu de données parent, elle est définitivement
supprimée.

Envoi de requêtes sur les informations des systèmes de
fichiers ZFS

La commande zfs list contient un mécanisme extensible permettant d'afficher et d'envoyer
des requêtes sur les informations des systèmes de fichiers. Cette section décrit les requêtes de
base ainsi que les requêtes plus complexes.

Affichage des informations de base des systèmes ZFS
La commande zfs list spécifiée sans option permet de répertorier les informations de base
sur les jeux de données. Cette commande affiche le nom de tous les jeux de données définis sur
le système ainsi que les propriétés used, available, referenced et mountpoint
correspondantes. Pour plus d'informations sur ces propriétés, reportez-vous à la section
“Présentation des propriétés ZFS” à la page 179.
Exemple :

# zfs list

NAME

pool

USED AVAIL REFER MOUNTPOINT

476K 16.5G

21K /pool

Chapitre 6 • Gestion des systèmes de fichiers ZFS

193

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

pool/clone

18K 16.5G

18K /pool/clone

pool/home

296K 16.5G

19K /pool/home

pool/home/marks

277K 16.5G

277K /pool/home/marks

pool/home/marks@snap

0

-

277K -

pool/test

18K 16.5G

18K /test

Cette commande permet d'afficher des jeux de données spécifiques. Pour cela, spécifiez le nom
du ou des jeux de données à afficher sur la ligne de commande. Vous pouvez également spécifier
l'option -r pour afficher de manière récursive tous les descendants des jeux de données.
Exemple :

# zfs list -r pool/home/marks

NAME

USED AVAIL REFER MOUNTPOINT

pool/home/marks

277K 16.5G

277K /pool/home/marks

pool/home/marks@snap

0

-

277K -

La commande zfs list s'utilise avec des noms de chemin absolus pour les jeux de données, les
instantanés et les volumes. Exemple :

# zfs list /pool/home/marks

NAME

USED AVAIL REFER MOUNTPOINT

pool/home/marks

277K 16.5G

277K /pool/home/marks

L'exemple suivant illustre la manière d'afficher tank/home/chua et tous ses jeux de données
descendants.

# zfs list -r tank/home/chua

NAME

USED AVAIL REFER MOUNTPOINT

tank/home/chua

26.0K 4.81G 10.0K /tank/home/chua

tank/home/chua/projects

16K 4.81G

9.0K /tank/home/chua/projects

tank/home/chua/projects/fs1

8K 4.81G

8K /tank/home/chua/projects/fs1

tank/home/chua/projects/fs2

8K 4.81G

8K /tank/home/chua/projects/fs2

Pour plus d'informations sur la commande zfs list, reportez-vous à la page de manuel
zfs(1M).

Création de requêtes ZFS complexes
Les options -o, -f et -H permettent de personnaliser la sortie de la commande zfs list.
Vous pouvez également personnaliser la sortie des valeurs de propriété en spécifiant l'option -o
ainsi que la liste des propriétés souhaitées séparées par une virgule. Les valeurs valides sont les
propriétés de jeu de données. Pour consulter la liste de toutes les propriétés de jeu de données
prises en charge, reportez-vous à la section “Présentation des propriétés ZFS” à la page 179.
Outre les propriétés répertoriées dans cette section, la liste de l'option -o peut également
contenir la valeur littérale name afin de définir l'inclusion du nom de jeu de données dans la
sortie.

194

Guide d'administration Solaris ZFS • Octobre 2009

Envoi de requêtes sur les informations des systèmes de fichiers ZFS

Les exemples suivants illustrent l'utilisation de la commande zfs list permettant d'afficher le
nom de jeu de données et des propriétés sharenfs et mountpoint.

# zfs list -o name,sharenfs,mountpoint

NAME

tank

tank/home

tank/home/ahrens

tank/home/bonwick

tank/home/chua

tank/home/eschrock

tank/home/moore

tank/home/tabriz

SHARENFS

MOUNTPOINT

off

/tank

on

on

on

on

on

on

ro

/tank/home

/tank/home/ahrens

/tank/home/bonwick

/tank/home/chua

legacy

/tank/home/moore

/tank/home/tabriz

L'option -t permet de spécifier le type de jeu de données à afficher. Les types corrects sont
décrits dans le tableau suivant.

TABLEAU 6–2 Types de jeux de données ZFS

Type

filesystem

Volume

Instantané

Description

Systèmes de fichiers et clones

Volumes

Instantanés

L'option -t permet de spécifier la liste des types de jeux de données à afficher, séparés par une
virgule. L'exemple suivant illustre l'affichage du nom et de la propriété -used de l'ensemble des
systèmes de fichiers via l'utilisation simultanée des options -t et o :

# zfs list -t filesystem -o name,used

NAME

pool

pool/clone

pool/home

USED

476K

18K

296K

pool/home/marks

277K

pool/test

18K

L'option -H permet d'exclure l'en-tête de la commande zfs list lors de la génération de la
sortie. Lorsque vous spécifiez l'option -H, les espaces sont générés sous la forme de tabulations.
Cette option permet notamment d'effectuer des analyses sur les sorties (par exemple, des
scripts). L'exemple suivant illustre la sortie de la commande zfs list spécifiée avec l'option -H :

# zfs list -H -o name

pool

pool/clone

pool/home

Chapitre 6 • Gestion des systèmes de fichiers ZFS

195

Gestion des propriétés ZFS

pool/home/marks

pool/home/marks@snap

pool/test

Gestion des propriétés ZFS

La gestion des propriétés de jeu de données s'effectue à l'aide des sous-commandes set,
inherit et get de la commande zfs.
■ “Définition des propriétés ZFS” à la page 196
■ “Héritage des propriétés ZFS” à la page 197
■ “Envoi de requêtes sur les propriétés ZFS” à la page 198

Définition des propriétés ZFS
La commande zfs set permet de modifier les propriétés de jeu de données pouvant être
définies. Vous pouvez également définir les propriétés lors de la création des jeux de données à
l'aide de la commande zfs create. Pour consulter la listes des propriétés de jeu de données
définies, reportez-vous à la section “Propriétés ZFS natives définies” à la page 189. La
commande zfs set utilise une séquence de propriété/valeur au format propriété=valeur et un
nom de jeu de données.

L'exemple suivant illustre la définition de la propriété atime sur la valeur off pour tank/home.
Chaque exécution de la commande zfs set autorise la définition ou la modification d'une seule
propriété.

# zfs set atime=off tank/home

Vous pouvez également définir les propriétés des systèmes de fichiers une fois ces derniers
créés. Exemple :

# zfs create -o atime=off tank/home

Les suffixes explicites suivants (répertoriés par ordre croissant) permettent de spécifier des
propriétés numériques : BKMGTPEZ. Ces suffixes peuvent être suivis de la lettre b (signifiant
"byte", octet) à l'exception du suffixe B, qui fait déjà référence à cette unité de mesure. Les quatre
formulations de la commande zfs set suivantes correspondent à des expressions numériques
équivalentes indiquant que la propriété quota doit être définie sur 50 Go sur le système de
fichiers tank/home/marks :

# zfs set quota=50G tank/home/marks

# zfs set quota=50g tank/home/marks

# zfs set quota=50GB tank/home/marks

# zfs set quota=50gb tank/home/marks

196

Guide d'administration Solaris ZFS • Octobre 2009

Gestion des propriétés ZFS

Les valeurs des propriétés non numériques prennent en charge la distinction
majuscules/minuscules et doivent être spécifiées sous la forme de minuscules, sauf pour les
propriétés mountpoint et sharenfs. Les valeurs de ces propriétés peuvent utiliser des
minuscules et des majuscules.

Pour plus d'informations sur la commande zfs set, reportez-vous à la page de manuel
zfs(1M).

Héritage des propriétés ZFS
Toutes les propriétés définies, à l'exception des propriétés de quotas et de réservations, héritent
de la valeur de leur parent (sauf si un quota ou une réservation est explicitement défini pour le
système enfant). Si aucune valeur explicite n'est définie pour une propriété d'un système
ascendant, la valeur par défaut de cette propriété est appliquée. Vous pouvez utiliser la
commande zfs inherit pour effacer la valeur d'une propriété et faire ainsi hériter la valeur du
parent.

L'exemple suivant illustre l'activation de la compression pour le système de fichiers
tank/home/bonwick à l'aide de la commande zfs set. La commande zfs inherit est ensuite
exécutée afin de supprimer la valeur de la propriété compression, entraînant ainsi l'héritage de
la valeur par défaut off. En effet, la propriété compression n'est définie localement ni pour
home, ni pour tank ; la valeur par défaut est donc appliquée. Si la compression avait été activée
pour ces deux systèmes, la valeur définie pour le système ascendant direct aurait été utilisée (en
l'occurrence, home).

# zfs set compression=on tank/home/bonwick

# zfs get -r compression tank

NAME

tank

PROPERTY

VALUE

compression

off

tank/home

compression

off

tank/home/bonwick compression

on

# zfs inherit compression tank/home/bonwick

# zfs get -r compression tank

NAME

tank

PROPERTY

VALUE

compression

off

tank/home

compression

off

tank/home/bonwick compression off

SOURCE

default

default

local

SOURCE

default

default

default

La sous-commande inherit est appliquée de manière récursive lorsque l'option -r est spécifiée.
Dans l'exemple suivant, la commande entraîne l'héritage de la valeur de la propriété
compression pour tank/home ainsi que pour ses éventuels descendants.

# zfs inherit -r compression tank/home

Chapitre 6 • Gestion des systèmes de fichiers ZFS

197

Gestion des propriétés ZFS

Remarque – L'utilisation de l'option -r entraîne la suppression de la valeur de propriété actuelle
pour l'ensemble des jeux de données descendants.

Pour plus d'informations sur la commande zfs, reportez-vous à la page de manuel zfs(1M).

Envoi de requêtes sur les propriétés ZFS
Le moyen le plus simple pour envoyer une requête sur les valeurs de propriété consiste à
exécuter la commande zfs list. Pour plus d'informations, reportez-vous à la section
“Affichage des informations de base des systèmes ZFS” à la page 193. Cependant, dans le cadre
de requêtes complexes et pour les scripts, utilisez la commande zfs get afin de fournir des
informations plus détaillées dans un format personnalisé.

La commande zfs get permet de récupérer les propriétés de jeu de données. L'exemple suivant
illustre la récupération d'une seule propriété au sein d'un jeu de données :

# zfs get checksum tank/ws

NAME

PROPERTY

VALUE

tank/ws

checksum

on

SOURCE

default

La quatrième colonne, SOURCE, indique l'emplacement à partir duquel la valeur de propriété a
été définie. Le tableau suivant indique la signification des valeurs possibles de la colonne
SOURCE.

TABLEAU 6–3 Valeurs possibles de la colonne SOURCE (zfs get)

Valeur

default

Description

Cette propriété n'a jamais été définie de manière explicite pour ce jeu
de données ni pour ses systèmes ascendants. La valeur par défaut est
utilisée.

inherited from nom-jeu-données

La valeur de propriété est héritée du parent spécifié par la chaîne
nom-jeu-données.

local

temporary

La valeur de propriété a été définie de manière explicite pour ce jeu de
données à l'aide de la commande zfs set.

La valeur de propriété a été définie à l'aide la commande zfs mount
spécifiée avec l'option - o et n'est valide que pour la durée de vie du
montage. Pour plus d'informations sur les propriétés de point de
montage temporaires, reportez-vous à la section “Utilisation de
propriétés de montage temporaires” à la page 205.

- (none)

Cette propriété est en lecture seule. Sa valeur est générée par ZFS.

198

Guide d'administration Solaris ZFS • Octobre 2009

Le mot-clé all permet de récupérer toutes les propriétés de jeu de données. Les exemples
suivants illustrent la récupération de toutes les propriétés de jeu de données à l'aide du mot-clé
all :

Gestion des propriétés ZFS

# zfs get all pool

NAME PROPERTY

VALUE

SOURCE

pool type

filesystem

-

pool creation

Thu Aug 27 9:33 2009 -

pool used

pool available

pool referenced

72K

66.9G

21K

pool compressratio

1.00x

pool mounted

pool quota

pool reservation

pool recordsize

pool mountpoint

pool sharenfs

pool checksum

pool compression

pool atime

pool devices

pool exec

pool setuid

pool readonly

pool zoned

yes

none

none

128K

/pool

off

on

on

on

on

on

on

off

off

pool snapdir

hidden

pool aclmode

groupmask

pool aclinherit

restricted

pool canmount

pool shareiscsi

pool xattr

pool copies

pool version

pool utf8only

pool normalization

on

off

on

1

4

off

none

pool casesensitivity

sensitive

pool vscan

pool nbmand

pool sharesmb

pool refquota

pool refreservation

pool primarycache

pool secondarycache

pool usedbysnapshots

pool usedbydataset

pool usedbychildren

off

off

off

none

none

all

all

0

21K

51K

pool usedbyrefreservation 0

-

-

-

-

-

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

default

-

-

-

-

default

default

default

default

default

default

default

-

-

-

-

Chapitre 6 • Gestion des systèmes de fichiers ZFS

199

Gestion des propriétés ZFS

Remarque – Les propriétés casesensitivity, nbmand, normalization, sharemgrutf8only et
vscan sont définies sur une valeur fixe et ne sont pas prises en charge dans la version Solaris 10.

L'option -s spécifiée avec la commande zfs get permet de spécifier, en fonction du type de
source, les propriétés à afficher. Cette option permet d'indiquer la liste des types de sources
souhaités, séparés par une virgule. Seules les propriétés associées au type de source spécifié sont
affichées. Les types de source valides sont local, default, inherited, temporary et none.
L'exemple suivant indique toutes les propriétés définies localement sur pool.

# zfs get -s local all pool

NAME

pool

PROPERTY

VALUE

compression

on

SOURCE

local

Les options décrites ci-dessus peuvent être associées à l'option -r afin d'afficher de manière
récursive les propriétés spécifiées sur les systèmes enfant du jeu de données. Dans l'exemple
suivant, les propriétés temporaires de tous les jeux de données définis sur tank sont affichées de
manière récursive :

# zfs get -r -s temporary all tank

NAME

PROPERTY

VALUE

tank/home

atime

tank/home/bonwick atime

tank/home/marks

atime

off

off

off

SOURCE

temporary

temporary

temporary

L'une des fonctionnalités récemment ajoutées permet d'envoyer des requêtes à l'aide de la
commande zfs get sans spécifier le système de fichiers cible (la requête porte sur tous les pools
ou sur tous les systèmes de fichiers). Exemple :

# zfs get -s local all

tank/home

atime

tank/home/bonwick

atime

tank/home/marks

quota

off

off

50G

local

local

local

Pour plus d'informations sur la commande zfs get, reportez-vous à la page de manuel
zfs(1M).

Envoi de requête sur les propriétés ZFS pour l'exécution de scripts
La commande zfs get prend en charge les options -H et -o, qui permettent l'exécution de
scripts. L'option -H indique que toute information d'en-tête doit être omise et que tous les
espaces doivent être remplacés par des caractères de tabulation. L'uniformisation des espaces
permet de faciliter l'analyse des données. L'option -o permet de personnaliser la sortie et
d'indiquer la liste des valeurs à inclure dans la sortie en les séparant par une virgule. Toutes les
propriétés définies à la section “Présentation des propriétés ZFS” à la page 179, ainsi que les
termes littéraux name, value, property et source peuvent être fournis dans la liste -o.

200

Guide d'administration Solaris ZFS • Octobre 2009

Montage et partage des systèmes de fichiers ZFS

L'exemple suivant illustre la commande permettant de récupérer une seule valeur en spécifiant
les options -H et -o de la commande zfs get.

# zfs get -H -o value compression tank/home

on

L'option -p indique les valeurs numériques sous leur forme exacte. Par exemple, 1 Mo serait
signalé sous la forme 1000000. Cette option peut être utilisée comme suit :

# zfs get -H -o value -p used tank/home

182983742

L'option -r permet de récupérer de manière récursive les valeurs demandées pour l'ensemble
des descendants et peut s'utiliser avec toutes les options mentionnées ci-dessus. Dans l'exemple
suivant, les options -r, -o et - H sont spécifiées afin de récupérer le nom du jeu de données ainsi
que la valeur de la propriété used pour export/home et ses descendants, tout en excluant les
en-têtes dans la sortie :

# zfs get -H -o name,value -r used export/home

export/home

5.57G

export/home/marks

1.43G

export/home/maybee

2.15G

Montage et partage des systèmes de fichiers ZFS

Cette section décrit le processus de gestion des points de montage et des systèmes de fichiers
partagés dans ZFS.
■ “Gestion des points de montage ZFS” à la page 201
■ “Montage de système de fichiers ZFS” à la page 204
■ “Utilisation de propriétés de montage temporaires” à la page 205
■ “Démontage des systèmes de fichiers ZFS” à la page 205
■ “Activation et annulation du partage des systèmes de fichiers ZFS” à la page 206

Gestion des points de montage ZFS
Par défaut, tous les systèmes de fichiers ZFS sont montés lors de l'initialisation à l'aide du service
SMF (Service Management Facility)svc://system/filesystem/local. Les systèmes de
fichiers sont montés sous /chemin, où chemin correspond au nom du système de fichiers.
Vous pouvez remplacer le point de montage par défaut en définissant la propriété mountpoint
sur un chemin spécifique à l'aide de la commande zfs set. La création du point de montage
(si nécessaire) et le montage du système de fichiers sont gérés automatiquement par le système
ZFS lors de l'exécution de la commande zfs mount -a. Il n'est pas nécessaire de modifier
manuellement le fichier /etc/vfstab.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

201

Montage et partage des systèmes de fichiers ZFS

La propriété mountpoint est héritée. Par exemple, si la propriété mountpoint de pool/home est
définie sur /export/stuff, pool/home/user hérite de la valeur /export/stuff/user pour sa
propriété mountpoint.

La propriété mountpoint peut être définie sur none afin d'empêcher le montage du système de
fichiers. La propriété canmount permet également de déterminer si les systèmes de fichiers
peuvent être montés. Pour plus d'informations sur la propriété canmount, reportez-vous à la
section “Propriété canmount” à la page 190.

Si vous le souhaitez, vous avez également la possibilité de gérer les systèmes de fichiers de
manière explicite à l'aide d'interfaces de montage héritées en définissant la propriété
mountpoint sur legacy dans la commande zfs set. Dans ce cas, le montage et la gestion du
système de fichiers ne sont pas gérés automatiquement par ZFS. Ces opérations s'effectuent
alors à l'aide des outils hérités, comme les commandes mount et umount et le fichier
/etc/vfstab. Pour plus d'informations sur les montages hérités, reportez-vous à la section
“Points de montage hérités” à la page 203.

Lorsque vous modifiez les stratégies de gestion des points de montage, les comportements
suivants s'appliquent :

■

■

comportement de point de montage automatique ;
comportement de point de montage hérité.

Points de montage automatiques

■

■

Si la propriété legacy ou none est remplacée par une autre valeur, le système de fichiers ZFS
est automatiquement monté.
Si le système de fichiers ZFS est géré automatiquement sans être monté et si la propriété
mountpoint est modifiée, le système de fichiers reste démonté.

Vous pouvez également définir le point de montage par défaut du jeu de données racine lors de
l'exécution de la commande de création zpool create en spécifiant l'option -m. Pour plus
d'informations sur la création de pools, reportez-vous à la section “Création d'un pool de
stockage ZFS” à la page 66.

Les jeux de données dont la propriété mountpoint n'est pas définie sur legacy sont gérés par le
système ZFS. L'exemple suivant illustre la création d'un jeu de données dont le point de
montage est géré automatiquement par le système ZFS.

# zfs create pool/filesystem

# zfs get mountpoint pool/filesystem

NAME

PROPERTY

VALUE

SOURCE

pool/filesystem mountpoint

/pool/filesystem

default

# zfs get mounted pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mounted

yes

SOURCE

-

202

Guide d'administration Solaris ZFS • Octobre 2009

Vous pouvez également définir la propriété mountpoint de manière explicite, comme dans
l'exemple suivant :

Montage et partage des systèmes de fichiers ZFS

# zfs set mountpoint=/mnt pool/filesystem

# zfs get mountpoint pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mountpoint

/mnt

# zfs get mounted pool/filesystem

NAME

PROPERTY

VALUE

pool/filesystem mounted

yes

SOURCE

local

SOURCE

-

Si la propriété mountpoint est modifiée, le système de fichiers est automatiquement démonté de
l'ancien point de montage et remonté sur le nouveau. Si nécessaire, les répertoires de point de
montage sont créés automatiquement. Si ZFS n'est pas en mesure de démonter un système de
fichiers parce qu'il est actif, une erreur est signalée et un démontage manuel forcé doit être
effectué.

Points de montage hérités
La gestion des systèmes de fichiers ZFS peut s'effectuer à l'aide d'outils hérités. Pour cela, la
propriété mountpoint doit être définie sur legacy. Les systèmes de fichiers hérités sont alors
gérés à l'aide des commandes mount et umount et du fichier /etc/vfstab. Les systèmes de
fichiers ZFS hérités ne sont pas montés automatiquement lors de l'initialisation et les
commandes ZFS mount et umount ne s'appliquent pas à ces jeux de données. Les exemples
suivants illustrent les commandes de définition et de gestion d'un jeu de données ZFS hérité :

# zfs set mountpoint=legacy tank/home/eschrock

# mount -F zfs tank/home/eschrock /mnt

Pour monter automatiquement un système de fichiers hérité, vous devez ajouter une entrée au
fichier /etc/vfstab. L'exemple suivant montre l'entrée telle qu'elle peut apparaître dans le
fichier /etc/vfstab :

#device

device

mount

FS

fsck

mount

mount

#to mount

to fsck

point

type

pass

at boot options

#

tank/home/eschrock -

/mnt

zfs

-

yes

-

Les entrées device to fsck et fsck pass sont définies sur -. Cette syntaxe s'explique par
l'absence d'application de la commande fsck aux systèmes de fichiers ZFS. Pour plus
d'informations sur l'intégrité des données et sur l'absence d'application de la commande fsck
au système ZFS, reportez-vous à la section “Sémantique transactionnelle” à la page 41.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

203

Montage et partage des systèmes de fichiers ZFS

Montage de système de fichiers ZFS
Le montage des systèmes de fichiers ZFS s'effectue automatiquement lors du processus de
création ou lors de l'initialisation du système. La commande zfs mount s'utilise uniquement
lors de la modification des options de montage ou lors du montage ou du démontage explicite
de systèmes de fichiers.

Spécifiée sans argument, la commande zfs mount répertorie tous les systèmes de fichiers
actuellement montés gérés par ZFS. Les points de montage hérités ne sont pas inclus. Exemple :

# zfs mount

tank

tank/home

/tank

/tank/home

tank/home/bonwick

/tank/home/bonwick

tank/ws

/tank/ws

L'option -a permet de monter tous les systèmes de fichiers ZFS. Les systèmes de fichiers hérités
ne sont pas montés. Exemple :

# zfs mount -a

Par défaut, le système ZFS autorise uniquement le montage sur les répertoires vides. Pour forcer
une opération de montage effectuée sur un répertoire non vide, spécifiez l'option -O. Exemple :

# zfs mount tank/home/lalt

cannot mount ’/export/home/lalt’: directory is not empty

use legacy mountpoint to allow this behavior, or use the -O flag

# zfs mount -O tank/home/lalt

La gestion des points de montage hérités doit s'effectuer à l'aide des outils hérités. Toute
tentative d'utilisation des outils ZFS génère une erreur. Exemple :

# zfs mount pool/home/billm

cannot mount ’pool/home/billm’: legacy mountpoint

use mount(1M) to mount this filesystem

# mount -F zfs tank/home/billm

Le montage d'un système de fichiers requiert l'utilisation d'un ensemble d'options basées sur les
valeurs des propriétés associées au jeu de données. Le tableau ci-dessous illustre la corrélation
entre les propriétés et les options de montage :
Propriété

Options de montage

devices

devices/nodevices

exec

exec/noexec

readonly

ro/rw

204

Guide d'administration Solaris ZFS • Octobre 2009

Montage et partage des systèmes de fichiers ZFS

setuid

setuid/nosetuid

L'option de montage nosuid représente un alias de nodevices,nosetuid.

Utilisation de propriétés de montage temporaires
Si les options sont définies de manière explicite en spécifiant l'option -o avec la commande zfs
mount, les valeurs des propriétés associées sont remplacées de manière temporaire. Ces valeurs
de propriété sont désignées par la chaîne temporary dans la commande zfs get et reprennent
leur valeur d'origine une fois le système de fichiers démonté. Si une valeur de propriété est
modifiée alors que le jeu de données est monté, la modification prend immédiatement effet et
remplace toute valeur temporaire.

L'exemple suivant illustre la définition temporaire de l'option de montage en lecture seule sur le
système de fichiers tank/home/perrin :

# zfs mount -o ro tank/home/perrin

Dans cet exemple, le système de fichiers est censé être démonté. Pour modifier temporairement
une propriété d'un système de fichiers monté, vous devez utiliser l'option spécifique remount.
Dans l'exemple suivant, la propriété atime est temporairement définie sur la valeur off pour un
système de fichiers monté :

# zfs mount -o remount,noatime tank/home/perrin

# zfs get atime tank/home/perrin

NAME

PROPERTY

VALUE

tank/home/perrin atime

off

SOURCE

temporary

Pour plus d'informations sur la commande zfs mount, reportez-vous à la page de manuel
zfs(1M).

Démontage des systèmes de fichiers ZFS
Le démontage des systèmes de fichiers peut s'effectuer à l'aide de la commande zfs unmount.
La commande unmount peut utiliser le point de montage ou le système de fichiers comme
argument.

L'exemple suivant illustre le démontage d'un système de fichiers avec l'argument de nom de
système de fichiers :

# zfs unmount tank/home/tabriz

Chapitre 6 • Gestion des systèmes de fichiers ZFS

205

Montage et partage des systèmes de fichiers ZFS

L'exemple suivant illustre le démontage d'un système de fichiers avec l'argument de point de
montage :

# zfs unmount /export/home/tabriz

Si le système de fichiers est actif ou occupé, la commande unmount échoue. L'option -f permet
de forcer le démontage d'un système de fichiers. Le démontage forcé d'un système de fichiers
requiert une attention particulière si le contenu de ce système est en cours d'utilisation. Ce type
d'opération peut entraîner des comportements d'application imprévisibles.

# zfs unmount tank/home/eschrock

cannot unmount ’/export/home/eschrock’: Device busy

# zfs unmount -f tank/home/eschrock

Pour garantir la compatibilité ascendante, vous pouvez démonter les systèmes de fichiers ZFS à
l'aide de la commande héritée umount. Exemple :

# umount /export/home/bob

Pour plus d'informations sur la commande zfs unmount, reportez-vous à la page de manuel
zfs(1M).

Activation et annulation du partage des systèmes de
fichiers ZFS
Comme pour les points de montage, ZFS permet de partager automatiquement les systèmes de
fichiers. Pour cela, vous devez spécifier la propriété sharenfs. Cette méthode évite d'avoir à
modifier le fichier /etc/dfs/dfstab lors de l'ajout d'un système de fichiers. La propriété
sharenfs correspond à une liste d'options séparées par une virgule spécifiée avec la commande
share. La valeur on constitue un alias des options de partage par défaut, qui attribuent les
droits read/write à tous les utilisateurs. La valeur off indique que le partage du système de
fichiers n'est pas géré par ZFS et qu'il s'effectue à l'aide des outils classiques (par exemple, à l'aide
du fichier /etc/dfs/dfstab). Tous les systèmes de fichiers dont la propriété sharenfs n'est pas
définie sur off sont partagés lors de l'initialisation.

Contrôle de la sémantique de partage
Par défaut, le partage est annulé pour tous les systèmes de fichiers. Pour partager un nouveau
système de fichiers, utilisez une syntaxe de la commande zfs set similaire à celle présentée à
l'exemple ci-dessous :

# zfs set sharenfs=on tank/home/eschrock

206

Guide d'administration Solaris ZFS • Octobre 2009

La propriété est héritée et les systèmes de fichiers sont automatiquement partagés lorsqu'ils sont
créés (si la propriété héritée n'est pas définie sur off). Exemple :

Montage et partage des systèmes de fichiers ZFS

# zfs set sharenfs=on tank/home

# zfs create tank/home/bricker

# zfs create tank/home/tabriz

# zfs set sharenfs=ro tank/home/tabriz

tank/home/bricker et tank/home/tabriz sont partagés en écriture dès leur création, car ils
héritent de la propriété sharenfs définie pour tank/home. Une fois la propriété définie sur ro
(lecture seule), tank/home/tabriz est partagé en lecture seule, quelle que soit la propriété
sharenfs définie pour tank/home.

Annulation du partage des systèmes de fichiers ZFS
L'activation et l'annulation du partage de la plupart de systèmes de fichiers s'effectuent lors de
l'initialisation, de la création ou de la destruction. Toutefois, il peut parfois s'avérer nécessaire
d'annuler explicitement le partage des systèmes de fichiers. Ce type d'opération s'effectue à l'aide
de la commande zfs unshare. Exemple :

# zfs unshare tank/home/tabriz

Cette commande entraîne l'annulation du partage du système de fichiers tank/home/tabriz.
Pour annuler le partage de tous les systèmes de fichiers ZFS du système, vous devez utiliser
l'option - a.

# zfs unshare -a

Partage des systèmes de fichiers ZFS
Le comportement automatique du système ZFS (partage à l'initialisation et à la création)
convient à la plupart des opérations classiques. Si vous êtes amené à annuler le partage d'un
système de fichiers, vous pouvez réactiver le partage à l'aide de la commande zfs share.
Exemple :

# zfs share tank/home/tabriz

L'option - a permet également de partager tous les systèmes de fichiers ZFS du système.

# zfs share -a

Chapitre 6 • Gestion des systèmes de fichiers ZFS

207

Définition des quotas et réservations ZFS

Comportement de partage hérité
Si la propriété sharenfs est définie sur off, l'activation et l'annulation du partage des systèmes
de fichiers ZFS sont désactivées de manière permanente. Cette valeur permet de gérer le partage
des systèmes de fichiers à l'aide des outils classiques (par exemple, à l'aide du fichier
/etc/dfs/dfstab).

Contrairement à la commande classique mount, les commandes share et unshare peuvent être
exécutées sur les systèmes de fichiers ZFS. Vous pouvez dès lors partager manuellement les
systèmes de fichiers en spécifiant des options différentes de celles définies pour la propriété
sharenfs. Ce modèle de gestion est déconseillé. La gestion des partages NFS doit s'effectuer
intégralement à l'aide du système ZFS ou intégralement à l'aide du fichier /etc/dfs/dfstab.
Les modèle ZFS a été conçu pour simplifier et pour faciliter les opérations de gestion par
rapport au modèle classique. Dans certains cas cependant, il peut s'avérer préférable de
contrôler le partage des systèmes de fichiers à l'aide du modèle classique.

Définition des quotas et réservations ZFS

La propriété quota permet de limiter la quantité d'espace disponible pour un système de
fichiers. La propriété reservation permet quant à elle de garantir la disponibilité d'une certaine
quantité d'espace pour un système de fichiers. Ces deux propriétés s'appliquent au jeu de
données sur lequel elles sont définies ainsi qu'à ses descendants.

Par exemple, si un quota est défini pour le jeu de données tank/home, la quantité d'espace totale
utilisée par tank/home et par tous ses descendants ne peut pas excéder le quota défini. De même,
si une réservation est définie pour le jeu de données tank/home, cette réservation s'applique à
tank/home et à tous ses descendants. La quantité d'espace utilisée par un jeu de données et par
tous ses descendants est indiquée par la propriété used.

Les propriétés refquota et refreservation vous permettent de gérer l'espace d'un système de
fichiers sans prendre en compte l'espace utilisé par les descendants, notamment les instantanés
et les clones.

Dans cette version de Solaris, vous pouvez définir un quota d'utilisateur ou de groupe sur la
quantité d'espace utilisée par les fichiers appartenant à un utilisateur ou à un groupe spécifique.
Les propriétés de quota d'utilisateur et de groupe ne peuvent pas être définies sur un volume,
sur un système de fichiers antérieur à la version 4, ou sur un pool antérieur à la version 15.

Considérez les points suivants pour déterminer quelles fonctions de quota et de réservation
conviennent le mieux à la gestion de vos systèmes de fichiers :
■ Les propriétés quota et reservation conviennent à la gestion de l'espace utilisé par les jeux

de données.

208

Guide d'administration Solaris ZFS • Octobre 2009

Définition des quotas et réservations ZFS

■ Les propriétés refquota et refreservation conviennent à la gestion de l'espace utilisé par

les jeux de données et les instantanés.

■ La définition d'une valeur refquota ou refreservation supérieure à une valeur

"reservation" n'a aucun effet. Lorsque vous définissez la propriété quota ou la propriété
refquota, les opérations qui tentent de dépasser l'une de ces valeurs échouent. Il est possible
de dépasser une valeur quota supérieure à une valeur refquota. Si certains blocs
d'instantanés sont modifiés, la valeur quota risque d'être dépassée avant la valeur refquota.

■ Les quotas d'utilisateurs et de groupes permettent d'augmenter plus facilement l'espace
disque contenant de nombreux comptes d'utilisateur, par exemple dans une université.

Pour plus d'informations sur la définition de quotas et réservations, reportez-vous aux sections
“Définitions de quotas sur les systèmes de fichiers ZFS” à la page 209 et “Définition de
réservations sur les systèmes de fichiers ZFS” à la page 213.

Définitions de quotas sur les systèmes de fichiers ZFS
La définition et l'affichage des quotas ZFS s'effectuent respectivement à l'aide des commandes
zfs set et zfs get. Dans l'exemple suivant, un quota de 10 Go est défini pour
tank/home/bonwick.

# zfs set quota=10G tank/home/bonwick

# zfs get quota tank/home/bonwick

NAME

PROPERTY

VALUE

tank/home/bonwick quota

10.0G

SOURCE

local

Les quotas ZFS affectent également la sortie des commandes zfs list et df. Exemple :

# zfs list

NAME

USED AVAIL REFER MOUNTPOINT

tank/home

16.5K 33.5G 8.50K /export/home

tank/home/bonwick

15.0K 10.0G 8.50K /export/home/bonwick

tank/home/bonwick/ws 6.50K 10.0G 8.50K /export/home/bonwick/ws

# df -h /export/home/bonwick

Filesystem

size

used avail capacity Mounted on

tank/home/bonwick

10G

8K

10G

1%

/export/home/bonwick

tank/home dispose de 33,5 Go d'espace. Toutefois, tank/home/bonwick et
tank/home/bonwick/ws disposent uniquement de 10 Go d'espace, en raison du quota défini
pour tank/home/bonwick.

Vous ne pouvez pas définir un quota sur une valeur inférieure à la quantité d'espace
actuellement utilisée par un jeu de données. Exemple :

Chapitre 6 • Gestion des systèmes de fichiers ZFS

209

Définition des quotas et réservations ZFS

# zfs set quota=10K tank/home/bonwick

cannot set quota for ’tank/home/bonwick’: size is less than current used or

reserved space

Vous pouvez définir une valeur refquota pour un jeu de données qui limite la quantité d'espace
qu'il peut utiliser. Cette limite maximale n'inclut pas l'espace utilisé par les instantanés et les
clones. Exemple :

# zfs set refquota=10g students/studentA

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

106K 33.2G

18K /profs

students

57.7M 33.2G

19K /students

students/studentA 57.5M 9.94G 57.5M /students/studentA

# zfs snapshot students/studentA@today

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

106K 33.2G

18K /profs

students

57.7M 33.2G

19K /students

students/studentA

57.5M 9.94G 57.5M /students/studentA

students/studentA@today

0

- 57.5M -

Par souci de commodité, vous pouvez définir un autre quota pour un jeu de données afin de
vous aider à gérer l'espace utilisé par les instantanés. Exemple :

# zfs set quota=20g students/studentA

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

106K 33.2G

18K /profs

students

57.7M 33.2G

19K /students

students/studentA

57.5M 9.94G 57.5M /students/studentA

students/studentA@today

0

- 57.5M -

Dans ce scénario, studentA peut atteindre la limite maximale de refquota (10 Go) mais peut
supprimer des fichiers pour libérer de l'espace même en présence d'instantanés.

Dans l'exemple ci-dessus, le plus petit des deux quotas (10 Go par opposition à 20 Go) s'affiche
dans la sortie de zfs list. Pour afficher la valeur des deux quotas, utilisez la commande zfs
get. Exemple :

# zfs get refquota,quota students/studentA

NAME

PROPERTY VALUE

students/studentA refquota 10G

students/studentA quota

20G

SOURCE

local

local

210

Guide d'administration Solaris ZFS • Octobre 2009

Définition des quotas et réservations ZFS

Définition de quotas d'utilisateurs ou de groupes sur un système de
fichiers ZFS
Vous pouvez définir un quota d'utilisateurs ou de groupes à l'aide des commandes zfs
userquota et zfs groupquota comme suit :

# zfs create students/compsci

# zfs set userquota@student1=10G students/compsci

# zfs create students/labstaff

# zfs set groupquota@staff=20GB students/labstaff

Affichez le quota d'utilisateurs ou de groupes actuel comme suit :

# zfs get userquota@student1 students/compsci

NAME

PROPERTY

VALUE

students/compsci userquota@student1 10G

# zfs get groupquota@staff students/labstaff

NAME

PROPERTY

VALUE

students/labstaff groupquota@staff 20G

SOURCE

local

SOURCE

local

Vous pouvez afficher l'utilisation générale de l'espace par les utilisateurs et les groupes en
interrogeant les propriétés suivantes :

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

227M

none

POSIX User student1 455M

10G

# zfs groupspace students/labstaff

TYPE

NAME

USED QUOTA

POSIX Group root

217M

none

POSIX Group staff 217M

20G

Si vous souhaitez identifier une utilisation de l'espace par un utilisateur ou un groupe
spécifique, interrogez les propriétés suivantes :

# zfs get userused@student1 students/compsci

NAME

PROPERTY

VALUE

students/compsci userused@student1 455M

# zfs get groupused@staff students/labstaff

NAME

PROPERTY

VALUE

students/labstaff groupused@staff 217M

SOURCE

local

SOURCE

local

Les propriétés de quota d'utilisateurs et de groupes ne sont pas affichées à l'aide de la commande
zfs get all dataset qui affiche une liste de toutes les propriétés de système de fichiers.

Vous pouvez supprimer un quota d'utilisateurs ou de groupes comme suit :

Chapitre 6 • Gestion des systèmes de fichiers ZFS

211

Définition des quotas et réservations ZFS

# zfs set userquota@user1=none students/compsci

# zfs set groupquota@staff=none students/labstaff

Les quotas d'utilisateurs et de groupes ZFS offrent les fonctions suivantes :
■ Lorsqu'un quota d'utilisateurs ou de groupes est défini sur un système de fichiers parent, il

n'est pas automatiquement hérité par un système de fichiers descendant.

■ Cependant, le quota d'utilisateurs ou de groupes est appliqué lorsqu'un clone ou un
instantané est créé à partir d'un système de fichiers lié à un quota d'utilisateurs ou de
groupes. De même, un quota d'utilisateurs ou de groupes est inclus avec le système de
fichiers lorsqu'un flux est créé à l'aide de la commande zfs send, même sans l'option -R.

■ Les utilisateurs dénués de privilèges peuvent uniquement disposer de leur propre utilisation

d'espace. L'utilisateur racine ou l'utilisateur qui s'est vu accorder le privilège userused ou
groupused peut accéder aux informations de comptabilité de l'espace utilisateur ou groupe
de tout le monde.

■ Les propriétés userquota et groupquota ne peuvent pas être définies sur les volumes ZFS,
sur un système de fichiers antérieur à la version 4, ou sur un pool antérieur à la version 15.

L'application des quotas d'utilisateurs ou de groupes peut être différée de quelques secondes. Ce
délai signifie que les utilisateurs peuvent dépasser leurs quotas avant que le système ne le
remarque et refuse d'autres écritures en affichant le message d'erreur EDQUOT .

Vous pouvez utiliser la commande quota héritée pour examiner les quotas d'utilisateurs dans
un environnement NFS où un système de fichiers ZFS est monté, par exemple. Sans aucune
option, la commande quota affiche uniquement la sortie en cas de dépassement du quota de
l'utilisateur. Exemple :

# zfs set userquota@student1=10m students/compsci

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

227M

none

POSIX User student1 455M

10M

# quota student1

Block limit reached on /students/compsci

Si vous réinitialisez le quota et si la limite du quota n'est plus dépassée, vous devez utiliser la
commande quota -v pour examiner le quota de l'utilisateur. Exemple :

# zfs set userquota@student1=10GB students/compsci

# zfs userspace students/compsci

TYPE

NAME

USED QUOTA

POSIX User root

227M

none

POSIX User student1 455M

10G

# quota student1

# quota -v student1

212

Guide d'administration Solaris ZFS • Octobre 2009

Définition des quotas et réservations ZFS

Disk quotas for student1 (uid 201):

Filesystem

usage quota limit

timeleft files quota limit

timeleft

/students/compsci

466029 10485760 10485760

Définition de réservations sur les systèmes de fichiers
ZFS
Une réservation ZFS désigne une quantité d'espace du pool garantie pour un jeu de données.
Dès lors, pour réserver une quantité d'espace pour un jeu de données, cette quantité doit être
actuellement disponible sur le pool. La quantité totale d'espace non utilisé des réservations ne
peut pas dépasser la quantité d'espace non utilisé du pool. La définition et l'affichage des
réservations ZFS s'effectuent respectivement à l'aide des commandes zfs set et zfs get.
Exemple :

# zfs set reservation=5G tank/home/moore

# zfs get reservation tank/home/moore

NAME

PROPERTY

VALUE

tank/home/moore reservation

5.00G

SOURCE

local

Les réservations ZFS peuvent affecter la sortie de la commande zfs list. Exemple :

# zfs list

NAME

USED AVAIL REFER MOUNTPOINT

tank/home

5.00G 33.5G 8.50K /export/home

tank/home/moore

15.0K 10.0G 8.50K /export/home/moore

Notez que tank/home utilise 5 Go d'espace bien que la quantité totale d'espace à laquelle
tank/home et ses descendants font référence est bien inférieure à 5 Go. L'espace utilisé
correspond à l'espace réservé pour tank/home/moore. Les réservations sont prises en compte
dans l'espace utilisé des jeux de données parent et non dans le quota, la réservation ou les deux.

# zfs set quota=5G pool/filesystem

# zfs set reservation=10G pool/filesystem/user1

cannot set reservation for ’pool/filesystem/user1’: size is greater than

available space

Un jeu de données peut utiliser une quantité d'espace supérieure à celle indiquée par sa
réservation. Pour cela, le pool doit contenir une quantité d'espace non réservée suffisante et la
quantité d'espace utilisée par le jeu de données doit être inférieur au quota défini pour ce jeu. Un
jeu de données ne peut pas utiliser l'espace réservé pour un autre jeu de données.

Chapitre 6 • Gestion des systèmes de fichiers ZFS

213

Définition des quotas et réservations ZFS

Les réservations ne sont pas cumulatives. En d'autres termes, l'exécution d'une nouvelle
commande zfs set pour un jeu de données déjà associé à une réservation n'entraîne pas l'ajout
de la nouvelle réservation à la réservation existante. La seconde réservation remplace la
première.

# zfs set reservation=10G tank/home/moore

# zfs set reservation=5G tank/home/moore

# zfs get reservation tank/home/moore

NAME

PROPERTY

VALUE

tank/home/moore reservation

5.00G

SOURCE

local

Vous pouvez définir une valeur refreservation afin de garantir l'espace pour un jeu de
données qui n'inclut pas l'espace utilisé par les instantanés ni les clones. La valeur
refreservation est prise en compte dans l'espace utilisé des jeux de données parent et vient en
déduction des quotas et réservations des jeux de données parent. Exemple :

# zfs set refreservation=10g profs/prof1

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

10.0G 23.2G

19K /profs

profs/prof1

10G 33.2G

18K /profs/prof1

Vous pouvez également définir une valeur de réservation pour le même jeu de données afin de
garantir l'espace du jeu de données et pas de l'espace des instantanés. Exemple :

# zfs set reservation=20g profs/prof1

# zfs list

NAME

profs

USED AVAIL REFER MOUNTPOINT

20.0G 13.2G

19K /profs

profs/prof1

10G 33.2G

18K /profs/prof1

Les réservations régulières sont prises en compte dans l'espace utilisé du parent.

Dans l'exemple ci-dessus, le plus petit des deux quotas (10 Go par opposition à 20 Go) s'affiche
dans la sortie de zfs list. Pour afficher la valeur des deux quotas, utilisez la commande zfs
get. Exemple :

# zfs get reservation,refreserv profs/prof1

NAME

PROPERTY

VALUE

SOURCE

profs/prof1 reservation

20G

profs/prof1 refreservation 10G

local

local

Lorsque la propriété refreservation est définie, un instantané n'est autorisé que si
suffisamment d'espace est disponible dans le pool au-delà de cette réservation afin de pouvoir
contenir le nombre actuel d'octets référencés dans le jeu de données.

214

Guide d'administration Solaris ZFS • Octobre 2009

7C H A P I T R E

7

Utilisation des instantanés et des clones ZFS

Ce chapitre fournit des informations sur la création et la gestion d'instantanés et de clones ZFS.
Il contient également des informations relatives à l'enregistrement d'instantanés.

Il contient les sections suivantes :
■ “Présentation des instantanés ZFS” à la page 215
■ “Création et destruction d'instantanés ZFS” à la page 216
■ “Affichage et accès des instantanés ZFS” à la page 218
■ “Restauration d'un instantané ZFS” à la page 219
■ “Présentation des clones ZFS” à la page 220
■ “Création d'un clone ZFS” à la page 221
■ “Destruction d'un clone ZFS” à la page 221
■ “Envoi et réception de données ZFS” à la page 223

Présentation des instantanés ZFS

Un instantané est une copie en lecture seule d'un système de fichiers ou d'un volume. La
création des instantanés est quasiment immédiate. Initialement, elle ne consomme pas d'espace
disque supplémentaire au sein du pool. Toutefois, à mesure que les données contenues dans le
jeu de données actif changent, l'instantané consomme de l'espace disque en continuant à faire
référence aux anciennes données et empêche donc de libérer de l'espace.

Les instantanés ZFS présentent les caractéristiques suivantes :
■ Persistance au cours des réinitialisations de système.
■ Théoriquement, le nombre maximal d'instantanés est de 264 instantanés.
■ Aucune sauvegarde de secours distincte n'est utilisée. Les instantanés consomment de

l'espace disque provenant directement du pool de stockage auquel appartient le système de
fichiers à partir duquel ils ont été créés.

215

Présentation des instantanés ZFS

■ Une seule opération, dite atomique, permet de créer rapidement des instantanés récursifs.

Ceux-ci sont tous créés simultanément ou ne sont pas créés du tout. Grâce à ce type de
sélection instantanée, les opérations atomiques assurent ainsi la cohérence des données, y
compris pour les systèmes de fichiers descendants.

Il n'est pas possible d'accéder directement aux instantanés de volumes, mais ils peuvent être
clonés, sauvegardés, restaurés, etc. Pour plus d'informations sur la sauvegarde d'un instantané
ZFS, reportez-vous à la section “Envoi et réception de données ZFS” à la page 223.

Création et destruction d'instantanés ZFS
La commande zfs snapshot permet de créer les instantanés. Elle ne prend pour argument que
le nom de l'instantané à créer. Le nom de l'instantané est spécifié comme suit :

filesystem@snapname
volume@snapname

Le nom de l'instantané doit respecter les conventions d'attribution de noms décrites dans la
section “Exigences d'attribution de noms de composants ZFS” à la page 46.

Dans l'exemple suivant, un instantané de tank/home/ahrens nommé friday est créé.

# zfs snapshot tank/home/ahrens@friday

Vous pouvez créer des instantanés pour tous les systèmes de fichiers descendants à l'aide de
l'option -r. Exemple :

# zfs snapshot -r tank/home@now

# zfs list -t snapshot

NAME

USED AVAIL REFER MOUNTPOINT

tank/home@now

tank/home/ahrens@now

tank/home/anne@now

tank/home/bob@now

tank/home/cindys@now

0

0

0

0

0

- 29.5K -

- 2.15M -

- 1.89M -

- 1.89M -

- 2.15M -

Les propriétés des instantanés ne sont pas modifiables. Les propriétés des jeux de données ne
peuvent pas être appliquées à un instantané.

# zfs set compression=on tank/home/ahrens@tuesday

cannot set compression property for ’tank/home/ahrens@tuesday’: snapshot

properties cannot be modified

La commande zfs destroy permet de détruire les instantanés. Exemple :

# zfs destroy tank/home/ahrens@friday

216

Guide d'administration Solaris ZFS • Octobre 2009

La destruction d'un jeu de données est impossible s'il existe des instantanés du jeu de données.
Exemple :

Présentation des instantanés ZFS

# zfs destroy tank/home/ahrens

cannot destroy ’tank/home/ahrens’: filesystem has children

use ’-r’ to destroy the following datasets:

tank/home/ahrens@tuesday

tank/home/ahrens@wednesday

tank/home/ahrens@thursday

En outre, afin de pouvoir détruire l'instantané utilisé pour créer les clones, vous devez au
préalable détruire les clones.

Pour de plus amples informations sur la sous-commande destroy, reportez-vous à la section
“Destruction d'un système de fichiers ZFS” à la page 177.

Renommage d'instantanés ZFS
Vous pouvez renommer les instantanés, mais vous devez procéder au sein du pool et du jeu de
données à partir desquels ils ont été créés. Exemple :

# zfs rename tank/home/cindys@083006 tank/home/cindys@today

En outre, le raccourci de syntaxe suivant donne une syntaxe de renommage d'instantané
similaire à l'exemple ci-dessus.

# zfs rename tank/home/cindys@083006 today

L'opération de renommage d'instantané n'est pas prise en charge, car le nom du pool cible et
celui du système de fichiers ne correspondent pas au pool et au système de fichiers dans lesquels
l'instantané a été créé.

# zfs rename tank/home/cindys@today pool/home/cindys@saturday

cannot rename to ’pool/home/cindys@today’: snapshots must be part of same

dataset

Vous pouvez renommer de manière récursive les instantanés à l'aide de la commande zfs
rename - r. Exemple :

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

270K 16.5G

22K /users

users/home

76K 16.5G

22K /users/home

users/home@yesterday

0

-

22K -

users/home/markm

18K 16.5G

18K /users/home/markm

users/home/markm@yesterday

0

-

18K -

users/home/marks

18K 16.5G

18K /users/home/marks

Chapitre 7 • Utilisation des instantanés et des clones ZFS

217

Présentation des instantanés ZFS

users/home/marks@yesterday

0

-

18K -

users/home/neil

18K 16.5G

18K /users/home/neil

users/home/neil@yesterday

0

-

18K -

# zfs rename -r users/home@yesterday @2daysago

# zfs list -r users/home

NAME

USED AVAIL REFER MOUNTPOINT

users/home

76K 16.5G

22K /users/home

users/home@2daysago

0

-

22K -

users/home/markm

18K 16.5G

18K /users/home/markm

users/home/markm@2daysago

0

-

18K -

users/home/marks

18K 16.5G

18K /users/home/marks

users/home/marks@2daysago

0

-

18K -

users/home/neil

18K 16.5G

18K /users/home/neil

users/home/neil@2daysago

0

-

18K -

Affichage et accès des instantanés ZFS
Vous pouvez activer ou désactiver l'affichage des listes d'instantanés de la sortie zfs list en
utilisant la propriété de pool listsnapshots. Cette propriété est activée par défaut.

Si vous désactivez cette propriété, vous pouvez utiliser la commande zfs list -t snapshot
pour afficher les informations relatives à un instantané. Ou activez la propriété de pool
listsnapshots. Exemple :

# zpool get listsnapshots tank

NAME PROPERTY

VALUE

SOURCE

tank listsnapshots on

default

# zpool set listsnapshots=off tank

# zpool get listsnapshots tank

NAME PROPERTY

VALUE

SOURCE

tank listsnapshots off

local

Les instantanés des systèmes de fichiers sont accessibles dans le répertoire .zfs/snapshot au
sein de la racine du système de fichiers qui le contient. Par exemple, si tank/home/ahrens est
monté sur /home/ahrens, les données de l'instantané tank/home/ahrens@thursday sont
accessibles dans le répertoire /home/ahrens/.zfs/snapshot/thursday.

# ls /tank/home/ahrens/.zfs/snapshot

tuesday wednesday thursday

Vous pouvez répertorier les instantanés comme suit :

# zfs list -t snapshot

NAME

USED AVAIL REFER MOUNTPOINT

pool/home/anne@monday

pool/home/bob@monday

0

0

-

780K -

- 1.01M -

218

Guide d'administration Solaris ZFS • Octobre 2009

Présentation des instantanés ZFS

tank/home/ahrens@tuesday

8.50K

-

780K -

tank/home/ahrens@wednesday 8.50K

- 1.01M -

tank/home/ahrens@thursday

0

- 1.77M -

tank/home/cindys@today

8.50K

-

524K -

Vous pouvez répertorier les instantanés qui ont été créés pour un système de fichiers particulier
comme suit :

# zfs list -r -t snapshot -o name,creation tank/home

NAME

CREATION

tank/home/ahrens@tuesday

Mon Aug 31 11:03 2009

tank/home/ahrens@wednesday Mon Aug 31 11:03 2009

tank/home/ahrens@thursday

Mon Aug 31 11:03 2009

tank/home/cindys@now

Mon Aug 31 11:04 2009

Comptabilisation de l'espace d'instantanés
Lors de la création d'un instantané, son espace est initialement partagé entre l'instantané et le
système de fichiers et éventuellement avec des instantanés précédents. Lorsque le système de
fichiers change, l'espace précédemment partagé devient dédié à l'instantané, et il est compté
dans la propriété used de l'instantané. De plus, la suppression d'instantanés peut augmenter la
quantité d'espace dédié à d'autres instantanés (et, par conséquent, utilisé par ceux-ci).
La propriété referenced de l'espace d'un instantané est identique à celle du système de fichiers
à la création de l'instantané.
Vous pouvez identifier des informations supplémentaires sur la façon dont les valeurs de la
propriété used sont utilisées. Les nouvelles propriétés de système de fichiers en lecture seule
décrivent l'utilisation de l'espace pour les clones, les systèmes de fichiers et les volumes.
Exemple :

$ zfs list -o space

NAME

rpool

AVAIL

USED USEDSNAP USEDDS USEDREFRESERV USEDCHILD

60.6G 6.37G

rpool/ROOT

60.6G 4.87G

rpool/ROOT/zfs1009BE 60.6G 4.87G

rpool/dump

60.6G 1.50G

rpool/swap

60.6G

16K

0

0

0

0

0

97K

21K

4.87G

1.50G

16K

0

0

0

0

0

6.37G

4.87G

0

0

0

Pour une description de ces propriétés, reportez-vous au Tableau 6–1.

Restauration d'un instantané ZFS
La commande zfs rollback permet de supprimer toutes les modifications effectuées depuis la
création d'un instantané spécifique. Le système de fichiers revient à l'état dans lequel il était lors
de la prise de l'instantané. Par défaut, la commande ne permet pas de restaurer un instantané
autre que le plus récent.

Chapitre 7 • Utilisation des instantanés et des clones ZFS

219

Présentation des clones ZFS

Pour restaurer un instantané précédent, tous les instantanés intermédiaires doivent être
détruits. Vous pouvez détruire les instantanés précédents en spécifiant l'option -r.

S'il existe des clones d'un instantané intermédiaire, vous devez spécifier l'option -R pour
détruire également les clones.

Remarque – Si le système de fichiers que vous souhaitez restaurer est actuellement monté, il doit
être démonté, puis remonté. Si le système de fichiers ne peut pas être démonté, la restauration
échoue. L'option -f force le démontage du système de fichiers, le cas échéant.

Dans l'exemple suivant, le système de fichiers tank/home/ahrens revient à l'instantané
tuesday :

# zfs rollback tank/home/ahrens@tuesday

cannot rollback to ’tank/home/ahrens@tuesday’: more recent snapshots exist

use ’-r’ to force deletion of the following snapshots:

tank/home/ahrens@wednesday

tank/home/ahrens@thursday

# zfs rollback -r tank/home/ahrens@tuesday

Dans l'exemple ci-dessus, les instantanés wednesday et thursday sont supprimés en raison de
la restauration de l'instantané tuesday précédent.

# zfs list -r -t snapshot -o name,creation tank/home/ahrens

NAME

CREATION

tank/home/ahrens@tuesday Wed Aug 27 16:35 2008

Présentation des clones ZFS

Un clone est un volume ou un système de fichiers accessible en écriture et dont le contenu initial
est similaire à celui du jeu de données à partir duquel il a été créé. Tout comme pour les
instantanés, la création d'un clone est quasiment instantanée et ne consomme initialement
aucun espace disque supplémentaire. Vous pouvez d'autre part créer un instantané d'un clone.
■ “Création d'un clone ZFS” à la page 221
■ “Destruction d'un clone ZFS” à la page 221
■ “Remplacement d'un système de fichiers ZFS par un clone ZFS” à la page 222

Les clones se créent uniquement à partir d'un instantané. Lors du clonage d'un instantané, une
dépendance implicite se crée entre le clone et l'instantané. Même en cas de création d'un clone à
un autre emplacement de la hiérarchie, l'instantané d'origine ne peut pas être détruit tant que le
clone existe. La propriété origin indique cette dépendance et la commande zfs destroy
répertorie ces dépendances, le cas échéant.

220

Guide d'administration Solaris ZFS • Octobre 2009

Présentation des clones ZFS

Un clone n'hérite pas des propriétés du jeu de données à partir duquel il a été créé. Les
commandes zfs get et zfs set permettent d'afficher et de modifier les propriétés d'un jeu de
données cloné. Pour de plus amples informations sur la configuration des propriétés de jeux de
données ZFS, reportez-vous à la section “Définition des propriétés ZFS” à la page 196.

Dans la mesure où un clone partage initialement son espace disque avec l'instantané d'origine,
la propriété used est initialement égale à zéro. À mesure que le clone est modifié, il utilise de
plus en plus d'espace. La propriété used de l'instantané d'origine ne tient pas compte de l'espace
disque consommé par le clone.

Création d'un clone ZFS
Pour créer un clone, utilisez la commande zfs clone en spécifiant l'instantané à partir duquel
créer le clone, ainsi que le nom du nouveau volume ou système de fichiers. Le nouveau volume
ou système de fichiers peut se trouver à tout emplacement de la hiérarchie ZFS. Le type du
nouveau jeu de données (par exemple, système de fichiers ou volume) est le même que celui de
l'instantané à partir duquel a été créé le clone. Vous ne pouvez pas créer le clone d'un système de
fichiers dans un autre pool que celui de l'instantané du système de fichiers d'origine.

Dans l'exemple suivant, un nouveau clone appelé tank/home/ahrens/bug123 avec le même
contenu initial que l'instantané tank/ws/gate@yesterday est créé.

# zfs snapshot tank/ws/gate@yesterday

# zfs clone tank/ws/gate@yesterday tank/home/ahrens/bug123

Dans l'exemple suivant, un espace de travail est créé à partir de l'instantané
projects/newproject@today pour un utilisateur temporaire, sous le nom
projects/teamA/tempuser. Ensuite, les propriétés sont configurées dans l'espace de travail
cloné.

# zfs snapshot projects/newproject@today

# zfs clone projects/newproject@today projects/teamA/tempuser

# zfs set sharenfs=on projects/teamA/tempuser

# zfs set quota=5G projects/teamA/tempuser

Destruction d'un clone ZFS
La commande zfs destroy permet de détruire les clones ZFS. Exemple :

# zfs destroy tank/home/ahrens/bug123

Les clones doivent être détruits préalablement à la destruction de l'instantané parent.

Chapitre 7 • Utilisation des instantanés et des clones ZFS

221

Présentation des clones ZFS

Remplacement d'un système de fichiers ZFS par un
clone ZFS
La commande zfs promote permet de remplacer un système de fichiers ZFS actif par un clone
de ce système de fichiers. Cette fonction facilite le clonage et le remplacement des systèmes de
fichiers pour que le système de fichiers d'origine devienne le clone du système de fichiers
spécifié. En outre, cette fonction permet de détruire le système de fichiers à partir duquel le
clone a été créé. Il est impossible de détruire un système de fichiers "d'origine" possédant des
clones actifs, sans le remplacer par l'un de ses clones. Pour plus d'informations sur la
destruction des clones, reportez-vous à la section “Destruction d'un clone ZFS” à la page 221.
Dans l'exemple suivant, le système de fichiers tank/test/productA est cloné, puis le clone du
système de fichiers (tank/test/productAbeta) devient le système de fichiers
tank/test/productA.

# zfs create tank/test

# zfs create tank/test/productA

# zfs snapshot tank/test/productA@today

# zfs clone tank/test/productA@today tank/test/productAbeta

# zfs list -r tank/test

NAME

USED AVAIL REFER MOUNTPOINT

tank/test

314K 8.24G 25.5K /tank/test

tank/test/productA

288K 8.24G

288K /tank/test/productA

tank/test/productA@today

0

-

288K -

tank/test/productAbeta

0 8.24G

288K /tank/test/productAbeta

# zfs promote tank/test/productAbeta

# zfs list -r tank/test

NAME

USED AVAIL REFER MOUNTPOINT

tank/test

316K 8.24G 27.5K /tank/test

tank/test/productA

0 8.24G

288K /tank/test/productA

tank/test/productAbeta

288K 8.24G

288K /tank/test/productAbeta

tank/test/productAbeta@today

0

-

288K -

Dans la sortie de zfs list, vous pouvez voir que la comptabilisation de l'espace du système de
fichiers productA d'origine a été remplacée par le système de fichiers productAbeta.
Pour terminer le processus de remplacement de clone, renommez les systèmes de fichiers.
Exemple :

# zfs rename tank/test/productA tank/test/productAlegacy

# zfs rename tank/test/productAbeta tank/test/productA

# zfs list -r tank/test

NAME

USED AVAIL REFER MOUNTPOINT

tank/test

316K 8.24G 27.5K /tank/test

tank/test/productA

288K 8.24G

288K /tank/test/productA

tank/test/productA@today

0

-

288K -

tank/test/productAlegacy

0 8.24G

288K /tank/test/productAlegacy

222

Guide d'administration Solaris ZFS • Octobre 2009

Vous pouvez également supprimer l'ancien système de fichiers si vous le souhaitez. Exemple :

Envoi et réception de données ZFS

# zfs destroy tank/test/productAlegacy

Envoi et réception de données ZFS

La commande zfs send crée une représentation de flux d'un instantané qui est écrite dans la
sortie standard. Un flux complet est généré par défaut. Vous pouvez rediriger la sortie vers un
fichier ou un système fichier. La commande zfs receive crée un instantané dont le contenu est
spécifié dans le flux fourni dans l'entrée standard. En cas de réception d'un flux complet, un
système de fichiers est également créé. Ces commandes permettent d'envoyer les données
d'instantané ZFS et de recevoir les systèmes de fichiers et les données d'instantané ZFS.
Reportez-vous aux exemples de la section suivante.
■ “Envoi d'un instantané ZFS” à la page 224
■ “Réception d'un instantané ZFS” à la page 225
■ “Réplication distante de données ZFS” à la page 229
■ “Enregistrement de données ZFS à l'aide d'autres produits de sauvegarde” à la page 229

Les solutions de sauvegarde suivantes sont disponibles pour enregistrer les données ZFS :
■ Produits de sauvegarde d'entreprise : si vous souhaitez disposer des fonctions suivantes,

considérez une solution de sauvegarde d'entreprise :
■ Restauration fichier par fichier
■ Vérification des médias de sauvegarde
■ Gestion des médias
Instantanés de systèmes de fichiers et restauration d'instantanés – Exécutez les commandes
zfs snapshot et zfs rollback pour créer facilement une copie d'un système de fichiers et
restaurer une version précédente de système de fichier, le cas échéant. Par exemple, cette
solution permet de restaurer un ou plusieurs fichiers issus d'une version précédente d'un
système de fichiers.
Pour de plus amples informations sur la création et la restauration d'instantané,
reportez-vous à la section “Présentation des instantanés ZFS” à la page 215.

■

■ Enregistrement d'instantanés : utilisez les commandes zfs send et zfs receive pour

envoyer et recevoir un instantané ZFS. Vous pouvez enregistrer les modifications
incrémentielles entre instantanés, mais la restauration individuelle de fichiers est
impossible. L'instantané du système doit être restauré dans son intégralité. Ces commandes
ne constituent pas une solution de sauvegarde complète pour l'enregistrement de vos
données ZFS.

■ Réplication distante – Utilisez les commandes zfs send et zfs receive lorsque vous

souhaitez copier un système de fichiers d'un système vers un autre. Ce processus diffère d'un
produit de gestion de volume classique qui pourrait mettre les périphériques en miroir dans
un WAN. Aucune configuration ni aucun matériel spécifique n'est requis. La réplication de

Chapitre 7 • Utilisation des instantanés et des clones ZFS

223

Envoi et réception de données ZFS

systèmes de fichiers ZFS a ceci d'avantageux qu'elle permet de recréer un système de fichiers
dans un pool de stockage et de spécifier différents niveaux de configuration pour le nouveau
pool, comme RAID-Z, mais avec des données de système de fichiers identiques.

■ Utilitaires d'archivage : enregistrez les données ZFS à l'aide d'utilitaires d'archivage tels que
tar, cpio et pax, ou des produits de sauvegarde tiers. Actuellement, les deux utilitaires tar
et cpio traduisent correctement les ACL de type NFSv4, contrairement à l'utilitaire pax.

Envoi d'un instantané ZFS
Vous pouvez utiliser la commande zfs send pour envoyer une copie d'un instantané et
recevoir cet instantané dans un autre pool du même système ou dans un autre pool d'un
système différent utilisé pour stocker les données de sauvegarde. Pour par exemple envoyer
l'instantané à un pool différent du même système, employez une syntaxe du type suivant :

# zfs send tank/data@snap1 | zfs recv spool/ds01

Si vous envoyez le flux de l'instantané à un système différent, envoyez la sortie de la commande
zfs send à la commande ssh. Exemple :

host1# zfs send tank/dana@snap1 | ssh host2 zfs recv newtank/dana

Lors de l'envoi d'un flux complet, le système de fichiers de destination ne doit pas exister.
Vous pouvez envoyer les données incrémentielles à l'aide de l'option zfs send - i. Exemple :

host1# zfs send -i tank/dana@snap1 tank/dana@snap2 | ssh host2 zfs recv newtank/dana

Le premier argument correspond à l'instantané le plus ancien (instantané1) et le second, à
l'instantané le plus récent (instantané2). Dans ce cas, le système de fichiers newtank/dana doit
exister pour que la réception incrémentielle s'effectue correctement.
La source de l'instantané1 incrémentiel peut être spécifiée comme étant le dernier composant
du nom de l'instantané. Grâce à ce raccourci, il suffit de spécifier le nom après le signe @ pour
l'instantané1, qui est considéré comme provenant du même système de fichiers que
l'instantané2. Exemple :

host1# zfs send -i snap1 tank/dana@snap2 > ssh host2 zfs recv newtank/dana

Cette syntaxe est équivalente à l'exemple de syntaxe incrémentielle ci-dessus.
Le message s'affiche en cas de tentative de génération d'un flux incrémentiel à partir d'un
instantané1 provenant d'un autre système de fichiers :

cannot send ’pool/fs@name’: not an earlier snapshot from the same fs

Si vous devez stocker de nombreuses copies, vous avez la possibilité de compresser une
représentation de flux d'instantané ZFS à l'aide de la commande gzip. Exemple :

224

Guide d'administration Solaris ZFS • Octobre 2009

# zfs send pool/fs@snap | gzip > backupfile.gz

Envoi et réception de données ZFS

Réception d'un instantané ZFS
Gardez les points suivants à l'esprit lorsque vous recevez un instantané d'un système de fichiers :
■ L'instantané et le système de fichiers sont reçus.
■ Le système de fichiers et tous les systèmes de fichiers descendants sont démontés.
■ Les systèmes de fichiers sont inaccessibles tant qu'ils sont en cours de réception.
■ Le système de fichiers d'origine à recevoir ne doit pas exister tant qu'il est en cours de

transfert.
Si le nom du système de fichiers choisi existe déjà, la commande zfs rename permet de
renommer le système de fichiers.

■

Exemple :

# zfs send tank/gozer@0830 > /bkups/gozer.083006

# zfs receive tank/gozer2@today < /bkups/gozer.083006

# zfs rename tank/gozer tank/gozer.old

# zfs rename tank/gozer2 tank/gozer

Vous pouvez utiliser zfs recv en tant qu'alias pour la commande zfs receive.
Si vous apportez des modifications au système de fichiers de destination et souhaitez effectuer
un autre envoi incrémentiel d'instantané, vous devez au préalable restaurer le système de
fichiers destinataire.
Par exemple, si vous effectuez les modifications dans le système de fichiers comme ci-dessous :

host2# rm newtank/dana/file.1

Si vous effectuez un envoi incrémentiel de tank/dana@snap3, vous devez tout d'abord restaurer
le système de fichiers pour recevoir le nouvel instantané incrémentiel. L'option -F permet
d'éviter l'étape de restauration. Exemple :

host1# zfs send -i tank/dana@snap2 tank/dana@snap3 | ssh host2 zfs recv -F newtank/dana

Lors de la réception d'un instantané incrémentiel, le système de fichiers de destination doit déjà
exister.
Si vous apportez des modifications au système de fichiers sans restaurer le système de fichiers
destinataire pour permettre la réception du nouvel instantané incrémentiel, ou si vous ne
spécifiez pas l'option -F, le message suivant s'affiche :

host1# zfs send -i tank/dana@snap4 tank/dana@snap5 | ssh host2 zfs recv newtank/dana

cannot receive: destination has been modified since most recent snapshot

Chapitre 7 • Utilisation des instantanés et des clones ZFS

225

Envoi et réception de données ZFS

Les vérifications suivantes sont requises pour assurer l'exécution de l'option -F :

■

■

Si l'instantané le plus récent ne correspond pas à la source incrémentielle, la restauration et
la réception ne s'effectuent pas intégralement et un message d'erreur s'affiche.
Si vous avez fourni accidentellement le nom d'un système de fichiers qui ne correspond pas
à la source incrémentielle à la commande zfs receive, la restauration et la réception ne
s'effectuent pas correctement et le message d'erreur suivant s'affiche.

cannot send ’pool/fs@name’: not an earlier snapshot from the same fs

Envoi et réception de flux d'instantanés ZFS
complexes
Cette section décrit l'utilisation des options zfs send -I et -R pour envoyer et recevoir des flux
d'instantanés plus complexes.

Gardez les points suivants à l'esprit lors de l'envoi et de la réception de flux d'instantanés ZFS :
■ Utilisez l'option zfs send -I pour envoyer tous les flux incrémentiels d'un instantané à un

instantané cumulé. Vous pouvez également utiliser cette option pour envoyer un flux
incrémentiel de l'instantané d'origine pour créer un clone. L'instantané d'origine doit déjà
exister sur le côté récepteur afin d'accepter le flux incrémentiel.

■ Utilisez l'option zfs send -R pour envoyer un flux de réplication de tous les systèmes de

fichiers descendants. Une fois reçus, les propriétés, instantanés, systèmes de fichiers
descendants et clones sont conservés.

■ Vous pouvez également utiliser les deux options pour envoyer un flux de réplication

incrémentiel.
■ Les modifications apportées aux propriétés, ainsi que les changements de nom et

■

destructions d'instantané et de système de fichiers sont conservés.
Si l'option zfs recv -F n'est pas spécifiée lors de la réception du flux de réplication, les
destructions de jeu de données sont ignorées. La syntaxe de zfs recv -F dans ce cas peut
conserve également sa signification de récupération le cas échéant.

■ Tout comme dans les autres cas (autres que zfs send -R) - i ou -I, si l'option -I est
utilisée, tous les instantanés créés entre instantanéA et instantanéD sont envoyés. Si
l'option -i est utilisée, seul l'instantanéD (de tous les descendants) est envoyé.

■ Pour recevoir ces nouveaux types de flux zfs send, le système récepteur doit exécuter une

version du logiciel capable de les envoyer. La version des flux est incrémentée.
Vous pouvez cependant accéder à des flux d'anciennes versions de pool en utilisant une
version plus récente du logiciel. Vous pouvez par exemple envoyer et recevoir des flux créés
à l'aide des nouvelles options à partir d'un pool de la version 3. Vous devez par contre
exécuter un logiciel récent pour recevoir un flux envoyé avec les nouvelles options.

226

Guide d'administration Solaris ZFS • Octobre 2009

Envoi et réception de données ZFS

EXEMPLE 7–1 Envoi et réception de flux d'instantanés ZFS complexes (exemples)
Plusieurs instantanés incrémentiels peuvent être regroupés en un seul instantané à l'aide de
l'option zfs send -I. Exemple :

# zfs send -I pool/fs@snapA pool/fs@snapD > /snaps/fs@all-I

Supprimez les instantanés B, C et D.

# zfs destroy pool/fs@snapB

# zfs destroy pool/fs@snapC

# zfs destroy pool/fs@snapD

Recevez l'instantané regroupé.

# zfs receive -d -F pool/fs < /snaps/fs@all-I

# zfs list

NAME

pool

USED AVAIL REFER MOUNTPOINT

428K 16.5G

20K /pool

pool/fs

71K 16.5G

21K /pool/fs

pool/fs@snapA

pool/fs@snapB

pool/fs@snapC

16K

17K

17K

- 18.5K -

-

20K -

- 20.5K -

pool/fs@snapD

0

-

21K -

Vous pouvez également utiliser la commande zfs send -I pour regrouper un instantané et un
clone d'instantané en un nouveau jeu de données. Exemple :

# zfs create pool/fs

# zfs snapshot pool/fs@snap1

# zfs clone pool/fs@snap1 pool/clone

# zfs snapshot pool/clone@snapA

# zfs send -I pool/fs@snap1 pool/clone@snapA > /snaps/fsclonesnap-I

# zfs destroy pool/clone@snapA

# zfs destroy pool/clone

# zfs receive -F pool/clone < /snaps/fsclonesnap-I

Utilisez la commande zfs send -R pour répliquer un système de fichiers ZFS et tous ses
systèmes de fichiers descendants, jusqu'à l'instantané nommé. Une fois reçus, les propriétés,
instantanés, systèmes de fichiers descendants et clones sont conservés.

Dans l'exemple suivant, des instantanés des systèmes de fichiers utilisateur sont créés. Un flux
de réplication de tous les instantanés utilisateur est créé. Les systèmes de fichiers et instantanés
d'origine sont ensuite détruits et récupérés.

# zfs snapshot -r users@today

# zfs list

NAME

USED AVAIL REFER MOUNTPOINT

Chapitre 7 • Utilisation des instantanés et des clones ZFS

227

Envoi et réception de données ZFS

EXEMPLE 7–1 Envoi et réception de flux d'instantanés ZFS complexes (exemples)

(Suite)

users

187K 33.2G

22K /users

users@today

0

-

22K -

users/user1

18K 33.2G

18K /users/user1

users/user1@today

0

-

18K -

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

# zfs send -R users@today > /snaps/users-R

# zfs destroy -r users

# zfs receive -F -d users < /snaps/users-R

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

196K 33.2G

22K /users

users@today

0

-

22K -

users/user1

18K 33.2G

18K /users/user1

users/user1@today

0

-

18K -

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

Vous pouvez utiliser la commande zfs send -R pour répliquer le jeu de données users et ses
descendants, puis envoyer le flux répliqué à un autre pool, users2.

# zfs create users2 mirror c0t1d0 c1t1d0

# zfs receive -F -d users2 < /snaps/users-R

# zfs list

NAME

users

USED AVAIL REFER MOUNTPOINT

224K 33.2G

22K /users

users@today

0

-

22K -

users/user1

33K 33.2G

18K /users/user1

users/user1@today

15K

-

18K -

users/user2

18K 33.2G

18K /users/user2

users/user2@today

0

-

18K -

users/user3

18K 33.2G

18K /users/user3

users/user3@today

0

-

18K -

users2

188K 16.5G

22K /users2

users2@today

0

-

22K -

users2/user1

18K 16.5G

18K /users2/user1

users2/user1@today

0

-

18K -

users2/user2

18K 16.5G

18K /users2/user2

users2/user2@today

0

-

18K -

users2/user3

18K 16.5G

18K /users2/user3

users2/user3@today

0

-

18K -

228

Guide d'administration Solaris ZFS • Octobre 2009

Envoi et réception de données ZFS

Réplication distante de données ZFS
Les commandes zfs send et zfs recv permettent d'effectuer une copie distante d'une
représentation de flux d'instantané d'un système vers un autre. Exemple :

# zfs send tank/cindy@today | ssh newsys zfs recv sandbox/restfs@today

Cette commande envoie les données de l'instantané tank/cindy@today, le reçoit dans le
système de fichiers sandbox/restfs et crée également un instantané restfs@today sur le
système newsys. Dans cet exemple, l'utilisateur a été configuré pour utiliser ssh dans le système
distant.

Enregistrement de données ZFS à l'aide d'autres
produits de sauvegarde
Outre les commandes zfs send et zfs receive, vous pouvez utiliser des utilitaires d'archivage,
tels que les commandes tar et cpio pour enregistrer des fichiers ZFS. Tous ces utilitaires
enregistrent et restaurent les attributs de fichiers et les ACL ZFS. Vérifiez les options adéquates
des commandes tar et cpio.

Pour connaître les toutes dernières informations relatives à ZFS et aux problèmes concernant
les produits de sauvegarde tiers, consultez les notes de version de Solaris 10 ou la FAQ de ZFS à
l'adresse :

http://opensolaris.org/os/community/zfs/faq/#backupsoftware

Chapitre 7 • Utilisation des instantanés et des clones ZFS

229

230

8C H A P I T R E

8

Utilisation des ACL pour la protection de
fichiers ZFS

Ce chapitre décrit l'utilisation des listes de contrôle d'accès (ACL, Access Control List) pour
protéger les fichiers ZFS en accordant plus de droits granulaires que de droits UNIX standard.

Il contient les sections suivantes :
■ “Nouveau modèle ACL Solaris” à la page 231
■ “Configuration d'ACL dans des fichiers ZFS” à la page 238
■ “Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé” à la page 240
■ “Configuration et affichage d'ACL dans des fichiers ZFS en format compact” à la page 254

Nouveau modèle ACL Solaris

Les versions précédentes les plus récentes de Solaris assuraient la prise en charge d'une
implémentation ACL reposant principalement sur la spécification POSIX-draft ACL. Les ACL
basées sur POSIX-draft sont utilisées pour protéger les fichiers UFS et sont traduites par les
versions de NFS antérieures à NFSv4.

Grâce à l'introduction de NFSv4, un nouveau modèle d'ACL assure entièrement la prise en
charge de l'interopérabilité qu'offre NFSv4 entre les clients UNIX et non UNIX. La nouvelle
implémentation d'ACL, telle que définie dans les spécifications NFSv4, fournit des sémantiques
bien plus riches, basées sur des ACL NT.

Les différences principales du nouveau modèle d'ACL sont les suivantes :
■ modèle basé sur la spécification NFSv4 et similaire aux ACL de type NT ;

■

■

jeu de privilèges d'accès bien plus granulaire ; Pour plus d'informations, reportez-vous au
Tableau 8–2.
configuration et affichage avec les commandes chmod et ls, et non les commandes setfacl
et getfacl ;

231

Nouveau modèle ACL Solaris

■

sémantique d'héritage bien plus riche pour déterminer comment les privilèges d'accès sont
appliqués d'un répertoire à un sous-répertoire, et ainsi de suite. Pour plus d'informations,
reportez-vous à la section “Héritage d'ACL” à la page 236.

Les deux modèles d'ACL assurent un contrôle d'accès bien plus fin, et disponible avec les droits
de fichier standard. De façon similaire aux listes de contrôle d'accès POSIX-draft, les nouvelles
ACL se composent de plusieurs ACE (Access Control Entry, entrées de contrôle d'accès).

Les ACL POSIX-draft utilisent une seule entrée pour définir les droits autorisés et celles qui ne
le sont pas. Le nouveau modèle d'ACL dispose de deux types d'ACE qui affectent la vérification
d'accès : ALLOW et DENY. Il est en soi impossible de déduire de toute entrée de contrôle d'accès
(ACE) définissant un groupe de droits si les droits qui n'ont pas été définis dans cette ACE sont
ou non autorisés.

La conversion entre les ACL NFSv4 et les ACL POSIX-draft s'effectue comme suit :

■

Si vous employez un utilitaire compatible avec les ACL (les commandes cp, mv, tar, cpio ou
rcp, par exemple) pour transférer des fichiers UFS avec des ACL vers un système de fichiers
ZFS, les ACL POSIX-draft sont converties en ACL NFSv4 équivalentes.

■ Les ACL NFSv4 sont converties en ACL POSIX-draft. Un message tel que le suivant s'affiche

si une ACL NFSv4 n'est pas convertie en ACL POSIX-draft :

# cp -p filea /var/tmp

cp: failed to set acl entries on /var/tmp/filea

■

Si vous créez une archive cpio ou tar UFS avec l'option de conservation des ACL (tar -p ou
cpio -P) dans un système exécutant la version actuelle de Solaris, les ACL sont perdues en
cas d'extraction de l'archive sur un système exécutant une version précédente de Solaris.
Tous les fichiers sont extraits avec les modes de fichier corrects, mais les entrées d'ACL sont
ignorées.

■ Vous pouvez utiliser la commande ufsrestore pour restaurer des données dans un système
de fichiers ZFS. Si les données d'origine incluent des ACL POSIX-style, elles sont converties
en ACL NFSv4-style.

■ En cas de tentative de configuration d'une ACL SFSv4 dans un fichier UFS, un message tel

que le suivant s'affiche :

chmod: ERROR: ACL type’s are different

■ En cas de tentative de configuration d'une ACL POSIX dans un fichier ZFS, un message tel

que le suivant s'affiche :

# getfacl filea

File system doesn’t support aclent_t style ACL’s.

See acl(5) for more information on Solaris ACL support.

232

Guide d'administration Solaris ZFS • Octobre 2009

Nouveau modèle ACL Solaris

Pour obtenir des informations sur les autres limitations des ACL et des produits de sauvegarde,
reportez-vous à la section “Enregistrement de données ZFS à l'aide d'autres produits de
sauvegarde” à la page 229.

Descriptions de syntaxe pour la configuration des ACL
Deux formats d'ACL de base sont fournis comme suit :
Syntax for Setting Trivial ACLs
chmod [options] A[index]{+|=}owner@ |group@ |everyone@: droits d'accès/...[:indicateurs
d'héritage]: deny | allow fichier
chmod [options] A-owner@, group@, everyone@:droits d'accès /...[:indicateurs
d'héritage]:deny | allow fichier ...
chmod [options] A[index]- fichier
Syntaxe pour la configuration d'ACL non triviales
chmod [options] A[index]{+|=}user|group:name:droits d'accès /...[:indicateurs
d'héritage]:deny | allow fichier
chmod [options] A-user|group:name:droits d'accès /...[:indicateurs d'héritage]:deny |
allow fichier ...
chmod [options] A[index]- fichier
owner@, group@, everyone@

Identifie le type d'entrée d'ACL pour la syntaxe d'ACL triviale. Pour obtenir une description
des types d'entrées d'ACL, reportez-vous au Tableau 8–1.

utilisateur ou groupe :ID-entrée-ACL=nomutilisateur ou nomgroupe

Identifie le type d'entrée d'ACL pour la syntaxe d'ACL explicite. Le type d'entrée d'ACL pour
l'utilisateur et le groupe doit également contenir l'ID d'entrée d'ACL, le nom d'utilisateur ou
le nom de groupe. Pour obtenir une description des types d'entrées d'ACL, reportez-vous au
Tableau 8–1.
droits-accès/.../

Identifie les droits d'accès accordés ou refusés. Pour obtenir une description des privilèges
d'accès d'ACL, reportez-vous au Tableau 8–2.

indicateurs-héritage

Identifie une liste optionnelle d'indicateurs d'héritage d'ACL. Pour une description des
indicateurs d'héritage d'ACL, reportez-vous au Tableau 8–3.

deny | allow

Détermine si les droits d'accès sont accordés ou refusés.

Dans l'exemple suivant, la valeur de l'ID d'entrée d'ACL n'est pas pertinente.

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

233

Nouveau modèle ACL Solaris

group@:write_data/append_data/execute:deny

L'exemple suivant inclut un ID d'entrée d'ACL car un utilisateur spécifique (type d'entrée d'ACL)
est inclus dans la liste.

0:user:gozer:list_directory/read_data/execute:allow

Lorsqu'une entrée d'ACL s'affiche, elle est similaire à celle-ci :

2:group@:write_data/append_data/execute:deny

La désignation 2 ou ID d'index dans cet exemple identifie l'entrée d'ACL dans la plus grande
ACL, qui peut présenter plusieurs entrées pour le propriétaire, des UID spécifiques, un groupe
et pour tous. Vous pouvez spécifier l'ID d'index avec la commande chmod pour identifier la
partie de l'ACL que vous souhaitez modifier. Par exemple, vous pouvez identifier l'ID d'index 3
par A3 dans la commande chmod comme ci-dessous :

chmod A3=user:venkman:read_acl:allow filename

Les types d'entrées d'ACL (qui sont les représentations d'ACL du propriétaire, du groupe et
autres) sont décrits dans le tableau suivant.

TABLEAU 8–1 Types d'entrées d'ACL

Type d'entrée d'ACL

Description

owner@

group@

everyone@

user

group

Spécifie l'accès accordé au propriétaire de l'objet.

Spécifie l'accès accordé au groupe propriétaire de l'objet.

Spécifie l'accès accordé à tout utilisateur ou groupe ne correspondant à aucune
autre entrée d'ACL.

Avec un nom d'utilisateur, spécifie l'accès accordé à un utilisateur supplémentaire
de l'objet. Doit inclure l'ID d'entrée d'ACL qui contient un nom d'utilisateur ou un
ID utilisateur. Le type d'entrée d'ACL est incorrect si la valeur n'est ni un UID
numérique, ni un nom d'utilisateur.

Avec un nom de groupe, spécifie l'accès accordé à un utilisateur supplémentaire de
l'objet. Doit inclure l'ID d'entrée d'ACL qui contient un nom de groupe ou un ID de
groupe. Le type d'entrée d'ACL est incorrect si la valeur n'est ni un GID numérique,
ni un nom de groupe.

Les privilèges d'accès sont décrits dans le tableau suivant.

234

Guide d'administration Solaris ZFS • Octobre 2009

TABLEAU 8–2 Privilèges d'accès d'ACL

Nouveau modèle ACL Solaris

Privilège d'accès

add_file

add_subdirectory

append_data

delete

delete_child

exécution

list_directory

read_acl

read_attributes

read_data

read_xattr

synchroniser

write_xattr

write_data

write_attributes

write_acl

Privilège d'accès
compact

Description

w

p

p

D

D

x

r

C

a

r

R

s

W

w

A

C

Droit d'ajouter un fichier à un répertoire.

Dans un répertoire, droit de créer un sous-répertoire.

Marque de réservation. Non implémentée actuellement.

Droit de supprimer un fichier.

Droit de supprimer un fichier ou un répertoire au sein d'un
répertoire.

Droit d'exécuter un fichier ou d'effectuer une recherche dans le
contenu d'un répertoire.

Droit de dresser la liste du contenu d'un répertoire.

Droit de lire l'ACL (ls).

Droit de lire les attributs de base (non ACL) d'un fichier.
Considérez les attributs de base comme les attributs de niveau stat.
L'autorisation de ce bit de masque d'accès signifie que l'entité peut
exécuter ls(1) et stat(2).

Droit de lire le contenu du fichier.

Droit de lire les attributs étendus d'un fichier ou d'effectuer une
recherche dans le répertoire d'attributs étendus d'un fichier.

Marque de réservation. Non implémentée actuellement.

Droit de créer des attributs étendus ou d'écrire dans le répertoire
d'attributs étendus.
L'attribution de ce droit à un utilisateur signifie que ce dernier peut
créer un répertoire d'attributs étendus pour un fichier. Les droits du
fichier d'attributs contrôlent l'accès de l'utilisateur à l'attribut.

Droit de modifier ou de remplacer le contenu d'un fichier.

Droit de remplacer les durées associées à un fichier ou un répertoire
par une valeur arbitraire.

Droit d'écriture sur l'ACL ou capacité de la modifier à l'aide de la
commande chmod.

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

235

Nouveau modèle ACL Solaris

TABLEAU 8–2 Privilèges d'accès d'ACL

(Suite)

Privilège d'accès

Privilège d'accès
compact

Description

write_owner

O

Droit de modifier le propriétaire ou le groupe d'un fichier. Ou
capacité d'exécuter les commandes chown ou chgrp sur le fichier.
Droit de devenir propriétaire d'un fichier ou droit de définir la
propriété de groupe du fichier sur un groupe dont fait partie
l'utilisateur. Le privilège PRIV_FILE_CHOWN est requis pour définir la
propriété de fichier ou de groupe sur un groupe ou un utilisateur
arbitraire.

Héritage d'ACL
L'héritage d'ACL a pour finalité de permettre à un fichier ou répertoire récemment créé
d'hériter des ACL qui leurs sont destinées, tout en tenant compte des bits de droits existants
dans le répertoire parent.

Par défaut, les ACL ne sont pas propagées. Si vous configurez une ACL non triviale dans un
répertoire, aucun répertoire subséquent n'en hérite. Vous devez spécifier l'héritage d'une ACL
dans un fichier ou un répertoire.

Les indicateurs d'héritage facultatifs sont décrits dans le tableau suivant.

TABLEAU 8–3

Indicateurs d'héritage d'ACL

Indicateur d'héritage

Indicateur d'héritage
compact

Description

file_inherit

dir_inherit

inherit_only

f

d

i

no_propagate

n

Hérite de l'ACL uniquement à partir du répertoire parent vers
les fichiers du répertoire.

Hérite de l'ACL uniquement à partir du répertoire parent vers
les sous-répertoires du répertoire.

Hérite de l'ACL à partir du répertoire parent mais ne s'applique
qu'aux fichiers et sous-répertoires récemment créés, pas au
répertoire lui-même. Cet indicateur requiert les indicateurs
file_inherit et/ou dir_inherit afin de spécifier ce qui doit
être hérité.

N'hérite que de l'ACL provenant du répertoire parent vers le
contenu de premier niveau du répertoire, et non les contenus
de second niveau et suivants. Cet indicateur requiert les
indicateurs file_inherit et/ou dir_inherit afin de spécifier
ce qui doit être hérité.

-

SO

Aucun droit n'est accordé.

236

Guide d'administration Solaris ZFS • Octobre 2009

Nouveau modèle ACL Solaris

De plus, vous pouvez configurer un héritage d'ACL par défaut plus ou moins strict sur le
système de fichiers à l'aide de la propriété de système de fichiers aclinherit. Pour de plus
amples informations, consultez la section suivante.

Modes de propriétés d'ACL
Le système de fichiers ZFS inclut deux modes de propriétés relatifs aux ACL :

■

aclinherit – Cette propriété détermine le comportement de l'héritage d'ACL. Les valeurs
possibles sont les suivantes :

■

■

■

■

■

discard – Pour les nouveaux objets, aucune entrée d'ACL n'est héritée lors de la création
d'un fichier ou d'un répertoire. L'ACL dans le fichier ou le répertoire est égale au mode de
droit du fichier ou répertoire.
noallow – Pour les nouveaux objets, seules les entrées d'ACL héritables dont le type
d'accès est deny sont héritées.
restricted – Pour les nouveaux objets, les droits write_owner et write_acl sont
supprimés lorsqu'une entrée d'ACL est héritée.
passthrough – Lorsqu'une valeur de propriété est définie sur passthrough, les fichiers
sont créés dans un mode déterminé par les ACE héritées. Si aucune ACE pouvant être
héritée n'affecte le mode, ce mode est alors défini en fonction du mode demandé à partir
de l'application.
passthrough-x : a la même sémantique que passthrough, si ce n'est que lorsque
passthrough-x est activé, les fichiers sont créés avec l'autorisation d'exécution (x), mais
uniquement si l'autorisation d'exécution est définie en mode de création de fichier et
dans une entrée de contrôle d'accès (ACE) pouvant être héritée et qui affecte le mode.

■

Le mode par défaut de aclinherit est restricted.
aclmode – Cette propriété modifie le comportement des ACL lorsqu'un fichier est créé ou
chaque fois que le mode d'un fichier ou d'un répertoire est modifié à l'aide de la commande
chmod. Les valeurs possibles sont les suivantes :

■

■

■

discard – Toutes les entrées d'ACL sont supprimées à l'exception des entrées
nécessaires à la définition du mode pour le fichier ou le répertoire.
groupmask – Les droits d'ACL de groupe ou d'utilisateur sont réduits de manière à ce
qu'ils ne soient pas supérieurs aux bits de droit du groupe, à moins qu'il ne s'agisse d'une
entrée d'utilisateur ayant le même UID que le propriétaire du fichier ou du répertoire.
Ensuite, les droits d'ACL sont réduits afin qu'ils ne dépassent pas les bits de droit du
propriétaire.
passthrough – Au cours d'une opération chmod, les ACE autres que owner@, group@ ou
everyone@ ne sont modifiées d'aucune manière. Les ACE owner@, group@ ou everyone@
sont désactivées afin de définir le mode de fichier comme demandé par l'opération
chmod.

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

237

Configuration d'ACL dans des fichiers ZFS

Le mode par défaut de la propriété aclmode est groupmask.

Configuration d'ACL dans des fichiers ZFS

Dans la mesure où elles sont implémentées avec ZFS, les ACL se composent d'un tableau
d'entrées d'ACL. ZFS fournit un modèle d'ACL pur, dans lequel tous les fichiers présentent une
ACL. L'ACL est habituellement insignifiante dans la mesure où elle ne représente que les entrées
UNIX traditionnelles owner/group/other.

Les fichiers ZFS disposent toujours de bits de droit et d'un mode, mais ces valeurs constituent
plus un cache de ce que représente une ACL. Par conséquent, si vous modifiez les droit du
fichier, l'ACL du fichier est mise à jour en conséquence. En outre, si vous supprimez une ACL
non triviale qui accordait à un utilisateur l'accès à un fichier ou à un répertoire, il est possible
que cet utilisateur y ait toujours accès en raison des bits de droit qui accordent l'accès à un
groupe ou à tous. L'ensemble des décisions de contrôle d'accès est régi par les droit représentés
dans l'ACL d'un fichier ou d'un répertoire.

Les règles principales d'accès aux ACL dans un fichier ZFS sont comme suit :
■ ZFS traite les entrées d'ACL dans l'ordre dans lesquelles elles sont répertoriées dans l'ACL,

en partant du haut.
Seules les entrées d'ACL disposant d'un " who " correspondant au demandeur d'accès sont
traitées.

■

■ Une fois le droit allow accordé, ce dernier ne peut plus être refusé par la suite par une entrée

d'ACL de refus dans le même jeu de droits d'ACL.

■ Le propriétaire du fichier dispose du droit write_acl de façon inconditionnelle, même si

celui-ci est explicitement refusé. Dans le cas contraire, tout droit non spécifié est refusé.
Dans les cas de droits deny ou lorsqu'un droit d'accès est manquant, le sous-système de
privilèges détermine la requête d'accès accordée pour le propriétaire du fichier ou pour le
superutilisateur. Ce mécanisme évite que les propriétaires de fichiers puissent accéder à
leurs fichiers et permet aux superutilisateurs de modifier les fichiers à des fins de
récupération.

Si vous configurez une ACL non triviale dans un répertoire, les enfants du répertoire n'en
héritent pas automatiquement. Si vous configurez une ACL non triviale, et souhaitez qu'elle soit
héritée par les enfants du répertoire, vous devez utiliser les indicateurs d'héritage d'ACL. Pour
plus d'informations, consultez le Tableau 8–3 et la section “Configuration d'héritage d'ACL
dans des fichiers ZFS en format détaillé” à la page 246.

Lorsque vous créez un fichier, en fonction de la valeur umask, une ACL triviale par défaut,
similaire à la suivante, est appliquée :

$ ls -v file.1

-r--r--r--

1 root

root

206663 Aug 31 11:53 file.1

238

Guide d'administration Solaris ZFS • Octobre 2009

Configuration d'ACL dans des fichiers ZFS

0:owner@:write_data/append_data/execute:deny

1:owner@:read_data/write_xattr/write_attributes/write_acl/write_owner

:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Notez que chaque catégorie d'utilisateur (owner@, group@, everyone@) de cet exemple dispose
de deux entrées d'ACL. Une entrée correspond aux droits deny et une autre, aux droits allow.

Voici une description de l'ACL de ce fichier :

0:owner@

1:owner@

2:group@

3:group@

4:everyone@

5:everyone@

Les droits d'écriture et d'exécution sur fichier sont refusés au propriétaire
(write_data/append_data/execute:deny).
Le propriétaire peut lire et modifier le contenu d'un fichier (
read_data/write_data/append_data). Il peut également modifier les
attributs du fichier tels que les horodatages, les attributs étendus et les ACL
(write_xattr/write_attributes /write_acl). De plus, le propriétaire
peut modifier la propriété du fichier (write_owner:allow).
Les droits de modification et d'exécution sur fichier sont refusés au groupe
(write_data/append_data/execute:deny).
Des droits de lecture du fichier sont accordés au groupe (read_data:allow).
Le droit d'exécuter ou de modifier le contenu ou tout attribut d'un fichier est
refusé à toute personne ne correspondant ni à un groupe ni à un utilisateur
(write_data/append_data/write_xattr/execute/
write_attributes/write_acl/write_owner:deny ).
Les droits de lecture du fichier et de ses attributs sont attribués à toute
personne ne correspondant ni à un utilisateur ni à un groupe
(read_data/read_xattr/read_attributes/read_acl/
synchronize:allow ). Le droit d'accès synchronize n'est actuellement pas
implémenté.

Lorsqu'un répertoire est créé, en fonction de la valeur umask, l'ACL par défaut du répertoire est
similaire à l'exemple suivant :

$ ls -dv dir.1

drwxr-xr-x

2 root

root

2 Aug 31 11:54 dir.1

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

239

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

2:group@:add_file/write_data/add_subdirectory/append_data:deny

3:group@:list_directory/read_data/execute:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Voici une description de l'ACL de ce répertoire :

0:owner@

1:owner@

2:group@

3:group@

4:everyone@

5:everyone@

La liste de refus du propriétaire est vide pour le répertoire (::deny).
Le propriétaire peut lire et modifier le contenu du répertoire
(list_directory/read_data/add_file/write_data/
add_subdirectory/append_data ), effectuer des recherches dans le contenu
(execute) et modifier les attributs du fichier, notamment les horodatages, les
attributs étendus et les ACL ( write_xattr/write_attributes/write_acl).
De plus, le propriétaire peut modifier la propriété du répertoire
(write_owner:allow).
Le groupe ne peut pas ajouter ni modifier le contenu du répertoire
(add_file/write_data/add_subdirectory/append_data
:deny).
Le groupe peut répertorier et lire le contenu du répertoire. De plus, le groupe
dispose de droits d'exécution pour effectuer des recherches dans le contenu
du répertoire (list_directory/read_data/execute:allow).
Le droit d'ajouter ou de modifier le contenu du répertoire est refusé à toute
personne ne correspondant ni à un utilisateur ni à un groupe
(add_file/write_data/add_subdirectory/append_data ). De plus, le
droit de modifier les attributs du répertoire est refusé. (write_xattr
/write_attributes/write_acl/write_owner:deny).
Toute personne n'étant ni un utilisateur ni un groupe dispose de droits de
lecture et d'exécution sur le contenu et les attributs du répertoire
(list_directory/read_data/read_xattr/execute/read_
attributes/read_acl/synchronize:allow ). Le droit d'accès synchronize
n'est actuellement pas implémenté.

Configuration et affichage d'ACL dans des fichiers ZFS en
format détaillé

Vous pouvez modifier les ACL dans des fichiers ZFS à l'aide de la commande chmod. La syntaxe
chmod suivante pour la modification de l'ACL utilise la spécification acl pour identifier le format

240

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

de la liste. Pour une description de la spécification ACL, reportez-vous à la section “Descriptions
de syntaxe pour la configuration des ACL” à la page 233.
■ Ajout d'entrées d'ACL

■ Ajout d'une entrée d'ACL pour un utilisateur

% chmod A+acl-specification filename

■ Ajout d'une entrée d'ACL par ID d'index

% chmod Aindex-ID+acl-specification filename

Cette syntaxe insère la nouvelle entrée d'ACL à l'emplacement d'ID d'index spécifié.

■ Remplacement d'une entrée d'ACL

% chmod A=acl-specification filename

% chmod Aindex-ID=acl-specification filename

■

Suppression d'entrées d'ACL

■

■

■

Suppression d'une entrée d'ACL par l'ID d'index

% chmod Aindex-ID- filename

Suppression d'une entrée d'ACL par utilisateur

% chmod A-acl-specification filename

Suppression de la totalité des ACE non triviales d'un fichier

% chmod A- filename

Les informations détaillées de l'ACL s'affichent à l'aide de la commande ls - v. Exemple :

# ls -v file.1

-rw-r--r--

1 root

root

206663 Aug 31 11:53 file.1

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Pour obtenir des informations sur l'utilisation du format d'ACL compact, consultez
“Configuration et affichage d'ACL dans des fichiers ZFS en format compact” à la page 254.

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

241

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–1 Modification des ACL triviales dans des fichiers ZFS
Cette section fournit des exemples de définition et d'affichage d'ACL insignifiantes.

Dans l'exemple suivant, une ACL triviale existe dans le fichier file.1 :

# ls -v file.1

-rw-r--r--

1 root

root

206663 Aug 31 11:53 file.1

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivants, les droits write_data sont accordés au groupe group@.

# chmod A2=group@:append_data/execute:deny file.1

# chmod A3=group@:read_data/write_data:allow file.1

# ls -v file.1

-rw-rw-r--

1 root

root

206663 Aug 31 11:53 file.1

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:append_data/execute:deny

3:group@:read_data/write_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivant, les droits du fichier file.1 sont reconfigurés sur 644.

# chmod 644 file.1

# ls -v file.1

-rw-r--r--

1 root

root

206663 Aug 31 11:53 file.1

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

242

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–2 Configuration d'ACL non triviales dans des fichiers ZFS

Cette section fournit des exemples de configuration et d'affichage d'ACL non triviales.

Dans l'exemple suivant, les droits read_data/execute sont ajoutés à l'utilisateur gozer dans le
répertoire test.dir.

# chmod A+user:gozer:read_data/execute:allow test.dir

# ls -dv test.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 12:02 test.dir

0:user:gozer:list_directory/read_data/execute:allow

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les droits read_data/execute sont retirés à l'utilisateur gozer.

# chmod A0- test.dir

# ls -dv test.dir

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

2:group@:add_file/write_data/add_subdirectory/append_data:deny

3:group@:list_directory/read_data/execute:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

EXEMPLE 8–3

Interactions entre les ACL et les droits dans les fichiers ZFS

Ces exemples d'ACL illustrent l'interaction entre la configuration d'ACL et la modification des
bits de droit du fichier ou du répertoire.

Dans l'exemple suivant, une ACL triviale existe dans le fichier file.2:

# ls -v file.2

-rw-r--r--

1 root

root

2836 Aug 31 12:06 file.2

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

243

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–3

Interactions entre les ACL et les droits dans les fichiers ZFS

(Suite)

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans l'exemple suivant, les droits d'ACL (allow) sont retirés à everyone@.

# chmod A5- file.2

# ls -v file.2

-rw-r-----+ 1 root

root

2836 Aug 31 12:06 file.2

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

Dans cette sortie, les bits de droit du fichier sont réinitialisés de 655 à 650. Les droits de lecture
de everyone@ ont été supprimés des bits de droit du fichier lorsque les droits "allow" des ACL
ont été supprimés de everyone@.

Dans l'exemple suivant, l'ACL existante est remplacée par des droits read_data/write_data
pour everyone@.

# chmod A=everyone@:read_data/write_data:allow file.3

# ls -v file.3

-rw-rw-rw-+ 1 root

root

2455 Aug 31 12:08 file.3

0:everyone@:read_data/write_data:allow

Dans cette sortie, la syntaxe chmod remplace effectivement l'ACL existante par les droits
read_data/write_data:allow pour les droits de lecture/écriture pour le propriétaire, le
groupe et everyone@. Dans ce modèle, everyone@ spécifie l'accès à tout utilisateur ou groupe.
Dans la mesure où aucune entrée d'ACL owner@ ou group@ n'existe pour ignorer les droits pour
l'utilisateur ou le groupe, les bits de droit sont définis sur 666.

Dans l'exemple suivant, l'ACL existante est remplacée par des droits de lecture pour l'utilisateur
gozer.

244

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–3

Interactions entre les ACL et les droits dans les fichiers ZFS

(Suite)

# chmod A=user:gozer:read_data:allow file.3

# ls -v file.3

----------+ 1 root

root

2455 Aug 31 12:08 file.3

0:user:gozer:read_data:allow

Dans cette sortie, les droits de fichier sont calculées pour être 000 car aucune entrée d'ACL
n'existe pour owner@, group@, ou everyone@, qui représentent les composant de droit classiques
d'un fichier. Le propriétaire du fichier peut résoudre ce problème en réinitialisant les droits (et
l'ACL) comme suit :

# chmod 655 file.3

# ls -v file.3

-rw-r-xr-x+ 1 root

root

2455 Aug 31 12:08 file.3

0:user:gozer::deny

1:user:gozer:read_data:allow

2:owner@:execute:deny

3:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

4:group@:write_data/append_data:deny

5:group@:read_data/execute:allow

6:everyone@:write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:deny

7:everyone@:read_data/read_xattr/execute/read_attributes/read_acl

/synchronize:allow

EXEMPLE 8–4 Restauration des ACL triviales dans des fichiers ZFS

Vous pouvez utiliser la commande chmod pour supprimer toutes les ACL non insignifiantes
d'un fichier ou d'un répertoire.

Dans l'exemple suivant, deux ACE non insignifiantes existent dans test5.dir.

# ls -dv test5.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 12:11 test5.dir

0:user:lp:read_data:file_inherit:deny

1:user:gozer:read_data:file_inherit:deny

2:owner@::deny

3:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

4:group@:add_file/write_data/add_subdirectory/append_data:deny

5:group@:list_directory/read_data/execute:allow

6:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

245

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–4 Restauration des ACL triviales dans des fichiers ZFS

(Suite)

7:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les ACL non triviales pour les utilisateurs gozer et lp sont supprimées.
L'ACL restante contient les six valeurs par défaut de owner@, group@ et everyone@.

# chmod A- test5.dir

# ls -dv test5.dir

drwxr-xr-x

2 root

root

2 Aug 31 12:11 test5.dir

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

2:group@:add_file/write_data/add_subdirectory/append_data:deny

3:group@:list_directory/read_data/execute:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Configuration d'héritage d'ACL dans des fichiers ZFS
en format détaillé
Vous pouvez déterminer comment les ACL sont héritées ou non dans les fichiers et répertoires.
Par défaut, les ACL ne sont pas propagées. Si vous configurez une ACL non triviale dans un
répertoire, aucun répertoire subséquent n'en hérite. Vous devez spécifier l'héritage d'une ACL
dans un fichier ou un répertoire.

En outre, deux propriétés d'ACL sont fournies afin de permettre leur configuration globale dans
les systèmes de fichiers : aclinherit et aclmode. Par défaut, aclinherit est définie sur
restricted et aclmode sur groupmask .

Pour plus d'informations, reportez-vous à la section “Héritage d'ACL” à la page 236.

EXEMPLE 8–5 Attribution d'héritage d'ACL par défaut
Par défaut, les ACL ne sont pas propagées par le biais d'une structure de répertoire.

Dans l'exemple suivant, une ACE non insignifiante de read_data/write_data/execute est
appliquée pour l'utilisateur gozer dans le fichier test.dir.

# chmod A+user:gozer:read_data/write_data/execute:allow test.dir

# ls -dv test.dir

246

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–5 Attribution d'héritage d'ACL par défaut

(Suite)

drwxr-xr-x+ 2 root

root

2 Aug 31 13:02 test.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute:allow

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Si un sous-répertoire test.dir est créé, l'ACE pour l'utilisateur gozer n'est pas propagée.
L'utilisateur gozer n'aurait accès à sub.dir que si les droits de sub.dir lui accordaient un accès
en tant que propriétaire de fichier, membre de groupe ou everyone@.

# mkdir test.dir/sub.dir

# ls -dv test.dir/sub.dir

drwxr-xr-x

2 root

root

2 Aug 31 13:26 test.dir/sub.dir

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

2:group@:add_file/write_data/add_subdirectory/append_data:deny

3:group@:list_directory/read_data/execute:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

EXEMPLE 8–6 Attribution d'héritage d'ACL dans les fichiers et les répertoires

Cette série d'exemples identifie les ACE du fichier et du répertoire qui sont appliquées lorsque
l'indicateur file_inherit est paramétré.

Dans l'exemple suivant, les droits read_data/write_data sont ajoutés pour les fichiers dans le
répertoire test.dir pour l'utilisateur gozer pour qu'il dispose de l'accès à tout nouveau fichier.

# chmod A+user:gozer:read_data/write_data:file_inherit:allow test2.dir

# ls -dv test2.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 13:26 test2.dir

0:user:gozer:read_data/write_data:file_inherit:allow

1:owner@::deny

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

247

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–6 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans l'exemple suivant, les droits de l'utilisateur gozer sont appliqués au fichier
test2.dir/file.2 récemment créé. L'héritage d'ACL étant accordé
(read_data:file_inherit:allow), l'utilisateur gozer peut lire le contenu de tout nouveau
fichier.

# touch test2.dir/file.2

# ls -v test2.dir/file.2

-rw-r--r--+ 1 root

root

0 Aug 31 13:27 test2.dir/file.2

0:user:gozer:write_data:deny

1:user:gozer:read_data/write_data:allow

2:owner@:execute:deny

3:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

4:group@:write_data/append_data/execute:deny

5:group@:read_data:allow

6:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

7:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Dans la mesure où la propriété aclmode pour ce fichier est paramétrée sur le mode par défaut,
groupmask, l'utilisateur gozer ne dispose pas du droit write_data pour le fichier file.2 car les
droits de groupe du fichier ne le permettent pas.

Notez que le droit inherit_only appliquée lorsque les indicateurs file_inherit ou
dir_inherit sont définis, est utilisée pour propager l'ACL dans la structure du répertoire.
Ainsi, l'utilisateur gozer se voit uniquement accorder ou refuser le droit des droits everyone@, à
moins qu'il ne soit le propriétaire du fichier ou membre du groupe propriétaire du fichier.
Exemple :

# mkdir test2.dir/subdir.2

# ls -dv test2.dir/subdir.2

drwxr-xr-x+ 2 root

root

2 Aug 31 13:28 test2.dir/subdir.2

0:user:gozer:list_directory/read_data/add_file/write_data:file_inherit

/inherit_only:allow

248

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–6 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

La série d'exemples suivants identifie les ACL du fichier et du répertoire appliquées lorsque les
indicateurs file_inherit et dir_inherit sont paramétrés.

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les droits de lecture, d'écriture et
d'exécution hérités des fichiers et répertoires récemment créés.

# chmod A+user:gozer:read_data/write_data/execute:file_inherit/dir_inherit:allow

test3.dir

# ls -dv test3.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 13:29 test3.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:file_inherit/dir_inherit:allow

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

# touch test3.dir/file.3

# ls -v test3.dir/file.3

-rw-r--r--+ 1 root

root

0 Jun 20 14:42 test3.dir/file.3

0:user:gozer:write_data/execute:deny

1:user:gozer:read_data/write_data/execute:allow

2:owner@:execute:deny

3:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

4:group@:write_data/append_data/execute:deny

5:group@:read_data:allow

6:everyone@:write_data/append_data/write_xattr/execute/write_attributes

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

249

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–6 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

/write_acl/write_owner:deny

7:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

# mkdir test3.dir/subdir.1

# ls -dv test3.dir/subdir.1

drwxr-xr-x+ 2 root

root

2 Aug 31 13:32 test3.dir/subdir.1

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:file_inherit/dir_inherit/inherit_only:allow

1:user:gozer:add_file/write_data:deny

2:user:gozer:list_directory/read_data/add_file/write_data/execute:allow

3:owner@::deny

4:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

5:group@:add_file/write_data/add_subdirectory/append_data:deny

6:group@:list_directory/read_data/execute:allow

7:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

8:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Dans ces exemples, les bits de droit du répertoire parent pour group@ et everyone@ n'accordent
pas les droits. Par conséquent, l'utilisateur gozer se voit refuser ces droits. La propriété par
défaut aclmode est restricted, ce qui signifie que les droits write_data et execute ne sont pas
hérités.

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les droits de lecture, d'écriture et
d'exécution qui sont héritées pour les fichiers récemment créés, mais ne sont pas propagées vers
tout contenu subséquent du répertoire.

# chmod A+user:gozer:read_data/write_data/execute:file_inherit/no_propagate:allow

test4.dir

# ls -dv test4.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 13:34 test4.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:file_inherit/no_propagate:allow

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

250

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–6 Attribution d'héritage d'ACL dans les fichiers et les répertoires

(Suite)

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'exemple suivant l'illustre, lors de la création d'un sous-répertoire, le droit
read_data/write_data/execute de l'utilisateur gozer pour les fichiers n'est pas propagée au
nouveau répertoire sub4.dir.

mkdir test4.dir/sub4.dir

# ls -dv test4.dir/sub4.dir

drwxr-xr-x

2 root

root

2 Aug 31 13:35 test4.dir/sub4.dir

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

2:group@:add_file/write_data/add_subdirectory/append_data:deny

3:group@:list_directory/read_data/execute:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'exemple suivant l'illustre, le droit read_data/write_data/execute de gozer pour
les fichiers est propagé vers le nouveau fichier.

# touch test4.dir/file.4

# ls -v test4.dir/file.4

-rw-r--r--+ 1 root

root

0 Aug 31 13:35 test4.dir/file.4

0:user:gozer:write_data/execute:deny

1:user:gozer:read_data/write_data/execute:allow

2:owner@:execute:deny

3:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

4:group@:write_data/append_data/execute:deny

5:group@:read_data:allow

6:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

7:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

EXEMPLE 8–7 Héritage d'ACL avec mode de liste défini sur Pass Through

Si la propriété aclmode dans le système de fichiers tank/cindys est définie sur passthrough,
l'utilisateur gozer hérite alors de l'ACL appliquée à test4.dir pour le fichier file.4
récemment créé, comme suit :

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

251

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–7 Héritage d'ACL avec mode de liste défini sur Pass Through

(Suite)

# zfs set aclmode=passthrough tank/cindys

# touch test4.dir/file.4

# ls -v test4.dir/file.4

-rw-r--r--+ 1 root

root

0 Aug 31 13:39 test4.dir/file.4

0:user:gozer:write_data/execute:deny

1:user:gozer:read_data/write_data/execute:allow

2:owner@:execute:deny

3:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

4:group@:write_data/append_data/execute:deny

5:group@:read_data:allow

6:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

7:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Cette sortie montre que l'ACL
read_data/write_data/execute:allow:file_inherit/dir_inherit définie sur le répertoire
parent, test4.dir, est transmise à l'utilisateur gozer.

EXEMPLE 8–8 Héritage d'ACL avec mode de liste défini sur Discard

Si la propriété aclmode d'un système de fichiers est définie sur discard, il est alors possible de
supprimer les ACL avec les bits de droit dans un changement de répertoire. Exemple :

# zfs set aclmode=discard tank/cindys

# chmod A+user:gozer:read_data/write_data/execute:dir_inherit:allow test5.dir

# ls -dv test5.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 13:40 test5.dir

0:user:gozer:list_directory/read_data/add_file/write_data/execute

:dir_inherit:allow

1:owner@::deny

2:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

3:group@:add_file/write_data/add_subdirectory/append_data:deny

4:group@:list_directory/read_data/execute:allow

5:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

6:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Si vous décidez ultérieurement de renforcer les bits de droit d'un répertoire, l'ACL non triviale
est supprimée. Exemple :

252

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé

EXEMPLE 8–8 Héritage d'ACL avec mode de liste défini sur Discard

(Suite)

# chmod 744 test5.dir

# ls -dv test5.dir

drwxr--r--

2 root

root

2 Aug 31 13:40 test5.dir

0:owner@::deny

1:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

2:group@:add_file/write_data/add_subdirectory/append_data/execute:deny

3:group@:list_directory/read_data:allow

4:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/execute/write_attributes/write_acl/write_owner:deny

5:everyone@:list_directory/read_data/read_xattr/read_attributes/read_acl

/synchronize:allow

EXEMPLE 8–9 Héritage d'ACL avec mode d'héritage de liste défini sur Noallow

Dans l'exemple suivant, deux ACL non triviales avec héritage de fichier sont définies. Une ACL
autorise le droit read_data, tandis qu'un autre refuse ce droit. Cet exemple illustre également
comment spécifier deux ACE dans la même commande chmod.

# zfs set aclinherit=noallow tank/cindys

# chmod A+user:gozer:read_data:file_inherit:deny,user:lp:read_data:file_inherit:allow

test6.dir

# ls -dv test6.dir

drwxr-xr-x+ 2 root

root

2 Aug 31 13:43 test6.dir

0:user:gozer:read_data:file_inherit:deny

1:user:lp:read_data:file_inherit:allow

2:owner@::deny

3:owner@:list_directory/read_data/add_file/write_data/add_subdirectory

/append_data/write_xattr/execute/write_attributes/write_acl

/write_owner:allow

4:group@:add_file/write_data/add_subdirectory/append_data:deny

5:group@:list_directory/read_data/execute:allow

6:everyone@:add_file/write_data/add_subdirectory/append_data/write_xattr

/write_attributes/write_acl/write_owner:deny

7:everyone@:list_directory/read_data/read_xattr/execute/read_attributes

/read_acl/synchronize:allow

Comme l'illustre l'exemple suivant, lors de la création d'un nouveau fichier, l'ACL qui autorise
le droit read_data est supprimée.

# touch test6.dir/file.6

# ls -v test6.dir/file.6

-rw-r--r--

1 root

root

0 Aug 31 13:44 test6.dir/file.6

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

253

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–9 Héritage d'ACL avec mode d'héritage de liste défini sur Noallow

(Suite)

0:owner@:execute:deny

1:owner@:read_data/write_data/append_data/write_xattr/write_attributes

/write_acl/write_owner:allow

2:group@:write_data/append_data/execute:deny

3:group@:read_data:allow

4:everyone@:write_data/append_data/write_xattr/execute/write_attributes

/write_acl/write_owner:deny

5:everyone@:read_data/read_xattr/read_attributes/read_acl/synchronize

:allow

Configuration et affichage d'ACL dans des fichiers ZFS en
format compact

Vous pouvez définir et afficher les droits relatifs aux fichiers ZFS en format compact utilisant 14
lettres uniques pour représenter les droits. Les lettres représentant les droits compacts sont
répertoriées dans le Tableau 8–2 et le Tableau 8–3.

Vous pouvez afficher les listes d'ACL compactes pour les fichiers et les répertoires à l'aide de la
commande ls -V. Exemple :

# ls -V file.1

-rw-r--r--

1 root

root

206663 Aug 31 13:54 file.1

owner@:--x-----------:------:deny

owner@:rw-p---A-W-Co-:------:allow

group@:-wxp----------:------:deny

group@:r-------------:------:allow

everyone@:-wxp---A-W-Co-:------:deny

everyone@:r-----a-R-c--s:------:allow

La sortie d'ACL compacte est décrite comme suit :

owner@

owner@

group@

group@

Le droit d'exécution sur le fichier est refusé au propriétaire (x= execute).
Le propriétaire peut lire et modifier le contenu du fichier (
rw=read_data/write_data), (p= append_data). Il peut également modifier les
attributs du fichier, par exemple l'horodatage, les attributs étendus et les ACL
(A=write_xattr, W=write_attributes, C= write_acl). De plus, le
propriétaire peut modifier la propriété du fichier (o=write_owner).
Les droits de modification et d'exécution sur le fichier sont refusés au groupe
(write_data, p=append_data et x=execute).
Les droits de lecture sur le fichier sont accordés au groupe (r= read_data).

254

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

everyone@

everyone@

Les droits d'exécution ou de modification du contenu du fichier, ou de
modification de tout attribut du fichier sont refusés à toute personne n'étant ni
un utilisateur ni un groupe (w=write_data, x= execute, p=append_data,
A=write_xattr, W=write_attributes , C=write_acl et o= write_owner).
Les droits de lecture sur le fichier et sur ses attributs sont accordés à toute
personne n'étant ni un utilisateur ni un groupe (r=read_data, a=append_data,
R=read_xattr , c=read_acl et s= synchronize). Le droit d'accès
synchronize n'est actuellement pas implémentée.

Le format d'ACL compact dispose des avantages suivants par rapport au format d'ACL détaillé :
■ Les droits peuvent être spécifiés en tant qu'arguments de position pour la commande chmod.
■ Les tirets (-), qui n'identifient aucun droit, peuvent être supprimés. Seules les lettres

nécessaires doivent être spécifiées.

■ Les indicateurs de droits et d'héritage sont configurés de la même manière.

Pour obtenir des informations sur l'utilisation du format d'ACL détaillé, consultez
“Configuration et affichage d'ACL dans des fichiers ZFS en format détaillé” à la page 240.

EXEMPLE 8–10 Configuration et affichage des ACL en format compact
Dans l'exemple suivant, une ACL triviale existe dans le fichier file.1 :

# ls -V file.1

-rw-r--r--

1 root

root

206663 Aug 31 13:54 file.1

owner@:--x-----------:------:deny

owner@:rw-p---A-W-Co-:------:allow

group@:-wxp----------:------:deny

group@:r-------------:------:allow

everyone@:-wxp---A-W-Co-:------:deny

everyone@:r-----a-R-c--s:------:allow

Dans cet exemple, les droits read_data/execute sont ajoutés pour l'utilisateur gozer dans le
fichier file.1.

# chmod A+user:gozer:rx:allow file.1

# ls -V file.1

-rw-r--r--+ 1 root

root

206663 Aug 31 13:54 file.1

user:gozer:r-x-----------:------:allow

owner@:--x-----------:------:deny

owner@:rw-p---A-W-Co-:------:allow

group@:-wxp----------:------:deny

group@:r-------------:------:allow

everyone@:-wxp---A-W-Co-:------:deny

everyone@:r-----a-R-c--s:------:allow

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

255

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–10 Configuration et affichage des ACL en format compact

(Suite)

Une autre méthode d'ajout des mêmes droits pour l'utilisateur gozer consiste à insérer une ACL
à un emplacement spécifique, par exemple 4. Ainsi, les ACL existantes aux emplacements 4–6
sont déplacées vers le bas. Exemple :

# chmod A4+user:gozer:rx:allow file.1

# ls -V file.1

-rw-r--r--+ 1 root

root

206663 Aug 31 14:05 file.1

owner@:--xp----------:------:deny

owner@:rw-----A-W-Co-:------:allow

group@:-wxp----------:------:deny

group@:r-------------:------:allow

user:gozer:r-x-----------:------:allow

everyone@:-wxp---A-W-Co-:------:deny

everyone@:r-----a-R-c--s:------:allow

Dans l'exemple suivant, l'utilisateur gozer se voit accorder les droits de lecture, d'écriture et
d'exécution qui sont hérités des fichiers et répertoires récemment créés grâce à l'utilisation de
l'ACL compacte.

# chmod A+user:gozer:rwx:fd:allow dir.2

# ls -dV dir.2

drwxr-xr-x+ 2 root

root

2 Aug 28 13:21 dir.2

user:gozer:rwx-----------:fd----:allow

owner@:--------------:------:deny

owner@:rwxp---A-W-Co-:------:allow

group@:-w-p----------:------:deny

group@:r-x-----------:------:allow

everyone@:-w-p---A-W-Co-:------:deny

everyone@:r-x---a-R-c--s:------:allow

Vous pouvez également couper et coller les droits et les indicateurs d'héritage à partir de la
sortie ls -V en format chmod compact. Par exemple, afin de dupliquer les droits et les
indicateurs d'héritage du fichier dir.2 de l'utilisateur gozer à l'utilisateur cindys dans le fichier
dir.2, copiez et collez les droits et les indicateurs d'héritage ( rwx-----------:f-----:allow)
dans la commande chmod. Exemple :

# chmod A+user:cindys:rwx-----------:fd----:allow dir.2

# ls -dV dir.2

drwxr-xr-x+ 2 root

root

2 Aug 28 14:12 dir.2

user:cindys:rwx-----------:fd----:allow

user:gozer:rwx-----------:fd----:allow

owner@:--------------:------:deny

owner@:rwxp---A-W-Co-:------:allow

group@:-w-p----------:------:deny

256

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–10 Configuration et affichage des ACL en format compact

(Suite)

group@:r-x-----------:------:allow

everyone@:-w-p---A-W-Co-:------:deny

everyone@:r-x---a-R-c--s:------:allow

EXEMPLE 8–11 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through

Un système de fichiers dont la propriété aclinherit est définie sur passthrough hérite de
toutes les entrées d'ACL pouvant être héritées, sans qu'aucune modification ne leur soit
apportée. Lorsque cette propriété est définie sur passthrough, les fichiers sont créés avec un
mode de droit déterminé par les ACE pouvant être héritées. Si aucune ACE pouvant être héritée
n'affecte le mode de droit, ce mode est alors défini en fonction du mode demandé à partir de
l'application.

Les exemples suivants utilisent la syntaxe ACL compacte pour illustrer le processus d'héritage
des bits de droit en définissant le mode aclinherit sur la valeur passthrough .

Dans cet exemple, une ACL est définie sur test1.dir pour forcer l'héritage. La syntaxe crée une
entrée d'ACL owner@, group@ et everyone@ pour les fichiers nouvellement créés. Les
répertoires nouvellement créés héritent d'une entrée d'ACL @owner, group@ et everyone@. En
outre, les répertoires héritent de six autres ACE qui appliquent les ACE aux répertoires et
fichiers nouvellement créés.

# zfs set aclinherit=passthrough tank/cindys

# pwd

/tank/cindys

# mkdir test1.dir

# chmod A=owner@:rwxpcCosRrWaAdD:fd:allow,group@:rwxp:fd:allow,everyone@::fd:allow

test1.dir

# ls -Vd test1.dir

drwxrwx---+ 2 root

root

2 Aug 31 14:11 test1.dir

owner@:rwxpdDaARWcCos:fd----:allow

group@:rwxp----------:fd----:allow

everyone@:--------------:fd----:allow

Dans cet exemple, un fichier nouvellement créé hérite de l'ACL dont les fichiers nouvellement
créés doivent hériter d'après ce qui a été spécifié.

# cd test1.dir

# touch file.1

# ls -V file.1

-rwxrwx---+ 1 root

root

0 Aug 31 14:14 file.1

owner@:rwxpdDaARWcCos:------:allow

group@:rwxp----------:------:allow

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

257

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–11 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through

(Suite)

everyone@:--------------:------:allow

Dans cet exemple, un répertoire nouvellement créé hérite à la fois des ACE contrôlant l'accès à
ce répertoire et des ACE à appliquer ultérieurement aux enfants de ce répertoire.

# mkdir subdir.1

# ls -dV subdir.1

drwxrwx---+ 2 root

root

2 Aug 31 14:15 subdir.1

owner@:rwxpdDaARWcCos:fdi---:allow

owner@:rwxpdDaARWcCos:------:allow

group@:rwxp----------:fdi---:allow

group@:rwxp----------:------:allow

everyone@:--------------:fdi---:allow

everyone@:--------------:------:allow

Les entrées -di-- et f-i--- permettent d'appliquer l'héritage et ne sont pas prises en compte
lors du contrôle d'accès. Dans cet exemple, un fichier est créé avec une ACL insignifiante dans
un autre répertoire ne contenant pas d'ACE héritées.

# cd /tank/cindys

# mkdir test2.dir

# cd test2.dir

# touch file.2

# ls -V file.2

-rw-r--r--

1 root

root

0 Aug 31 14:16 file.2

owner@:--x-----------:------:deny

owner@:rw-p---A-W-Co-:------:allow

group@:-wxp----------:------:deny

group@:r-------------:------:allow

everyone@:-wxp---A-W-Co-:------:deny

everyone@:r-----a-R-c--s:------:allow

EXEMPLE 8–12 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through-X

Lorsque aclinherit=passthrough-x est activé, les fichiers sont créés avec l'autorisation
d'exécution (x) pour propriétaire@, groupe@ ou tous les utilisateurs@, mais seulement si
l'autorisation d'exécution est définie dans le mode de création de fichier et dans une ACE
héritable qui affecte le mode.

L'exemple suivant montre comment hériter l'autorisation d'exécution en définissant le mode
aclinherit sur passthrough-x.

# zfs set aclinherit=passthrough-x tank/cindys

258

Guide d'administration Solaris ZFS • Octobre 2009

Configuration et affichage d'ACL dans des fichiers ZFS en format compact

EXEMPLE 8–12 Héritage d'ACL avec mode d'héritage ACL défini sur Pass Through-X

(Suite)

L'ACL suivante est définie sur /tank/cindys/test1.dir pour permettre l'héritage des ACL
exécutable pour les fichiers de propriétaire@.

# chmod A=owner@:rwxpcCosRrWaAdD:fd:allow,group@:rwxp:fd:allow,everyone@::fd:allow test1.dir

# ls -Vd test1.dir

drwxrwx---+ 2 root

root

2 Aug 31 15:05 test1.dir

owner@:rwxpdDaARWcCos:fd----:allow

group@:rwxp----------:fd----:allow

everyone@:--------------:fd----:allow

Un fichier (file1) est créé avec les autorisations demandées 0666. Les autorisations obtenues
sont 0660. L'autorisation d'exécution n'était pas héritée car le mode de création ne le requérait
pas.

# touch test1.dir/file1

# ls -V test1.dir/file1

-rw-rw----+ 1 root

root

0 Aug 31 15:06 test1.dir/file1

owner@:rw-pdDaARWcCos:------:allow

group@:rw-p----------:------:allow

everyone@:--------------:------:allow

Ensuite, un fichier exécutable appelé t est généré à l'aide du compilateur cc dans le répertoire
testdir.

# cc -o t t.c

# ls -V t

-rwxrwx---+ 1 root

root

7396 Dec 3 15:19 t

owner@:rwxpdDaARWcCos:------:allow

group@:rwxp----------:------:allow

everyone@:--------------:------:allow

Les autorisations obtenues sont 0770 car cc a demandé des autorisations 0777, ce qui a entraîné
l'héritage de l'autorisation d'exécution à partir des entrées propriétaire@, groupe@ et tous les
utilisateurs@.

Chapitre 8 • Utilisation des ACL pour la protection de fichiers ZFS

259

260

9C H A P I T R E

9

Administration déléguée de ZFS

Ce chapitre décrit la méthode d'utilisation de l'administration déléguée pour permettre aux
utilisateurs ne disposant pas de privilèges appropriés d'effectuer des tâches d'administration
ZFS.
■ “Présentation de l'administration déléguée de ZFS” à la page 261
■ “Délégation de droits ZFS” à la page 262
■ “Affichage des droits ZFS délégués (exemples)” à la page 266
■ “Délégation de droits ZFS (exemples)” à la page 268
■ “Suppression de droits ZFS (exemples)” à la page 272

Présentation de l'administration déléguée de ZFS

Cette fonction vous permet de distribuer des droits précis à des utilisateurs ou des groupes
spécifiques, voire à tous les utilisateurs. Deux types de droits délégués sont pris en charge :
■ Des droits individuels peuvent être explicitement spécifiés, notamment de création (create),

de destruction (destroy), de montage (mount), d'instantané (snapshot), etc.

■ Des groupes de droits appelés jeux de droits peuvent être définis. Tout utilisateur d'un jeu de

droits est automatiquement affecté par les modifications apportées à celui-ci dans le cadre
d'une mis à jour. Les jeux de droits commencent par la lettre @ et sont limités à 64 caractères.
Les caractères suivant le caractère @ dans le nom de jeu ont les mêmes restrictions que ceux
des noms de systèmes de fichiers ZFS standard.

L'administration déléguée de ZFS offre des fonctions similaires au modèle de sécurité RBAC. Le
modèle de délégation ZFS offre les avantages suivants pour la gestion des pools de stockage et
systèmes de fichiers ZFS :
■ Les droits sont transférés avec le pool de stockage ZFS lorsque celui-ci est migré.
■ Offre un héritage dynamique vous permettant de contrôler la propagation des droits dans

les systèmes de fichiers.

261

Délégation de droits ZFS

■ Peut être configuré de manière à ce que seul le créateur d'un système de fichiers puisse

détruire celui-ci.

■ Les droits peuvent être distribués à des systèmes de fichiers spécifiques. Tout nouveau

système de fichiers peut automatiquement récupérer des droits.

■ Ce modèle offre une administration NFS simple. Un utilisateur disposant de droits

explicites peut par exemple créer un instantané sur un système NFS dans le répertoire
.zfs/snapshot approprié.

Considérez l'utilisation de l'administration déléguée pour la répartition des tâches ZFS. Pour
plus d'informations sur l'utilisation de RBAC pour gérer les tâches d'administration générales
de Solaris, reportez-vous à Partie III, “Roles, Rights Profiles, and Privileges” du System
Administration Guide: Security Services (en anglais).

Désactivation des droits délégués de ZFS
Vous pouvez activer ou désactiver l'administration déléguée en définissant la propriété
delegation du pool. Par exemple :

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation on

default

# zpool set delegation=off users

# zpool get delegation users

NAME PROPERTY

VALUE

SOURCE

users delegation off

local

Par défaut, la propriété delegation est activée.

Délégation de droits ZFS

Vous pouvez utiliser la commande zfs allow pour accorder des droits applicables aux jeux de
données ZFS aux utilisateurs non root, de la manière suivante :
■ Vous pouvez accorder des droits individuels à un utilisateur, à un groupe, voire à tous les

utilisateurs.

■ Vous pouvez accorder des groupes de droits individuels sous forme de jeu de droits à un

utilisateur, à un groupe, voire à tous les utilisateurs.

■ Vous pouvez accorder des droits localement uniquement au jeu de données actuel ou à tous

les descendants de celui-ci.

Le tableau suivant décrit les opérations pouvant être déléguées et tout droit dépendant devant
réaliser ces opérations déléguées.

262

Guide d'administration Solaris ZFS • Octobre 2009

Droit (sous-commande)

Description

Dépendances

Délégation de droits ZFS

allow

Clone

create

destroy

monter

promote

receive

rename

Capacité à accorder des droits qui vous
ont été octroyés à un autre utilisateur.

Doit également disposer du droit à
autoriser.

Capacité à cloner tout instantané du jeu
de données.

Doit également disposer de la capacité
create et de la capacité mount dans le
système de fichiers d'origine.

Capacité à créer des jeux de données
descendants.

Doit également disposer de la capacité
mount.

Capacité à détruire un jeu de données.

Doit également disposer de la capacité
mount.

Capacité à monter et démonter un jeu de
données, et à créer et détruire les liens
vers des périphériques de volume.

Capacité à promouvoir le clonage d'un
jeu de données.

Capacité à créer un système de fichiers
descendant à l'aide de la commande zfs
receive.

Doit également disposer de la capacité
mount et de la capacité promote dans le
système de fichiers d'origine.

Doit également disposer de la capacité
mount et de la capacité create.

Capacité à renommer un jeu de données. Doit également disposer de la capacité
create et de la capacité mount dans le
nouveau parent.

rollback

Capacité à restaurer un instantané.

Doit également disposer de la capacité
mount.

send

share

Instantané

Capacité à envoyer un flux d'instantané.

Capacité à partager et annuler le partage
d'un·jeu de données.

Capacité à prendre un instantané de·jeu
de données.

Vous pouvez déléguer l'ensemble d'autorisations suivant mais l'autorisation peut être limitée à
l'accès, la lecture et la modification :

■

■

■

■

■

groupquota

groupused

userprop

userquota

userused

Vous pouvez en outre déléguer les propriétés ZFS suivantes à des utilisateurs non root :

Chapitre 9 • Administration déléguée de ZFS

263

Délégation de droits ZFS

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

■

aclinherit

aclmode

atime

canmount

casesensitivity

Somme de contrôle

compression

copies

devices

exec

mountpoint

nbmand

normalization

primarycache

quota

readonly

recordsize

refreservation

reservation

secondarycache

setuid

shareiscsi

sharenfs

sharesmb

snapdir

utf8only

version

volblocksize

volsize

vscan

xattr

zoned

Certaines de ces propriétés ne peuvent être définies qu'à la création d'un jeu de données. Pour
une description de ces propriétés, reportez-vous à la section “Présentation des propriétés ZFS”
à la page 179.

Description de la syntaxe de délégation des
autorisations (zfs allow)
La syntaxe de zfs allow est la suivante :

# zfs allow -[ldugecs] everyone|user|group[,,...] perm|@setname,...] filesystem| volume

264

Guide d'administration Solaris ZFS • Octobre 2009

Délégation de droits ZFS

La syntaxe de zfs allow suivante (en gras) identifie les utilisateurs auxquels les droits sont
délégués :

zfs allow [-uge]|user|group|everyone [,...] filesystem | volume
Vous pouvez spécifier plusieurs entrées sous forme de liste séparée par des virgules. Si aucune
option -uge n'est spécifiée, l'argument est interprété en premier comme le mot clé everyone,
puis comme un nom d'utilisateur et enfin, comme un nom de groupe. Pour spécifier un
utilisateur ou un groupe nommé "everyone", utilisez l'option -u ou l'option -g. Pour spécifier
un groupe portant le même nom qu'un utilisateur, utilisez l'option -g. L'option -c accorde des
droits create-time.
La syntaxe de zfs allow suivante (en gras) identifie la méthode de spécification des droits et
jeux de droits :

zfs allow [-s] ... perm|@setname [,...] filesystem | volume
Vous pouvez spécifier plusieurs droits sous forme de liste séparée par des virgules. Les noms de
droits sont identiques aux sous-commandes et propriétés ZFS. Pour plus d'informations,
reportez-vous à la section précédente.
Les droits peuvent être regroupés en jeux de droits et sont identifiés par l'option -s. Les jeux de
droits peuvent être utilisés par d'autres commandes zfs allow pour le système de fichiers
spécifié et ses descendants. Les jeux de droits sont évalués dynamiquement et de ce fait, toute
modification apportée à un jeu est immédiatement mise à jour. Les jeux de droits doivent se
conformer aux mêmes conventions d'attribution de noms que les systèmes de fichiers ZFS, à
ceci près que leurs noms doivent commencer par le caractère arobase (@) et ne pas dépasser 64
caractères.
La syntaxe de zfs allow suivante (en gras) identifie la méthode de délégation des droits :

zfs allow [-ld] ... ... filesystem | volume
L'option -l indique que le droit est accordé au jeu de données spécifié mais pas à ses
descendants, à moins de spécifier également l'option -d. L'option -d indique que le droit est
accordé pour les jeux de données descendants mais pas pour l'actuel jeu de données, à moins de
spécifier également l'option -l. Si aucun des droits -ld n'est spécifié, les droits sont accordés au
système de fichiers ou au volume, ainsi qu'à leurs descendants.

Suppression des droits délégués de ZFS (zfs unallow)
Vous pouvez supprimer des droits précédemment accordés, à l'aide de la commande zfs
unallow.
Supposons par exemple que vous déléguiez les droits create, destroy, mount et snapshot de la
manière suivante :

Chapitre 9 • Administration déléguée de ZFS

265

Utilisation de l'administration déléguée de ZFS

# zfs allow cindys create,destroy,mount,snapshot tank/cindys

# zfs allow tank/cindys

-------------------------------------------------------------

Local+Descendent permissions on (tank/cindys)

user cindys create,destroy,mount,snapshot

-------------------------------------------------------------

Pour supprimer ces autorisations, vous devez utiliser une syntaxe du type suivant :

# zfs unallow cindys tank/cindys

# zfs allow tank/cindys

Utilisation de l'administration déléguée de ZFS

Cette section contient des exemples d'affichage et de délégation de droits ZFS délégués.

Affichage des droits ZFS délégués (exemples)
Vous pouvez vous servir de la commande suivante pour afficher les droits :

# zfs allow dataset

Cette commande affiche les droits définis ou accordés à ce jeu de données. La sortie contient les
composants suivants :

Jeux de droits

■

■

■

■

■ Droits spécifiques ou droits à la création

Jeu de données local
Jeux de données locaux et descendants
Jeux de données descendants uniquement

EXEMPLE 9–1 Affichage des droits d'administration déléguée de base
La sortie suivante de cet exemple indique que l'utilisateur cindys dispose des droits de création,
de destruction, de montage et d'instantané dans le système de fichiers tank/cindys.

# zfs allow tank/cindys

-------------------------------------------------------------

Local+Descendent permissions on (tank/cindys)

user cindys create,destroy,mount,snapshot

EXEMPLE 9–2 Affichage des droits d'administration déléguée complexes

La sortie de cet exemple indique les droits suivants sur les systèmes de fichiers pool/fred et
pool.

266

Guide d'administration Solaris ZFS • Octobre 2009

Utilisation de l'administration déléguée de ZFS

EXEMPLE 9–2 Affichage des droits d'administration déléguée complexes

(Suite)

Pour le système de fichiers pool/fred :
■ Deux jeux de droits sont définis :

■

■

@eng (create, destroy, snapshot, mount, clone, promote, rename)
@simple (create, mount)

■ Les droits à la création sont définis pour le jeu de droits @eng et la propriété mountpoint. "À

la création" signifie qu'une fois qu'un jeu de données est créé, le jeu de droits @eng et la
propriété mountpoint sont accordés.

■ Le jeu de droits @eng est accordé à l'utilisateur tom et les droits create, destroy et mount

pour les systèmes de fichiers locaux sont accordés à l'utilisateur joe.

■ Le jeu de droits @basic, ainsi que les droits share et rename pour les systèmes de fichiers

locaux et descendants sont accordés à l'utilisateur fred.

■ Le jeu de droits @basic pour les systèmes de fichiers descendants uniquement est accordé à

l'utilisateur barney et au groupe staff.

Pour le système de fichiers pool :
■ Le jeu de droits @simple (create, destroy, mount) est défini.
■ Le jeu de droits sur le système de fichiers local @simple est accordé au groupe staff.

La sortie de cet exemple est la suivante :

$ zfs allow pool/fred

------------------------------------------------------------------------------

Permission sets on (pool/fred)

@eng create,destroy,snapshot,mount,clone,promote,rename

@simple create,mount

Create time permissions on (pool/fred)

@eng,mountpoint

Local permissions on (pool/fred)

user tom @eng

user joe create,destroy,mount

Local+Descendent permissions on (pool/fred)

user fred @basic,share,rename

Descendent permissions on (pool/fred)

user barney @basic

group staff @basic

------------------------------------------------------------------------------

Permission sets on (pool)

@simple create,destroy,mount

Local permissions on (pool)

group staff @simple

------------------------------------------------------------------------------

Chapitre 9 • Administration déléguée de ZFS

267

Utilisation de l'administration déléguée de ZFS

Délégation de droits ZFS (exemples)

EXEMPLE 9–3 Délégation de droits à un utilisateur individuel
Lorsque vous accordez les droits create et mount à un utilisateur individuel, vous devez vous
assurer que cet utilisateur dispose de droits sur le point de montage sous-jacent.
Pour accorder par exemple à l'utilisateur marks les droits create et mount sur tank, définissez
au préalable ces droits :

# chmod A+user:marks:add_subdirectory:fd:allow /tank

Utilisez ensuite la commande zfs allow pour accorder les droits create, destroy et mount.
Exemple :

# zfs allow marks create,destroy,mount tank

L'utilisateur marks peut dorénavant créer ses propres systèmes de fichiers dans le système de
fichiers tank. Exemple :

# su marks

marks$ zfs create tank/marks

marks$ ^D

# su lp

$ zfs create tank/lp

cannot create ’tank/lp’: permission denied

EXEMPLE 9–4 Délégation des droits de création (create) et de destruction (destroy) à un groupe

L'exemple suivant illustre la configuration d'un système de fichiers pour que tout membre du
groupe staff puisse créer et monter des systèmes de fichiers dans le système de fichiers tank,
ainsi que détruire ses propres systèmes de fichiers. Toutefois, les membres du groupe staff ne
sont pas autorisés à détruire les systèmes de fichiers des autres utilisateurs.

# zfs allow staff create,mount tank

# zfs allow -c create,destroy tank

# zfs allow tank

-------------------------------------------------------------

Create time permissions on (tank)

create,destroy

Local+Descendent permissions on (tank)

group staff create,mount

-------------------------------------------------------------

# su cindys

cindys% zfs create tank/cindys

cindys% exit

# su marks

268

Guide d'administration Solaris ZFS • Octobre 2009

EXEMPLE 9–4 Délégation des droits de création (create) et de destruction (destroy) à un groupe

(Suite)

Utilisation de l'administration déléguée de ZFS

marks% zfs create tank/marks/data

marks% exit

cindys% zfs destroy tank/marks/data

cannot destroy ’tank/mark’: permission denied

EXEMPLE 9–5 Délégation de droits au niveau approprié d'un système de fichiers

Assurez-vous d'accorder les droits aux utilisateurs au niveau approprié du système de fichiers.
Par exemple, les droits create, destroy et mount pour les systèmes de fichiers locaux et
descendants sont accordés à l'utilisateur marks. Le droit local de prendre un instantané du
système de fichiers tank a été accordé à l'utilisateur marks, mais pas celui de prendre un
instantané de son propre système de fichiers. Le droit snapshot ne lui donc a pas été accordé au
niveau approprié du système de fichiers.

# zfs allow -l marks snapshot tank

# zfs allow tank

-------------------------------------------------------------

Local permissions on (tank)

user marks snapshot

Local+Descendent permissions on (tank)

user marks create,destroy,mount

-------------------------------------------------------------

# su marks

marks$ zfs snapshot tank/@snap1

marks$ zfs snapshot tank/marks@snap1

cannot create snapshot ’mark/marks@snap1’: permission denied

Pour accorder à l'utilisateur marks ce droit au niveau des descendants, utilisez l'option zfs
allow - d. Exemple :

# zfs unallow -l marks snapshot tank

# zfs allow -d marks snapshot tank

# zfs allow tank

-------------------------------------------------------------

Descendent permissions on (tank)

user marks snapshot

Local+Descendent permissions on (tank)

user marks create,destroy,mount

-------------------------------------------------------------

# su marks

$ zfs snapshot tank@snap2

cannot create snapshot ’tank@snap2’: permission denied

$ zfs snapshot tank/marks@snappy

L'utilisateur marks ne peut maintenant créer un instantané qu'à un niveau inférieur à tank.

Chapitre 9 • Administration déléguée de ZFS

269

Utilisation de l'administration déléguée de ZFS

EXEMPLE 9–6 Définition et utilisation de droits délégués complexes

Vous pouvez accorder des droits spécifiques à des utilisateurs ou des groupes. Par exemple, la
commande zfs allow suivante accorde des droits spécifiques au groupe staff. En outre, les
droits destroy et snapshot sont accordés après la création de systèmes de fichiers tank.

# zfs allow staff create,mount tank

# zfs allow -c destroy,snapshot tank

# zfs allow tank

-------------------------------------------------------------

Create time permissions on (tank)

destroy,snapshot

Local+Descendent permissions on (tank)

group staff create,mount

-------------------------------------------------------------

Étant donné que l'utilisateur marks est membre du groupe staff, il peut créer des systèmes de
fichiers dans tank. En outre, l'utilisateur marks peut créer un instantané de tank/marks2 parce
qu'il dispose des droits spécifiques pour le faire. Exemple :

# su marks

$ zfs create tank/marks2

$ zfs allow tank/marks2

-------------------------------------------------------------

Local permissions on (tank/marks2)

user marks destroy,snapshot

-------------------------------------------------------------

Create time permissions on (tank)

destroy,snapshot

Local+Descendent permissions on (tank)

group staff create

everyone mount

-------------------------------------------------------------

Il ne peut par contre pas créer d'instantané dans tank/marks parce qu'il ne dispose pas des
droits spécifiques pour le faire. Exemple :

$ zfs snapshot tank/marks2@snap1

$ zfs snapshot tank/marks@snappp

cannot create snapshot ’tank/marks@snappp’: permission denied

Si vous disposez des droits create dans votre répertoire personnel, vous pouvez créer vos
propres répertoires d'instantanés. Ce scénario s'avère utile lorsque votre système de fichiers est
monté sur un système NFS. Exemple :

$ cd /tank/marks2

$ ls

$ cd .zfs

270

Guide d'administration Solaris ZFS • Octobre 2009

EXEMPLE 9–6 Définition et utilisation de droits délégués complexes

(Suite)

Utilisation de l'administration déléguée de ZFS

$ ls

snapshot

$ cd snapshot

$ ls -l

total 3

drwxr-xr-x

2 marks

staff

2 Dec 15 13:53 snap1

$ pwd

/tank/marks2/.zfs/snapshot

$ mkdir snap2

$ zfs list

NAME

tank

USED AVAIL REFER MOUNTPOINT

264K 33.2G 33.5K /tank

tank/marks

24.5K 33.2G 24.5K /tank/marks

tank/marks2

46K 33.2G 24.5K /tank/marks2

tank/marks2@snap1

21.5K

- 24.5K -

tank/marks2@snap2

0

- 24.5K -

$ ls

snap1 snap2

$ rmdir snap2

$ ls

snap1

EXEMPLE 9–7 Définition et utilisation d'un jeu de droits délégué ZFS

L'exemple suivant illustre la création d'un jeu de droits intitulé @myset et, accorde ce jeu de
droits ainsi que le droit de renommage au groupe staff pour le système de fichiers tank.
L'utilisateur cindys, membre du groupe staff, a le droit de créer un système de fichiers dans
tank. Par contre, l'utilisateur lp ne dispose pas de ce droit de création de systèmes de fichiers
dans tank.

# zfs allow -s @myset create,destroy,mount,snapshot,promote,clone,readonly tank

# zfs allow tank

-------------------------------------------------------------

Permission sets on (tank)

@myset clone,create,destroy,mount,promote,readonly,snapshot

-------------------------------------------------------------

# zfs allow staff @myset,rename tank

# zfs allow tank

-------------------------------------------------------------

Permission sets on (tank)

@myset clone,create,destroy,mount,promote,readonly,snapshot

Local+Descendent permissions on (tank)

group staff @myset,rename

# chmod A+group:staff:add_subdirectory:fd:allow tank

# su cindys

Chapitre 9 • Administration déléguée de ZFS

271

Utilisation de l'administration déléguée de ZFS

EXEMPLE 9–7 Définition et utilisation d'un jeu de droits délégué ZFS

(Suite)

cindys% zfs create tank/data

Cindys% zfs allow tank

-------------------------------------------------------------

Permission sets on (tank)

@myset clone,create,destroy,mount,promote,readonly,snapshot

Local+Descendent permissions on (tank)

group staff @myset,rename

-------------------------------------------------------------

cindys% ls -l /tank

total 15

drwxr-xr-x

2 cindys

staff

2 Aug 8 14:10 data

cindys% exit

# su lp

$ zfs create tank/lp

cannot create ’tank/lp’: permission denied

Suppression de droits ZFS (exemples)
Vous pouvez utiliser la commande zfs unallow pour supprimer des droits accordés. Par
exemple, l'utilisateur cindys a le droit de créer, détruire, monter et réaliser des instantanés dans
le système de fichiers tank/cindys .

# zfs allow cindys create,destroy,mount,snapshot tank/cindys

# zfs allow tank/cindys

-------------------------------------------------------------

Local+Descendent permissions on (tank/cindys)

user cindys create,destroy,mount,snapshot

-------------------------------------------------------------

La syntaxe suivante de la commande zfs unallow supprime le droit de réaliser des instantanés
du système de fichiers tank/cindys accordé à l'utilisateur cindys :

# zfs unallow cindys snapshot tank/cindys

# zfs allow tank/cindys

-------------------------------------------------------------

Local+Descendent permissions on (tank/cindys)

user cindys create,destroy,mount

-------------------------------------------------------------

cindys% zfs create tank/cindys/data

cindys% zfs snapshot tank/cindys@today

cannot create snapshot ’tank/cindys@today’: permission denied

Autre exemple : l'utilisateur marks dispose des droits suivants dans tank/marks :

272

Guide d'administration Solaris ZFS • Octobre 2009

Utilisation de l'administration déléguée de ZFS

# zfs allow tank/marks

-------------------------------------------------------------

Local+Descendent permissions on (tank/marks)

user marks create,destroy,mount

-------------------------------------------------------------

Dans cet exemple, la syntaxe suivante de la commande zfs unallow supprime tous les droits
accordés à l'utilisateur marks pour tank/marks:

# zfs unallow marks tank/marks

La syntaxe suivante de la commande zfs unallow supprime un jeu de droits sur le système de
fichiers tank.

# zfs allow tank

-------------------------------------------------------------

Permission sets on (tank)

@myset clone,create,destroy,mount,promote,readonly,snapshot

Create time permissions on (tank)

create,destroy,mount

Local+Descendent permissions on (tank)

group staff create,mount

-------------------------------------------------------------

# zfs unallow -s @myset tank

$ zfs allow tank

-------------------------------------------------------------

Create time permissions on (tank)

create,destroy,mount

Local+Descendent permissions on (tank)

group staff create,mount

-------------------------------------------------------------

Chapitre 9 • Administration déléguée de ZFS

273

274

10C H A P I T R E

1 0

Sections avancées de ZFS

Ce chapitre décrit les volumes ZFS, l'utilisation de ZFS dans un système Solaris avec zones
installées, les pools racine de remplacement ZFS et les profils de droits ZFS.
Il contient les sections suivantes :
■ “Volumes ZFS” à la page 275
■ “Utilisation de ZFS dans un système Solaris avec zones installées” à la page 278
■ “Utilisation de pools racine ZFS de remplacement” à la page 284
■ “Profils de droits ZFS” à la page 285

Volumes ZFS

Un volume ZFS est un jeu de données qui représente un périphérique en mode bloc. Les
volumes ZFS sont identifiés en tant que périphériques dans le répertoire
/dev/zvol/{dsk,rdsk}/path.
Dans l'exemple suivant, le volume ZFS tank/vol de 5 Go a été créé :

# zfs create -V 5gb tank/vol

Lors de la création d'un volume, une réservation est automatiquement configurée à la taille
initiale du volume. La taille de la réservation est maintenue à égalité avec la taille du volume
pour éviter tout comportement inattendu. Si, par exemple, la taille du volume diminue, les
données risquent d'être corrompues. Vous devez faire preuve de prudence lors de la
modification de la taille du volume.
En outre, en cas de création de l'instantané d'un volume dont la taille change, cela peut entraîner
des incohérences dans le système de fichiers si vous tentez un retour arrière de l'instantané ou
de créer un clone à partir de l'instantané.
Pour de plus amples informations concernant les propriétés de systèmes de fichiers applicables
aux volumes, reportez-vous au Tableau 6–1.

275

Volumes ZFS

En cas d'utilisation d'un système Solaris avec zones installées, la création ou le clonage d'un
volume ZFS dans une zone non globale est impossible. Toute tentative de création ou de
clonage d'un volume à partir d'une zone non globale échoue. Pour obtenir des informations
relatives à l'utilisation de volumes ZFS dans une zone globale, reportez-vous à la section “Ajout
de volumes ZFS à une zone non globale” à la page 281.

Utilisation d'un volume ZFS en tant que périphérique
de swap ou de dump
Lors de l'installation d'un système de fichiers racine ZFS ou d'une migration à partir d'un
système de fichiers racine UFS, un périphérique de swap est créé sur un volume ZFS du pool
racine ZFS. Exemple :

# swap -l

swapfile

dev

swaplo

blocks

free

/dev/zvol/dsk/rpool/swap 253,3

16 8257520 8257520

Lors de l'installation d'un système de fichiers racine ZFS ou d'une migration à partir d'un
système de fichiers racine UFS, un périphérique de vidage est créé sur un volume ZFS du pool
racine ZFS. Le périphérique de vidage ne nécessite aucune administration une fois configuré.
Exemple :

# dumpadm

Dump content: kernel pages

Dump device: /dev/zvol/dsk/rpool/dump (dedicated)

Savecore directory: /var/crash/t2000

Savecore enabled: yes

En raison de CR 6724860, vous devez exécuter la commande savecore manuellement pour
enregistrer un vidage mémoire sur incident lorsque vous utilisez un volume de vidage ZFS.

Pour modifier la zone de swap ou le périphérique de vidage une fois le système installé ou mis à
niveau, utilisez les commandes swap et dumpadm de la même façon que dans les versions Solaris
précédentes. Pour définir une zone de swap supplémentaire, créez un volume ZFS d'une taille
spécifique et activez le swap sur ce périphérique. Exemple :

# zfs create -V 2G rpool/swap2

# swap -a /dev/zvol/dsk/rpool/swap2

# swap -l

swapfile

dev swaplo blocks

free

/dev/zvol/dsk/rpool/swap 256,1

16 2097136 2097136

/dev/zvol/dsk/rpool/swap2 256,5

16 4194288 4194288

N'effectuez pas de swap vers un fichier dans un système de fichiers ZFS. La configuration de
fichier swap ZFS n'est pas prise en charge.

276

Guide d'administration Solaris ZFS • Octobre 2009

Volumes ZFS

Pour plus d'informations sur l'ajustement de la taille des volumes de swap et de vidage,
reportez-vous à la section “Ajustement de la taille de vos périphériques de swap et de vidage
ZFS” à la page 159.

Utilisation d'un volume ZFS en tant que cible iSCSI
Solaris
Les initiateurs et les cibles iSCSI Solaris sont pris en charge dans cette version de Solaris.

De plus, la création d'un volume ZFS en tant que cible iSCSI s'effectue facilement, en
configurant la propriété shareiscsi dans le volume. Exemple :

# zfs create -V 2g tank/volumes/v2

# zfs set shareiscsi=on tank/volumes/v2

# iscsitadm list target

Target: tank/volumes/v2

iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a

Connections: 0

Une fois la cible iSCSI créée, configurez l'initiateur iSCSI. Pour de plus amples informations sur
les cibles iSCSI et les initiateurs Solaris, reportez-vous au Chapitre 14, “Configuring Solaris
iSCSI Targets and Initiators (Tasks)” du System Administration Guide: Devices and File Systems.

Remarque – La commande iscsitadm permet la création et la gestion de cibles iSCSI. Si vous
avez configuré la propriété shareiscsi dans un volume ZFS, n'utilisez pas la commande
iscsitadm pour créer le même périphérique cible. Dans le cas contraire, vous obtiendrez des
informations de cible dupliquées pour le même périphérique.

La gestion d'un volume ZFS en tant que cible iSCSI s'effectue comme pour tout jeu de données
ZFS. Cependant, les opérations de renommage, d'exportation et d'importation fonctionnent de
façon différente pour les cibles iSCSI.
■ Lors du renommage d'un volume ZFS, le nom de la cible iSCSI ne change pas. Exemple :

# zfs rename tank/volumes/v2 tank/volumes/v1

# iscsitadm list target

Target: tank/volumes/v1

iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a

Connections: 0

■ L'exportation d'un pool contenant un volume ZFS entraîne la suppression de la cible.

L'importation d'un pool contenant un volume ZFS entraîne le partage de la cible. Exemple :

Chapitre 10 • Sections avancées de ZFS

277

Utilisation de ZFS dans un système Solaris avec zones installées

# zpool export tank

# iscsitadm list target

# zpool import tank

# iscsitadm list target

Target: tank/volumes/v1

iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a

Connections: 0

L'ensemble des informations de configuration de cible iSCSI est stocké dans le jeu de données.
Tout comme un système de fichiers NFS partagé, une cible iSCSI importée dans un système
différent est partagée adéquatement.

Utilisation de ZFS dans un système Solaris avec zones
installées

Les sections suivantes décrivent l'utilisation de ZFS dans un système avec zones Solaris.
■ “Ajout de systèmes de fichiers ZFS à une zone non globale” à la page 279
■ “Délégation de jeux de données à une zone non globale” à la page 280
■ “Ajout de volumes ZFS à une zone non globale” à la page 281
■ “Utilisation de pools de stockage ZFS au sein d'une zone” à la page 281
■ “Gestion de propriétés ZFS au sein d'une zone” à la page 282
■ “Explication de la propriété zoned” à la page 283

Pour plus d'informations sur la configuration de zones d'un système de fichiers racine ZFS
migré ou auquel des patchs ont été appliqués à l'aide de Solaris Live Upgrade, reportez-vous à la
section “Utilisation de Solaris Live Upgrade pour migrer ou mettre à jour un système avec zones
(Solaris 10 10/08)” à la page 141 ou “Utilisation de Solaris Live Upgrade pour migrer ou mettre
à jour un système avec zones (Solaris 10 5/09 et Solaris 10 10/09)” à la page 147.

Tenez compte des points suivants lors de l'association de jeux de données à des zones :

■

■

Il est possible d'ajouter un système de fichiers ZFS ou un clone ZFS à une zone non globale
en déléguant ou non le contrôle administratif.
Il est possible d'ajouter un volume ZFS en tant que périphérique à des zones non globales.

■ L'association d'instantanés ZFS à des zones est impossible à l'heure actuelle.

Dans les sections ci-dessous, un jeu de données ZFS fait référence à un système de fichiers ou à
un clone.

L'ajout d'un jeu de données permet à la zone non globale de partager l'espace avec la zone
globale, mais l'administrateur de zone ne peut pas contrôler les propriétés ou créer de nouveaux
systèmes de fichiers dans la hiérarchie de systèmes de fichiers sous-jacents. Cette opération est
identique à l'ajout de tout autre type de système de fichiers à une zone. Effectuez-la lorsque vous
souhaitez simplement partager de l'espace commun.

278

Guide d'administration Solaris ZFS • Octobre 2009

Utilisation de ZFS dans un système Solaris avec zones installées

ZFS autorise également la délégation de jeux de données à une zone non globale, ce qui permet
à l'administrateur de zone de contrôler parfaitement le jeu de données et ses enfants.
L'administrateur de zone peut créer et détruire les systèmes de fichiers ou les clones au sein de
ce jeu de données et modifier les propriétés des jeux de données. L'administrateur de zone ne
peut pas modifier les jeux de données non ajoutés à la zone et ne peut pas dépasser les quotas
maximum définis pour les jeux de données exportés.

Tenez compte des interactions suivantes lorsque vous travaillez avec ZFS dans un système ZFS
sur lequel des zones Solaris sont installées :
■ La propriété mountpoint d'un système de fichiers ZFS ajouté à une zone non globale doit

être définie sur legacy.

■ En raison du problème 6449301, n'ajoutez pas de jeu de données ZFS à une zone non globale

lorsque celle-ci est configurée. Ajoutez plutôt un jeu de données ZFS une fois la zone
installée.

■ Lorsqu'un emplacement zonepath source et l'emplacement zonepath cible résident sur ZFS

et se trouvent dans le même pool, la commande zoneadm clone utilise dorénavant ZFS
automatiquement pour cloner une zone. La commande zoneadm clone prend un
instantané ZFS de l'emplacement zonepath source et configure l'emplacement zonepath
cible. Vous ne pouvez pas utiliser la commande zfs clone pour cloner une zone. Pour plus
d'informations, reportez-vous à la Partie II, “Zones” du Guide d’administration système :
Gestion des ressources conteneurs Solaris et des zones Solaris.
Si vous déléguez un système de fichiers ZFS à une zone non globale, vous devez supprimer
ce système de fichiers de la zone non globale avant d'utiliser Solaris Live Upgrade. Sinon,
l'opération Live Upgrade échoue en raison d'une erreur de système de fichiers en lecture
seule.

■

Ajout de systèmes de fichiers ZFS à une zone non
globale
Vous pouvez ajouter un système de fichiers ZFS en tant que système de fichiers générique
lorsqu'il s'agit simplement de partager de l'espace avec la zone globale. La propriété mountpoint
d'un système de fichiers ZFS ajouté à une zone non globale doit être définie sur legacy.

La sous-commande add fs de la commande zonecfg permet d'ajouter un système de fichiers
ZFS à une zone non globale. Exemple :

Dans l'exemple suivant, un système de fichiers ZFS est ajouté à une zone non globale par un
administrateur global dans la zone globale.

# zonecfg -z zion

zonecfg:zion> add fs

zonecfg:zion:fs> set type=zfs

Chapitre 10 • Sections avancées de ZFS

279

Utilisation de ZFS dans un système Solaris avec zones installées

zonecfg:zion:fs> set special=tank/zone/zion

zonecfg:zion:fs> set dir=/export/shared

zonecfg:zion:fs> end

Cette syntaxe permet d'ajouter le système de fichiers ZFS tank/zone/zion à la zone zion déjà
configurée, montée à /export/shared. La propriété mountpoint du système de fichiers doit
être définie sur legacy et le système de fichiers ne peut pas être déjà monté à un autre
emplacement. L'administrateur de zone peut créer et détruire des fichiers au sein du système de
fichiers. Le système de fichiers ne peut pas être remonté à un emplacement différent. En outre,
l'administrateur de zone ne peut pas modifier les propriétés dans le système de fichiers, telles
que atime, readonly, compression etc. L'administrateur de zone globale est chargé de la
configuration et du contrôle des propriétés du système de fichiers.

Pour plus d'informations sur la commande zonecfg et sur la configuration des types de
ressources à l'aide de zonecfg, reportez-vous à la Partie II, “Zones” du Guide d’administration
système : Gestion des ressources conteneurs Solaris et des zones Solaris.

Délégation de jeux de données à une zone non
globale
Si le principal objectif est de déléguer l'administration du stockage à une zone, ZFS prend alors
en charge l'ajout de jeux de données à une zone non globale à l'aide de la sous-commande add
dataset de la commande zonecfg.

Dans l'exemple suivant, un système de fichiers ZFS est délégué à une zone non globale par un
administrateur global dans la zone globale.

# zonecfg -z zion

zonecfg:zion> add dataset

zonecfg:zion:dataset> set name=tank/zone/zion

zonecfg:zion:dataset> end

Contrairement à l'ajout d'un système de fichiers, cette syntaxe entraîne la visibilité du système
de fichiers ZFS tank/zone/zion dans la zone zion déjà configurée. L'administrateur de zone
peut définir les propriétés de système de fichiers et créer des enfants. En outre, il peut prendre
des instantanés, créer des clones et contrôler la hiérarchie complète du système de fichiers.

Si vous utilisez Solaris Live Upgrade pour mettre à niveau votre environnement d'initialisation
ZFS avec des zones non globales, supprimez tout jeu de données délégué avant l'opération Live
Upgrade, faute de quoi celle-ci échouera avec une erreur de système de fichiers en lecture seule.
Exemple :

zonecfg:zion>

zonecfg:zone1> remove dataset name=tank/zone/zion

zonecfg:zone1> exit

280

Guide d'administration Solaris ZFS • Octobre 2009

Utilisation de ZFS dans un système Solaris avec zones installées

Pour de plus amples informations relatives aux actions autorisées au sein des zones,
reportez-vous à la section “Gestion de propriétés ZFS au sein d'une zone” à la page 282.

Ajout de volumes ZFS à une zone non globale
Les volumes ZFS ne peuvent pas être ajoutés à une zone non globale à l'aide de la
sous-commande add dataset de la commande zonecfg. En cas de détection d'une tentative
d'ajout d'un volume ZFS, la zone ne peut pas se réinitialiser. Il est cependant possible d'ajouter
des volumes à une zone à l'aide de la sous-commande add device de la commande zonecfg.

Dans l'exemple suivant, un volume ZFS est ajouté à une zone non globale par un administrateur
global dans la zone globale :

# zonecfg -z zion

zion: No such zone configured

Use ’create’ to begin configuring a new zone.

zonecfg:zion> create

zonecfg:zion> add device

zonecfg:zion:device> set match=/dev/zvol/dsk/tank/vol

zonecfg:zion:device> end

Cette syntaxe exporte le volume tank/vol dans la zone. Notez que l'ajout d'un volume brut à
une zone comporte des risques de sécurité implicites, même si le volume ne correspond pas à un
périphérique physique. L'administrateur risque notamment de créer des systèmes de fichiers
non conformes qui généreraient des erreurs graves dans le système en cas de tentative de
montage. Pour de plus amples informations sur l'ajout de périphériques à de zones et les risques
de sécurités associés, reportez-vous à la section “Explication de la propriété zoned” à la page 283.

Pour plus d'informations sur l'ajout de périphériques à des zones, reportez-vous à la Partie II,
“Zones” du Guide d’administration système : Gestion des ressources conteneurs Solaris et des
zones Solaris.

Utilisation de pools de stockage ZFS au sein d'une
zone
Il est impossible de créer ou de modifier des pools de stockage ZFS au sein d'une zone. Le
modèle d'administration délégué centralise le contrôle de périphériques de stockage physique
au sein de la zone globale et le contrôle du stockage virtuel dans les zones non globales. Bien
qu'un jeu de données au niveau du pool puisse être ajouté à une zone, toute commande
modifiant les caractéristiques physiques du pool, comme la création, l'ajout ou la suppression
de périphériques est interdite au sein de la zone. Même si les périphériques physiques sont
ajoutés à une zone à l'aide de la sous-commande add device de la commande zonecfg, ou si les
fichiers sont utilisés, la commande zpool n'autorise pas la création de nouveaux pools au sein de
la zone.

Chapitre 10 • Sections avancées de ZFS

281

Utilisation de ZFS dans un système Solaris avec zones installées

Gestion de propriétés ZFS au sein d'une zone
Après l'ajout d'un jeu de données à une zone, l'administrateur de zone peut contrôler des
propriétés de jeux de données spécifiques. Lorsqu'un jeu de données est ajouté à une zone, tous
ses ancêtres sont visibles en tant que jeux de données en lecture seule, tandis que le jeu de
données lui-même est accessible en écriture, de même que ses enfants. Considérez par exemple
la configuration suivante :

global# zfs list -Ho name

tank

tank/home

tank/data

tank/data/matrix

tank/data/zion

tank/data/zion/home

En cas d'ajout de tank/data/zion à une zone, chaque jeu de données dispose des propriétés
suivantes.

Jeu de données

Visible

Accessible en écriture

Propriétés immuables

tank

tank/home

tank/data

tank/data/matrix

tank/data/zion

tank/data/zion/home

Oui

Non

Oui

Non

Oui

Oui

Non

-

Non

-

Oui

Oui

-

-

-

-

sharenfs, zoned, quota,

reservation

sharenfs, zoned

Notez que chaque parent de tank/zone/zion est visible en lecture seule, que tous les enfants
sont accessibles en écriture et les jeux de données qui ne font pas partie de la hiérarchie parent
sont invisibles. L'administrateur de zone ne peut pas modifier la propriété sharenfs car les
zones non globales ne peuvent pas faire office de serveurs NFS. Il ne peut pas non plus modifier
la propriété zoned car cela entraînerait un risque de sécurité, tel que décrit dans la section
suivante.
Toute autre propriété configurable peut être modifiée, à l'exception de la propriété quota et du
jeu de données lui-même. Ce comportement permet à l'administrateur de zone globale de
contrôler la consommation d'espace de l'ensemble des jeux de données utilisés par la zone non
globale.
En outre, l'administrateur de zone globale ne peut pas modifier les propriétés sharenfs et
mountpoint après l'ajout d'un jeu de données à une zone non globale.

282

Guide d'administration Solaris ZFS • Octobre 2009

Utilisation de ZFS dans un système Solaris avec zones installées

Explication de la propriété zoned
Lors qu'un jeu de données est ajouté à une zone non globale, il doit être marqué spécialement
pour que certaines propriétés ne soient pas interprétées dans le contexte de la zone globale.
Lorsqu'un jeu de données est ajouté à une zone non globale sous le contrôle d'un administrateur
de zone, son contenu n'est plus fiable. Comme pour tout système de fichiers, cela peut entraîner
la présence de binaires Setuid, de liens symboliques ou d'autres contenus douteux qui
pourraient compromettre la sécurité de la zone globale. De plus, l'interprétation de la propriété
mountpoint est impossible dans le contexte de la zone globale. Dans le cas contraire,
l'administrateur de zone pourrait affecter l'espace de noms de la zone globale. Afin de résoudre
ceci, ZFS utilise la propriété zoned pour indiquer qu'un jeu de données a été délégué à une zone
non globale à un moment donné.

La propriété zoned est une valeur booléenne automatiquement activée lors de la première
initialisation d'une zone contenant un jeu de données ZFS. L'activation manuelle de cette
propriété par un administrateur de zone n'est pas nécessaire. Si la propriété zoned est définie, le
montage ou le partage du jeu de données dans la zone globale est impossible et le jeu de données
est ignoré lors de l'exécution de la commande zfs share -a ou de la commande zfs mount -a.
Dans l'exemple suivant, tank/zone/zion a été ajouté à une zone, tandis que tank/zone/global
ne l'a pas été :

# zfs list -o name,zoned,mountpoint -r tank/zone

NAME

ZONED MOUNTPOINT

tank/zone/global

off /tank/zone/global

tank/zone/zion

on /tank/zone/zion

# zfs mount

tank/zone/global

/tank/zone/global

tank/zone/zion

/export/zone/zion/root/tank/zone/zion

Notez la différence entre la propriété mountpoint et le répertoire dans lequel le jeu de données
tank/zone/zion est actuellement monté. La propriété mountpoint correspond à la propriété
telle qu'elle est stockée dans le disque, pas à l'emplacement auquel est monté le jeu de données
sur le système.

Lors de la suppression d'un jeu de données d'une zone ou de la destruction d'une zone, la
propriété zoned n'est pas effacée automatiquement. Ce comportement est dû aux risques de
sécurité inhérents associés à ces tâches. Dans la mesure où un utilisateur qui n'est pas fiable
dispose de l'accès complet au jeu de données et à ses enfants, la propriété mountpoint risque
d'être configurée sur des valeurs erronées, ou des binaires Setuid peuvent exister dans les
systèmes de fichiers.

Afin de prévenir les risques de sécurité accidentels, l'administrateur global doit effacer
manuellement la propriété zoned pour que le jeu de données puisse être utilisé à nouveau.
Avant de configurer la propriété zoned sur off, assurez-vous que la propriété mountpoint pour
le jeu de données et tous ses enfants est configurée sur des valeurs raisonnables et qu'il n'existe
aucun binaire Setuid, ou désactivez la propriété setuid.

Chapitre 10 • Sections avancées de ZFS

283

Utilisation de pools racine ZFS de remplacement

Après avoir vérifié qu'aucune vulnérabilité n'existe au niveau de la sécurité, il est possible de
désactiver la propriété zoned à l'aide de la commande zfs set ou zfs inherit. Si la propriété
zoned est désactivée alors que le jeu de données est en cours d'utilisation au sein d'une zone, le
système peut se comporter de façon imprévue. Ne modifiez la propriété que si vous êtes sûr que
le jeu de données n'est plus en cours d'utilisation dans une zone non globale.

Utilisation de pools racine ZFS de remplacement

Lors de sa création, un pool est intrinsèquement lié au système hôte. Le système hôte assure la
maintenance des connaissances relatives au pool, ce qui lui permet de détecter l'indisponibilité
du pool, le cas échéant. Bien que ces connaissances soient utiles aux opérations normales, elles
peuvent causer des interférences lors de l'initialisation à partir d'autres supports ou lors de la
création d'un pool dans un média amovible. La fonction de pool racine de remplacement de ZFS
permet de résoudre ce problème. Un pool racine de remplacement n'est pas conservé d'une
réinitialisation système à une autre et tous les points de montage sont modifiés de sorte à être
relatifs à la racine du pool.

Création de pools racine de remplacement ZFS
Le plus souvent, la création d'un pool racine de remplacement s'effectue en vue d'une utilisation
avec un média amovible. Dans ces circonstances, les utilisateurs souhaitent employer un
système de fichiers unique et le monter à l'emplacement de leur choix dans le système cible.
Lorsqu'un pool racine de remplacement est créé à l'aide de l'option -R, le point de montage du
système de fichiers racine est automatiquement défini sur /, qui est l'équivalent de la racine de
remplacement elle-même.

Dans l'exemple suivant, un pool nommé morpheus est créé à l'aide /mnt en tant que chemin de
racine de remplacement :

# zpool create -R /mnt morpheus c0t0d0

# zfs list morpheus

NAME

USED AVAIL REFER MOUNTPOINT

morpheus

32.5K 33.5G

8K /mnt/

Notez le système de fichiers morpheus dont le point de montage est la racine de remplacement
du pool, /mnt. Le point de montage stocké sur le disque est / et le chemin complet de /mnt n'est
interprété que dans le contexte du pool racine de remplacement. Ce système de fichiers peut
ensuite être exporté ou importé sous un pool racine de remplacement arbitraire dans un
système différent.

284

Guide d'administration Solaris ZFS • Octobre 2009

Profils de droits ZFS

Importation de pools racine de remplacement
L'importation de pool s'effectue également à l'aide d'une racine de remplacement. Cette
fonction permet de récupérer les données, le cas échéant, lorsque les points de montage ne
doivent pas être interprétés dans le contexte de la racine actuelle, mais sous un répertoire
temporaire où pourront s'effectuer les réparations. Cette fonction peut également être utilisée
lors du montage de médias amovibles comme décrit ci-dessus.

Dans l'exemple suivant, un pool nommé morpheus est importé à l'aide de /mnt en tant que
chemin racine de remplacement : Cet exemple part du principe que morpheus a été
précédemment exporté.

# zpool import -R /mnt morpheus

# zpool list morpheus

NAME

SIZE

USED

AVAIL

CAP HEALTH

ALTROOT

morpheus

33.8G

68.0K

33.7G

0% ONLINE

/mnt

# zfs list morpheus

NAME

USED AVAIL REFER MOUNTPOINT

morpheus

32.5K 33.5G

8K /mnt/morpheus

Profils de droits ZFS

Si vous souhaitez effectuer des tâches de gestion ZFS sans utiliser le compte superutilisateur
(racine), vous pouvez adopter un rôle disposant de l'une des propriétés suivantes afin d'effectuer
des tâches d'administration ZFS :
■ Gestion de stockage ZFS – Permet de créer, détruire ou manipuler les périphériques au sein

d'un pool de stockage ZFS.

■ Gestion de système de fichiers ZFS – Permet de créer, détruire et modifier les systèmes de

fichiers ZFS.

Pour de plus amples informations sur la création ou l'assignation de rôles, reportez-vous au
System Administration Guide: Security Services.

Outre les rôles RBAC permettant de gérer les systèmes de fichiers ZFS, vous pouvez également
vous servir de l'administration déléguée de ZFS pour effectuer des tâches d'administration ZFS
distribuée. Pour plus d'informations, reportez-vous au Chapitre 9, “Administration déléguée de
ZFS”.

Chapitre 10 • Sections avancées de ZFS

285

286

11C H A P I T R E

1 1

Résolution de problèmes et récupération de
données ZFS

Ce chapitre décrit les méthodes d'identification et de résolution des modes de panne de ZFS.
Des informations relatives à la prévention des pannes sont également fournies.

Il contient les sections suivantes :
■ “Modes de panne ZFS” à la page 287
■ “Vérification de l'intégrité des données ZFS” à la page 289
■ “Identification de problèmes dans ZFS” à la page 291
■ “Réparation d'un configuration ZFS endommagée” à la page 297
■ “Réparation d'un périphérique manquant” à la page 297
■ “Remplacement ou réparation d'un périphérique endommagé ” à la page 299
■ “Réparation de données endommagées” à la page 308
■ “Réparation d'un système impossible à réinitialiser” à la page 311

Modes de panne ZFS

En tant que système de fichiers et gestionnaire de volumes combinés, ZFS peut rencontrer
différents modes de panne. Ce chapitre commence par une description des différents modes de
panne, puis explique comment les identifier sur un système en cours d'exécution. Il se conclut
en expliquant comment résoudre les problèmes. ZFS peut faire face à trois types d'erreurs de
base :
■ “Périphériques manquants dans un pool de stockage ZFS” à la page 288
■ “Périphériques endommagés dans un pool de stockage ZFS” à la page 288
■ “Données ZFS corrompue” à la page 288

Notez que les trois types d'erreurs peuvent se produire dans un même pool. Une procédure de
réparation complète implique de trouver et de corriger une erreur, de passer à la suivante et
ainsi de suite.

287

Modes de panne ZFS

Périphériques manquants dans un pool de stockage
ZFS
Si un périphérique est supprimé du système, ZFS détecte que son ouverture est impossible et le
met dans l'état UNAVAIL. En fonction du niveau de réplication des données du pool, cela peut
résulter ou pas en une indisponibilité de la totalité du pool. Le pool reste accessible en cas de
suppression d'un périphérique mis en miroir ou RAID-Z. Si tous les composants d'un miroir
sont supprimés, si plusieurs périphériques d'un périphérique RAID-Z sont supprimés, ou si un
périphérique de niveau supérieur à un disque est supprimé, l'état du pool devient FAULTED.
Aucune donnée n'est accessible tant que le périphérique n'est pas reconnecté.

Périphériques endommagés dans un pool de stockage
ZFS
Le terme " endommagé " fait référence à une grande variété d'erreurs possibles. Les exemples
incluent les erreurs suivantes :

■

■

■

■

erreurs d'E/S transitoires causées par un disque ou un contrôleur défaillant ;
corruption de données sur disque causée par les rayons cosmiques ;
bogues de pilotes entraînant des transferts de données vers ou à partir d'un emplacement
erroné ;
écrasement accidentel de parties du périphérique physique par un autre utilisateur.

Certaines erreurs sont transitoires, par exemple une erreur d'E/S aléatoire alors que le
contrôleur rencontre des problèmes. Dans d'autres cas, les dommages sont permanents, par
exemple lors de la corruption sur disque. En outre, même si les dommages sont permanents,
cela ne signifie pas que l'erreur est susceptible de se reproduire. Par exemple, si un
administrateur écrase une partie d'un disque par accident, aucune panne matérielle ne s'est
produite et il est inutile de remplacer le périphérique. L'identification de la séquence d'erreurs
dans un périphérique n'est pas une tâche aisée. Elle est abordée plus en détail dans une section
ultérieure.

Données ZFS corrompue
La corruption de données se produit lorsqu'une ou plusieurs erreurs de périphériques
(indiquant des périphériques manquants ou endommagés) affectent un périphérique virtuel de
niveau supérieur. Par exemple, la moitié d'un miroir peut subir des milliers d'erreurs sans
jamais causer de corruption de données. Si une erreur se produit sur l'autre côté du miroir au
même emplacement, les données sont alors endommagées.

288

Guide d'administration Solaris ZFS • Octobre 2009

Vérification de l'intégrité des données ZFS

La corruption de données est toujours permanente et nécessite une soin particulier lors de la
réparation. Même en cas de réparation ou de remplacement des périphériques sous-jacents, les
données d'origine sont irrémédiablement perdues. La plupart du temps, ce scénario requiert la
restauration des données à partir de sauvegardes. Les erreurs de données sont enregistrées à
mesure qu'elles sont détectées et peuvent être contrôlées à l'aide de nettoyages de disques de
routine, comme expliqué dans la section suivante. Lorsqu'un bloc corrompu est supprimé, le
nettoyage de disque suivant reconnaît que la corruption n'est plus présente et supprime toute
trace de l'erreur dans le système.

Vérification de l'intégrité des données ZFS

Il n'existe pas d'utilitaire fsck équivalent pour ZFS. Cet utilitaire remplissait deux fonctions : la
réparation et la validation des données.

Réparation de données
Avec les systèmes de fichiers classiques, la méthode d'écriture des données est affectée par les
pannes inattendues entraînant des incohérences de données. Un système de fichiers classique
n'étant pas transactionnel, les blocs non référencés, les comptes de liens défectueux ou autres
structures de données incohérentes sont possibles. L'ajout de la journalisation résout certains de
ces problèmes, mais peut entraîner des problèmes supplémentaires lorsque la restauration du
journal est impossible. Grâce à ZFS, ces problèmes ne se posent pas. Une incohérence des
données sur disque ne se produit qu'à la suite d'une panne de matérielle (auquel cas le pool
aurait dû être redondant) ou en présence d'un bogue dans le logiciel ZFS.

L'utilitaire fsck étant conçu pour réparer les problèmes spécifiques aux systèmes de fichiers
individuels, l'écriture d'un tel utilitaire pour un système de fichiers ne présentant pas de
problème connu est impossible. La preuve pourrait être apportée à l'avenir que certains
problèmes de corruption de données sont suffisamment fréquents et simples pour justifier le
développement d'un tel utilitaire de réparation, mais ces problèmes peuvent toujours être évités
à l'aide de pools redondants.

Si le pool n'est pas redondant, le risque qu'une corruption de données puisse rendre tout ou
partie de vos données inaccessibles est toujours présent.

Validation de données
Outre la réparation de données, l'utilitaire fsck valide l'absence de problème relatif aux
données sur le disque. Cette tâche s'effectue habituellement en démontant le système de fichiers
et en exécutant l'utilitaire fsck, éventuellement en mettant le système en mode utilisateur
unique lors du processus. Ce scénario entraîne une indisponibilité proportionnelle à la taille du

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

289

Vérification de l'intégrité des données ZFS

système de fichiers en cours de vérification. Plutôt que de requérir à un utilitaire explicite pour
effectuer la vérification nécessaire, ZFS fournit un mécanisme pour effectuer une vérification de
routine des données. Cette fonctionnalité, appelée nettoyage, est fréquemment utilisée dans les
systèmes de mémoire et autres systèmes comme méthode de détection et de prévention
d'erreurs pour éviter qu'elles entraînent des pannes matérielles ou logicielles.

Contrôle du nettoyage de données ZFS
Si ZFS rencontre une erreur, soit via le nettoyage ou lors de l'accès à un fichier à la demande,
l'erreur est journalisée en interne pour vous donner une vue d'ensemble rapide de toutes les
erreurs connues au sein du pool.

Nettoyage explicite de données ZFS
La façon la plus simple de vérifier l'intégrité des données est de lancer un nettoyage explicite de
toutes les données au sein du pool. Cette opération traverse toutes les données dans le pool une
fois et vérifie que tous les blocs sont lisibles. Le nettoyage va aussi vite que le permettent les
périphériques, mais la priorité de toute E/S reste inférieure à celle de toute opération normale.
Cette opération peut affecter les performances, bien que le système de fichiers reste utilisable et
sa réactivité est quasiment la même lors du nettoyage. La commande zpool scrubpermet de
lancer un nettoyage explicite. Exemple :

# zpool scrub tank

La sortie de zpool status permet d'afficher l'état du nettoyage actuel. Exemple :

# zpool status -v tank

pool: tank

state: ONLINE

scrub: scrub completed after 0h7m with 0 errors on Tue Sep 1 09:20:52 2009

config:

NAME

tank

ONLINE

mirror

ONLINE

c1t0d0 ONLINE

c1t1d0 ONLINE

STATE

READ WRITE CKSUM

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Notez qu'une seule opération de nettoyage actif par pool peut se produire à la fois.

L'option -s permet d'interrompre un nettoyage en cours. Exemple :

# zpool scrub -s tank

290

Guide d'administration Solaris ZFS • Octobre 2009

Identification de problèmes dans ZFS

Dans la plupart des cas, une opération de nettoyage pour assurer l'intégrité des données doit
être menée à son terme. Vous pouvez cependant interrompre une telle opération si les
performances du système sont affectées.

Un nettoyage de routine garantit également des E/S continues pour l'ensemble des disques du
système. Cet opération a cependant pour effet secondaire d'empêcher la gestion de
l'alimentation de placer des disques inactifs en mode basse consommation. Si le système réalise
en général des E/S en permanence, ou si la consommation n'est pas une préoccupation, ce
problème peut être ignoré.

Pour de plus amples informations sur l'interprétation de la sortie de zpool status,
reportez-vous à la section “Requête d'état de pool de stockage ZFS” à la page 96.

Nettoyage et réargenture de données ZFS
Lors du remplacement d'un périphérique, une opération de réargenture est amorcée pour
déplacer les données des copies correctes vers le nouveau périphérique. Cette action est une
forme de nettoyage de disque. Par conséquent, une seule action de ce type peut être effectuée à
un moment donné dans le pool. Lorsqu'une opération de nettoyage est en cours, toute
opération de réargenture suspend le nettoyage et le redémarre une fois qu'elle a terminé.

Pour de plus amples informations sur la réargenture, reportez-vous à la section “Affichage de
l'état de réargenture” à la page 306.

Identification de problèmes dans ZFS

Les sections suivantes décrivent l'identification des problèmes dans les systèmes de fichiers ZFS
ou les pools de stockage.
■ “Recherche de problèmes éventuels dans un pool de stockage ZFS” à la page 293
■ “Consultation de la sortie de zpool status” à la page 293
■ “Rapport système de messages d'erreur ZFS” à la page 296

Les fonctions suivantes permettent d'identifier les problèmes au sein de la configuration ZFS :
■ La commande zpool status permet d'obtenir des informations relatives au pool de

stockage ZFS.

■ Les défaillances de pool et de périphérique sont rapportées par le biais de messages de

diagnostics ZFS/FMA.

■ La commande zpool history permet d'afficher les commandes ZFS précédentes qui ont

modifié les informations d'état de pool.

La plupart des solutions de problèmes ZFS sont axées autour de la commande zpool status.
Cette commande analyse les différentes erreurs système et identifie les problèmes les plus
sévères. En outre, elle propose des actions à effectuer et un lien vers un article de connaissances

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

291

Identification de problèmes dans ZFS

pour de plus amples informations. Notez que cette commande n'identifie qu'un seul problème
dans le pool, même si plusieurs problèmes existent. Par exemple, les erreurs de corruption de
données impliquent toujours la panne d'un périphérique. Le remplacement d'un périphérique
défaillant ne règle pas les problèmes de corruption de données.

En outre, un moteur de diagnostic ZFS est fourni pour diagnostiquer et rapporter les
défaillances de pool ou de périphériques. Les erreurs liées aux sommes de contrôle, aux E/S et
aux périphériques font également l'objet d'un rapport lorsqu'elles sont liées à la défaillance d'un
pool ou d'un périphérique. Les défaillances ZFS telles que rapportées par fmd s'affichent sur la
console ainsi que les dans le fichier de messages système. Dans la plupart des cas, le message fmd
vous dirige vers la commande zpool status pour obtenir des instructions supplémentaires de
récupération.

Le processus de récupération est comme décrit ci-après :
■ Le cas échéant, la commande zpool history permet d'identifier les commandes ZFS

précédentes ayant entraîné le scénario d'erreur. Exemple :

# zpool history tank

History for ’tank’:

2009-09-01.09:26:15 zpool create tank mirror c0t1d0 c0t2d0 c0t3d0

2009-09-01.09:26:34 zfs create tank/erick

2009-09-01.09:26:41 zfs set checksum=off tank/erick

Notez dans la sortie ci-dessous que les sommes de contrôle sont désactivées pour le système
de fichiers tank/erick. Cette configuration est déconseillée.
Identifiez les erreurs à l'aide des messages fmd affichés sur la console système ou dans les
fichiers /var/adm/messages.

■

■ Obtenez des instructions de réparation supplémentaires grâce à la commande zpool

status -x.

■ Réparez les pannes, par exemple :

■ Remplacez le périphérique défaillant ou manquant et mettez-le en ligne.
■ Restaurez la configuration défaillante ou les données corrompues à partir d'une

sauvegarde.

■ Vérifiez la récupération à l'aide de la commande zpool status - x.

■

Sauvegardez la configuration restaurée, le cas échéant.

Ce chapitre décrit les méthodes d'interprétation de la sortie de zpool status afin de
diagnostiquer le type la panne et vous indique la section à consulter pour résoudre le problème.
Bien que la commande effectue la plupart du travail de façon automatique, il est important de
bien comprendre les problèmes identifiés afin de diagnostiquer le type de panne.

292

Guide d'administration Solaris ZFS • Octobre 2009

Identification de problèmes dans ZFS

Recherche de problèmes éventuels dans un pool de
stockage ZFS
La méthode la plus simple pour déterminer s'il existe des problèmes connus sur le système
consiste à exécuter la commande zpool status -x. Cette commande décrit uniquement les
pools présentant des problèmes. En l'absence de pools défaillants dans le système, la commande
affiche un message simple, comme ci-dessous :

# zpool status -x

all pools are healthy

Sans l'indicateur -x, la commande affiche l'état complet de tous les pools (ou du pool demandé
s'il est spécifié sur la ligne de commande), même si les pools sont autrement fonctionnels.

Pour de plus amples informations sur les options de ligne de commande de la commande zpool
status, reportez-vous à la section “Requête d'état de pool de stockage ZFS” à la page 96.

Consultation de la sortie de zpool status
La sortie complète de zpool status est similaire à ce qui suit :

# zpool status tank

pool: tank

state: DEGRADED

status: One or more devices has been taken offline by the administrator.

Sufficient replicas exist for the pool to continue functioning in a

degraded state.

action: Online the device using ’zpool online’ or replace the device with

’zpool replace’.

scrub: none requested

config:

NAME

tank

STATE

READ WRITE CKSUM

DEGRADED

mirror

DEGRADED

c1t0d0

ONLINE

c1t1d0

OFFLINE

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Cette sortie est divisée en plusieurs sections :

Informations globales d'état des pools
Cette section d'en-tête de la sortie zpool status se compose des champs suivants, certains
d'entre eux n'étant affichés que pour les pools présentant des problèmes :

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

293

Identification de problèmes dans ZFS

pool

state

status

action

see

scrub

errors

Nom du pool.
État de maintenance actuel du pool. Ces informations concernent uniquement la
capacité de pool à fournir le niveau de réplication requis. Les pools dont l'état est
ONLINE peuvent contenir des périphériques défaillants ou des données corrompues.
Description du problème dans le pool. Ce champ est absent si aucun problème n'est
détecté.
Action recommandée pour la réparation des erreurs. Ce champ constitue un
formulaire abrégé qui redirige l'utilisateur vers l'une des sections suivantes. Ce
champ est absent si aucun problème n'est détecté.
Référence à un article de connaissances contenant des informations de réparation
détaillées. Les articles en ligne sont actualisés plus régulièrement que ce guide.
Normalement, ils sont toujours référencés pour fournir les procédures de
réparation les plus à jour. Ce champ est absent si aucun problème n'est détecté.
Identifie l'état actuel d'une opération de nettoyage. Ce champ peut indiquer la date
et l'heure du dernier nettoyage, un nettoyage en cours ou l'absence de requête de
nettoyage.
Identifie les erreurs de données ou l'absence d'erreurs de données connues.

Informations de configuration de pool
Le champ config de la sortie zpool status décrit la configuration des périphériques incluant
le pool, ainsi que leur état et toute erreur générée à partir des périphériques. L'état peut être l'un
des suivants : ONLINE, FAULTED, DEGRADED, UNAVAILABLE ou OFFLINE. Si l'état n'est pas ONLINE, la
tolérance de pannes du pool a été compromise.

La deuxième section de la sortie de configuration affiche des statistiques d'erreurs. Ces erreurs
se divisent en trois catégories :

■

■

■

READ : des erreurs d'E/S se sont produites lors de l'envoi d'une requête de lecture.
WRITE : des erreurs d'E/S se sont produites lors de l'envoi d'une requête d'écriture.
CKSUM : erreurs de somme de contrôle. Le périphérique a renvoyé des données corrompues
en réponse à une requête de lecture.

Il est possible d'utiliser ces erreurs pour déterminer si les dommages sont permanents. Des
erreurs d'E/S peu nombreuses peuvent indiquer une interruption de service temporaire. Si elles
sont nombreuses, il est possible que le périphérique présente un problème permanent. Ces
erreurs ne correspondent pas nécessairement à la corruption de données telle qu'interprétée par
les applications. Si la configuration du périphérique est redondante, les périphériques de disque
peuvent présenter des erreurs impossibles à corriger, même si aucune erreur ne s'affiche au

294

Guide d'administration Solaris ZFS • Octobre 2009

Identification de problèmes dans ZFS

niveau du périphérique RAID-Z ou du miroir. Si c'est ce scénario qui se produit, ZFS a récupéré
les données adéquates et a réussi à réparer les données endommagées pour les répliques
existantes.

Pour de plus amples informations sur l'interprétation de ces erreurs pour déterminer une panne
de périphérique, reportez-vous à la section “Détermination du type de panne de périphérique”
à la page 299.

Enfin, les informations auxiliaires supplémentaire sont affichées dans la dernière colonne de la
sortie de zpool status. Ces informations s'étendent dans le champ state et facilitent le
diagnostic de modes de panne. Si l'état d'un périphérique est FAULTED, ce champ indique si
périphérique est inaccessible ou si les données du périphérique sont corrompues. Si le
périphérique est en cours de réargenture, ce champ affiche la progression du processus.

Pour de plus amples informations sur le contrôle de la progression de la réargenture,
reportez-vous à la section “Affichage de l'état de réargenture” à la page 306.

État du nettoyage
La troisième section de la sortie zpool status décrit l'état actuel de tout nettoyage explicite.
Ces informations sont distinctes de la détection d'erreurs dans le système, mais il est possible de
les utiliser pour déterminer l'exactitude du rapport d'erreurs de corruption de données. Si le
dernier nettoyage s'est récemment terminé, toute corruption de données existante aura
probablement déjà été détecté.

Pour de plus amples informations sur le nettoyage de données et l'interprétation de ces
informations, reportez-vous à la section “Vérification de l'intégrité des données ZFS”
à la page 289.

Erreurs de corruption de données
La commande zpool status indique également si des erreurs connues sont associées au pool.
La détection de ces erreurs a pu s'effectuer lors du nettoyage de disque ou lors des opérations
normales. ZFS conserve un journal persistant des erreurs de données associées au pool. Ce
journal tourne à chaque fois qu'un nettoyage complet du système est terminé.

Les erreurs de corruption de données constituent toujours des erreurs fatales. Elles indiquent
une erreur d'E/S dans au moins une application, en raison de la présence de données
corrompues au sein du pool. Les erreurs de périphérique dans un pool redondant n'entraînent
pas de corruption de données et ne sont pas enregistrées en tant que partie de ce journal. Par
défaut, seul le nombre d'erreurs trouvées s'affiche. Vous pouvez obtenir la liste complète des
erreurs et de leurs spécificités à l'aide de l'option zpool status -v. Exemple :

# zpool status -v

pool: tank

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

295

Identification de problèmes dans ZFS

state: UNAVAIL

status: One or more devices are faulted in response to IO failures.

action: Make sure the affected devices are connected, then run ’zpool clear’.

see: http://www.sun.com/msg/ZFS-8000-HC

scrub: scrub completed after 0h0m with 0 errors on Tue Sep 1 09:51:01 2009

config:

NAME

tank

UNAVAIL

c1t0d0

ONLINE

c1t1d0

UNAVAIL

STATE

READ WRITE CKSUM

0

0

4

0

0

1

0 insufficient replicas

0

0 cannot open

errors: Permanent errors have been detected in the following files:

/tank/data/aaa

/tank/data/bbb

/tank/data/ccc

La commande fmd affiche également un message similaire dans la console système et le fichier
/var/adm/messages. La commande fmdump permet également de réaliser le suivi de ces
messages.

Pour de plus amples informations sur l'interprétation d'erreurs de corruption de données,
reportez-vous à la section “Identification du type de corruption de données” à la page 309.

Rapport système de messages d'erreur ZFS
Outre le suivi permanent des erreur au sein du pool, ZFS affiche également des messages syslog
lorsque des événements intéressants se produisent. Les scénarios suivants génèrent des
événements pour notifier l'administrateur :
■ Transition d'état de périphérique – Si l'état d'un périphérique devient FAULTED, ZFS

consigne un message indiquant que la tolérance de pannes du pool risque d'être
compromise. Un message similaire est envoyé si le périphérique est mis en ligne
ultérieurement, restaurant la maintenance du pool.

■ Corruption de données – En cas de détection de corruption de données, ZFS consigne un

message indiquant où et quand s'est produit la détection. Ce message n'est consigné que lors
de la première détection. Les accès ultérieurs ne génèrent pas de message.

■ Défaillances de pool et de périphérique – En cas de défaillance d'un pool ou d'un

périphérique, le démon du gestionnaire de pannes rapporte ces erreurs par le biais de
messages syslog et de la commande fmdump.

Si ZFS détecte un erreur de périphérique et la corrige automatiquement, aucune notification
n'est générée. De telles erreurs ne constituent pas une défaillance de redondance de pool ou de

296

Guide d'administration Solaris ZFS • Octobre 2009

l'intégrité des données. En outre, de telles erreurs sont typiquement dues à un problème de
pilote accompagné de son propre jeu de messages d'erreur.

Réparation d'un périphérique manquant

Réparation d'un configuration ZFS endommagée

ZFS conserve un cache des pools actifs et de leur configuration dans le système de fichiers
racine. Si ce fichier est corrompu ou n'est plus synchronisé avec ce qui est stocké dans le disque,
l'ouverture du pool n'est plus possible. ZFS tente d'éviter ces situations, mais la corruption
arbitraire reste possible en raison des caractéristiques du système de fichiers sous-jacent et du
stockage. En général, cette situation est due à la disparition d'un pool du système alors qu'il
devrait être disponible. Parfois, elle correspond à une configuration partielle, dans laquelle il
manque un nombre inconnu de périphériques virtuels de niveau supérieur. Quel que soit le cas,
la configuration peut être récupérée en exportant le pool (s'il est visible à tous) et en le
réimportant.

Pour de plus amples informations sur l'importation et l'exportation de pools, reportez-vous à la
section “Migration de pools de stockage ZFS” à la page 105.

Réparation d'un périphérique manquant

Si l'ouverture d'un périphérique est impossible, ce dernier s'affiche dans l'état UNAVAILABLE dans
la sortie de zpool status. Cet état indique que ZFS n'a pas pu ouvrir le périphérique lors du
premier accès au pool ou que le périphérique est devenu indisponible par la suite. Si le
périphérique rend un périphérique de niveau supérieur indisponible, l'intégralité du pool
devient inaccessible. Dans le cas contraire, la tolérance de pannes du pool risque d'être
compromise. Quel que soit le cas, le périphérique doit simplement être reconnecté au système
pour refonctionner normalement.

Par exemple, après une panne de périphérique, fmd peut afficher un message similaire au
suivant :

SUNW-MSG-ID: ZFS-8000-FD, TYPE: Fault, VER: 1, SEVERITY: Major

EVENT-TIME: Tue Sep 1 09:36:46 MDT 2009

PLATFORM: SUNW,Sun-Fire-T200, CSN: -, HOSTNAME: neo

SOURCE: zfs-diagnosis, REV: 1.0

EVENT-ID: a1fb66d0-cc51-cd14-a835-961c15696fed

DESC: The number of I/O errors associated with a ZFS device exceeded

acceptable levels. Refer to http://sun.com/msg/ZFS-8000-FD for more information.

AUTO-RESPONSE: The device has been offlined and marked as faulted. An attempt

will be made to activate a hot spare if available.

IMPACT: Fault tolerance of the pool may be compromised.

REC-ACTION: Run ’zpool status -x’ and replace the bad device.

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

297

Réparation d'un périphérique manquant

L'étape suivante consiste à exécuter la commande zpool status -x pour visualiser des
informations plus détaillées sur le problème du périphérique et sa résolution. Exemple :

Cette sortie indique que le périphérique manquant c1t1d0 ne fonctionne pas. Si vous
déterminez que le périphérique est défectueux, remplacez-le.

Exécutez ensuite la commande zpool online pour mettre le périphérique remplacé en ligne.
Exemple :

# zpool online tank c1t1d0

Confirmez que le pool dont le périphérique a été remplacé est fonctionnel.

# zpool status -x tank

pool ’tank’ is healthy

Reconnexion physique du périphérique
La reconnexion d'un périphérique dépend du périphérique en question. S'il s'agit d'un disque
connecté au réseau, la connectivité doit être restaurée. S'il s'agit d'un support USB ou autre
support amovible, il doit être reconnecté au système. S'il s'agit d'un disque local, un contrôleur
est peut-être tombé en panne, rendant le périphérique invisible au système. Dans ce cas, il faut
remplacer le contrôleur pour que les disques soient à nouveau disponibles. D'autres problèmes
existent et dépendent du type de matériel et de sa configuration. Si un disque tombe en panne et
n'est plus visible pour le système (événement peu probable), le périphérique doit être traité
comme un périphérique endommagé. Suivez les procédures décrites dans la section
“Remplacement ou réparation d'un périphérique endommagé ” à la page 299.

Notification relative à la disponibilité de
périphériques dans ZFS
Une fois le périphérique reconnecté au système, sa disponibilité peut être détectée
automatiquement ou non dans ZFS. Si le pool était précédemment défaillant ou si le system a
été réinitialisé en tant que partie de la procédure de reconnexion, alors ZFS rebalayer
automatiquement tous les périphériques lors de la tentative d'ouverture du pool. Si le pool était
endommagé et que le périphérique a été remplacé alors que le système était activé, vous devez
indiquer à ZFS que le périphérique est dorénavant disponible et qu'il est prêt à être rouvert à
l'aide de la commande zpool online. Exemple :

# zpool online tank c0t1d0

298

Guide d'administration Solaris ZFS • Octobre 2009

Remplacement ou réparation d'un périphérique endommagé

Pour de plus amples informations sur la remise en ligne de périphériques, reportez-vous à la
section “Mise en ligne d'un périphérique” à la page 85.

Remplacement ou réparation d'un périphérique endommagé

Cette section explique comment déterminer les types de panne de périphériques, effacer les
erreurs transitoires et remplacer un périphérique.

Détermination du type de panne de périphérique
Le terme périphérique endommagé peut décrire un grand nombre de situations :
■ Bit rot – Sur la durée, des événements aléatoires, tels que les influences magnétiques et les
rayons cosmiques, peuvent entraîner une inversion des bits stockés dans le disque dans des
moments imprévisibles. Ces événements sont relativement rares mais, cependant, assez
courants pour entraîner des corruptions de données potentielles dans des systèmes de
grande taille ou de longue durée. Ces erreurs sont typiquement transitoires.

■ Lectures ou écritures mal dirigées – Les bogues de microprogrammes ou les pannes de

matériel peuvent entraîner un référencement incorrect de l'emplacement du disque par des
lectures ou écritures de blocs entiers. Ces erreurs sont généralement transitoires, mais un
grand nombre d'entre elles peut indiquer un disque défectueux.

■ Erreur d'administrateur – Les administrateurs peuvent écraser par erreur des parties du

disque avec des données erronées (la copie de /dev/zero sur des partie du disque, par
exemple) qui entraînent la corruption permanente du disque. Ces erreurs sont toujours
transitoires.
Interruption temporaire de service : un disque peut être temporairement indisponible,
entraînant l'échec des E/S. En général, cette situation est associée aux périphériques
connectés au réseau, mais les disques locaux peuvent également connaître des interruptions
temporaires de service. Ces erreurs peuvent être transitoires ou non.

■

■ Matériel défectueux ou peu fiable – Cette situation englobe tous les problèmes liés à un

matériel défectueux. Il peut s'agir d'erreurs d'E/S constantes, de transports défectueux
entraînant des corruptions aléatoires ou d'autres défaillances. Ces erreurs sont typiquement
permanentes.

■ Périphérique mis hors ligne – Si un périphérique est hors ligne, il est considéré comme

ayant été mis hors ligne par l'administrateur, parce qu'il était défectueux. L'administrateur
qui a mis ce dispositif hors ligne peut déterminer si cette hypothèse est exacte.

Il est parfois difficile de déterminer la nature exacte de la panne. La première étape consiste à
examiner le décompte d'erreurs dans la sortie de zpool status comme suit :

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

299

Remplacement ou réparation d'un périphérique endommagé

# zpool status -v pool

Les erreurs sont divisées en erreurs d'E/S et en erreurs de sommes de contrôle. Ces deux
catégories peuvent indiquer le type de panne possible. Une opération typique renvoie un très
petit nombre d'erreurs (quelques-unes sur une longue période). Si les erreurs sont nombreuses,
un périphérique est probablement en panne ou sur le point de tomber en panne. Cependant, les
problèmes générés par l'erreur d'administrateur peuvent entraîner un grand nombre d'erreurs.
Le journal système constitue une autre source d'informations. Si le journal présente un grand
nombre de messages SCSI ou de pilote Fibre Channel, il existe probablement de graves
problèmes matériels. L'absence de messages syslog indique que les dommages sont
probablement transitoires.

L'objectif est de répondre à la question suivante :

Est-il possible qu'une autre erreur se produise dans ce périphérique ?

Les erreurs qui ne se produisent qu'une fois sont considérées transitoires et n'indiquent pas une
panne potentielle. Les erreurs suffisamment persistantes ou sévères pour indiquer une panne
matérielle potentielle sont considérées comme étant des "erreurs fatales". Aucun logiciel
automatisé actuellement disponible avec ZFS ne permet de déterminer le type d'erreur. Par
conséquent, l'administrateur doit procéder manuellement. Une fois l'erreur déterminée, vous
pouvez réaliser l'action adéquate. En cas d'erreurs fatales, effacez les erreurs transitoires ou
remplacez le périphérique. Ces procédures de réparation sont décrites dans les sections
suivantes.

Même si les erreurs de périphériques sont considérées comme étant transitoires, elles peuvent
tout de même entraîner des erreurs de données impossibles à corriger au sein du pool. Ces
erreurs requièrent des procédures de réparation spéciales, même si le périphérique sous-jacent
est considéré comme étant fonctionnel ou réparé. Pour de plus amples informations sur la
réparation d'erreurs de données, reportez-vous à la section “Réparation de données
endommagées” à la page 308.

Suppression des erreurs transitoires
Si les erreurs de périphérique sont considérées comme étant transitoires, dans la mesure où il
est peu probable qu'elles affectent la maintenance du périphérique, les erreurs de périphériques
peuvent être effacées en toute sécurité pour indiquer qu'aucune erreur fatale ne s'est produite.
Pour effacer les compteurs d'erreurs pour les périphériques mis en miroir ou RAID-Z, utilisez la
commande zpool clear. Exemple :

# zpool clear tank c1t1d0

Cette syntaxe efface toutes les erreurs associées au périphérique et tout décompte d'erreurs de
données associées au périphérique.

300

Guide d'administration Solaris ZFS • Octobre 2009

Remplacement ou réparation d'un périphérique endommagé

Pour effacer toutes les erreurs associées aux périphériques virtuels du pool et tout décompte
d'erreurs de données associées au pool, utilisez la syntaxe suivante :

# zpool clear tank

Pour de plus amples informations relatives à la suppression des erreurs de pool, reportez-vous à
la section “Suppression des périphériques de pool de stockage” à la page 86.

Remplacement d'un périphérique dans un pool de
stockage ZFS
Si le périphérique présente ou risque de présenter une panne permanente, il doit être remplacé.
Le remplacement du périphérique dépend de la configuration.
■ “Détermination de la possibilité de remplacement du périphérique” à la page 301
■ “Périphériques impossibles à remplacer” à la page 302
■ “Remplacement d'un périphérique dans un pool de stockage ZFS” à la page 302
■ “Affichage de l'état de réargenture” à la page 306

Détermination de la possibilité de remplacement du périphérique
Pour qu'un périphérique puisse être remplacé, l'état du pool doit être ONLINE. Le périphérique
doit faire partie d'une configuration redondante ou être fonctionnel (état ONLINE). Si le disque
fait partie d'une configuration redondante, il doit exister suffisamment de répliques pour
permettre la récupération des données correctes. Si deux disques d'un miroir à quatre directions
sont défaillants, chaque disque peut être remplacé car des répliques fonctionnelles sont
disponibles. Cependant, en cas de panne de deux disques dans un périphérique RAID-Z à
quatre directions, aucun disque ne peut être remplacé en l'absence de répliques suffisantes
permettant de récupérer les données. Si le périphérique est endommagé mais en ligne, il peut
être remplacé tant que l'état du pool n'est pas FAULTED. Toutefois, toute donnée endommagée
sur le périphérique est copiée sur le nouveau périphérique à moins que le nombre de copies des
données non endommagées soit déjà suffisant.

Dans la configuration suivante, le disque c1t1d0 peut être remplacé et toute donnée du pool est
copiée à partir de la réplique correcte, c1t0d0.

mirror

c1t0d0

c1t1d0

DEGRADED

ONLINE

FAULTED

Le disque c1t0d0 peut également être remplacé, mais un autorétablissement des données est
impossible, car il n'existe aucune réplique correcte.

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

301

Remplacement ou réparation d'un périphérique endommagé

Dans la configuration suivante, aucun des disques défaillants ne peut être remplacé. Les disques
ONLINE ne peuvent pas l'être non plus, car le pool lui-même est défaillant.

raidz

c1t0d0

c2t0d0

c3t0d0

c3t0d0

FAULTED

ONLINE

FAULTED

FAULTED

ONLINE

Dans la configuration suivante, chacun des disques de niveau supérieur peut être remplacé.
Cependant, les données incorrectes seront également copiées dans le nouveau disque, le cas
échéant.

c1t0d0

ONLINE

c1t1d0

ONLINE

Si les deux disques étaient défectueux, alors tout remplacement serait impossible car le pool
lui-même serait défectueux.

Périphériques impossibles à remplacer
Si la perte d'un périphérique entraîne une défaillance du pool ou si le périphérique contient trop
d'erreurs de données dans une configuration non redondante, alors le remplacement du
périphérique en toute sécurité est impossible. En l'absence de redondance suffisante, il n'existe
pas de données correctes avec lesquelles réparer le périphérique défectueux. Dans ce cas, la
seule option est de détruire le pool et de recréer la configuration, en restaurant les données au
cours du processus.

Pour de plus amples informations sur la restauration d'un pool entier, reportez-vous à la section
“Réparation de dommages présents dans l'ensemble du pool de stockage ZFS” à la page 311.

Remplacement d'un périphérique dans un pool de stockage ZFS
Après avoir déterminé qu'il est possible de remplacer un périphérique, exécutez la commande
zpool replace pour le remplacer effectivement. Exécutez la commande suivante si vous
remplacez le périphérique endommagé par un autre périphérique :

# zpool replace tank c1t1d0 c2t0d0

Cette commande lance la migration de données vers le nouveau périphérique, soit à partir du
périphérique endommagé, soit à partir d'autres périphériques du pool s'il s'agit d'une
configuration redondante. Une fois l'exécution de la commande terminée, le périphérique
endommagé est séparé de la configuration. Il peut dorénavant être retiré du système. Si vous
avez déjà retiré le périphérique et que vous l'avez remplacé par un autre dans le même
emplacement, utilisez la forme "périphérique unique" de la commande. Exemple :

302

Guide d'administration Solaris ZFS • Octobre 2009

Remplacement ou réparation d'un périphérique endommagé

# zpool replace tank c1t1d0

Cette commande formate adéquatement un disque non formaté et commence ensuite la
réargenture de données à partir du reste de la configuration.

Pour de plus amples informations sur la commande zpool replace reportez-vous à la section
“Remplacement de périphériques dans un pool de stockage” à la page 87.

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS
L'exemple suivant illustre le remplacement d'un périphérique (c1t3d0) du pool de stockage mis
en miroir tank sur un système Sun Fire x4500. Pour remplacer le disque c1t3d0 par un nouveau
au même emplacement (c1t3d0), annulez la configuration du disque avant de procéder au
remplacement. Les étapes de base sont les suivantes :
■ Mettez au préalable le disque à remplacer hors ligne. Vous ne pouvez pas annuler la

configuration d'un disque utilisé.
Identifiez le disque (c1t3d0) dont la configuration doit être annulée puis annulez sa
configuration. Dans cette configuration mise en miroir, le pool est endommagé et le disque
est hors ligne mais le pool reste disponible.

■

■ Remplacez le disque (c1t3d0). Vérifiez que la DEL bleue, indiquant que le périphérique est

prêt à être retiré, est allumée avant de retirer le lecteur défaillant.

■ Reconfigurez le disque (c1t3d0).
■ Remettez le disque (c1t3d0) en ligne.
■ Exécutez la commande zpool replace pour remplacer le disque (c1t3d0).

Remarque – Si vous avez précédemment défini la propriété de pool autoreplace=on, tout
nouveau périphérique détecté au même emplacement physique qu'un périphérique
appartenant précédemment au pool est automatiquement formaté et remplacé sans recourir
à la commande zpool replace. Cette fonction n'est pas prise en charge sur tous les types de
matériel.

■

Si un disque défectueux est automatiquement remplacé par un disque hot spare, vous devrez
peut-être déconnecter le disque hot spare une fois le disque défectueux remplacé. Par
exemple, si c2t4d0 reste actif comme disque de remplacement une fois le disque défectueux
remplacé, déconnectez-le.

# zpool detach tank c2t4d0

# zpool offline tank c1t3d0

# cfgadm | grep c1t3d0

sata1/3::dsk/c1t3d0

disk

connected

configured

ok

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

303

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS

(Suite)

# cfgadm -c unconfigure sata1/3

Unconfigure the device at: /devices/pci@0,0/pci1022,7458@2/pci11ab,11ab@1:3

This operation will suspend activity on the SATA device

Continue (yes/no)? yes

# cfgadm | grep sata1/3

sata1/3

disk

connected

unconfigured ok

<Replace the physical disk c1t3d0>

# cfgadm -c configure sata1/3

# cfgadm | grep sata3/7

sata3/7::dsk/c5t7d0

disk

connected

configured

ok

# zpool online tank c1t3d0

# zpool replace tank c1t3d0

# zpool status

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Tue Apr 22 14:44:46 2008

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror

ONLINE

c0t3d0 ONLINE

c1t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

Notez que la commande zpool output affiche parfois l'ancien disque et le nouveau sous
l'en-tête de remplacement. Exemple :

replacing

DEGRADED

c1t3d0s0/o FAULTED

c1t3d0

ONLINE

0

0

0

0

0

0

0

0

0

Ce texte signifie que la procédure de remplacement et la réargenture du nouveau disque sont en
cours.

Pour remplacer un disque (c1t3d0) par un autre disque (c4t3d0), il suffit d'exécuter la
commande zpool replace une fois le disque remplacé physiquement. Exemple :

304

Guide d'administration Solaris ZFS • Octobre 2009

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–1 Remplacement d'un périphérique dans un pool de stockage ZFS

(Suite)

# zpool replace tank c1t3d0 c4t3d0

# zpool status

pool: tank

state: DEGRADED

scrub: resilver completed after 0h0m with 0 errors on Tue Apr 22 14:54:50 2008

config:

NAME

tank

STATE

READ WRITE CKSUM

DEGRADED

mirror

ONLINE

c0t1d0

ONLINE

c1t1d0

ONLINE

mirror

ONLINE

c0t2d0

ONLINE

c1t2d0

ONLINE

mirror

DEGRADED

c0t3d0

ONLINE

replacing DEGRADED

c1t3d0

OFFLINE

c4t3d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

errors: No known data errors

La commande zpool status doit parfois être exécutée plusieurs fois jusqu'à la fin du
remplacement du disque.

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Tue Apr 22 14:54:50 2008

config:

NAME

tank

STATE

READ WRITE CKSUM

ONLINE

mirror

ONLINE

c0t1d0 ONLINE

c1t1d0 ONLINE

mirror

ONLINE

c0t2d0 ONLINE

c1t2d0 ONLINE

mirror

ONLINE

c0t3d0 ONLINE

c4t3d0 ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

305

Remplacement ou réparation d'un périphérique endommagé

EXEMPLE 11–2 Remplacement d'un périphérique de journal défaillant

L'exemple suivant montre comment récupérer les données d'un périphérique de journal
défaillant c0t5d0 dans le pool de stockage pool. Les étapes de base sont les suivantes :
■ Vérifiez la sortie zpool status -x et le message de diagnostic FMA, décrits ici :

http://www.sun.com/msg/ZFS-8000-K4

■ Remplacez physiquement le périphérique de journal défaillant.
■ Mettez le périphérique de journal en ligne.
■ Effacez la condition d'erreur du pool.

# zpool status -x

pool: pool

state: FAULTED

status: One or more of the intent logs could not be read.

Waiting for adminstrator intervention to fix the faulted pool.

action: Either restore the affected device(s) and run ’zpool online’,

or ignore the intent log records by running ’zpool clear’.

scrub: none requested

config:

NAME

pool

STATE

READ WRITE CKSUM

FAULTED

mirror

ONLINE

c0t1d0 ONLINE

c0t4d0 ONLINE

logs

FAULTED

c0t5d0

UNAVAIL

0

0

0

0

0

0

0

0

0

0

0

0

0 bad intent log

0

0

0

0 bad intent log

0 cannot open

<Physically replace the failed log device>

# zpool online pool c0t5d0

# zpool clear pool

Affichage de l'état de réargenture
Le processus de remplacement d'un disque peut prendre beaucoup de temps, selon la taille du
disque et la quantité de données dans le pool. Le processus de déplacement de données d'un
périphérique à un autre s'appelle la réargenture. Vous pouvez la contrôler à l'aide de la
commande zpool status.

Les systèmes de fichiers traditionnels effectuent la réargenture de données au niveau du bloc.
Dans la mesure où ZFS élimine la séparation en couches artificielles du gestionnaire de volume,
il peut effectuer la réargenture de façon bien plus puissante et contrôlée. Les deux avantages de
cette fonction sont comme suite :

306

Guide d'administration Solaris ZFS • Octobre 2009

Remplacement ou réparation d'un périphérique endommagé

■ ZFS n'effectue la réargenture que de la quantité minimale de données requises. Dans le cas

d'une brève interruption de service (et non pas du remplacement complet d'un
périphérique), vous pouvez effectuer la réargenture du disque en quelques minutes ou
quelques secondes, plutôt que d'effectuer la réargenture de l'intégralité du disque ou de
compliquer l'opération avec la journalisation de " régions sales " prise en charge par certains
gestionnaires de volume. Lors du remplacement d'un disque entier, la durée du processus de
réargenture est proportionnelle à la quantité de données utilisées dans le disque. Le
remplacement d'un disque de 500 Go ne dure que quelques secondes si le pool ne contient
que quelques giga-octets d'espace utilisé.

■ La réargenture est un processus fiable qui peut être interrompu, le cas échéant. En cas de
mise hors-tension ou de réinitialisation du système, le processus de réargenture reprend
exactement là où il s'est arrêté, sans requérir une intervention manuelle.

La commande zpool status permet de visualiser le processus de réargenture. Exemple :

# zpool status tank

pool: tank

state: ONLINE

status: One or more devices is currently being resilvered. The pool will

continue to function, possibly in a degraded state.

action: Wait for the resilver to complete.

scrub: resilver in progress for 0h2m, 16.43% done, 0h13m to go

config:

NAME

tank

mirror

STATE

READ WRITE CKSUM

DEGRADED

DEGRADED

replacing

DEGRADED

c1t0d0

ONLINE

c2t0d0

ONLINE

c1t1d0

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

Dans cet exemple, le disque c1t0d0 est remplacé par c2t0d0. Cet événement est observé dans la
sortie d'état par la présence du périphérique virtuel de remplacement (replacing) de la
configuration. Ce périphérique n'est pas réel et ne permet pas de créer un pool. L'objectif de ce
périphérique consiste uniquement à afficher le processus de réargenture et à identifier le
périphérique en cours de remplacement.

Notez que tout pool en cours de réargenture se voit attribuer l'état ONLINE ou DEGRADED car il ne
peut pas fournir le niveau souhaité de redondance tant que le processus n'est pas terminé. La
réargenture s'effectue aussi rapidement que possible, mais les E/S sont toujours programmées
avec une priorité inférieure à celle des E/S requises par l'utilisateur afin de minimiser l'impact
sur le système. Une fois le processus terminé, la nouvelle configuration complète s'applique,
remplaçant l'ancienne configuration. Exemple :

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

307

Réparation de données endommagées

# zpool status tank

pool: tank

state: ONLINE

scrub: resilver completed after 0h0m with 0 errors on Tue Sep 1 10:55:54 2009

config:

NAME

tank

mirror

ONLINE

c2t0d0 ONLINE

c1t1d0 ONLINE

errors: No known data errors

STATE

READ WRITE CKSUM

ONLINE

0

0

0

0

0

0

0

0

0

0

0

0

L'état du pool est à nouveau ONLINE et le disque défectueux d'origine (c1t0d0) a été supprimé de
la configuration.

Réparation de données endommagées

Les sections suivantes décrivent comment identifier le type de corruption de données et
comment réparer les données le cas échéant.
■ “Identification du type de corruption de données” à la page 309
■ “Réparation d'un fichier ou répertoire corrompu” à la page 310
■ “Réparation de dommages présents dans l'ensemble du pool de stockage ZFS” à la page 311

ZFS utilise les sommes de contrôle, la redondance et les données autorétablissantes afin de
réduire les risques de corruption de données. Cependant, la corruption de données peut se
produire si le pool n'est pas redondant, si la corruption s'est produite alors que le pool était
endommagé ou si une série d'événements improbables a corrompu plusieurs copies d'un
élément de données. Quelle que soit la source, le résultat est le même : les données sont
corrompues et par conséquent inaccessibles. Les actions à effectuer dépendent du type de
données corrompue et de leurs valeurs relatives. Deux types de données peuvent être
corrompus :
■ Métadonnées de pool – ZFS requiert une certaine quantité de données à analyser afin

d'ouvrir un pool et d'accéder aux jeux de données. Si ces données sont corrompues, le pool
entier ou des parties complètes de la hiérarchie du jeu de données sont indisponibles.

■ Données d'objet – Dans ce cas, la corruption se produit au sein d'un fichier ou périphérique

spécifique. Ce problème peut rendre une partie du fichier ou répertoire inaccessible ou
endommager l'objet.

Les données sont vérifiées lors des opérations normales et lors du nettoyage. Pour de plus
amples informations sur la vérification de l'intégrité des données du pool, reportez-vous à la
section “Vérification de l'intégrité des données ZFS” à la page 289.

308

Guide d'administration Solaris ZFS • Octobre 2009

Réparation de données endommagées

Identification du type de corruption de données
Par défaut, la commande zpool status indique qu'une corruption s'est produite, mais
n'indique pas à quel endroit. Exemple :

# zpool status

pool: monkey

state: ONLINE

status: One or more devices has experienced an error resulting in data

corruption. Applications may be affected.

action: Restore the file in question if possible. Otherwise restore the

entire pool from backup.

see: http://www.sun.com/msg/ZFS-8000-8A

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

monkey

ONLINE

c1t1d0s6 ONLINE

c1t1d0s7 ONLINE

0

0

0

0

0

0

0

0

0

errors: 8 data errors, use ’-v’ for a list

Toute erreur indique seulement qu'une erreur s'est produite à un moment donné. Il est possible
que certaines erreurs ne soient plus présentes dans le système. Dans des circonstances
normales, cette situation est vraie. Certaines interruptions de service temporaires peuvent
entraîner une corruption de données qui est automatiquement réparée une fois l'interruption
de service terminée. Un nettoyage complet du pool examine chaque bloc actif dans le pool.
Ainsi, le journal d'erreur est réinitialisé à la fin de chaque nettoyage. Si vous déterminez que les
erreurs ne sont plus présentes et ne souhaitez pas attendre la fin du nettoyage, la commande
zpool online permet de réinitialiser toutes les erreurs du pool.

Si la corruption de données se produit dans des métadonnées au niveau du pool, la sortie est
légèrement différente. Exemple :

# zpool status -v morpheus

pool: morpheus

id: 1422736890544688191

state: FAULTED

status: The pool metadata is corrupted.

action: The pool cannot be imported due to damaged devices or data.

see: http://www.sun.com/msg/ZFS-8000-72

config:

morpheus

FAULTED

corrupted data

c1t10d0

ONLINE

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

309

Réparation de données endommagées

Dans le cas d'une corruption au niveau du pool, ce dernier se voit attribuer l'état FAULTED, car le
pool ne peut pas fournir le niveau de redondance requis.

Réparation d'un fichier ou répertoire corrompu
En cas de corruption d'un fichier ou d'un répertoire, le système peut tout de même continuer à
fonctionner, selon le type de corruption. Tout dommage est irréversible, à moins que des copies
correctes des données n'existent sur le système. Si les données sont importantes, vous n'avez pas
d'autre choix que de restaurer les données affectées à partir d'une sauvegarde. Vous pouvez
pourtant récupérer les données suite à corruption sans avoir à restaurer tout le pool.

En cas de dommages au sein d'un bloc de données de fichiers, le fichier peut être supprimé en
toute sécurité. L'erreur est alors effacée du système. La commande zpool status -v permet
d'afficher une liste de noms de fichiers comportant des erreurs persistantes. Exemple :

# zpool status -v

pool: monkey

state: ONLINE

status: One or more devices has experienced an error resulting in data

corruption. Applications may be affected.

action: Restore the file in question if possible. Otherwise restore the

entire pool from backup.

see: http://www.sun.com/msg/ZFS-8000-8A

scrub: none requested

config:

NAME

STATE

READ WRITE CKSUM

monkey

ONLINE

c1t1d0s6 ONLINE

c1t1d0s7 ONLINE

0

0

0

0

0

0

0

0

0

errors: Permanent errors have been detected in the following files:

/monkey/a.txt

/monkey/bananas/b.txt

/monkey/sub/dir/d.txt

/monkey/ghost/e.txt

/monkey/ghost/boo/f.txt

La sortie précédente est décrite comme suit :

■

Si le chemin complet du fichier est trouvé et si le jeu de données est monté, le chemin
complet du fichier s'affiche. Exemple :

/monkey/a.txt

310

Guide d'administration Solaris ZFS • Octobre 2009

Réparation d'un système impossible à réinitialiser

■

■

Si chemin complet du fichier est trouvé mais que le jeu de données n'est pas monté, le nom
du jeu de données non précédé d'un slash (/) s'affiche, suivi du chemin du fichier au sein du
jeu de données. Exemple :

monkey/ghost/e.txt

Si le nombre d'objet vers un chemin de fichiers ne peut pas être converti, soit en raison d'une
erreur soit parce qu'aucun chemin de fichiers réel n'est associé à l'objet, tel que c'est le cas
pour dnode_t, alors le nom du jeu de données s'affiche, suivi du numéro de l'objet. Exemple :

monkey/dnode:<0x0>

■ En cas de corruption d'un MOS (Meta-Object Set, jeu de méta-objet), la balise spéciale

<metadata> s'affiche, suivie du numéro de l'objet.

Si la corruption se situe au sein des métadonnées d'un répertoire ou d'un fichier, vous devez
déplacer le fichier vers un autre emplacement. Vous pouvez déplacer en toute sécurité un fichier
ou répertoire vers un emplacement moins pratique, ce qui permettra de restaurer l'objet
d'origine sur place.

Réparation de dommages présents dans l'ensemble
du pool de stockage ZFS
Si les dommages sont présents dans les métadonnées du pool, il est impossible d'ouvrir ce
dernier. Vous devez alors restaurer le pool et l'ensemble de ses données à partir de sauvegardes.
Le mécanisme utilisé varie énormément selon la configuration du pool et la stratégie de
sauvegarde. Tout d'abord, enregistrez la configuration telle qu'elle s'affiche dans zpool status
pour pouvoir le recréer une fois le pool détruit. Ensuite, détruisez le pool à l'aide de la
commande zpool destroy -f. Conservez également un fichier décrivant la disposition des
jeux de données et les diverses propriétés définies localement dans un emplacement sûr, car ces
informations deviennent inaccessibles lorsque le pool est lui-même inaccessible. Avec la
configuration du pool et la disposition des jeux de données, vous pouvez reconstruire la
configuration complète après destruction du pool. Les données peuvent ensuite être
renseignées par la stratégie de sauvegarde ou de restauration de votre choix.

Réparation d'un système impossible à réinitialiser

ZFS a été conçu pour être robuste et stable malgré les erreurs. Cependant, les bogues de logiciels
ou certains problèmes inattendus peuvent entraîner la panique du système lors de l'accès à un
pool. Dans le cadre du processus de réinitialisation, chaque pool doit être ouvert. En raison de

Chapitre 11 • Résolution de problèmes et récupération de données ZFS

311

Réparation d'un système impossible à réinitialiser

ces défaillances, le système effectue des réinitialisations en boucle. Pour pouvoir reprendre les
opérations dans cette situation, vous devez indiquer à ZFS de ne pas rechercher de pool au
démarrage.

ZFS conserve un cache interne de pools disponibles et de leurs configurations dans
/etc/zfs/zpool.cache. L'emplacement et le contenu de ce fichier sont privés et sujets à
modification. Si le système devient impossible à initialiser, redémarrez au jalon none à l'aide de
l'option d'initialisation -m milestone=none. Une fois le système rétabli, remontez le système
de fichiers racine en tant que système accessible en écriture,·puis renommez ou placez le fichier
/etc/zfs/zpool.cache à un autre emplacement. En raison de ces actions, ZFS oublie
l'existence de pools dans le système, ce qui l'empêche d'accéder au pool défectueux à l'origine du
problème. Vous pouvez ensuite passer à un état normal de système en exécutant la commande
svcadm milestone all. Vous pouvez utiliser un processus similaire lors de la réinitialisation à
partir d'une racine de remplacement pour effectuer des réparations.

Une fois le system démarré, vous pouvez tenter d'importer le pool à l'aide de la commande
zpool import. Cependant, dans ce cas, l'erreur qui s'est produite lors de l'initialisation risque
de se reproduire car la commande utilise le même mécanisme d'accès aux pools. Si le système
contient plusieurs pools, procédez comme suit :
■ Renommez ou déplacez le fichier zpool.cache vers un autre emplacement, comme décrit

ci-dessus.

■ Utilisez la commande fmdump -eV pour afficher les pools présentant des erreurs fatales et

déterminer ainsi quel pool pose des problèmes.
Importez les pools un à un en ignorant ceux qui posent problème, comme décrit dans
fmdump.

■

312

Guide d'administration Solaris ZFS • Octobre 2009

Index

A
Accès

ACL

Instantané ZFS

(exemple), 218

ACL dans un fichier ZFS

Description détaillée, 240

ACL sur un fichier ZFS

Description détaillée, 239

Configuration d'ACL dans un fichier ZFS (mode

ACL, Modification d'une ACL insignifiante dans un
fichier ZFS (mode détaillé) (Suite)

(Exemple), 242

Privilèges d'accès, 234
Restauration d'une ACL insignifiante sur un fichier

ZFS (mode détaillé)
(exemple), 245
Type d'entrée, 234

ACL, modèle de Solaris, Différences existant entre les

systèmes de fichiers traditionnels et ZFS, 57

ACL NFSv4

Description de format, 233
Différences avec les ACL POSIX-draft, 232
Héritage d'ACL, 236
Indicateurs d'héritage d'ACL, 236
Mode de propriété d'ACL, 237
Modèle

Description, 231

ACL POSIX-draft, Description, 232
ACL Solaris

Configuration d'héritage d'ACL dans un fichier ZFS

Définition d'ACL sur un fichier ZFS (mode compact)

Définition des ACL sur un fichier ZFS (mode

compact)
Exemple, 255

(mode détaillé)
(exemple), 246

Description, 254

détaillé)
Description, 241

Définition sur les fichiers ZFS

Description, 238

Description, 231
Description de format, 233
Différences avec les ACL POSIX-draft, 232
Héritage d'ACL, 236
Indicateurs d'héritage d'ACL, 236
Mode de propriété aclmode, 237
Mode de propriété d'ACL, 237
Mode de propriétéaclinherit, 237
Modification d'une ACL insignifiante dans un fichier

ZFS (mode détaillé)

Description de format, 233
Différences avec les ACL POSIX-draft, 232
Héritage d'ACL, 236
Indicateurs d'héritage d'ACL, 236
Mode de propriété d'ACL, 237
Nouveau modèle

Description, 231

Administration déléguée , présentation, 261
Administration déléguée de ZFS, présentation, 261
Administration simplifiée, Description, 43
Affichage

Droits délégués (exemple), 266

313

Disques, à une configuration RAID-Z (exemple), 79
Périphérique à un pool de stockage ZFS (zpool add)

noms, 46

Index

Affichage (Suite)

État de maintenance d'un pool de stockage ZFS

État détaillé du fonctionnement du pool de stockage

Exemple, 103

ZFS
(exemple), 104

État fonctionnel d'un pool de stockage

Description, 102

history, commande, 33
Rapport syslog de messages d'erreur ZFS

Description, 296

Statistiques d'E/S à l'échelle du pool de stockage ZFS

Statistiques d'E/S de pool de stockage vdev ZFS

Exemple, 100

Exemple, 101

Statistiques d'E/S de pools de stockage ZFS

Description, 100

Ajout

Périphérique de journalisation mis en miroir

Exemple, 77

(exemple), 79

Périphériques de cache (exemple), 80
Système de fichiers ZFS à une zone non globale

Volume ZFS à une zone non globale

Exemple, 279

(exemple), 281

Ajustement, Périphériques de swap et de vidage, 159
Annulation du partage

Système de fichiers ZFS

exemple, 207

atime, propriété, Description, 180
Autorétablissement, Description, 65
available, propriété, Description, 181

B
Bloc d'initialisation, Installation à l'aide de installboot

et de installgrub, 162

314

Guide d'administration Solaris ZFS • Octobre 2009

C
Cache primaire Propriété, Description, 183
Cache secondaire Propriété, Description, 185
canmount, propriété
Description, 181
Description détaillée, 190

checksum, propriété, Description, 181
Clone

Création

Exemple, 221

Définition, 44
Destruction

Exemple, 221

Fonctions, 220

Comportement d'espace saturé, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 57

Composants, Pool de stockage ZFS, 59
Composants de ZFS, Exigences d'attribution de

compression, propriété, Description, 182
compressratio, propriété, Description, 182
Comptabilisation de l'espace ZFS, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 56

Configuration

ACL dans un fichier ZFS (mode compact)

Exemple, 255

Configuration en miroir

Description, 63
Fonction de redondance, 63
Vue conceptuelle, 63
Configuration RAID-Z

Double parité, description, 64
Exemple, 68
Parité simple, description, 64
Redondance, fonction, 64
Vue conceptuelle, 64

Configuration RAID-Z, ajout de disques,

Configuration requise, Installation et Live

(exemple), 79

Upgrade, 117

Périphérique, à un pool de stockage ZFS (zpool

D
Définition

Connexion

attach)
Exemple, 82

Contrôle

Intégrité des données ZFS, 289
Validation des données (nettoyage), 290

copies, propriété, Description, 182
Création

Clone ZFS

Exemple, 221

Hiérarchie d'un système de fichiers ZFS, 50
Instantané ZFS

(exemple), 216

Pool de stockage, à l'aide d'un périphérique de

journalisation (exemple), 70

Pool de stockage avec périphériques de cache

Pool de stockage RAID-Z à double parité (zpool

Pool de stockage RAID-Z à parité simple (zpool

(exemple), 70

create)
Exemple, 69

create)
Exemple, 68

Pool de stockage ZFS
Description, 66

Pool de stockage ZFS (zpool create)

(exemple), 48, 66

Pool de stockage ZFS mis en miroir (zpool create)

(exemple), 67

Pool racine de remplacement

(exemple), 284

Système de fichiers ZFS, 52

(exemple), 176
Description, 176

(exemple), 48

Volume ZFS

(exemple), 275

creation, propriété, Description, 182

Système de fichiers ZFS de base (zpool create)

Index

ACL sur les fichiers ZFS

Description, 238

ACL sur un fichier ZFS (mode compact)

Description, 254

ACL sur un fichier ZFS (mode détaillé)

Description, 241
atime, propriété ZFS

(exemple), 196

compression, propriété

(exemple), 52

Héritage d'ACL dans un fichier ZFS (mode détaillé)

(exemple), 246

mountpoint, propriété, 52
Point de montage ZFS (zfs set mountpoint)

(exemple), 203

Points de montage hérités

(exemple), 203

quota, propriété (exemple), 53
Quota d'un système de fichiers ZFS (zfs set quota)

Réservation de système de fichiers ZFS

Exemple, 209

Quota ZFS

(exemple), 197

Exemple, 213

sharenfs, propriété

(exemple), 52

Délégation

Droit (exemple), 268
Jeu de données à une zone non globale

(exemple), 280

Délégation de droits, zfs allow, 264
Délégation de droits à un groupe, (exemple), 268
Délégation de droits à un utilisateur individuel,

(exemple), 268

Démontage

Système de fichiers ZFS

(exemple), 205

Dépannage

Détection de problèmes éventuels (zpool status

Déterminer si un périphérique peut être remplacé

-x), 293

Description, 301

315

Index

Dépannage (Suite)

Périphérique manquant (défaillant), 288
Problème d'identification, 292
Remplacement d'un périphérique (zpool replace)

Remplacement d'un périphérique manquant

Réparation d'un fichier ou répertoire endommagé

Réparation d'un système qui ne peut être initialisé

(exemple), 307

(exemple), 297

Description, 310

Description, 312

Réparation d'une configuration ZFS

endommagée, 297

Réparation de dommages au niveau d'un pool

Description, 311

Destruction
Clone ZFS

Exemple, 221

Instantané ZFS

(exemple), 217

Pool de stockage ZFS
Description, 66

Différences existant entre les systèmes de fichiers
traditionnels et ZFS (Suite)

Comptabilisation de l'espace ZFS, 56
Gestion d'un volume traditionnel, 57
Granularité d'un système de fichiers, 55
Montage d'un système de fichiers ZFS, 57
Nouveau modèle ACL de Solaris, 57

Disque, Composant de pool de stockage ZFS, 60
Disque entier, Composant de pool de stockage ZFS, 60
Disque hot spare

Corruption identifiée (zpool status -v)

Création

(exemple), 89

Description

Exemple, 89

Données

Exemple, 295

Endommagées, 288
Nettoyage

Exemple, 290

Réargenture

Description, 291

Pool de stockage ZFS (zpool destroy)

(exemple), 76

Système de fichiers ZFS

(exemple), 177

Système de fichiers ZFS comportant des systèmes

dépendants
(exemple), 178

Détection

Niveaux de réplication incohérents

Exemple, 74

Périphériques en cours d'utilisation

Exemple, 73

Détermination

Type de panne de périphérique

Description, 299

Déterminer

Remplacement d'un périphérique

Description, 301

devices, propriété, Description, 182
Différences existant entre les systèmes de fichiers

traditionnels et ZFS
Comportement d'espace saturé, 57

Réparation, 289
Validation (nettoyage), 290

Données vérifiées par somme de contrôle,

Description, 42

Périphérique d'un pool de stockage ZFS (zpool

E
Effacement

clear)
Description, 86

Enregistrement

Données d'un système de fichiers ZFS (zfs send)

(exemple), 224

Entrelacement dynamique

Description, 65
Fonction de pool de stockage, 65

Envoi et réception

Données d'un système de fichiers ZFS

Description, 223

Etiquette EFI

Description, 60

316

Guide d'administration Solaris ZFS • Octobre 2009

Etiquette EFI (Suite)

Interaction avec ZFS, 60

exec, propriété, Description, 182
Exigences d'attribution de noms, Composants ZFS, 46
Exigences matérielles et logicielles, 47
Exportation

Pool de stockage ZFS

(exemple), 106

F
Fichiers, En tant que composants d'un pool de stockage

Fonctions de réplication de ZFS, Mise en miroir ou

ZFS, 63

RAID-Z, 63

G
Gestion d'un volume traditionnel, Différences existant

entre les systèmes de fichiers traditionnels et
ZFS, 57

Granularité d'un système de fichiers, Différences

existant entre les systèmes de fichiers traditionnels et
ZFS, 55

H
Héritage

Propriété ZFS (zfs inherit)

Description, 197

Hiérarchie d'un système de fichiers, Création, 50
history, commande, zpool history, 33

I
Identification

Pool de stockage ZFS à importer (zpool import -a)

(exemple), 107
Stockage requis, 49
Type de corruption de données (zpool status -v)

Exemple, 309

Importation

Pool de stockage ZFS

(exemple), 110

Pool de stockage ZFS, à partir de répertoires

alternatifs (zpool import -d)
(exemple), 109

Pool racine de remplacement

(exemple), 285

Initialisation

Environnement d'initialisation ZFS avec boot -L et

boot -Z sur un système SPARC, 164

Système de fichiers racine, 161

Index

317

Installation initiale d'un système de fichiers racine ZFS,

Installation

Système de fichiers racine ZFS
(installation initiale), 120
Configuration requise, 117
Fonctions, 116
Installation JumpStart, 130
Installation de blocs d'initialisation

installboot et installgrup

(exemple), 162

(exemple), 121

Installation JumpStart

Système de fichiers racine

Exemples de profils, 130
Problèmes, 133

Instantané

Accès

(exemple), 218

Comptabilisation de l'espace, 219
Création

(exemple), 216

Définition, 45
Destruction

(exemple), 217

Fonctions, 215
Renommer

Exemple, 217

Restauration

Exemple, 220

Index

J
Jeu de données

Définition, 44
Description, 175

Jeu de droits défini, 261
Journal d'intention ZFS (ZIL), Description, 28

L
Lecture seule, propriétés ZFS, mounted, 183
Liste

Descendants d'un système de fichiers ZFS

(exemple), 194

Informations sur le pool ZFS, 50
Pool de stockage ZFS
Description, 96

Pools de stockage ZFS

Exemple, 97

Propriété ZFS (zfs list)

(exemple), 198

Propriété ZFS de script

(exemple), 200

Propriété ZFS par valeur source

(exemple), 200

Système de fichiers ZFS

(exemple), 193

Système de fichiers ZFS (zfs list)

Système de fichiers ZFS sans l'en-tête

Types de systèmes de fichiers ZFS

(exemple), 53

Exemple, 195

Exemple, 195

listsnapshots propriété, description, 95

luactivate

Système de fichier racine

(exemple), 138

lucreate

Environnement d'initialisation ZFS à partir d'un

environnement d'initialisation ZFS
(exemple), 140

Migration d'un système de fichiers racine

(exemple), 137

318

Guide d'administration Solaris ZFS • Octobre 2009

M
Migration

Système de fichiers racine UFS vers système de

fichiers racine ZFS
(Solaris Live Upgrade), 134

Système de fichiers racine UFS vers un système de

fichiers racine ZFS
Problèmes, 135

Migration d'un pool de stockage ZFS, Description, 105
Miroir, Définition, 44
Mise à niveau

Pool de stockage ZFS
Description, 112

Mise en ligne d'un périphérique

Pool de stockage ZFS (zpool online)

(exemple), 86

Mise en ligne et hors ligne de périphériques

Mise hors ligne d'un périphérique (zpool offline)

Pool de stockage ZFS
Description, 84

Pool de stockage ZFS

(exemple), 84
Mode de panne, 287

Périphérique endommagé, 288
Mode de propriété aclmode, 237
Mode de propriétéaclinherit, 237
Mode de propriétés d'ACL, aclinherit, 180
Mode panne

Données endommagées, 288
Périphérique manquant (défaillant), 288

Modes de propriétés d'ACL, aclmode, 180
Modification

ACL insignifiante dans un fichier ZFS (mode

détaillé)
(Exemple), 242

Montage

Système de fichiers ZFS

Exemple, 204

Montage d'un système de fichiers ZFS, Différences

existant entre les systèmes de fichiers traditionnels et
ZFS, 57

Mot clé de profil JumpStart, Système de fichiers racine

ZFS, 131

mounted, propriété, Description, 183

Index

Détection

Exemple, 73

Point de montage

Automatique, 202
Gestion de ZFS

Description, 201

Héritage, 202
Par défaut pour les pools de stockage ZFS, 75
Par défaut pour un système de fichiers ZFS, 176

Pool, Définition, 45
Pool de stockage mis en miroir (zpool create),

(exemple), 67

Pool de stockage ZFS

Affichage de l'état de maintenance

Affichage de l'état détaillé du fonctionnement

Exemple, 103

(exemple), 104

Affichage de l'état fonctionnel, 102
Affichage du processus de réargenture

(exemple), 307

Ajout de périphérique (zpool add)

Exemple, 77
Composants, 59
Configuration en miroir, description, 63
Configuration RAID-Z, création (zpool create)

Connexion de périphériques (zpool attach)

Exemple, 68

Exemple, 82

Corruption de données identifiée (zpool status -v)

Exemple, 295

Création (zpool create)

(exemple), 66

Création d'une configuration mise en miroir (zpool

N
Nettoyage

Exemple, 290
Validation des données, 290

Niveaux de réplication incohérents

Détection

Exemple, 74

Notification

Exemple, 298

périphérique reconnecté dans ZFS(zpool online)

O
origin, propriété, Description, 183

P
Partage

Système de fichiers ZFS

Description, 206
Exemple, 207

Périphérique de journalisation mis en miroir, Création

d'un pool (exemple), 70

Périphérique de journalisation mis en miroir, ajout,

(exemple), 79

Périphérique virtuel

Composant de pools de stockage ZFS, 71
Définition, 45

Périphériques de cache

Considérations d'utilisation, 70
Création d'un pool (exemple), 70

Périphériques de cache , suppression, (exemple), 80
Périphériques de cache, ajout, (exemple), 80
Périphériques de journalisation distincts,

considérations, 28

Périphériques de swap et de vidage

Ajustement de la taille, 159
Description, 158
Points à prendre en compte, 159

mountpoint, propriété, Description, 183

Périphériques en cours d'utilisation

create)
(exemple), 67

(exemple), 76

-x)
Description, 293

Description, 299

Destruction (zpool destroy)

Détection de problèmes éventuels (zpool status

Détermination du type de panne de périphérique

319

Index

Pool de stockage ZFS (Suite)

Déterminer si un périphérique peut être remplacé

Pool de stockage ZFS, Notification d'un périphérique
reconnecté dans ZFS (zpool online) (Suite)

Description, 301

Données endommagées

Description, 288

Entrelacement dynamique, 65
Exportation

(exemple), 106

Identification du type de corruption de données

Identification pour l'importation (zpool import -a)

Importation à partir de répertoires alternatifs (zpool

Informations globales d'état des pools pour la

Exemple, 298

Périphérique endommagé

Description, 288

Périphérique manquant (défaillant)

Description, 288

Périphérique virtuel, 71
Point de montage par défaut, 75
Pool

Définition, 45

Pool racine de remplacement, 284
Problème d'identification

Description, 292
Profil de droits, 285
RAID-Z

Définition, 45

RAID-Z, description, 64
Réargenture

Définition, 45

Récupération d'un pool détruit

(zpool status -v)
Exemple, 309

(exemple), 107

Importation

(exemple), 110

import -d)
(exemple), 109

résolution de problèmes
Description, 293

Liste

Exemple, 97

Message d'erreur système

Description, 296

Migration

Description, 105

Miroir

Définition, 44

Mise à niveau

Description, 112

(exemple), 84

Mode de panne, 287
Nettoyage de données

Exemple, 290

Nettoyage des données

Description, 290

Description, 291

(zpool online)

Remplacement d'un périphérique (zpool replace)

Exemple, 111

(exemple), 87

(exemple), 297

Exemple, 302

Description, 310

Description, 312

Remplacement d'un périphérique manquant

Remplacement de périphérique (zpool replace)

Réparation d'un fichier ou répertoire endommagé

Réparation d'un système qui ne peut être initialisé

Réparation d'une configuration ZFS

Réparation de dommages au niveau d'un pool

Description, 311

Réparation des données

Description, 289

Script de sortie de pool de stockage

Mise en ligne et hors ligne de périphériques

Description, 84

Mise hors ligne d'un périphérique (zpool offline)

endommagée, 297

Nettoyage des données et réargenture

Séparation des périphériques (zpool detach)

Notification d'un périphérique reconnecté dans ZFS

Statistiques d'E/S à l'échelle du pool

Exemple, 98

Exemple, 83

Exemple, 100

320

Guide d'administration Solaris ZFS • Octobre 2009

Pool de stockage ZFS (Suite)

Statistiques d'E/S vdev

Exemple, 101

Suppression d'un périphérique

Exemple, 86

clear)
Exemple, 300

Suppression des erreurs de périphérique (zpool

Propriété de pool ZFS (Suite)

version, 96

propriété delegation, désactivation, 262
Propriété delegation, description, 94
Propriété failmode, 95
Propriété guid, description, 95
Propriété health, description, 95
Propriété size, description, 95
Propriété used, description, 95
Propriété version, description, 96
Propriété ZFS

Gestion de systèmes de fichiers et pools de stockage

Test (zpool create -n)

Exemple, 75

Utilisation de disques entiers, 60
Utilisation de fichiers, 63
Validation des données

Description, 290

Pool de stockage ZFS (zpool online)

Mise en ligne d'un périphérique

(exemple), 86

Pool racine de remplacement

Création

(exemple), 284
Description, 284
Importation

(exemple), 285

Profil de droits

ZFS
Description, 285

Propriété altroot, description, 94
Propriété autoreplace, description, 94
Propriété available, description, 94
Propriété bootfs, description, 94
Propriété capacity, description, 94
Propriété de pool ZFS

altroot, 94
autoreplace, 94
available, 94
bootfs, 94
capacity, 94
delegation, 94, 95
guid, 95
health, 95
listsnapshots, 95
size, 95
used, 95

Index

321

Description détaillée, 190

aclinherit, 180
aclmode, 180
atime, 180
available, 181
canmount, 181

checksum, 181
compression, 182
compressratio, 182
copies, 182
creation, 182
devices, 182
exec, 182
lecture seule, 188
mountpoint, 183
origin, 183
quota, 184
read-only, 184
recordsize, 184

referenced, 184
refquota, 185
refreservation, 185
reservation, 185
setuid, 186
sharenfs, 186
snapdir, 186
type, 186
used, 186

Description détaillée, 191

Description détaillée, 188

volblocksize, 187
volsize, 187

Description détaillée, 192

Index

Propriété ZFS (Suite)

xattr, 187
zoned, 187
zoned, propriété

Description détaillée, 283

Propriété ZFS en lecture seule

Propriété ZFS pouvant être définie

Description détaillée, 190

available, 181
compression, 182
creation, 182
Description, 188
origin, 183
referenced, 184
type, 186
used, 186

aclinherit, 180
aclmode, 180
atime, 180
canmount, 181

checksum, 181
compression, 182
copies, 182
devices, 182
exec, 182
mountpoint, 183
quota, 184
read-only, 184
recordsize, 184

Description détaillée, 191

refquota, 185
refreservation, 185
reservation, 185
setuid, 186
sharenfs, 186
snapdir, 186

Propriétés de ZFS

Description, 179

Propriétés ZFS

Cache secondaire, 183, 185
Définies par l'utilisateur

Description détaillée, 192

Description des propriétés héritées, 179
Gestion au sein d'une zone

Description, 282

Héritées, description, 179
mounted, 183
Pouvant être définies, 189
usedbychildren, 186
usedbydataset, 186
usedbyrefreservation, 187
usedbysnapshots, 187

Propriétés ZFS en lecture seule

usedbychildren, 186
usedbydataset, 186
usedbyrefreservation, 187
usedbysnapshots, 187

Propriétés ZFS pouvant être définies

Cache primaire, 183
Cache secondaire, 185
Description, 189

Q
quota, propriété, Description, 184
Quota et réservation, Description, 208

R
RAID-Z, Définition, 45
read-only, propriété, Description, 184
Réargenture, Définition, 45
Réargenture et nettoyage des données,

used

Description détaillée, 188

volblocksize, 187
volsize, 187

Description détaillée, 192

xattr, 187
zoned, 187

322

Guide d'administration Solaris ZFS • Octobre 2009

Description, 291

Réception

(exemple), 225
recordsize, propriété

Description, 184

Données de système de fichiers ZFS (zfs receive)

Index

Résolution de problèmes (Suite)

Remplacement de périphérique (zpool replace)

Suppression des erreurs de périphérique (zpool

ACL insignifiante sur un fichier ZFS (mode détaillé)

Exemple, 302

clear)
Exemple, 300

Restauration

(exemple), 245

Instantané ZFS

Exemple, 220

S
Script

recordsize, propriété (Suite)
Description détaillée, 191

Récupération

Pool de stockage ZFS détruit

Exemple, 111

referenced, propriété, Description, 184
refquota, propriété, Description, 185
refreservation, propriété, Description, 185
Remplacement

Périphérique (zpool replace)

(exemple), 87, 307
Exemple, 302

Périphérique manquant

(exemple), 297

Renommer

Instantané ZFS

Exemple, 217

Réparation

Configuration ZFS endommagée

Description, 297

Dommages au niveau d'un pool

Description, 311

Réparation d'un fichier ou répertoire endommagé

Description, 310

Système qui ne peut être initialisé

Description, 312

reservation, propriété, Description, 185
Résolution de problèmes

Sortie de pool de stockage ZFS

Exemple, 98

Sémantique transactionnelle, Description, 41
Séparation

Périphérique, d'un pool de stockage ZFS (zpool

detach)
Exemple, 83

setuid, propriété, Description, 186
sharenfs, propriété

Description, 186, 206

snapdir, propriété, Description, 186
Solaris Live Upgrade

Corruption de données identifiée (zpool status -v)

Migration d'un système de fichiers racine, 134

Détermination du type de corruption de données

Problèmes de migration d'un système de fichiers

Exemple, 295

(zpool status -v)
Exemple, 309

Détermination du type de panne de périphérique

Informations globales d'état des pools

Description, 299

Description, 293

Mode de panne ZFS, 287
Notification d'un périphérique reconnecté dans ZFS

(zpool online)
Exemple, 298

(exemple), 137

racine, 135

Somme de contrôle, Définition, 43
Stockage requis, Identification, 49
Stockage sur pool, Description, 41
Suppression

Erreurs de périphérique (zpool clear)

Exemple, 300

Périphériques de cache (exemple), 80
Suppression d'un droit, zfs unallow, 265
Suppression d'un périphérique

Périphérique endommagé, 288
Rapport syslog de messages d'erreur ZFS, 296

Pool de stockage ZFS

Exemple, 86

323

Système de fichiers, Définition, 44
Système de fichiers racine ZFS, Problèmes de migration

d'un système de fichiers racine, 135

Index

Système de

fichiers ZFS

Exemple, 178

Système de fichiers ZFS

ACL dans un répertoire ZFS
Description détaillée, 240

Administration simplifiée

Description, 43

globale
Exemple, 279

(exemple), 281

Annulation du partage

exemple, 207

Clone

Ajout d'un système de fichiers ZFS à une zone non

Ajout d'un volume ZFS à une zone non globale

Délégation d'un jeu de données à une zone non

Système de fichiers ZFS (Suite)

Définition d'un point de montage hérité

(exemple), 203

Définition d'une réservation

Exemple, 213

Définition de la propriété quota

(exemple), 197

Définition des ACL sur les fichiers ZFS

Définition des ACL sur un fichier ZFS (mode

Description, 238

détaillé)
Description, 241

Définitionatime, propriété

(exemple), 196

globale
(exemple), 280

Démontage

(exemple), 205

Description, 40, 175
Destruction

(exemple), 177

Destruction avec les systèmes dépendants

Données vérifiées par somme de contrôle

(exemple), 178

Description, 42

Enregistrement d'un flux de données (zfs send)

(exemple), 224
Envoi et réception

Description, 223

Exigences d'attribution de noms de composant, 46
Gestion de points de montage automatiques, 202
Gestion de propriété au sein d'une zone

Description, 282

Gestion des points de montage

Description, 201

Gestion des points de montage hérités

Description, 202

(exemple), 197

ZFS avec boot -L et boot -Z
(exemple SPARC), 164

Création, 221
Définition, 44
Destruction, 221
Remplacement d'un système de fichiers

(exemple), 222

Clones

Description, 220

Comptabilisation de l'espace d'un instantané, 219
Configuration d'ACL dans un fichier ZFS (mode

Configuration d'héritage d'ACL dans un fichier ZFS

Configuration requise pour l'installation et Live

compact)
Exemple, 255

(mode détaillé)
(exemple), 246

Upgrade, 117

Création

(exemple), 176

Création d'un volume ZFS

(exemple), 275

Description, 254

mountpoint)
(exemple), 203

324

Guide d'administration Solaris ZFS • Octobre 2009

Définition d'ACL sur un fichier ZFS (mode compact)

Héritage d'une propriété (zfs inherit)

Définition d'un point de montage (zfs set

Initialisation d'un environnement d'initialisation

Système de fichiers ZFS (Suite)

Initialisation d'un système de fichiers racine

Description, 161

Installation d'un système de fichiers racine, 116
Installation initiale d'un système de fichiers racine

Installation JumpStart d'un système de fichiers

Système de fichiers ZFS, Partage (Suite)

Exemple, 207

Périphériques de swap et de vidage

Ajustement de la taille, 159
Description, 158
Points à prendre en compte, 159

Point de montage par défaut

Index

(exemple), 176
Profil de droits, 285
Réception de flux de données (zfs receive)

(exemple), 225

Restauration d'une ACL insignifiante sur un fichier

ZFS (mode détaillé)
(exemple), 245

Sémantique transactionnelle

Description, 41
Somme de contrôle
Définition, 43
Stockage sur pool

Description, 41
Système de fichiers
Définition, 44

Type de jeu de données

Description, 195

Description, 279

Volume

Définition, 46

Définition d'un quota

Exemple, 209

Système de stockage ZFS
Périphérique virtuel

Définition, 45

Systèmes de fichiers ZFS
ACL sur un fichier ZFS

Utilisation sur un système Solaris doté de zones

Système de fichiers ZFS (zfs set quota)

T
Terminologie
Clone, 44
Instantané, 45
Jeu de données, 44

325

ZFS, 120

racine, 130

Instantané

Accès, 218
Création, 216
Définition, 45
Description, 215
Destruction, 217
Renommer, 217
Restauration, 220

Jeu de données

Définition, 44

Liste

(exemple), 193
Liste de descendants
(exemple), 194

Liste de propriétés (zfs list)

(exemple), 198

Liste de propriétés de script

(exemple), 200

Liste de propriétés par valeur source

(exemple), 200

Liste des types

Exemple, 195
Liste sans en-tête
Exemple, 195

ZFS (mode détaillé)
(Exemple), 242

Modification du nom

Exemple, 178

Montage

Partage

Exemple, 204

Description, 206

Migration d'un système de fichiers racine avec

Solaris Live Upgrade, 134
(exemple), 137

Modification d'une ACL insignifiante dans un fichier

Description détaillée, 239

Index

Terminologie (Suite)

Miroir, 44
Périphérique virtuel, 45
Pool, 45
RAID-Z, 45
Réargenture, 45
Somme de contrôle, 43
Système de fichiers, 44
Volume, 46

Test

Création de pool de stockage ZFS (zpool create -n)

Exemple, 75

type, propriété, Description, 186
Type de jeu de données, Description, 195

U
used, propriété

Description, 186
Description détaillée, 188

usedbychildren Propriété, Description, 186
usedbydataset Propriété, Description, 186
usedbyrefreservation Propriété, Description, 187
usedbysnapshotsPropriété, Description, 187
Utilisateur, propriétés ZFS

Description détaillée, 192
Exemple, 192

V
volblocksize, propriété, Description, 187
volsize, propriété

Description, 187
Description détaillée, 192

Volume, Définition, 46
Volume ZFS, Description, 275

X
xattr, propriété, Description, 187

326

Guide d'administration Solaris ZFS • Octobre 2009

Z

zfs allow

Affichage des droits délégués, 266
Description, 264

zfs create

(exemple), 52, 176
Description, 176

zfs destroy -r, (exemple ), 178
zfs Destruction, (exemple), 177
zfs get, (exemple), 198
zfs get -H -o, (exemple), 200
zfs get -s, (exemple), 200
zfs inherit, (exemple), 197

zfs list

(exemple), 53, 193

zfs list -H, Exemple, 195
zfs list -r, (exemple), 194
zfs list -t, Exemple, 195
zfs mount, Exemple, 204
zfs promote, Promotion d'un clone (exemple), 222
zfs receive, (exemple), 225
zfs rename, Exemple, 178
zfs send, (exemple), 224
zfs set atime, (exemple), 196
zfs set compression, (exemple), 52

zfs set mountpoint

(exemple), 52, 203

zfs set mountpoint=legacy, (exemple), 203

zfs set quota

(exemple), 53

zfs set quota, (exemple), 197

zfs set quota

Exemple, 209

zfs set reservation, Exemple, 213
zfs set sharenfs, (exemple), 52
zfs set sharenfs=on, Exemple, 207
zfs unallow, Description, 265
zfs unmount, (exemple), 205
Zone

Ajout d'un système de fichiers ZFS à une zone non

globale
Exemple, 279

Description, 282

Gestion de propriétés ZFS au sein d'une zone

Zone (Suite)

Utilisation de systèmes de fichiers ZFS

Description, 279

zoned, propriété

Description, 187
Description détaillée, 283

Zones

Ajout d'un volume ZFS à une zone non globale

Délégation d'un jeu de données à une zone non

(exemple), 281

globale
(exemple), 280

zoned, propriété

Description détaillée, 283

zpool add, Exemple, 77
zpool attach, Exemple, 82

zpool clear

Description, 86
Exemple, 86

zpool create

(exemple), 48, 50
Pool de base

(exemple), 66

Pool de stockage mis en miroir

(exemple), 67

Pool de stockage RAID-Z

Exemple, 68

zpool create -n

Test

Exemple, 75

zpool destroy, (exemple), 76
zpool detach, Exemple, 83
zpool export, (exemple), 106
zpool history, Exemple, 33
zpool import -a, (exemple), 107
zpool import -D, Exemple, 111
zpool import -d, (exemple), 109
zpool import nom, (exemple), 110
zpool iostat, pool complet, exemple, 100
zpool iostat -v, vdev, exemple, 101

zpool list

(exemple), 50
Description, 96
Exemple, 97

zpool list -Ho name, Exemple, 98
zpool offline, (exemple), 84
zpool online, (exemple), 86
zpool replace, (exemple), 87
zpool status -v, (exemple), 104
zpool status -x, Exemple, 103
zpool upgrade, 112

Index

327

328

